<!DOCTYPE html><html><head><title>Help for package psychmeta</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {psychmeta}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.attenuate_r_bvdrr'><p>Internal function to undo the Case V correction for bivariate indirect range restriction</p></a></li>
<li><a href='#.attenuate_r_bvirr'><p>Internal function to undo the Case V correction for bivariate indirect range restriction</p></a></li>
<li><a href='#.attenuate_r_rb'><p>Internal function to attenuate correlations using Raju and Burke's formula for univariate direct range restriction</p></a></li>
<li><a href='#.attenuate_r_uvdrr'><p>Internal function to undo the Case II correction for univariate direct range restriction</p></a></li>
<li><a href='#.attenuate_r_uvirr'><p>Internal function to undo the Case IV correction for univariate indirect range restriction</p></a></li>
<li><a href='#.convert_metatab'><p>Function to convert a meta-analysis of correlations to a meta-analysis of d values or vice-versa (does one table)</p></a></li>
<li><a href='#.correct_r_bvdrr'><p>Internal function to compute the correction for bivariate direct range restriction</p></a></li>
<li><a href='#.correct_r_bvirr'><p>Internal function to compute the Case V correction for bivariate indirect range restriction</p></a></li>
<li><a href='#.correct_r_rb'><p>Internal function to compute Raju and Burke's correction for univariate direct range restriction</p></a></li>
<li><a href='#.correct_r_uvdrr'><p>Internal function to compute the Case II correction for univariate direct range restriction</p></a></li>
<li><a href='#.correct_r_uvirr'><p>Internal function to compute the Case IV correction for univariate indirect range restriction</p></a></li>
<li><a href='#.create_ad_int'><p>Create a tabular array of artifact information summarizing values and weights of values in an interactive artifact distribution</p></a></li>
<li><a href='#.descriptives_database'><p>Compute weighted descriptive statistics for a database</p></a></li>
<li><a href='#.estimate_attenuation'><p>Estimate the compound attenuation factors (i.e., &quot;A&quot;) for correlations</p></a></li>
<li><a href='#.heterogeneity'><p>Computation of heterogeneity indices from meta-analytic results</p></a></li>
<li><a href='#.identify_ma_cols'><p>Identify meta-analysis type and provide new column names for a meta-analysis</p></a></li>
<li><a href='#.integrate_dmod'><p>Integration function for computing parametric signed or unsigned <code class="reqn">d_{Mod}</code> effect sizes</p>
for a single focal group</a></li>
<li><a href='#.lambda_bvirr'><p>Internal function to produce lambda coefficients to use in the Case V correction for bivariate indirect range restriction.</p></a></li>
<li><a href='#.ma_artifacts'><p>Internal function for computing meta-analyses of artifacts</p></a></li>
<li><a href='#.ma_bootstrap'><p>Wrapper function to facilitate bootstrapped meta-analyses</p></a></li>
<li><a href='#.ma_cumulative'><p>Cumulative meta-analyses</p></a></li>
<li><a href='#.ma_d_bb'><p>Internal function for computing bare-bones meta-analyses of d values</p></a></li>
<li><a href='#.ma_d_bb_boot'><p>Internal function for computing bootstrapped bare-bones meta-analyses of d values</p></a></li>
<li><a href='#.ma_generic'><p>Internal function for computing bare-bones meta-analyses of generic effect sizes</p></a></li>
<li><a href='#.ma_generic_boot'><p>Internal function for computing bootstrapped bare-bones meta-analyses of generic effect sizes</p></a></li>
<li><a href='#.ma_leave1out'><p>Leave-one-out (i.e., jackknife) meta-analyses</p></a></li>
<li><a href='#.ma_r_bb'><p>Internal function for computing bare-bones meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_bb_boot'><p>Internal function for computing bootstrapped bare-bones meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_ic'><p>Internal function for computing individual-correction meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_ic_boot'><p>Internal function for computing bootstrapped individual-correction meta-analyses of all varieties</p></a></li>
<li><a href='#.ma_r_icts_boot'><p>Internal function for computing bootstrapped individual-correction meta-analyses of true-score correlations</p></a></li>
<li><a href='#.ma_r_icvgx_boot'><p>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for X</p></a></li>
<li><a href='#.ma_r_icvgy_boot'><p>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for Y</p></a></li>
<li><a href='#.ma_r_order2'><p>Internal function for computing individual-correction meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_order2_ad_boot'><p>Internal function for computing bootstrapped second-order artifact-distribution meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_order2_bb_boot'><p>Internal function for computing bootstrapped second-order bare-bones meta-analyses of correlations</p></a></li>
<li><a href='#.ma_r_order2_ic_boot'><p>Internal function for computing bootstrapped second-order individual-correction meta-analyses of correlations</p></a></li>
<li><a href='#.metabulate'><p>Internal function for .metabulating results tables</p></a></li>
<li><a href='#.plot_forest_meta'><p>Internal plotting function for forest plots</p></a></li>
<li><a href='#.plot_funnel'><p>Internal funnel-plot generator</p></a></li>
<li><a href='#.print.lm_mat'><p>Print method for objects of the class &quot;lm_mat&quot;</p></a></li>
<li><a href='#.print.summary.lm_mat'><p>Print method for objects of the class &quot;summary.lm_mat&quot;</p></a></li>
<li><a href='#.rbeta'><p>Generate values from a beta distribution, given a mean and standard deviation of the distribution</p></a></li>
<li><a href='#.refine_var_rr'><p>Range-restriction refinement factor (i.e., &quot;a&quot;) for correlations' corrected sampling variances</p></a></li>
<li><a href='#.tau_squared_m_solver'><p>tau_m_squared Solver</p></a></li>
<li><a href='#.tau_squared_r_solver'><p>tau_r_squared Solver</p></a></li>
<li><a href='#adjust_n_d'><p>Adjusted sample size for a non-Cohen <em>d</em> value for use in a meta-analysis of Cohen's <em>d</em> values</p></a></li>
<li><a href='#adjust_n_r'><p>Adjusted sample size for a non-Pearson correlation coefficient for use in a meta-analysis of Pearson correlations</p></a></li>
<li><a href='#anova.ma_psychmeta'><p>Wald-type tests for moderators in psychmeta meta-analyses</p></a></li>
<li><a href='#check_wt_type'><p>Check whether wt_type argument is valid and determine which package to use for weights</p></a></li>
<li><a href='#clean_warning'><p>Clean warnings and remove warnings present in the environment before running the function of interest</p></a></li>
<li><a href='#composite_d_matrix'><p>Matrix formula to estimate the standardized mean difference associated with a weighted or unweighted composite variable</p></a></li>
<li><a href='#composite_d_scalar'><p>Scalar formula to estimate the standardized mean difference associated with a composite variable</p></a></li>
<li><a href='#composite_r_matrix'><p>Matrix formula to estimate the correlation between two weighted or unweighted composite variables</p></a></li>
<li><a href='#composite_r_scalar'><p>Scalar formula to estimate the correlation between a composite and another variable or between two composite variables</p></a></li>
<li><a href='#composite_rel_matrix'><p>Matrix formula to estimate the reliability of a weighted or unweighted composite variable</p></a></li>
<li><a href='#composite_rel_scalar'><p>Scalar formula to estimate the reliability of a composite variable</p></a></li>
<li><a href='#composite_u_matrix'><p>Matrix formula to estimate the u ratio of a composite variable</p></a></li>
<li><a href='#composite_u_scalar'><p>Scalar formula to estimate the u ratio of a composite variable</p></a></li>
<li><a href='#compute_alpha'><p>Compute coefficient alpha</p></a></li>
<li><a href='#compute_dmod'><p>Comprehensive <code class="reqn">d_{Mod}</code> calculator</p></a></li>
<li><a href='#compute_dmod_npar'><p>Function for computing non-parametric <code class="reqn">d_{Mod}</code> effect sizes for a single focal group</p></a></li>
<li><a href='#compute_dmod_par'><p>Function for computing parametric <code class="reqn">d_{Mod}</code> effect sizes for any number of focal groups</p></a></li>
<li><a href='#conf.limits.nc.chisq'><p>Confidence limits for noncentral chi square parameters (function and documentation from package 'MBESS' version 4.4.3)</p>
Function to determine the noncentral parameter that leads to the observed <code>Chi.Square</code>-value,
so that a confidence interval for the population noncentral chi-squrae value can be formed.</a></li>
<li><a href='#confidence'><p>Construct a confidence interval</p></a></li>
<li><a href='#confidence_r'><p>Construct a confidence interval for correlations using Fisher's z transformation</p></a></li>
<li><a href='#confint'><p>Confidence interval method for objects of classes deriving from &quot;lm_mat&quot;</p></a></li>
<li><a href='#control_intercor'><p>Control function to curate intercorrelations to be used in automatic compositing routine</p></a></li>
<li><a href='#control_psychmeta'><p>Control for <span class="pkg">psychmeta</span> meta-analyses</p></a></li>
<li><a href='#convert_consistency2reltype'><p>Convert logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate to a string variable of generic reliability types</p></a></li>
<li><a href='#convert_es'><p>Convert effect sizes</p></a></li>
<li><a href='#convert_ma'><p>Function to convert meta-analysis of correlations to d values or vice-versa</p></a></li>
<li><a href='#convert_pq_to_p'><p>Convert the dichotomous variable variance to a proportion</p></a></li>
<li><a href='#convert_reltype2consistency'><p>Convert string variable containing reliability type indicators to a logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate</p></a></li>
<li><a href='#convert_sdd_to_sdr'><p>Convert the SD of d to the SD of r via TSA</p></a></li>
<li><a href='#convert_sdr_to_sdd'><p>Convert the SD of r to the SD of d via TSA</p></a></li>
<li><a href='#convert_vard_to_varr'><p>Convert the variance of d to the variance of r via TSA</p></a></li>
<li><a href='#convert_varr_to_vard'><p>Convert the variance of r to the variance of d via TSA</p></a></li>
<li><a href='#correct_d'><p>Correct <code class="reqn">d</code> values for measurement error and/or range restriction</p></a></li>
<li><a href='#correct_d_bias'><p>Correct for small-sample bias in Cohen's <code class="reqn">d</code> values</p></a></li>
<li><a href='#correct_glass_bias'><p>Correct for small-sample bias in Glass' <code class="reqn">\Delta</code> values</p></a></li>
<li><a href='#correct_matrix_mvrr'><p>Multivariate select/correction for covariance matrices</p></a></li>
<li><a href='#correct_means_mvrr'><p>Multivariate select/correction for vectors of means</p></a></li>
<li><a href='#correct_r'><p>Correct correlations for range restriction and/or measurement error</p></a></li>
<li><a href='#correct_r_bias'><p>Correct correlations for small-sample bias</p></a></li>
<li><a href='#correct_r_coarseness'><p>Correct correlations for scale coarseness</p></a></li>
<li><a href='#correct_r_dich'><p>Correct correlations for artificial dichotomization of one or both variables</p></a></li>
<li><a href='#correct_r_split'><p>Correct correlations for uneven/unrepresentative splits</p></a></li>
<li><a href='#create_ad'><p>Generate an artifact distribution object for use in artifact-distribution meta-analysis programs.</p></a></li>
<li><a href='#create_ad_array'><p>Create an array of all possible combinations of artifact values from 2-4 artifact distributions</p></a></li>
<li><a href='#create_ad_group'><p>Generate an artifact distribution object for a dichotomous grouping variable.</p></a></li>
<li><a href='#create_ad_tibble'><p>Create a tibble of artifact distributions by construct</p></a></li>
<li><a href='#credibility'><p>Construct a credibility interval</p></a></li>
<li><a href='#data_d_bb_multi'><p>Hypothetical <em>d</em> value dataset simulated with sampling error only</p></a></li>
<li><a href='#data_d_meas_multi'><p>Hypothetical <em>d</em> value dataset simulated to satisfy the assumptions of the correction for measurement error only in multiple constructs</p></a></li>
<li><a href='#data_r_bvdrr'><p>Hypothetical dataset simulated to satisfy the assumptions of the bivariate correction for direct range restriction</p></a></li>
<li><a href='#data_r_bvirr'><p>Hypothetical dataset simulated to satisfy the assumptions of the bivariate correction for indirect range restriction</p></a></li>
<li><a href='#data_r_gonzalezmule_2014'><p>Meta-analysis of OCB correlations with other constructs</p></a></li>
<li><a href='#data_r_mcdaniel_1994'><p>Artifact-distribution meta-analysis of the validity of interviews</p></a></li>
<li><a href='#data_r_mcleod_2007'><p>Bare-bones meta-analysis of parenting and childhood depression</p></a></li>
<li><a href='#data_r_meas'><p>Hypothetical dataset simulated to satisfy the assumptions of the correction for measurement error only</p></a></li>
<li><a href='#data_r_meas_multi'><p>Hypothetical correlation dataset simulated to satisfy the assumptions of the correction for measurement error only in multiple constructs</p></a></li>
<li><a href='#data_r_oh_2009'><p>Second order meta-analysis of operational validities of big five personality measures across East Asian countries</p></a></li>
<li><a href='#data_r_roth_2015'><p>Artifact-distribution meta-analysis of the correlation between school grades and cognitive ability</p></a></li>
<li><a href='#data_r_uvdrr'><p>Hypothetical dataset simulated to satisfy the assumptions of the univariate correction for direct range restriction</p></a></li>
<li><a href='#data_r_uvirr'><p>Hypothetical dataset simulated to satisfy the assumptions of the univariate correction for indirect range restriction</p></a></li>
<li><a href='#estimate_artifacts'><p>Estimation of applicant and incumbent reliabilities and of true- and observed-score u ratios</p></a></li>
<li><a href='#estimate_length_sb'><p>Inverse Spearman-Brown formula to estimate the amount by which a measure would have to be lengthened or shorted to achieve a desired level of reliability</p></a></li>
<li><a href='#estimate_matrix_prods'><p>Estimate covariance matrices and mean vectors containing product terms</p></a></li>
<li><a href='#estimate_prod'><p>Estimation of statistics computed from products of random, normal variables</p></a></li>
<li><a href='#estimate_q_dist'><p>Estimate descriptive statistics of square-root reliabilities</p></a></li>
<li><a href='#estimate_rel_dist'><p>Estimate descriptive statistics of reliabilities</p></a></li>
<li><a href='#estimate_rel_sb'><p>Spearman-Brown prophecy formula to estimate the reliability of a lengthened measure</p></a></li>
<li><a href='#estimate_u'><p>Estimate u ratios from available artifact information</p></a></li>
<li><a href='#estimate_var_artifacts'><p>Taylor series approximations for the variances of estimates artifact distributions.</p></a></li>
<li><a href='#estimate_var_rho_int'><p>Non-linear estimate of variance of <code class="reqn">\rho</code> corrected for psychometric artifacts using numeric integration</p></a></li>
<li><a href='#estimate_var_rho_tsa'><p>Taylor Series Approximation of variance of <code class="reqn">\rho</code> corrected for psychometric artifacts</p></a></li>
<li><a href='#estimate_var_tsa'><p>Taylor Series Approximation of effect-size variances corrected for psychometric artifacts</p></a></li>
<li><a href='#filter_listnonnull'><p>Filter a list to remove NULL entries</p></a></li>
<li><a href='#filter_ma'><p>Filter meta-analyses</p></a></li>
<li><a href='#filter_r'><p>Filter to detect and remove impossible values in vectors of correlations and sample sizes.</p></a></li>
<li><a href='#filter_r_bar'><p>Filter to detect and remove impossible values in vectors of meta-analytic mean correlations and numbers of studies</p></a></li>
<li><a href='#filter_rel'><p>Filter to remove impossible values from vectors of reliabilities and corresponding weights.</p></a></li>
<li><a href='#filter_u'><p>Filter to remove impossible values from vectors of u ratios and corresponding weights.</p></a></li>
<li><a href='#format_long'><p>Create long-format datasets in simulate_r_database</p></a></li>
<li><a href='#format_num'><p>Format numbers for presentation</p></a></li>
<li><a href='#format_wide'><p>Create wide-format datasets in simulate_r_database</p></a></li>
<li><a href='#generate_bib'><p>Generate a list of references included in meta-analyses</p></a></li>
<li><a href='#generate_directory'><p>Generate a system of folders from a file path to a new directory</p></a></li>
<li><a href='#get_stuff'><p>Extract results from a psychmeta meta-analysis object</p></a></li>
<li><a href='#heterogeneity'><p>Supplemental heterogeneity statistics for meta-analyses</p></a></li>
<li><a href='#impute_artifacts'><p>Impute missing and impossible artifact values</p></a></li>
<li><a href='#interval_warning'><p>Warning message for the widths of uncertainty intervals</p></a></li>
<li><a href='#limits_tau'><p>Confidence limits of tau</p></a></li>
<li><a href='#limits_tau2'><p>Confidence limits of tau-squared</p></a></li>
<li><a href='#lm_mat'><p>Compute linear regression models and generate &quot;lm&quot; objects from covariance matrices.</p></a></li>
<li><a href='#ma_d'><p>Meta-analysis of <em>d</em> values</p></a></li>
<li><a href='#ma_d_order2'><p>Second-order meta-analysis function for <em>d</em> values</p></a></li>
<li><a href='#ma_generic'><p>Bare-bones meta-analysis of generic effect sizes</p></a></li>
<li><a href='#ma_r'><p>Meta-analysis of correlations</p></a></li>
<li><a href='#ma_r_ad.int_bvdrr'><p>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.int_bvirr'><p>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.int_meas'><p>Interactive artifact-distribution meta-analysis correcting for measurement error</p></a></li>
<li><a href='#ma_r_ad.int_none'><p>Null artifact distribution result: No corrections performed</p></a></li>
<li><a href='#ma_r_ad.int_rbAdj'><p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.int_rbOrig'><p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.int_uvdrr'><p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.int_uvirr'><p>Interactive artifact-distribution meta-analysis correcting for Case IV indirect range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_bvdrr'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_bvirr'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_meas'><p>Taylor series approximation artifact-distribution meta-analysis correcting for measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_rb1Adj'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_rb1Orig'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_rb2Adj'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_rb2Orig'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_uvdrr'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_ad.tsa_uvirr'><p>Taylor series approximation artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</p></a></li>
<li><a href='#ma_r_order2'><p>Second-order meta-analysis function for correlations</p></a></li>
<li><a href='#ma_wrapper'><p>Wrapper function to compute meta-analytic results for all analyses.</p></a></li>
<li><a href='#manage_arglength'><p>Check the length of x against the length of y and replicate z if necessary</p></a></li>
<li><a href='#merge_simdat_d'><p>Merge multiple &quot;simdat_d_database&quot; class objects</p></a></li>
<li><a href='#merge_simdat_r'><p>Merge multiple &quot;simdat_r_database&quot; class objects</p></a></li>
<li><a href='#metabulate'><p>Write a summary table of meta-analytic results</p></a></li>
<li><a href='#metabulate_rmd_helper'><p>Add metabulate equation commands and LaTeX dependencies</p></a></li>
<li><a href='#metareg'><p>Compute meta-regressions</p></a></li>
<li><a href='#mix_dist'><p>Descriptive statistics for a mixture distribution</p></a></li>
<li><a href='#mix_matrix'><p>Estimate mixture covariance matrix from within-group covariance matrices</p></a></li>
<li><a href='#mix_r_2group'><p>Estimate the mixture correlation for two groups</p></a></li>
<li><a href='#organize_database'><p>Organize a database of multi-construct or moderated information</p></a></li>
<li><a href='#organize_moderators'><p>Organization of moderator data for use in meta-analyses</p></a></li>
<li><a href='#plot_forest'><p>Create forest plots</p></a></li>
<li><a href='#plot_funnel'><p>Create funnel plots</p></a></li>
<li><a href='#predict'><p>Prediction method for objects of classes deriving from &quot;lm_mat&quot;</p></a></li>
<li><a href='#print'><p>Print methods for <strong><code>psychmeta</code></strong></p></a></li>
<li><a href='#psychmeta-package'><p><span class="pkg">psychmeta</span>: Psychometric meta-analysis toolkit</p></a></li>
<li><a href='#reattribute'><p>Copy class and attributes from the original version of an object to a modified version.</p></a></li>
<li><a href='#record_fyis'><p>Summary of FYI messages generated within a function</p></a></li>
<li><a href='#record_warnings'><p>Summary of warning messages generated within a function</p></a></li>
<li><a href='#reshape_mat2dat'><p>Extract a long-format correlation database from a correlation matrix and its supporting vectors/matrices of variable information</p></a></li>
<li><a href='#reshape_vec2mat'><p>Assemble a variance-covariance matrix</p></a></li>
<li><a href='#reshape_wide2long'><p>Reshape database from wide format to long format</p></a></li>
<li><a href='#round2char'><p>Round numeric values to an exact number of digits and return as a character</p></a></li>
<li><a href='#scalar_arg_warning'><p>Warning message for scalar arguments receiving multiple values</p></a></li>
<li><a href='#screen_ad_int'><p>Screen to detect invalid interactive artifact distribution objects</p></a></li>
<li><a href='#screen_ad_tsa'><p>Screen to detect invalid Taylor series artifact distribution objects</p></a></li>
<li><a href='#screen_r'><p>Screen to detect impossible values in vectors of correlations and sample sizes.</p></a></li>
<li><a href='#screen_rel'><p>Screen to detect impossible values in vectors of reliability estimates.</p></a></li>
<li><a href='#screen_u'><p>Screen to detect impossible values in vectors of u ratios.</p></a></li>
<li><a href='#sensitivity'><p>Sensitivity analyses for meta-analyses</p></a></li>
<li><a href='#simulate_alpha'><p>Generate a vector of simulated sample alpha coefficients</p></a></li>
<li><a href='#simulate_d_database'><p>Simulate d value databases of primary studies</p></a></li>
<li><a href='#simulate_d_sample'><p>Simulate a sample of psychometric d value data with measurement error, direct range restriction, and/or indirect range restriction</p></a></li>
<li><a href='#simulate_matrix'><p>Generate a list of simulated sample matrices sampled from the Wishart distribution</p></a></li>
<li><a href='#simulate_psych'><p>Simulate Monte Carlo psychometric data (observed, true, and error scores)</p></a></li>
<li><a href='#simulate_r_database'><p>Simulate correlation databases of primary studies</p></a></li>
<li><a href='#simulate_r_sample'><p>Simulation of data with measurement error and range-restriction artifacts</p></a></li>
<li><a href='#sparsify_simdat_d'><p>Create sparse artifact information in a &quot;simdat_d_database&quot; class object</p></a></li>
<li><a href='#sparsify_simdat_r'><p>Create sparse artifact information in a &quot;simdat_r_database&quot; class object</p></a></li>
<li><a href='#summarize_ads'><p>Summarize artifact information from meta-analyses into table format</p></a></li>
<li><a href='#summary'><p>Summary methods for <span class="pkg">psychmeta</span></p></a></li>
<li><a href='#truncate_dist'><p>Truncation function for normal distributions (truncates both mean and variance)</p></a></li>
<li><a href='#truncate_mean'><p>Truncation function for means</p></a></li>
<li><a href='#truncate_var'><p>Truncation function for variances</p></a></li>
<li><a href='#unmix_matrix'><p>Estimate average within-group covariance matrices from a mixture covariance matrix</p></a></li>
<li><a href='#unmix_r_2group'><p>Estimate the average within-group correlation from a mixture correlation for two groups</p></a></li>
<li><a href='#var_error_A'><p>Estimate the error variance of the probability-based effect size (<code class="reqn">A</code>, AUC, the common language effect size [CLES])</p></a></li>
<li><a href='#var_error_alpha'><p>Analytic estimate of the sampling variance of coefficient <code class="reqn">\alpha</code></p></a></li>
<li><a href='#var_error_d'><p>Estimate the error variance Cohen's <code class="reqn">d</code> values</p></a></li>
<li><a href='#var_error_delta'><p>Estimate the error variance of Glass's <code class="reqn">\Delta</code> values</p></a></li>
<li><a href='#var_error_g'><p>Estimate the error variance Hedges's <code class="reqn">g</code> values</p></a></li>
<li><a href='#var_error_mult_R'><p>Estimate the error variance of linear regression multiple <em>R</em>(-squared)</p></a></li>
<li><a href='#var_error_q'><p>Estimate the error variance of square roots of reliability estimates</p></a></li>
<li><a href='#var_error_r'><p>Estimate the error variance of correlations</p></a></li>
<li><a href='#var_error_r_bvirr'><p>Taylor series approximation of the sampling variance of correlations corrected using the bivariate indirect range restriction correction (Case V)</p></a></li>
<li><a href='#var_error_rel'><p>Estimate the error variance of reliability estimates</p></a></li>
<li><a href='#var_error_spearman'><p>Estimate the error variance of Spearman rank correlations</p></a></li>
<li><a href='#var_error_u'><p>Estimate the error variance of <code class="reqn">u</code> ratios</p></a></li>
<li><a href='#warning_variance'><p>Warning message for invalid variances</p></a></li>
<li><a href='#wt_cov'><p>Compute weighted covariances</p></a></li>
<li><a href='#wt_dist'><p>Weighted descriptive statistics for a vector of numbers</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Psychometric Meta-Analysis Toolkit</td>
</tr>
<tr>
<td>Version:</td>
<td>2.6.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-08-25</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/psychmeta/psychmeta/issues">https://github.com/psychmeta/psychmeta/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <a href="https://github.com/psychmeta/psychmeta/issues">https://github.com/psychmeta/psychmeta/issues</a> or &lt;issues@psychmeta.com&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, boot, metafor, ggplot2, progress, curl, dplyr, tibble,
tidyr (&ge; 1.0.0), rlang, purrr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, mvtnorm, nor1mix, bib2df, rmarkdown, knitr, stringi,
cli, crayon, testthat (&ge; 2.1.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1.9000</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-25 22:46:12 UTC; jeffreydahlke</td>
</tr>
<tr>
<td>Author:</td>
<td>Jeffrey A. Dahlke [aut, cre],
  Brenton M. Wiernik [aut],
  Wesley Gardiner [ctb] (Unit tests),
  Michael T. Brannick [ctb] (Testing),
  Jack Kostal [ctb] (Code for reshape_mat2dat function),
  Sean Potter [ctb] (Testing; Code for cumulative and leave1out plots),
  John Sakaluk [ctb] (Code for funnel and forest plots),
  Yuejia (Mandy) Teng [ctb] (Testing)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jeffrey A. Dahlke &lt;jdahlke@humrro.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-26 12:30:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='.attenuate_r_bvdrr'>Internal function to undo the Case V correction for bivariate indirect range restriction</h2><span id='topic+.attenuate_r_bvdrr'></span>

<h3>Description</h3>

<p>Internal function to undo the Case V correction for bivariate indirect range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.attenuate_r_bvdrr(rtpa, qxa = 1, qya = 1, ux = 1, uy = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".attenuate_r_bvdrr_+3A_rtpa">rtpa</code></td>
<td>
<p>Correlation corrected for range restriction and measurement error.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvdrr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvdrr_+3A_qya">qya</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvdrr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvdrr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of attenuated correlations.
</p>

<hr>
<h2 id='.attenuate_r_bvirr'>Internal function to undo the Case V correction for bivariate indirect range restriction</h2><span id='topic+.attenuate_r_bvirr'></span>

<h3>Description</h3>

<p>Internal function to undo the Case V correction for bivariate indirect range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.attenuate_r_bvirr(
  rtpa,
  qxa = 1,
  qya = 1,
  ux = 1,
  uy = 1,
  sign_rxz = 1,
  sign_ryz = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".attenuate_r_bvirr_+3A_rtpa">rtpa</code></td>
<td>
<p>Correlation corrected for range restriction and measurement error.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_qya">qya</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id=".attenuate_r_bvirr_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of attenuated correlations.
</p>

<hr>
<h2 id='.attenuate_r_rb'>Internal function to attenuate correlations using Raju and Burke's formula for univariate direct range restriction</h2><span id='topic+.attenuate_r_rb'></span>

<h3>Description</h3>

<p>Internal function to attenuate correlations using Raju and Burke's formula for univariate direct range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.attenuate_r_rb(rtpa, qx = 1, qy = 1, ux = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".attenuate_r_rb_+3A_rtpa">rtpa</code></td>
<td>
<p>Vector of fully corrected correlations.</p>
</td></tr>
<tr><td><code id=".attenuate_r_rb_+3A_qx">qx</code></td>
<td>
<p>Vector of square-root of reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_rb_+3A_qy">qy</code></td>
<td>
<p>Vector square-root of reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_rb_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.attenuate_r_uvdrr'>Internal function to undo the Case II correction for univariate direct range restriction</h2><span id='topic+.attenuate_r_uvdrr'></span>

<h3>Description</h3>

<p>Internal function to undo the Case II correction for univariate direct range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.attenuate_r_uvdrr(rtpa, qxa = 1, qyi = 1, ux = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".attenuate_r_uvdrr_+3A_rtpa">rtpa</code></td>
<td>
<p>Vector of fully corrected correlations.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvdrr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvdrr_+3A_qyi">qyi</code></td>
<td>
<p>Vector square-root of incumbent reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvdrr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of attenuated correlations.
</p>

<hr>
<h2 id='.attenuate_r_uvirr'>Internal function to undo the Case IV correction for univariate indirect range restriction</h2><span id='topic+.attenuate_r_uvirr'></span>

<h3>Description</h3>

<p>Internal function to undo the Case IV correction for univariate indirect range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.attenuate_r_uvirr(rtpa, qxi = 1, qyi = 1, ut = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".attenuate_r_uvirr_+3A_rtpa">rtpa</code></td>
<td>
<p>Vector of fully corrected correlations.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvirr_+3A_qxi">qxi</code></td>
<td>
<p>Vector of square-root of incumbent reliability coefficient for X.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvirr_+3A_qyi">qyi</code></td>
<td>
<p>Vector of square-root of incumbent reliability coefficient for Y.</p>
</td></tr>
<tr><td><code id=".attenuate_r_uvirr_+3A_ut">ut</code></td>
<td>
<p>Vector of true-score u ratios for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of attenuated correlations.
</p>

<hr>
<h2 id='.convert_metatab'>Function to convert a meta-analysis of correlations to a meta-analysis of d values or vice-versa (does one table)</h2><span id='topic+.convert_metatab'></span>

<h3>Description</h3>

<p>Function to convert a meta-analysis of correlations to a meta-analysis of d values or vice-versa (does one table)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.convert_metatab(
  ma_table,
  p_vec = rep(0.5, nrow(ma_table)),
  conf_level = 0.95,
  cred_level = 0.8,
  conf_method = "t",
  cred_method = "t"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".convert_metatab_+3A_ma_table">ma_table</code></td>
<td>
<p>Meta-analysis table.</p>
</td></tr>
<tr><td><code id=".convert_metatab_+3A_p_vec">p_vec</code></td>
<td>
<p>Vector of proportions associated with the rows of <code>ma_table</code>.</p>
</td></tr>
<tr><td><code id=".convert_metatab_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id=".convert_metatab_+3A_cred_level">cred_level</code></td>
<td>
<p>Credibility level to define the width of the credibility interval (default = .80).</p>
</td></tr>
<tr><td><code id=".convert_metatab_+3A_conf_method">conf_method</code></td>
<td>
<p>Distribution to be used to compute the width of confidence intervals. Available options are &quot;t&quot; for t distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
<tr><td><code id=".convert_metatab_+3A_cred_method">cred_method</code></td>
<td>
<p>Distribution to be used to compute the width of credibility intervals. Available options are &quot;t&quot; for t distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Meta-analysis table converted to a new metric
</p>

<hr>
<h2 id='.correct_r_bvdrr'>Internal function to compute the correction for bivariate direct range restriction</h2><span id='topic+.correct_r_bvdrr'></span>

<h3>Description</h3>

<p>Internal function to compute the correction for bivariate direct range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.correct_r_bvdrr(rxyi, qxa = 1, qya = 1, ux = 1, uy = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".correct_r_bvdrr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".correct_r_bvdrr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".correct_r_bvdrr_+3A_qya">qya</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_bvdrr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id=".correct_r_bvdrr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.correct_r_bvirr'>Internal function to compute the Case V correction for bivariate indirect range restriction</h2><span id='topic+.correct_r_bvirr'></span>

<h3>Description</h3>

<p>Internal function to compute the Case V correction for bivariate indirect range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.correct_r_bvirr(
  rxyi,
  qxa = 1,
  qya = 1,
  ux = 1,
  uy = 1,
  sign_rxz = 1,
  sign_ryz = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".correct_r_bvirr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_qya">qya</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id=".correct_r_bvirr_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.correct_r_rb'>Internal function to compute Raju and Burke's correction for univariate direct range restriction</h2><span id='topic+.correct_r_rb'></span>

<h3>Description</h3>

<p>Internal function to compute Raju and Burke's correction for univariate direct range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.correct_r_rb(rxyi, qx = 1, qy = 1, ux = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".correct_r_rb_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".correct_r_rb_+3A_qx">qx</code></td>
<td>
<p>Vector of square-root of reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".correct_r_rb_+3A_qy">qy</code></td>
<td>
<p>Vector square-root of reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_rb_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.correct_r_uvdrr'>Internal function to compute the Case II correction for univariate direct range restriction</h2><span id='topic+.correct_r_uvdrr'></span>

<h3>Description</h3>

<p>Internal function to compute the Case II correction for univariate direct range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.correct_r_uvdrr(rxyi, qxa = 1, qyi = 1, ux = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".correct_r_uvdrr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".correct_r_uvdrr_+3A_qxa">qxa</code></td>
<td>
<p>Vector of square-root of applicant reliability coefficients for X.</p>
</td></tr>
<tr><td><code id=".correct_r_uvdrr_+3A_qyi">qyi</code></td>
<td>
<p>Vector square-root of incumbent reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_uvdrr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.correct_r_uvirr'>Internal function to compute the Case IV correction for univariate indirect range restriction</h2><span id='topic+.correct_r_uvirr'></span>

<h3>Description</h3>

<p>Internal function to compute the Case IV correction for univariate indirect range restriction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.correct_r_uvirr(rxyi, qxi = 1, qyi = 1, ut = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".correct_r_uvirr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".correct_r_uvirr_+3A_qxi">qxi</code></td>
<td>
<p>Vector of square-root of incumbent reliability coefficient for X.</p>
</td></tr>
<tr><td><code id=".correct_r_uvirr_+3A_qyi">qyi</code></td>
<td>
<p>Vector of square-root of incumbent reliability coefficient for Y.</p>
</td></tr>
<tr><td><code id=".correct_r_uvirr_+3A_ut">ut</code></td>
<td>
<p>Vector of true-score u ratio for X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of corrected correlations.
</p>

<hr>
<h2 id='.create_ad_int'>Create a tabular array of artifact information summarizing values and weights of values in an interactive artifact distribution</h2><span id='topic+.create_ad_int'></span>

<h3>Description</h3>

<p>This is an internal function that constructs a data frame of artifact estimates (in the Value column) and corresponding weights (in the Weight column), consolidated according to the specified number of digits used in rounding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.create_ad_int(art_vec, wt_vec = rep(1, length(art_vec)), decimals = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".create_ad_int_+3A_art_vec">art_vec</code></td>
<td>
<p>Vector of artifact values (i.e., u ratios, reliability coefficients, square-root reliabilities).</p>
</td></tr>
<tr><td><code id=".create_ad_int_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Vector for weights to assign to individual artifact values.</p>
</td></tr>
<tr><td><code id=".create_ad_int_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimals to which artifact values should be rounded and consolidated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with two columns: One containing artifact values and the other containing weights associated with artifact values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># .create_ad_int(art_vec = c(.8, .8, .9), wt_vec = c(100, 200, 100), decimals = 2)

</code></pre>

<hr>
<h2 id='.descriptives_database'>Compute weighted descriptive statistics for a database</h2><span id='topic+.descriptives_database'></span>

<h3>Description</h3>

<p>Compute weighted descriptive statistics for a database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.descriptives_database(dat, wt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".descriptives_database_+3A_dat">dat</code></td>
<td>
<p>Numeric matrix or data frame</p>
</td></tr>
<tr><td><code id=".descriptives_database_+3A_wt">wt</code></td>
<td>
<p>Vector of weights to be applied to all columns of dat</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of weighted descriptive statistics
</p>

<hr>
<h2 id='.estimate_attenuation'>Estimate the compound attenuation factors (i.e., &quot;A&quot;) for correlations</h2><span id='topic+.estimate_attenuation'></span>

<h3>Description</h3>

<p>For use with all artifact corrections except the Case V correction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.estimate_attenuation(r_observed, r_corrected)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".estimate_attenuation_+3A_r_observed">r_observed</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".estimate_attenuation_+3A_r_corrected">r_corrected</code></td>
<td>
<p>Vector of corrected correlations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of compound attenuation factors.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. p. 144.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
.estimate_attenuation(r_observed = .3, r_corrected = .5)

## End(Not run)
</code></pre>

<hr>
<h2 id='.heterogeneity'>Computation of heterogeneity indices from meta-analytic results</h2><span id='topic+.heterogeneity'></span>

<h3>Description</h3>

<p>Computation of heterogeneity indices from meta-analytic results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.heterogeneity(
  mean_es,
  var_es,
  var_pre,
  var_res,
  var_e = NA,
  var_art = NA,
  wt_vec,
  N,
  k,
  es_vec,
  vare_vec,
  es_failsafe = NULL,
  conf_level = 0.95,
  es_type = "es",
  wt_type,
  ma_method,
  var_unbiased,
  var_res_ci_method
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".heterogeneity_+3A_mean_es">mean_es</code></td>
<td>
<p>The mean effect size.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_es">var_es</code></td>
<td>
<p>The observed variances of effect sizes.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_pre">var_pre</code></td>
<td>
<p>The total predicted variance of effect sizes due to sampling error and other artifacts.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_res">var_res</code></td>
<td>
<p>The estimated residual variance of effect sizes.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_e">var_e</code></td>
<td>
<p>The predicted variance of effect sizes due to sampling error.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_art">var_art</code></td>
<td>
<p>The predicted variance of effect sizes predicted from other artifacts.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_wt_vec">wt_vec</code></td>
<td>
<p>The vector of weights used in the meta-analysis.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_n">N</code></td>
<td>
<p>The total sample size of the meta-analysis.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_k">k</code></td>
<td>
<p>The number of effect sizes included in the meta-analysis.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_es_vec">es_vec</code></td>
<td>
<p>The vector of effect sizes used in the meta-analysis.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_vare_vec">vare_vec</code></td>
<td>
<p>The vector of sampling-error variances used in the meta-analysis.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_es_failsafe">es_failsafe</code></td>
<td>
<p>Failsafe value of the effect size for use in file-drawer analyses.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_es_type">es_type</code></td>
<td>
<p>Name of effect-size type.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_wt_type">wt_type</code></td>
<td>
<p>Weighting method.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_ma_method">ma_method</code></td>
<td>
<p>What artifact correction method is used. Options are &quot;bb&quot;, &quot;ic&quot;, and &quot;ad&quot;.</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Are variances calculated using the unbiased (<code>TRUE</code>) or maximum likelihood (<code>FALSE</code>) estimator?</p>
</td></tr>
<tr><td><code id=".heterogeneity_+3A_var_res_ci_method">var_res_ci_method</code></td>
<td>
<p>Method to use to estimate a confidence interval for <code>var_res</code>. See <code><a href="#topic+heterogeneity">heterogeneity()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of heterogeneity statistics.
</p>

<hr>
<h2 id='.identify_ma_cols'>Identify meta-analysis type and provide new column names for a meta-analysis</h2><span id='topic+.identify_ma_cols'></span>

<h3>Description</h3>

<p>Identify meta-analysis type and provide new column names for a meta-analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.identify_ma_cols(col_names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".identify_ma_cols_+3A_col_names">col_names</code></td>
<td>
<p>Column names of a meta-analysis table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Meta-analysis type, old column names of table, column names of table after effect-size conversion, and a vector categorizing the types of entries supplied in the table.
</p>

<hr>
<h2 id='.integrate_dmod'>Integration function for computing parametric signed or unsigned <code class="reqn">d_{Mod}</code> effect sizes
for a single focal group</h2><span id='topic+.integrate_dmod'></span>

<h3>Description</h3>

<p>This internal function exists to support the <code><a href="#topic+compute_dmod_par">compute_dmod_par()</a></code> function, but may also be useful as a bare-bones tool for computing signed and unsigned <code class="reqn">d_{Mod}</code> effect sizes.
Please note that this function does not include an option for re-scaling its result to compensate for cumulative densities smaller than 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.integrate_dmod(
  referent_int,
  referent_slope,
  focal_int,
  focal_slope,
  focal_mean_x,
  focal_sd_x,
  referent_sd_y,
  focal_min_x,
  focal_max_x,
  signed = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".integrate_dmod_+3A_referent_int">referent_int</code></td>
<td>
<p>Referent group's intercept.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_referent_slope">referent_slope</code></td>
<td>
<p>Referent group's slope.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_int">focal_int</code></td>
<td>
<p>Focal group's intercept.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_slope">focal_slope</code></td>
<td>
<p>Focal group's slope.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_mean_x">focal_mean_x</code></td>
<td>
<p>Focal group's predictor-score mean.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_sd_x">focal_sd_x</code></td>
<td>
<p>Focal group's predictor-score standard deviation.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_referent_sd_y">referent_sd_y</code></td>
<td>
<p>Referent group's criterion standard deviation.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_min_x">focal_min_x</code></td>
<td>
<p>Focal group's minimum predictor score.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_focal_max_x">focal_max_x</code></td>
<td>
<p>Focal group's maximum predictor score.</p>
</td></tr>
<tr><td><code id=".integrate_dmod_+3A_signed">signed</code></td>
<td>
<p>Logical argument that indicates whether the function should compute <code class="reqn">d_{Mod_{Signed}}</code> (<code>TRUE</code>; default) or <code class="reqn">d_{Mod_{Unsigned}}</code> (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">d_{Mod_{Signed}}</code> effect size (i.e., the average of differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Signed}}=\frac{1}{SD_{Y_{1}}}\intop f_{2}(X)\left[X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right] dX</code>
</p>
<p>,
where
</p>

<ul>
<li> <p><code class="reqn">SD_{Y_{1}}</code> is the referent group's criterion standard deviation;
</p>
</li>
<li> <p><code class="reqn">f_{2}(X)</code> is the normal-density function for the distribution of focal-group predictor scores;
</p>
</li>
<li> <p><code class="reqn">b_{1_{1}}</code> and <code class="reqn">b_{1_{2}}</code> are the slopes of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively;
</p>
</li>
<li> <p><code class="reqn">b_{0_{1}}</code> and <code class="reqn">b_{0_{2}}</code> are the intercepts of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively; and
</p>
</li>
<li> <p>the integral spans all <code class="reqn">X</code> scores within the operational range of predictor scores for the focal group.
</p>
</li></ul>

<p>The <code class="reqn">d_{Mod_{Unsigned}}</code> effect size (i.e., the average of absolute differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Unsigned}}=\frac{1}{SD_{Y_{1}}}\intop f_{2}(X)\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|dX.</code>
</p>



<h3>Value</h3>

<p>A <code class="reqn">d_{Mod_{Signed}}</code> or <code class="reqn">d_{Mod_{Unsigned}}</code> effect size, depending on the <code>signed</code> argument.
</p>


<h3>References</h3>

<p>Nye, C. D., &amp; Sackett, P. R. (2017).
New effect sizes for tests of categorical moderation and differential prediction.
<em>Organizational Research Methods, 20</em>(4), 639–664. doi: <a href="https://doi.org/10.1177/1094428116644505">10.1177/1094428116644505</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example for computing \eqn{d_{Mod_{Signed}}}{d_Mod_Signed}
.integrate_dmod(referent_int = -.05, referent_slope = .5,
              focal_int = -.05, focal_slope = .3,
              focal_mean_x = -.5, focal_sd_x = 1,
              referent_sd_y = 1, focal_min_x = -Inf, focal_max_x = Inf,
              signed = TRUE)

# Example for computing \eqn{d_{Mod_{Unsigned}}}{d_Mod_Unsigned}
.integrate_dmod(referent_int = -.05, referent_slope = .5,
              focal_int = -.05, focal_slope = .3,
              focal_mean_x = -.5, focal_sd_x = 1,
              referent_sd_y = 1, focal_min_x = -Inf, focal_max_x = Inf,
              signed = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='.lambda_bvirr'>Internal function to produce lambda coefficients to use in the Case V correction for bivariate indirect range restriction.</h2><span id='topic+.lambda_bvirr'></span>

<h3>Description</h3>

<p>Internal function to produce lambda coefficients to use in the Case V correction for bivariate indirect range restriction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.lambda_bvirr(ux, uy, sign_rxz = 1, sign_ryz = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".lambda_bvirr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id=".lambda_bvirr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id=".lambda_bvirr_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id=".lambda_bvirr_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of lambda values.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>

<hr>
<h2 id='.ma_artifacts'>Internal function for computing meta-analyses of artifacts</h2><span id='topic+.ma_artifacts'></span>

<h3>Description</h3>

<p>Internal function for computing meta-analyses of artifacts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_artifacts(data, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_artifacts_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_artifacts_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bare-bones meta-analyses of correlations.
</p>

<hr>
<h2 id='.ma_bootstrap'>Wrapper function to facilitate bootstrapped meta-analyses</h2><span id='topic+.ma_bootstrap'></span>

<h3>Description</h3>

<p>Wrapper function to facilitate bootstrapped meta-analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_bootstrap(
  data,
  ma_fun_boot,
  boot_iter = 1000,
  boot_conf_level = 0.95,
  boot_ci_type = "norm",
  ma_arg_list,
  convert_ma = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_bootstrap_+3A_data">data</code></td>
<td>
<p>Data to be meta-analyzed.</p>
</td></tr>
<tr><td><code id=".ma_bootstrap_+3A_ma_fun_boot">ma_fun_boot</code></td>
<td>
<p>Meta-analysis function.</p>
</td></tr>
<tr><td><code id=".ma_bootstrap_+3A_boot_iter">boot_iter</code></td>
<td>
<p>Number of bootstrap iterations to be computed.</p>
</td></tr>
<tr><td><code id=".ma_bootstrap_+3A_boot_conf_level">boot_conf_level</code></td>
<td>
<p>Width of confidence intervals to be constructed for all bootstrapped statistics.</p>
</td></tr>
<tr><td><code id=".ma_bootstrap_+3A_boot_ci_type">boot_ci_type</code></td>
<td>
<p>Type of bootstrapped confidence interval (see &quot;type&quot; options for boot::boot.ci for possible arguments).</p>
</td></tr>
<tr><td><code id=".ma_bootstrap_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing (1) a summary matrix of means, variances, and confidence intervals of bootstrapped values and (2) the raw
output of the bootstrapping function.
</p>

<hr>
<h2 id='.ma_cumulative'>Cumulative meta-analyses</h2><span id='topic+.ma_cumulative'></span>

<h3>Description</h3>

<p>Cumulative meta-analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_cumulative(
  data,
  sort_method = c("n", "inv_var", "weight"),
  ma_fun_boot,
  ma_arg_list
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_cumulative_+3A_data">data</code></td>
<td>
<p>Data to be meta-analyzed.</p>
</td></tr>
<tr><td><code id=".ma_cumulative_+3A_sort_method">sort_method</code></td>
<td>
<p>Method to sort samples in the cumulative meta-analysis. Options are &quot;weight&quot; to sort by weight (default), &quot;n&quot; to sort by sample size, and &quot;inv_var&quot; to sort by inverse variance.</p>
</td></tr>
<tr><td><code id=".ma_cumulative_+3A_ma_fun_boot">ma_fun_boot</code></td>
<td>
<p>Meta-analysis function.</p>
</td></tr>
<tr><td><code id=".ma_cumulative_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cumulative meta-analysis table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Analysis TBD
</code></pre>

<hr>
<h2 id='.ma_d_bb'>Internal function for computing bare-bones meta-analyses of d values</h2><span id='topic+.ma_d_bb'></span>

<h3>Description</h3>

<p>Internal function for computing bare-bones meta-analyses of d values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_d_bb(data, ma_arg_list, run_lean = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_d_bb_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_d_bb_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
<tr><td><code id=".ma_d_bb_+3A_run_lean">run_lean</code></td>
<td>
<p>If TRUE, the meta-analysis will not generate an escalc object. Meant to speed up bootstrap analyses that do not require supplmental output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bare-bones meta-analyses of d values.
</p>

<hr>
<h2 id='.ma_d_bb_boot'>Internal function for computing bootstrapped bare-bones meta-analyses of d values</h2><span id='topic+.ma_d_bb_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped bare-bones meta-analyses of d values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_d_bb_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_d_bb_boot_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_d_bb_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_d_bb_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped bare-bones meta-analyses of d values.
</p>

<hr>
<h2 id='.ma_generic'>Internal function for computing bare-bones meta-analyses of generic effect sizes</h2><span id='topic+.ma_generic'></span>

<h3>Description</h3>

<p>Internal function for computing bare-bones meta-analyses of generic effect sizes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_generic(data, run_lean = FALSE, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_generic_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_generic_+3A_run_lean">run_lean</code></td>
<td>
<p>If TRUE, the meta-analysis will not generate an escalc object. Meant to speed up bootstrap analyses that do not require supplemental output.</p>
</td></tr>
<tr><td><code id=".ma_generic_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bare-bones meta-analyses of generic effect sizes.
</p>

<hr>
<h2 id='.ma_generic_boot'>Internal function for computing bootstrapped bare-bones meta-analyses of generic effect sizes</h2><span id='topic+.ma_generic_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped bare-bones meta-analyses of generic effect sizes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_generic_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_generic_boot_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_generic_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_generic_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped bare-bones meta-analyses of generic effect sizes.
</p>

<hr>
<h2 id='.ma_leave1out'>Leave-one-out (i.e., jackknife) meta-analyses</h2><span id='topic+.ma_leave1out'></span>

<h3>Description</h3>

<p>Leave-one-out (i.e., jackknife) meta-analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_leave1out(data, ma_fun_boot, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_leave1out_+3A_data">data</code></td>
<td>
<p>Data to be meta-analyzed.</p>
</td></tr>
<tr><td><code id=".ma_leave1out_+3A_ma_fun_boot">ma_fun_boot</code></td>
<td>
<p>Meta-analysis function.</p>
</td></tr>
<tr><td><code id=".ma_leave1out_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Leave-one-out results for the specified meta-analysis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Analysis TBD
</code></pre>

<hr>
<h2 id='.ma_r_bb'>Internal function for computing bare-bones meta-analyses of correlations</h2><span id='topic+.ma_r_bb'></span>

<h3>Description</h3>

<p>Internal function for computing bare-bones meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_bb(data, run_lean = FALSE, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_bb_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_r_bb_+3A_run_lean">run_lean</code></td>
<td>
<p>If TRUE, the meta-analysis will not generate an escalc object. Meant to speed up bootstrap analyses that do not require supplemental output.</p>
</td></tr>
<tr><td><code id=".ma_r_bb_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bare-bones meta-analyses of correlations.
</p>

<hr>
<h2 id='.ma_r_bb_boot'>Internal function for computing bootstrapped bare-bones meta-analyses of correlations</h2><span id='topic+.ma_r_bb_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped bare-bones meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_bb_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_bb_boot_+3A_data">data</code></td>
<td>
<p>Data frame of bare-bones information.</p>
</td></tr>
<tr><td><code id=".ma_r_bb_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_bb_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped bare-bones meta-analyses of correlations.
</p>

<hr>
<h2 id='.ma_r_ic'>Internal function for computing individual-correction meta-analyses of correlations</h2><span id='topic+.ma_r_ic'></span>

<h3>Description</h3>

<p>Internal function for computing individual-correction meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_ic(data, type = "all", run_lean = FALSE, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_ic_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_ic_+3A_type">type</code></td>
<td>
<p>Type of correlation to be meta-analyzed: &quot;ts&quot; for true score, &quot;vgx&quot; for validity generalization with &quot;X&quot; as the predictor,
&quot;vgy&quot; for for validity generalization with &quot;X&quot; as the predictor, and &quot;all&quot; for the complete set of results.</p>
</td></tr>
<tr><td><code id=".ma_r_ic_+3A_run_lean">run_lean</code></td>
<td>
<p>If TRUE, the meta-analysis will not generate an escalc object. Meant to speed up bootstrap analyses that do not require supplemental output.</p>
</td></tr>
<tr><td><code id=".ma_r_ic_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of individual-correction meta-analyses of correlations.
</p>

<hr>
<h2 id='.ma_r_ic_boot'>Internal function for computing bootstrapped individual-correction meta-analyses of all varieties</h2><span id='topic+.ma_r_ic_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped individual-correction meta-analyses of all varieties
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_ic_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_ic_boot_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_ic_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_ic_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped individual-correction meta-analyses of true-score correlations.
</p>

<hr>
<h2 id='.ma_r_icts_boot'>Internal function for computing bootstrapped individual-correction meta-analyses of true-score correlations</h2><span id='topic+.ma_r_icts_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped individual-correction meta-analyses of true-score correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_icts_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_icts_boot_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_icts_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_icts_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped individual-correction meta-analyses of true-score correlations.
</p>

<hr>
<h2 id='.ma_r_icvgx_boot'>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for X</h2><span id='topic+.ma_r_icvgx_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for X
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_icvgx_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_icvgx_boot_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_icvgx_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_icvgx_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped individual-correction meta-analyses of validity generalization correlations.
</p>

<hr>
<h2 id='.ma_r_icvgy_boot'>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for Y</h2><span id='topic+.ma_r_icvgy_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped individual-correction meta-analyses of validity generalization correlations for Y
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_icvgy_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_icvgy_boot_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_icvgy_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_icvgy_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped individual-correction meta-analyses of validity generalization correlations.
</p>

<hr>
<h2 id='.ma_r_order2'>Internal function for computing individual-correction meta-analyses of correlations</h2><span id='topic+.ma_r_order2'></span>

<h3>Description</h3>

<p>Internal function for computing individual-correction meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_order2(data, type = "all", run_lean = FALSE, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_order2_+3A_data">data</code></td>
<td>
<p>Data frame of individual-correction information.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_+3A_type">type</code></td>
<td>
<p>Type of correlation to be meta-analyzed: &quot;ts&quot; for true score, &quot;vgx&quot; for validity generalization with &quot;X&quot; as the predictor,
&quot;vgy&quot; for for validity generalization with &quot;X&quot; as the predictor, and &quot;all&quot; for the complete set of results.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_+3A_run_lean">run_lean</code></td>
<td>
<p>If TRUE, the meta-analysis will not generate a data object. Meant to speed up bootstrap analyses that do not require supplemental output.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be used in the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analytic table and a data frame.
</p>

<hr>
<h2 id='.ma_r_order2_ad_boot'>Internal function for computing bootstrapped second-order artifact-distribution meta-analyses of correlations</h2><span id='topic+.ma_r_order2_ad_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped second-order artifact-distribution meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_order2_ad_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_order2_ad_boot_+3A_data">data</code></td>
<td>
<p>Data frame of meta-analytic information.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_ad_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_ad_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped second-order artifact-distribution meta-analyses of correlations
</p>

<hr>
<h2 id='.ma_r_order2_bb_boot'>Internal function for computing bootstrapped second-order bare-bones meta-analyses of correlations</h2><span id='topic+.ma_r_order2_bb_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped second-order bare-bones meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_order2_bb_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_order2_bb_boot_+3A_data">data</code></td>
<td>
<p>Data frame of meta-analytic information.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_bb_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_bb_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped second-order bare-bones meta-analyses of correlations
</p>

<hr>
<h2 id='.ma_r_order2_ic_boot'>Internal function for computing bootstrapped second-order individual-correction meta-analyses of correlations</h2><span id='topic+.ma_r_order2_ic_boot'></span>

<h3>Description</h3>

<p>Internal function for computing bootstrapped second-order individual-correction meta-analyses of correlations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ma_r_order2_ic_boot(data, i, ma_arg_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ma_r_order2_ic_boot_+3A_data">data</code></td>
<td>
<p>Data frame of meta-analytic information.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_ic_boot_+3A_i">i</code></td>
<td>
<p>Vector of indexes to select studies from 'data'.</p>
</td></tr>
<tr><td><code id=".ma_r_order2_ic_boot_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing the results of bootstrapped second-order individual-correction meta-analyses of correlations
</p>

<hr>
<h2 id='.metabulate'>Internal function for .metabulating results tables</h2><span id='topic+.metabulate'></span>

<h3>Description</h3>

<p>Internal function for .metabulating results tables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.metabulate(
  meta_tables,
  ma_type = "ad_ts",
  output_format = "word",
  caption = caption,
  show_msd = TRUE,
  show_conf = TRUE,
  show_cred = TRUE,
  show_se = FALSE,
  show_var = FALSE,
  collapse_construct_labels = TRUE,
  bold_headers = TRUE,
  es_type = NULL,
  symbol_es = "ES",
  digits = 2L,
  decimal.mark = getOption("OutDec"),
  leading0 = "figure",
  neg.sign = "&amp;minus;",
  pos.sign = "figure",
  drop0integer = TRUE,
  big.mark = "&amp;#8239;",
  big.interval = 3L,
  small.mark = "&amp;#8239;",
  small.interval = 3L,
  na.mark = "&amp;mdash;",
  lgl.mark = c("+", "&amp;minus;"),
  inf.mark = c("&amp;#8199;&amp;infin;", "&amp;minus;&amp;infin;"),
  conf_format = "parentheses",
  cred_format = "parentheses",
  verbose = FALSE,
  unicode = unicode,
  conf_level = 0.95,
  cred_level = 0.8
)
</code></pre>

<hr>
<h2 id='.plot_forest_meta'>Internal plotting function for forest plots</h2><span id='topic+.plot_forest_meta'></span>

<h3>Description</h3>

<p>Internal plotting function for forest plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.plot_forest_meta(ma_mat, ma_vec = NULL, analysis = "leave1out")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".plot_forest_meta_+3A_ma_mat">ma_mat</code></td>
<td>
<p>Matrix of meta-analytic results to be plotted.</p>
</td></tr>
<tr><td><code id=".plot_forest_meta_+3A_ma_vec">ma_vec</code></td>
<td>
<p>An optional vector of overall meta-analytic results to use as reference points on the plots.</p>
</td></tr>
<tr><td><code id=".plot_forest_meta_+3A_analysis">analysis</code></td>
<td>
<p>Type of analysis to be plotted: leave-one-out or cumulative.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of forest plots
</p>

<hr>
<h2 id='.plot_funnel'>Internal funnel-plot generator</h2><span id='topic+.plot_funnel'></span>

<h3>Description</h3>

<p>Internal funnel-plot generator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.plot_funnel(
  x,
  label_es = "Effect Size",
  conf_level = c(0.95, 0.99),
  conf_linetype = c("dashed", "dotted"),
  conf_fill = NA,
  conf_alpha = 1,
  null_effect = NA,
  null_conf_level = c(0.9, 0.95, 0.99),
  null_conf_linetype = c("solid", "dashed", "dotted"),
  null_conf_fill = "black",
  null_conf_alpha = c(0, 0.2, 0.4)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".plot_funnel_+3A_x">x</code></td>
<td>
<p>An escalc-class object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A funnel plot.
</p>


<h3>Author(s)</h3>

<p>John Sakaluk and Brenton Wiernik
</p>

<hr>
<h2 id='.print.lm_mat'>Print method for objects of the class &quot;lm_mat&quot;</h2><span id='topic+.print.lm_mat'></span>

<h3>Description</h3>

<p>Print method for objects of the class &quot;lm_mat&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.print.lm_mat(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>

<hr>
<h2 id='.print.summary.lm_mat'>Print method for objects of the class &quot;summary.lm_mat&quot;</h2><span id='topic+.print.summary.lm_mat'></span>

<h3>Description</h3>

<p>Print method for objects of the class &quot;summary.lm_mat&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.print.summary.lm_mat(
  x,
  digits = max(3L, getOption("digits") - 3L),
  symbolic.cor = x$symbolic.cor,
  signif.stars = getOption("show.signif.stars"),
  ...
)
</code></pre>

<hr>
<h2 id='.rbeta'>Generate values from a beta distribution, given a mean and standard deviation of the distribution</h2><span id='topic+.rbeta'></span>

<h3>Description</h3>

<p>Generate values from a beta distribution, given a mean and standard deviation of the distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.rbeta(n, mean, sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".rbeta_+3A_n">n</code></td>
<td>
<p>Number of values to generate</p>
</td></tr>
<tr><td><code id=".rbeta_+3A_mean">mean</code></td>
<td>
<p>Mean of the distribution</p>
</td></tr>
<tr><td><code id=".rbeta_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of the distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of simulated values from a distribution bounded at 0 and 1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
.rbeta(n = 10, mean = .8, sd, .2)

## End(Not run)
</code></pre>

<hr>
<h2 id='.refine_var_rr'>Range-restriction refinement factor (i.e., &quot;a&quot;) for correlations' corrected sampling variances</h2><span id='topic+.refine_var_rr'></span>

<h3>Description</h3>

<p>For use with Case II and Case IV range restriction (not for use with Case V).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.refine_var_rr(
  rxyi,
  ux,
  rxx = NULL,
  indirect_rr = rep(TRUE, length(rxyi)),
  ux_observed = rep(TRUE, length(rxyi)),
  rxx_restricted = rep(TRUE, length(rxyi))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".refine_var_rr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id=".refine_var_rr_+3A_ux">ux</code></td>
<td>
<p>Vector of u ratios.</p>
</td></tr>
<tr><td><code id=".refine_var_rr_+3A_rxx">rxx</code></td>
<td>
<p>Vector of reliability estimates.</p>
</td></tr>
<tr><td><code id=".refine_var_rr_+3A_indirect_rr">indirect_rr</code></td>
<td>
<p>Logical vector determining whether a correction for indirect range restriction was performed (TRUE) or not (FALSE).</p>
</td></tr>
<tr><td><code id=".refine_var_rr_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector determining whether each element of ux is an observed-score u ratio (TRUE) or a true-score u ratio (FALSE).</p>
</td></tr>
<tr><td><code id=".refine_var_rr_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector determining whether each element of rxx is an incumbent reliability (TRUE) or an applicant reliability (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of range-restriction refinement factors.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.)</em>.
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. p. 145.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
.refine_var_rr(rxyi = .3, ux = .8, rxx = .8, indirect_rr = TRUE,
         ux_observed = TRUE, rxx_restricted = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='.tau_squared_m_solver'>tau_m_squared Solver</h2><span id='topic+.tau_squared_m_solver'></span>

<h3>Description</h3>

<p>Function to solve for tau_m_squared (outlier-robust estimator of tau_squared based on absolute deviations from median)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.tau_squared_m_solver(Q_m, wi, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".tau_squared_m_solver_+3A_q_m">Q_m</code></td>
<td>
<p>The Q_r statistic.</p>
</td></tr>
<tr><td><code id=".tau_squared_m_solver_+3A_wi">wi</code></td>
<td>
<p>Vector of inverse within-study sampling variances.</p>
</td></tr>
<tr><td><code id=".tau_squared_m_solver_+3A_k">k</code></td>
<td>
<p>The number of effect sizes included in the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tau_r_squared
</p>


<h3>Author(s)</h3>

<p>Lifeng Lin and Haitao Chu
</p>

<hr>
<h2 id='.tau_squared_r_solver'>tau_r_squared Solver</h2><span id='topic+.tau_squared_r_solver'></span>

<h3>Description</h3>

<p>Function to solve for tau_r_squared (outlier-robust estimator of tau_squared based on absolute deviations from mean)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.tau_squared_r_solver(Q_r, wi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".tau_squared_r_solver_+3A_q_r">Q_r</code></td>
<td>
<p>The Q_r statistic.</p>
</td></tr>
<tr><td><code id=".tau_squared_r_solver_+3A_wi">wi</code></td>
<td>
<p>Vector of inverse within-study sampling variances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tau_r_squared
</p>


<h3>Author(s)</h3>

<p>Lifeng Lin and Haitao Chu
</p>

<hr>
<h2 id='adjust_n_d'>Adjusted sample size for a non-Cohen <em>d</em> value for use in a meta-analysis of Cohen's <em>d</em> values</h2><span id='topic+adjust_n_d'></span>

<h3>Description</h3>

<p>This function is used to convert a non-Cohen <code class="reqn">d</code> value (e.g., Glass' <code class="reqn">\Delta</code>) to a Cohen's <code class="reqn">d</code> value by identifying the sample size of a Cohen's <code class="reqn">d</code> that has the
same standard error as the non-Cohen <code class="reqn">d</code>. This function permits users to account for the influence of sporadic corrections on the sampling variance of <code class="reqn">d</code> prior to use in a meta-analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_n_d(d, var_e, p = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_n_d_+3A_d">d</code></td>
<td>
<p>Vector of non-Cohen <code class="reqn">d</code> standardized mean differences.</p>
</td></tr>
<tr><td><code id="adjust_n_d_+3A_var_e">var_e</code></td>
<td>
<p>Vector of error variances of standardized mean differences.</p>
</td></tr>
<tr><td><code id="adjust_n_d_+3A_p">p</code></td>
<td>
<p>Proportion of participants in a study belonging to one of the two groups being contrasted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The adjusted sample size is computed as:
</p>
<p style="text-align: center;"><code class="reqn">n_{adjusted}=\frac{d^{2}p(1-p)+2}{2p(1-p)var_{e}}</code>
</p>



<h3>Value</h3>

<p>A vector of adjusted sample sizes.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 7 (Equations 7.23 and 7.23a).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>adjust_n_d(d = 1, var_e = .03, p = NA)
</code></pre>

<hr>
<h2 id='adjust_n_r'>Adjusted sample size for a non-Pearson correlation coefficient for use in a meta-analysis of Pearson correlations</h2><span id='topic+adjust_n_r'></span>

<h3>Description</h3>

<p>This function is used to compute the adjusted sample size of a non-Pearson correlation (e.g., a tetrachoric correlation) based on the correlation and its estimated error variance.
This function allows users to adjust the sample size of a correlation corrected for sporadic artifacts (e.g., unequal splits of dichotomous variables, artificial dichotomization of continuous variables) prior to use in a meta-analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_n_r(r, var_e)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_n_r_+3A_r">r</code></td>
<td>
<p>Vector of correlations.</p>
</td></tr>
<tr><td><code id="adjust_n_r_+3A_var_e">var_e</code></td>
<td>
<p>Vector of error variances.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The adjusted sample size is computed as:
</p>
<p style="text-align: center;"><code class="reqn">n_{adjusted}=\frac{(r^{2}-1)^{2}+var_{e}}{var_{e}}</code>
</p>



<h3>Value</h3>

<p>A vector of adjusted sample sizes.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
*Methods of meta-analysis: Correcting error and bias in research findings* (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Equation 3.7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>adjust_n_r(r = .3, var_e = .01)
</code></pre>

<hr>
<h2 id='anova.ma_psychmeta'>Wald-type tests for moderators in psychmeta meta-analyses</h2><span id='topic+anova.ma_psychmeta'></span>

<h3>Description</h3>

<p>This function computes Wald-type pairwise comparisons for each level of
categorical moderators for an <code>ma_psychmeta</code> object, as well as an ombnibus
one-way ANOVA test (equal variance not assumed).
</p>
<p>Currently, samples across moderator levels are assumed to be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ma_psychmeta'
anova(
  object,
  ...,
  analyses = "all",
  moderators = NULL,
  L = NULL,
  ma_obj2 = NULL,
  ma_method = c("bb", "ic", "ad"),
  correction_type = c("ts", "vgx", "vgy"),
  conf_level = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.ma_psychmeta_+3A_object">object</code></td>
<td>
<p>A psychmeta meta-analysis object.</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to to test moderators for? Can be either <code>"all"</code> to test moderators for all meta-analyses in the object (default) or a list containing one or more of the arguments <code>construct</code>, <code>construct_pair</code>, <code>pair_id</code>, <code>k_min</code>, and <code>N_min</code>. See <code><a href="#topic+filter_ma">filter_ma()</a></code> for details. Note that <code>analysis_id</code> should not be used. If <code>k_min</code> is not supplied, it is set to 2.</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_moderators">moderators</code></td>
<td>
<p>A character vector of moderators to test. If <code>NULL</code>, all categorical moderators are tested.</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_l">L</code></td>
<td>
<p>A named list with with elements specifying set of linear contrasts for each variable in <code>moderators</code>. (Not yet implemented.)</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_ma_obj2">ma_obj2</code></td>
<td>
<p>A second psychmeta meta-analysis object to compare to <code>object</code> (Not yet implemented.)</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_ma_method">ma_method</code></td>
<td>
<p>Meta-analytic methods to be included. Valid options are: &quot;bb&quot;, &quot;ic&quot;, and &quot;ad&quot;</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_correction_type">correction_type</code></td>
<td>
<p>Types of meta-analytic corrections to be included. Valid options are: &quot;ts&quot;, &quot;vgx&quot;, and &quot;vgy&quot;</p>
</td></tr>
<tr><td><code id="anova.ma_psychmeta_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of confidence intervals (defaults to value set when <code>object</code> was fit)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>anova.ma_psychmeta</code>. A tibble with a row for each construct pair in <code>object</code> and a column for each moderator tested. Cells lists of contrasts tested.
</p>


<h3>Note</h3>

<p>Currently, only simple (single) categorical moderators (one-way ANOVA) are supported.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ma_obj &lt;- ma_r(rxyi, n, construct_x = x_name, construct_y = y_name,
moderators = moderator, data = data_r_meas_multi)

anova(ma_obj)
</code></pre>

<hr>
<h2 id='check_wt_type'>Check whether wt_type argument is valid and determine which package to use for weights</h2><span id='topic+check_wt_type'></span>

<h3>Description</h3>

<p>Check whether wt_type argument is valid and determine which package to use for weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_wt_type(wt_type, generic = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_wt_type_+3A_wt_type">wt_type</code></td>
<td>
<p>wt_type argument passed to a meta-analysis function</p>
</td></tr>
<tr><td><code id="check_wt_type_+3A_generic">generic</code></td>
<td>
<p>Logical scalar determining whether the effect size is generic (TRUE) or one for which the meta-analysis function estimates error variances (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character object determining which package should be used to compute weights
</p>

<hr>
<h2 id='clean_warning'>Clean warnings and remove warnings present in the environment before running the function of interest</h2><span id='topic+clean_warning'></span>

<h3>Description</h3>

<p>Clean warnings and remove warnings present in the environment before running the function of interest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_warning(warn_obj1, warn_obj2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clean_warning_+3A_warn_obj1">warn_obj1</code></td>
<td>
<p>Initial warning object.</p>
</td></tr>
<tr><td><code id="clean_warning_+3A_warn_obj2">warn_obj2</code></td>
<td>
<p>Second warning object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cleaned warning table
</p>

<hr>
<h2 id='composite_d_matrix'>Matrix formula to estimate the standardized mean difference associated with a weighted or unweighted composite variable</h2><span id='topic+composite_d_matrix'></span>

<h3>Description</h3>

<p>This function is a wrapper for <code><a href="#topic+composite_r_matrix">composite_r_matrix</a></code> that converts <em>d</em> values to correlations, computes the composite correlation implied by the <em>d</em> values, and transforms the result back to the <em>d</em> metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_d_matrix(d_vec, r_mat, wt_vec, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_d_matrix_+3A_d_vec">d_vec</code></td>
<td>
<p>Vector of standardized mean differences associated with variables in the composite to be formed.</p>
</td></tr>
<tr><td><code id="composite_d_matrix_+3A_r_mat">r_mat</code></td>
<td>
<p>Correlation matrix from which the composite is to be computed.</p>
</td></tr>
<tr><td><code id="composite_d_matrix_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Weights to be used in forming the composite (by default, all variables receive equal weight).</p>
</td></tr>
<tr><td><code id="composite_d_matrix_+3A_p">p</code></td>
<td>
<p>The proportion of cases in one of the two groups used the compute the standardized mean differences.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The composite <em>d</em> value is computed by converting the vector of <em>d</em> values to correlations, computing the composite correlation (see <code>composite_r_matrix</code>), and transforming that composite back into the <em>d</em> metric.
See &quot;Details&quot; of <code><a href="#topic+composite_r_matrix">composite_r_matrix</a></code> for the composite computations.
</p>


<h3>Value</h3>

<p>The estimated standardized mean difference associated with the composite variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_d_matrix(d_vec = c(1, 1), r_mat = matrix(c(1, .7, .7, 1), 2, 2),
                   wt_vec = c(1, 1), p = .5)
</code></pre>

<hr>
<h2 id='composite_d_scalar'>Scalar formula to estimate the standardized mean difference associated with a composite variable</h2><span id='topic+composite_d_scalar'></span>

<h3>Description</h3>

<p>This function estimates the <em>d</em> value of a composite of X variables, given the mean <em>d</em> value of the individual X values and the mean correlation among those variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_d_scalar(
  mean_d,
  mean_intercor,
  k_vars,
  p = 0.5,
  partial_intercor = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_d_scalar_+3A_mean_d">mean_d</code></td>
<td>
<p>The mean standardized mean differences associated with variables in the composite to be formed.</p>
</td></tr>
<tr><td><code id="composite_d_scalar_+3A_mean_intercor">mean_intercor</code></td>
<td>
<p>The mean correlation among the variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_d_scalar_+3A_k_vars">k_vars</code></td>
<td>
<p>The number of variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_d_scalar_+3A_p">p</code></td>
<td>
<p>The proportion of cases in one of the two groups used the compute the standardized mean differences.</p>
</td></tr>
<tr><td><code id="composite_d_scalar_+3A_partial_intercor">partial_intercor</code></td>
<td>
<p>Logical scalar determining whether the <code>intercor</code> represents the partial (i.e., within-group) correlation among variables (<code>TRUE</code>) or the overall correlation between variables (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two different methods available for computing such a composite, one that uses the partial intercorrelation among the X variables (i.e., the average within-group correlation)
and one that uses the overall correlation among the X variables (i.e., the total or mixture correlation across groups).
</p>
<p>If a partial correlation is provided for the interrelationships among variables, the following formula is used to estimate the composite <em>d</em> value:
</p>
<p style="text-align: center;"><code class="reqn">d_{X}=\frac{\bar{d}_{x_{i}}k}{\sqrt{\bar{\rho}_{x_{i}x_{j}}k^{2}+\left(1-\bar{\rho}_{x_{i}x_{j}}\right)k}}</code>
</p>

<p>where <code class="reqn">d_{X}</code> is the composite d value, <code class="reqn">\bar{d}_{x_{i}}</code> is the mean <em>d</em> value, <code class="reqn">\bar{\rho}_{x_{i}x_{j}}</code> is the mean intercorrelation among the variables in the composite, and <em>k</em> is the number of variables in the composite.
Otherwise, the composite <em>d</em> value is computed by converting the mean <em>d</em> value to a correlation, computing the composite correlation (see <code><a href="#topic+composite_r_scalar">composite_r_scalar</a></code> for formula), and transforming that composite back into the <em>d</em> metric.
</p>


<h3>Value</h3>

<p>The estimated standardized mean difference associated with the composite variable.
</p>


<h3>References</h3>

<p>Rosenthal, R., &amp; Rubin, D. B. (1986). Meta-analytic procedures for combining studies with multiple effect sizes.
<em>Psychological Bulletin, 99</em>(3), 400–406.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_d_scalar(mean_d = 1, mean_intercor = .7, k_vars = 2, p = .5)
</code></pre>

<hr>
<h2 id='composite_r_matrix'>Matrix formula to estimate the correlation between two weighted or unweighted composite variables</h2><span id='topic+composite_r_matrix'></span>

<h3>Description</h3>

<p>This function computes the weighted (or unweighted, by default) composite correlation between a set of X variables and a set of Y variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_r_matrix(
  r_mat,
  x_col,
  y_col,
  wt_vec_x = rep(1, length(x_col)),
  wt_vec_y = rep(1, length(y_col))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_r_matrix_+3A_r_mat">r_mat</code></td>
<td>
<p>Correlation matrix from which composite correlations are to be computed.</p>
</td></tr>
<tr><td><code id="composite_r_matrix_+3A_x_col">x_col</code></td>
<td>
<p>Column indices of variables from 'r_mat' in the X composite (specify a single variable if Y is an observed variable rather than a composite).</p>
</td></tr>
<tr><td><code id="composite_r_matrix_+3A_y_col">y_col</code></td>
<td>
<p>Column indices of variables from 'r_mat' in the Y composite (specify a single variable if Y is an observed variable rather than a composite).</p>
</td></tr>
<tr><td><code id="composite_r_matrix_+3A_wt_vec_x">wt_vec_x</code></td>
<td>
<p>Weights to be used in forming the X composite (by default, all variables receive equal weight).</p>
</td></tr>
<tr><td><code id="composite_r_matrix_+3A_wt_vec_y">wt_vec_y</code></td>
<td>
<p>Weights to be used in forming the Y composite (by default, all variables receive equal weight).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY}\frac{\mathbf{w}_{X}^{T}\mathbf{R}_{XY}\mathbf{w}_{Y}}{\sqrt{\left(\mathbf{w}_{X}^{T}\mathbf{R}_{XX}\mathbf{w}_{X}\right)\left(\mathbf{w}_{Y}^{T}\mathbf{R}_{YY}\mathbf{w}_{Y}\right)}}</code>
</p>

<p>where <code class="reqn">\rho_{XY}</code> is the composite correlation, <code class="reqn">\mathbf{w}</code> is a vector of weights, and <code class="reqn">\mathbf{R}</code> is a correlation matrix. The subscripts of <code class="reqn">\mathbf{w}</code> and <code class="reqn">\mathbf{R}</code> indicate the variables indexed within the vector or matrix.
</p>


<h3>Value</h3>

<p>A composite correlation
</p>


<h3>References</h3>

<p>Mulaik, S. A. (2010). <em>Foundations of factor analysis</em>.
Boca Raton, FL: CRC Press. pp. 83–84.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_r_scalar(mean_rxy = .3, k_vars_x = 4, mean_intercor_x = .4)
R &lt;- reshape_vec2mat(.4, order = 5)
R[-1,1] &lt;- R[1,-1] &lt;- .3
composite_r_matrix(r_mat = R, x_col = 2:5, y_col = 1)
</code></pre>

<hr>
<h2 id='composite_r_scalar'>Scalar formula to estimate the correlation between a composite and another variable or between two composite variables</h2><span id='topic+composite_r_scalar'></span>

<h3>Description</h3>

<p>This function estimates the correlation between a set of X variables and a set of Y variables using a scalar formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_r_scalar(
  mean_rxy,
  k_vars_x = NULL,
  mean_intercor_x = NULL,
  k_vars_y = NULL,
  mean_intercor_y = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_r_scalar_+3A_mean_rxy">mean_rxy</code></td>
<td>
<p>Mean correlation between sets of X and Y variables.</p>
</td></tr>
<tr><td><code id="composite_r_scalar_+3A_k_vars_x">k_vars_x</code></td>
<td>
<p>Number of X variables.</p>
</td></tr>
<tr><td><code id="composite_r_scalar_+3A_mean_intercor_x">mean_intercor_x</code></td>
<td>
<p>Mean correlation among X variables.</p>
</td></tr>
<tr><td><code id="composite_r_scalar_+3A_k_vars_y">k_vars_y</code></td>
<td>
<p>Number of Y variables.</p>
</td></tr>
<tr><td><code id="composite_r_scalar_+3A_mean_intercor_y">mean_intercor_y</code></td>
<td>
<p>Mean correlation among Y variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula to estimate a correlation between one composite variable and one external variable is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{Xy}=\frac{\bar{\rho}_{x_{i}y}}{\sqrt{\frac{1}{k_{x}}+\frac{k_{x}-1}{k_{x}}\bar{\rho}_{x_{i}x_{j}}}}</code>
</p>

<p>and the formula to estimate the correlation between two composite variables is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY}=\frac{\bar{\rho}_{x_{i}y_{j}}}{\sqrt{\frac{1}{k_{x}}+\frac{k-1}{k_{x}}\bar{\rho}_{x_{i}x_{j}}}\sqrt{\frac{1}{k_{y}}+\frac{k_{y}-1}{k_{y}}\bar{\rho}_{y_{i}y_{j}}}}</code>
</p>

<p>where <code class="reqn">\bar{\rho}_{x_{i}y}</code> and <code class="reqn">\bar{\rho}_{x_{i}y{j}}</code> are mean correlations between the x variables and the y variable(s),
<code class="reqn">\bar{\rho}_{x_{i}x_{j}}</code> is the mean correlation among x variables,
<code class="reqn">\bar{\rho}_{y_{i}y_{j}}</code> is the mean correlation among y variables,
<code class="reqn">{k}_{x}</code> is the number of x variables, and <code class="reqn">{k}_{y}</code> is the number of y variables.
</p>


<h3>Value</h3>

<p>A vector of composite correlations
</p>


<h3>References</h3>

<p>Ghiselli, E. E., Campbell, J. P., &amp; Zedeck, S. (1981).
<em>Measurement theory for the behavioral sciences</em>.
San Francisco, CA: Freeman. p. 163-164.
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Thousand Oaks, CA: Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 441 - 447.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Composite correlation between 4 variables and an outside variable with which
## the four variables correlate .3 on average:
composite_r_scalar(mean_rxy = .3, k_vars_x = 4, mean_intercor_x = .4)

## Correlation between two composites:
composite_r_scalar(mean_rxy = .3, k_vars_x = 2, mean_intercor_x = .5,
                   k_vars_y = 2, mean_intercor_y = .2)
</code></pre>

<hr>
<h2 id='composite_rel_matrix'>Matrix formula to estimate the reliability of a weighted or unweighted composite variable</h2><span id='topic+composite_rel_matrix'></span>

<h3>Description</h3>

<p>This function computes the reliability of a variable that is a weighted or unweighted composite of other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_rel_matrix(rel_vec, r_mat, sd_vec, wt_vec = rep(1, length(rel_vec)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_rel_matrix_+3A_rel_vec">rel_vec</code></td>
<td>
<p>Vector of reliabilities associated with variables in the composite to be formed.</p>
</td></tr>
<tr><td><code id="composite_rel_matrix_+3A_r_mat">r_mat</code></td>
<td>
<p>Correlation matrix from which the composite is to be computed.</p>
</td></tr>
<tr><td><code id="composite_rel_matrix_+3A_sd_vec">sd_vec</code></td>
<td>
<p>Vector of standard deviations associated with variables in the composite to be formed.</p>
</td></tr>
<tr><td><code id="composite_rel_matrix_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Weights to be used in forming the composite (by default, all variables receive equal weight).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function treats measure-specific variance as reliable.
</p>
<p>The Mosier composite formula is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX}=\frac{\mathbf{w}^{T}\left(\mathbf{r}\circ\mathbf{s}\right)+\mathbf{w}^{T}\mathbf{S}\mathbf{w}-\mathbf{w}^{T}\mathbf{s}}{\mathbf{w}^{T}\mathbf{S}\mathbf{w}}</code>
</p>

<p>where <code class="reqn">\rho_{XX}</code> is a composite reliability estimate, <code class="reqn">\mathbf{r}</code> is a vector of reliability estimates, <code class="reqn">\mathbf{w}</code> is a vector of weights, <code class="reqn">\mathbf{S}</code> is a covariance matrix, and <code class="reqn">\mathbf{s}</code> is a vector of variances (i.e., the diagonal elements of <code class="reqn">\mathbf{S}</code>).
</p>


<h3>Value</h3>

<p>The estimated reliability of the composite variable.
</p>


<h3>References</h3>

<p>Mosier, C. I. (1943). On the reliability of a weighted composite.
<em>Psychometrika, 8</em>(3), 161–168. doi: <a href="https://doi.org/10.1007/BF02288700">10.1007/BF02288700</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Thousand Oaks, CA: Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 441 - 447.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_rel_matrix(rel_vec = c(.8, .8),
r_mat = matrix(c(1, .4, .4, 1), 2, 2), sd_vec = c(1, 1))
</code></pre>

<hr>
<h2 id='composite_rel_scalar'>Scalar formula to estimate the reliability of a composite variable</h2><span id='topic+composite_rel_scalar'></span>

<h3>Description</h3>

<p>This function computes the reliability of a variable that is a unit-weighted composite of other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_rel_scalar(mean_rel, mean_intercor, k_vars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_rel_scalar_+3A_mean_rel">mean_rel</code></td>
<td>
<p>The mean reliability of variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_rel_scalar_+3A_mean_intercor">mean_intercor</code></td>
<td>
<p>The mean correlation among the variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_rel_scalar_+3A_k_vars">k_vars</code></td>
<td>
<p>The number of variables in the composite.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Mosier composite formula is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX}=\frac{\bar{\rho}_{x_{i}x_{i}}k+k\left(k-1\right)\bar{\rho}_{x_{i}x_{j}}}{k+k\left(k-1\right)\bar{\rho}_{x_{i}x_{j}}}</code>
</p>

<p>where <code class="reqn">\bar{\rho}_{x_{i}x_{i}}</code> is the mean reliability of variables in the composite, <code class="reqn">\bar{\rho}_{x_{i}x_{j}}</code> is the mean intercorrelation among variables in the composite, and <em>k</em> is the number of variables in the composite.
</p>


<h3>Value</h3>

<p>The estimated reliability of the composite variable.
</p>


<h3>References</h3>

<p>Mosier, C. I. (1943). On the reliability of a weighted composite.
<em>Psychometrika, 8</em>(3), 161–168. doi: <a href="https://doi.org/10.1007/BF02288700">10.1007/BF02288700</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Thousand Oaks, CA: Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 441 - 447.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_rel_scalar(mean_rel = .8, mean_intercor = .4, k_vars = 2)
</code></pre>

<hr>
<h2 id='composite_u_matrix'>Matrix formula to estimate the u ratio of a composite variable</h2><span id='topic+composite_u_matrix'></span>

<h3>Description</h3>

<p>This function estimates the u ratio of a composite variable when at least one matrix of correlations (restricted or unrestricted) among the variables is available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_u_matrix(
  ri_mat = NULL,
  ra_mat = NULL,
  u_vec,
  wt_vec = rep(1, length(u_vec)),
  sign_r_vec = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_u_matrix_+3A_ri_mat">ri_mat</code></td>
<td>
<p>Range-restricted correlation matrix from which the composite is to be computed (if <code>NULL</code>, <code>ri_mat</code> is estimated from <code>ra_mat</code>).</p>
</td></tr>
<tr><td><code id="composite_u_matrix_+3A_ra_mat">ra_mat</code></td>
<td>
<p>Unrestricted correlation matrix from which the composite is to be computed (if <code>NULL</code>, <code>ra_mat</code> is estimated from <code>ri_mat</code>).</p>
</td></tr>
<tr><td><code id="composite_u_matrix_+3A_u_vec">u_vec</code></td>
<td>
<p>Vector of u ratios associated with variables in the composite to be formed.</p>
</td></tr>
<tr><td><code id="composite_u_matrix_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Weights to be used in forming the composite (by default, all variables receive equal weight).</p>
</td></tr>
<tr><td><code id="composite_u_matrix_+3A_sign_r_vec">sign_r_vec</code></td>
<td>
<p>The signs of the relationships between the variables in the composite and the variable by which range restriction was induced.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is computed as:
</p>
<p style="text-align: center;"><code class="reqn">u_{composite}=\sqrt{\frac{\left(\mathbf{w}\circ\mathbf{u}\right)^{T}\mathbf{R}_{i}\left(\mathbf{w}\circ\mathbf{u}\right)}{\mathbf{w}^{T}\mathbf{R}_{a}\mathbf{w}}}</code>
</p>

<p>where <code class="reqn">u_{composite}</code> is the composite u ratio, <code class="reqn">\mathbf{u}</code> is a vector of u ratios, <code class="reqn">\mathbf{R}_{i}</code> is a range-restricted correlation matrix, <code class="reqn">\mathbf{R}_{a}</code> is an unrestricted correlation matrix, and <code class="reqn">\mathbf{w}</code> is a vector of weights.
</p>


<h3>Value</h3>

<p>The estimated <em>u</em> ratio of the composite variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_u_matrix(ri_mat = matrix(c(1, .3, .3, 1), 2, 2), u_vec = c(.8, .8))
</code></pre>

<hr>
<h2 id='composite_u_scalar'>Scalar formula to estimate the u ratio of a composite variable</h2><span id='topic+composite_u_scalar'></span>

<h3>Description</h3>

<p>This function provides an approximation of the u ratio of a composite variable based on the u ratios of the component variables, the mean restricted intercorrelation among those variables,
and the mean unrestricted correlation among those variables. If only one of the mean intercorrelations is known, the other will be estimated using the bivariate indirect range-restriction formula.
This tends to compute a conservative estimate of the u ratio associated with a composite.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite_u_scalar(mean_ri = NULL, mean_ra = NULL, mean_u, k_vars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite_u_scalar_+3A_mean_ri">mean_ri</code></td>
<td>
<p>The mean range-restricted correlation among variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_u_scalar_+3A_mean_ra">mean_ra</code></td>
<td>
<p>The mean unrestricted correlation among variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_u_scalar_+3A_mean_u">mean_u</code></td>
<td>
<p>The mean u ratio of variables in the composite.</p>
</td></tr>
<tr><td><code id="composite_u_scalar_+3A_k_vars">k_vars</code></td>
<td>
<p>The number of variables in the composite.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is computed as:
</p>
<p style="text-align: center;"><code class="reqn">u_{composite}=\sqrt{\frac{\bar{\rho}_{i}\bar{u}^{2}k(k-1)+k\bar{u}^{2}}{\bar{\rho}_{a}k(k-1)+k}}</code>
</p>

<p>where <code class="reqn">u_{composite}</code> is the composite u ratio, <code class="reqn">\bar{u}</code> is the mean univariate u ratio, <code class="reqn">\bar{\rho}_{i}</code> is the mean restricted correlation among variables,
<code class="reqn">\bar{\rho}_{a}</code> is the mean unrestricted correlation among variables, and <em>k</em> is the number of variables in the composite.
</p>


<h3>Value</h3>

<p>The estimated <em>u</em> ratio of the composite variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>composite_u_scalar(mean_ri = .3, mean_ra = .4, mean_u = .8, k_vars = 2)
</code></pre>

<hr>
<h2 id='compute_alpha'>Compute coefficient alpha</h2><span id='topic+compute_alpha'></span>

<h3>Description</h3>

<p>Compute coefficient alpha
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_alpha(sigma = NULL, data = NULL, standardized = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_alpha_+3A_sigma">sigma</code></td>
<td>
<p>Covariance matrix (must be supplied if data argument is not supplied)</p>
</td></tr>
<tr><td><code id="compute_alpha_+3A_data">data</code></td>
<td>
<p>Data matrix or data frame (must be supplied if sigma argument is not supplied)</p>
</td></tr>
<tr><td><code id="compute_alpha_+3A_standardized">standardized</code></td>
<td>
<p>Logical scalar determining whether alpha should be computed from an unstandardized covariance matrix (<code>TRUE</code>) or a correlation matrix (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="compute_alpha_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to <code>cov()</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Coefficient alpha
</p>


<h3>Examples</h3>

<pre><code class='language-R'>compute_alpha(sigma = reshape_vec2mat(cov = .4, order = 10))
</code></pre>

<hr>
<h2 id='compute_dmod'>Comprehensive <code class="reqn">d_{Mod}</code> calculator</h2><span id='topic+compute_dmod'></span>

<h3>Description</h3>

<p>This is a general-purpose function to compute <code class="reqn">d_{Mod}</code> effect sizes from raw data and to perform bootstrapping.
It subsumes the functionalities of the <code>compute_dmod_par</code> (for parametric computations) and <code>compute_dmod_npar</code> (for non-parametric computations)
functions and automates the generation of regression equations and descriptive statistics for computing <code class="reqn">d_{Mod}</code> effect sizes. Please see documentation
for <code>compute_dmod_par</code> and <code>compute_dmod_npar</code> for details about how the effect sizes are computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_dmod(
  data,
  group,
  predictors,
  criterion,
  referent_id,
  focal_id_vec = NULL,
  conf_level = 0.95,
  rescale_cdf = TRUE,
  parametric = TRUE,
  bootstrap = TRUE,
  boot_iter = 1000,
  stratify = FALSE,
  empirical_ci = FALSE,
  cross_validate_wts = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_dmod_+3A_data">data</code></td>
<td>
<p>Data frame containing the data to be analyzed (if not a data frame, must be an object convertible to a data frame via the as.data.frame function).
The data set must contain a criterion variable, at least one predictor variable, and a categorical variable that identifies the group to which each case (i.e., row) in the data set belongs.</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_group">group</code></td>
<td>
<p>Name or column-index number of the variable that identifies group membership in the data set.</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_predictors">predictors</code></td>
<td>
<p>Name(s) or column-index number(s) of the predictor variable(s) in the data set. No predictor can be a factor-type variable.
If multiple predictors are specified, they will be combined into a regression-weighted composite that will be carried forward to compute <code class="reqn">d_{Mod}</code> effect sizes.
</p>

<ul>
<li> <p><em>Note</em>: If weights other than regression weights should be used to combine the predictors into a composite, the user must manually compute such a composite,
include the composite in the <code>dat</code> data set, and identify the composite variable in <code>predictors</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="compute_dmod_+3A_criterion">criterion</code></td>
<td>
<p>Name or column-index number of the criterion variable in the data set. The criterion cannot be a factor-type variable.</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_referent_id">referent_id</code></td>
<td>
<p>Label used to identify the referent group in the <code>group</code> variable.</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_focal_id_vec">focal_id_vec</code></td>
<td>
<p>Label(s) to identify the focal group(s) in the <code>group</code> variable. If <code>NULL</code> (the default), the specified referent group will be compared to all other groups.</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level (between <code>0</code> and <code>1</code>) to be used in generating confidence intervals. Default is <code>.95</code></p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_rescale_cdf">rescale_cdf</code></td>
<td>
<p>Logical argument that indicates whether parametric <code class="reqn">d_{Mod}</code> results should be rescaled to account for using a cumulative density &lt; 1 in the computations (<code>TRUE</code>; default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_parametric">parametric</code></td>
<td>
<p>Logical argument that indicates whether <code class="reqn">d_{Mod}</code> should be computed using an assumed normal distribution (<code>TRUE</code>; default) or observed frequencies (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Logical argument that indicates whether <code class="reqn">d_{Mod}</code> should be bootstrapped (<code>TRUE</code>; default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_boot_iter">boot_iter</code></td>
<td>
<p>Number of bootstrap iterations to compute (default = <code>1000</code>).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_stratify">stratify</code></td>
<td>
<p>Logical argument that indicates whether the random bootstrap sampling should be stratified (<code>TRUE</code>) or unstratified (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_empirical_ci">empirical_ci</code></td>
<td>
<p>Logical argument that indicates whether the bootstrapped confidence invervals should be computed from the observed empirical distributions (<code>TRUE</code>) or computed using
bootstrapped means and standard errors via the normal-theory approach (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="compute_dmod_+3A_cross_validate_wts">cross_validate_wts</code></td>
<td>
<p>Only relevant when multiple predictors are specified and bootstrapping is performed.
Logical argument that indicates whether regression weights derived from the full sample should be used to combine predictors in the bootstrapped samples (<code>TRUE</code>)
or if a new set of weights should be derived during each iteration of the bootstrapping procedure (<code>FALSE</code>; default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If bootstrapping is selected, the list will include:
</p>

<ul>
<li> <p><code>point_estimate</code> A matrix of effect sizes (<code class="reqn">d_{Mod_{Signed}}</code>,
<code class="reqn">d_{Mod_{Unsigned}}</code>, <code class="reqn">d_{Mod_{Under}}</code>,
<code class="reqn">d_{Mod_{Over}}</code>), proportions of under- and over-predicted criterion scores,
minimum and maximum differences, and the scores associated with minimum and maximum differences.
All of these values are computed using the full data set.
</p>
</li>
<li> <p><code>bootstrap_mean</code> A matrix of the same statistics as the <code>point_estimate</code> matrix,
but the values in this matrix are the means of the results from bootstrapped samples.
</p>
</li>
<li> <p><code>bootstrap_se</code> A matrix of the same statistics as the <code>point_estimate</code> matrix,
but the values in this matrix are bootstrapped standard errors (i.e., the standard deviations of the results from bootstrapped samples).
</p>
</li>
<li> <p><code>bootstrap_CI_Lo</code> A matrix of the same statistics as the <code>point_estimate</code> matrix,
but the values in this matrix are the lower confidence bounds of the results from bootstrapped samples.
</p>
</li>
<li> <p><code>bootstrap_CI_Hi</code> A matrix of the same statistics as the <code>point_estimate</code> matrix,
but the values in this matrix are the upper confidence bounds of the results from bootstrapped samples.
</p>
</li></ul>

<p>If no bootstrapping is performed, the output will be limited to the <code>point_estimate</code> matrix.
</p>


<h3>References</h3>

<p>Nye, C. D., &amp; Sackett, P. R. (2017).
New effect sizes for tests of categorical moderation and differential prediction.
<em>Organizational Research Methods, 20</em>(4), 639–664. doi: <a href="https://doi.org/10.1177/1094428116644505">10.1177/1094428116644505</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate some hypothetical data for a referent group and three focal groups:
set.seed(10)
refDat &lt;- MASS::mvrnorm(n = 1000, mu = c(.5, .2),
                        Sigma = matrix(c(1, .5, .5, 1), 2, 2), empirical = TRUE)
foc1Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(-.5, -.2),
                         Sigma = matrix(c(1, .5, .5, 1), 2, 2), empirical = TRUE)
foc2Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(0, 0),
                         Sigma = matrix(c(1, .3, .3, 1), 2, 2), empirical = TRUE)
foc3Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(-.5, -.2),
                         Sigma = matrix(c(1, .3, .3, 1), 2, 2), empirical = TRUE)
colnames(refDat) &lt;- colnames(foc1Dat) &lt;- colnames(foc2Dat) &lt;- colnames(foc3Dat) &lt;- c("X", "Y")
dat &lt;- rbind(cbind(G = 1, refDat), cbind(G = 2, foc1Dat),
             cbind(G = 3, foc2Dat), cbind(G = 4, foc3Dat))

# Compute point estimates of parametric d_mod effect sizes:
compute_dmod(data = dat, group = "G", predictors = "X", criterion = "Y",
     referent_id = 1, focal_id_vec = 2:4,
     conf_level = .95, rescale_cdf = TRUE, parametric = TRUE,
     bootstrap = FALSE)

# Compute point estimates of non-parametric d_mod effect sizes:
compute_dmod(data = dat, group = "G", predictors = "X", criterion = "Y",
     referent_id = 1, focal_id_vec = 2:4,
     conf_level = .95, rescale_cdf = TRUE, parametric = FALSE,
     bootstrap = FALSE)

# Compute unstratified bootstrapped estimates of parametric d_mod effect sizes:
compute_dmod(data = dat, group = "G", predictors = "X", criterion = "Y",
     referent_id = 1, focal_id_vec = 2:4,
     conf_level = .95, rescale_cdf = TRUE, parametric = TRUE,
     boot_iter = 10, bootstrap = TRUE, stratify = FALSE, empirical_ci = FALSE)

# Compute unstratified bootstrapped estimates of non-parametric d_mod effect sizes:
compute_dmod(data = dat, group = "G", predictors = "X", criterion = "Y",
     referent_id = 1, focal_id_vec = 2:4,
     conf_level = .95, rescale_cdf = TRUE, parametric = FALSE,
     boot_iter = 10, bootstrap = TRUE, stratify = FALSE, empirical_ci = FALSE)
</code></pre>

<hr>
<h2 id='compute_dmod_npar'>Function for computing non-parametric <code class="reqn">d_{Mod}</code> effect sizes for a single focal group</h2><span id='topic+compute_dmod_npar'></span>

<h3>Description</h3>

<p>This function computes non-parametric <code class="reqn">d_{Mod}</code> effect sizes from user-defined descriptive statistics
and regression coefficients, using a distribution of observed scores as weights.
This non-parametric function is best used when the assumption of normally distributed predictor
scores is not reasonable and/or the distribution of scores observed in a sample is likely to represent the
distribution of scores in the population of interest.
If one has access to the full raw data set, the <code>dMod</code> function may be used
as a wrapper to this function so that the regression equations and descriptive statistics can
be computed automatically within the program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_dmod_npar(
  referent_int,
  referent_slope,
  focal_int,
  focal_slope,
  focal_x,
  referent_sd_y
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_dmod_npar_+3A_referent_int">referent_int</code></td>
<td>
<p>Referent group's intercept.</p>
</td></tr>
<tr><td><code id="compute_dmod_npar_+3A_referent_slope">referent_slope</code></td>
<td>
<p>Referent group's slope.</p>
</td></tr>
<tr><td><code id="compute_dmod_npar_+3A_focal_int">focal_int</code></td>
<td>
<p>Focal group's intercept.</p>
</td></tr>
<tr><td><code id="compute_dmod_npar_+3A_focal_slope">focal_slope</code></td>
<td>
<p>Focal group's slope.</p>
</td></tr>
<tr><td><code id="compute_dmod_npar_+3A_focal_x">focal_x</code></td>
<td>
<p>Focal group's vector of predictor scores.</p>
</td></tr>
<tr><td><code id="compute_dmod_npar_+3A_referent_sd_y">referent_sd_y</code></td>
<td>
<p>Referent group's criterion standard deviation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">d_{Mod_{Signed}}</code> effect size (i.e., the average of differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Signed}}=\frac{\sum_{i=1}^{m}n_{i}\left[X_{i}\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right]}{SD_{Y_{1}}\sum_{i=1}^{m}n_{i}},</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">SD_{Y_{1}}</code> is the referent group's criterion standard deviation;
</p>
</li>
<li> <p><code class="reqn">m</code> is the number of unique scores in the distribution of focal-group predictor scores;
</p>
</li>
<li> <p><code class="reqn">X</code> is the vector of unique focal-group predictor scores, indexed <code class="reqn">i=1</code> through <code class="reqn">m</code>;
</p>
</li>
<li> <p><code class="reqn">X_{i}</code> is the <code class="reqn">i^{th}</code> unique score value;
</p>
</li>
<li> <p><code class="reqn">n</code> is the vector of frequencies associated with the elements of <code class="reqn">X</code>;
</p>
</li>
<li> <p><code class="reqn">n_{i}</code> is the number of cases with a score equal to <code class="reqn">X_{i}</code>;
</p>
</li>
<li> <p><code class="reqn">b_{1_{1}}</code> and <code class="reqn">b_{1_{2}}</code> are the slopes of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively; and
</p>
</li>
<li> <p><code class="reqn">b_{0_{1}}</code> and <code class="reqn">b_{0_{2}}</code> are the intercepts of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively.
</p>
</li></ul>

<p>The <code class="reqn">d_{Mod_{Under}}</code> and <code class="reqn">d_{Mod_{Over}}</code> effect sizes are computed
using the same equation as <code class="reqn">d_{Mod_{Signed}}</code>, but <code class="reqn">d_{Mod_{Under}}</code> is
the weighted average of all scores in the area of underprediction (i.e., the differences in prediction with
negative signs) and <code class="reqn">d_{Mod_{Over}}</code> is the weighted average of all scores in the area of
overprediction (i.e., the differences in prediction with negative signs).
</p>
<p>The <code class="reqn">d_{Mod_{Unsigned}}</code> effect size (i.e., the average of absolute differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Unsigned}}=\frac{\sum_{i=1}^{m}n_{i}\left|X_{i}\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|}{SD_{Y_{1}}\sum_{i=1}^{m}n_{i}}.</code>
</p>

<p>The <code class="reqn">d_{Min}</code> effect size (i.e., the smallest absolute difference in prediction observed over the
range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Min}=\frac{1}{SD_{Y_{1}}}Min\left[\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|\right].</code>
</p>

<p>The <code class="reqn">d_{Max}</code> effect size (i.e., the largest absolute difference in prediction observed over the
range of predictor scores)is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Max}=\frac{1}{SD_{Y_{1}}}Max\left[\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|\right].</code>
</p>

<p><em>Note</em>: When <code class="reqn">d_{Min}</code> and <code class="reqn">d_{Max}</code> are computed in this package, the output will display the
signs of the differences (rather than the absolute values of the differences) to aid in interpretation.
</p>


<h3>Value</h3>

<p>A vector of effect sizes (<code class="reqn">d_{Mod_{Signed}}</code>,
<code class="reqn">d_{Mod_{Unsigned}}</code>, <code class="reqn">d_{Mod_{Under}}</code>,
<code class="reqn">d_{Mod_{Over}}</code>), proportions of under- and over-predicted criterion scores,
minimum and maximum differences (i.e., <code class="reqn">d_{Mod_{Under}}</code> and <code class="reqn">d_{Mod_{Over}}</code>),
and the scores associated with minimum and maximum differences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate some hypothetical data for a referent group and three focal groups:
set.seed(10)
refDat &lt;- MASS::mvrnorm(n = 1000, mu = c(.5, .2),
                        Sigma = matrix(c(1, .5, .5, 1), 2, 2), empirical = TRUE)
foc1Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(-.5, -.2),
                         Sigma = matrix(c(1, .5, .5, 1), 2, 2), empirical = TRUE)
foc2Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(0, 0),
                         Sigma = matrix(c(1, .3, .3, 1), 2, 2), empirical = TRUE)
foc3Dat &lt;- MASS::mvrnorm(n = 1000, mu = c(-.5, -.2),
                         Sigma = matrix(c(1, .3, .3, 1), 2, 2), empirical = TRUE)
colnames(refDat) &lt;- colnames(foc1Dat) &lt;- colnames(foc2Dat) &lt;- colnames(foc3Dat) &lt;- c("X", "Y")

# Compute a regression model for each group:
refRegMod &lt;- lm(Y ~ X, data.frame(refDat))$coef
foc1RegMod &lt;- lm(Y ~ X, data.frame(foc1Dat))$coef
foc2RegMod &lt;- lm(Y ~ X, data.frame(foc2Dat))$coef
foc3RegMod &lt;- lm(Y ~ X, data.frame(foc3Dat))$coef

# Use the subgroup regression models to compute d_mod for each referent-focal pairing:

# Focal group #1:
compute_dmod_npar(referent_int = refRegMod[1], referent_slope = refRegMod[2],
             focal_int = foc1RegMod[1], focal_slope = foc1RegMod[2],
             focal_x = foc1Dat[,"X"], referent_sd_y = 1)

# Focal group #2:
compute_dmod_npar(referent_int = refRegMod[1], referent_slope = refRegMod[2],
             focal_int = foc2RegMod[1], focal_slope = foc1RegMod[2],
             focal_x = foc2Dat[,"X"], referent_sd_y = 1)

# Focal group #3:
compute_dmod_npar(referent_int = refRegMod[1], referent_slope = refRegMod[2],
             focal_int = foc3RegMod[1], focal_slope = foc3RegMod[2],
             focal_x = foc3Dat[,"X"], referent_sd_y = 1)
</code></pre>

<hr>
<h2 id='compute_dmod_par'>Function for computing parametric <code class="reqn">d_{Mod}</code> effect sizes for any number of focal groups</h2><span id='topic+compute_dmod_par'></span>

<h3>Description</h3>

<p>This function computes <code class="reqn">d_{Mod}</code> effect sizes from user-defined descriptive statistics
and regression coefficients. If one has access to a raw data set, the <code>dMod</code> function may be used
as a wrapper to this function so that the regression equations and descriptive statistics can
be computed automatically within the program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_dmod_par(
  referent_int,
  referent_slope,
  focal_int,
  focal_slope,
  focal_mean_x,
  focal_sd_x,
  referent_sd_y,
  focal_min_x,
  focal_max_x,
  focal_names = NULL,
  rescale_cdf = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_dmod_par_+3A_referent_int">referent_int</code></td>
<td>
<p>Referent group's intercept.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_referent_slope">referent_slope</code></td>
<td>
<p>Referent group's slope.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_int">focal_int</code></td>
<td>
<p>Focal groups' intercepts.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_slope">focal_slope</code></td>
<td>
<p>Focal groups' slopes.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_mean_x">focal_mean_x</code></td>
<td>
<p>Focal groups' predictor-score means.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_sd_x">focal_sd_x</code></td>
<td>
<p>Focal groups' predictor-score standard deviations.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_referent_sd_y">referent_sd_y</code></td>
<td>
<p>Referent group's criterion standard deviation.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_min_x">focal_min_x</code></td>
<td>
<p>Focal groups' minimum predictor scores.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_max_x">focal_max_x</code></td>
<td>
<p>Focal groups' maximum predictor scores.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_focal_names">focal_names</code></td>
<td>
<p>Focal-group names. If <code>NULL</code> (the default), the focal groups will be given numeric labels ranging from 1 through the number of groups.</p>
</td></tr>
<tr><td><code id="compute_dmod_par_+3A_rescale_cdf">rescale_cdf</code></td>
<td>
<p>Logical argument that indicates whether parametric <code class="reqn">d_{Mod}</code> results
should be rescaled to account for using a cumulative density &lt; 1 in the computations (<code>TRUE</code>; default) or not (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">d_{Mod_{Signed}}</code> effect size (i.e., the average of differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Signed}}=\frac{1}{SD_{Y_{1}}}\intop f_{2}(X)\left[X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right] dX,</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">SD_{Y_{1}}</code> is the referent group's criterion standard deviation;
</p>
</li>
<li> <p><code class="reqn">f_{2}(X)</code> is the normal-density function for the distribution of focal-group predictor scores;
</p>
</li>
<li> <p><code class="reqn">b_{1_{1}}</code> and <code class="reqn">b_{1_{0}}</code> are the slopes of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively;
</p>
</li>
<li> <p><code class="reqn">b_{0_{1}}</code> and <code class="reqn">b_{0_{0}}</code> are the intercepts of the regression of <code class="reqn">Y</code> on <code class="reqn">X</code> for the referent and focal groups, respectively; and
</p>
</li>
<li> <p>the integral spans all <code class="reqn">X</code> scores within the operational range of predictor scores for the focal group.
</p>
</li></ul>

<p>The <code class="reqn">d_{Mod_{Under}}</code> and <code class="reqn">d_{Mod_{Over}}</code> effect sizes are computed
using the same equation as <code class="reqn">d_{Mod_{Signed}}</code>, but <code class="reqn">d_{Mod_{Under}}</code> is
the weighted average of all scores in the area of underprediction (i.e., the differences in prediction with
negative signs) and <code class="reqn">d_{Mod_{Over}}</code> is the weighted average of all scores in the area of
overprediction (i.e., the differences in prediction with negative signs).
</p>
<p>The <code class="reqn">d_{Mod_{Unsigned}}</code> effect size (i.e., the average of absolute differences in prediction over
the range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Mod_{Unsigned}}=\frac{1}{SD_{Y_{1}}}\intop f_{2}(X)\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|dX.</code>
</p>

<p>The <code class="reqn">d_{Min}</code> effect size (i.e., the smallest absolute difference in prediction observed over the
range of predictor scores) is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Min}=\frac{1}{SD_{Y_{1}}}Min\left[\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|\right].</code>
</p>

<p>The <code class="reqn">d_{Max}</code> effect size (i.e., the largest absolute difference in prediction observed over the
range of predictor scores)is computed as
</p>
<p style="text-align: center;"><code class="reqn">d_{Max}=\frac{1}{SD_{Y_{1}}}Max\left[\left|X\left(b_{1_{1}}-b_{1_{2}}\right)+b_{0_{1}}-b_{0_{2}}\right|\right].</code>
</p>

<p><em>Note</em>: When <code class="reqn">d_{Min}</code> and <code class="reqn">d_{Max}</code> are computed in this package, the output will display the
signs of the differences (rather than the absolute values of the differences) to aid in interpretation.
</p>
<p>If <code class="reqn">d_{Mod}</code> effect sizes are to be rescaled to compensate for a cumulative density less than 1 (see the <code>rescale_cdf</code> argument), the result of each
effect size involving integration will be divided by the ratio of the cumulative density of the observed range of scores (i.e., the range bounded by the <code>focal_min_x</code>
and <code>focal_max_x</code> arguments) to the cumulative density of scores bounded by <code>-Inf</code> and <code>Inf</code>.
</p>


<h3>Value</h3>

<p>A matrix of effect sizes (<code class="reqn">d_{Mod_{Signed}}</code>,
<code class="reqn">d_{Mod_{Unsigned}}</code>, <code class="reqn">d_{Mod_{Under}}</code>,
<code class="reqn">d_{Mod_{Over}}</code>), proportions of under- and over-predicted criterion scores,
minimum and maximum differences (i.e., <code class="reqn">d_{Mod_{Under}}</code> and <code class="reqn">d_{Mod_{Over}}</code>),
and the scores associated with minimum and maximum differences.
Note that if the regression lines are parallel and infinite <code>focal_min_x</code> and <code>focal_max_x</code> values were
specified, the extrema will be defined using the scores 3 focal-group SDs above and below the corresponding focal-group means.
</p>


<h3>References</h3>

<p>Nye, C. D., &amp; Sackett, P. R. (2017).
New effect sizes for tests of categorical moderation and differential prediction.
<em>Organizational Research Methods, 20</em>(4), 639–664. doi: <a href="https://doi.org/10.1177/1094428116644505">10.1177/1094428116644505</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>compute_dmod_par(referent_int = -.05, referent_slope = .5,
                 focal_int = c(.05, 0, -.05), focal_slope = c(.5, .3, .3),
                 focal_mean_x = c(-.5, 0, -.5), focal_sd_x = rep(1, 3),
                 referent_sd_y = 1,
                 focal_min_x = rep(-Inf, 3), focal_max_x = rep(Inf, 3),
                 focal_names = NULL, rescale_cdf = TRUE)
</code></pre>

<hr>
<h2 id='conf.limits.nc.chisq'>Confidence limits for noncentral chi square parameters (function and documentation from package 'MBESS' version 4.4.3)
Function to determine the noncentral parameter that leads to the observed <code>Chi.Square</code>-value,
so that a confidence interval for the population noncentral chi-squrae value can be formed.</h2><span id='topic+conf.limits.nc.chisq'></span>

<h3>Description</h3>

<p>Confidence limits for noncentral chi square parameters (function and documentation from package 'MBESS' version 4.4.3)
Function to determine the noncentral parameter that leads to the observed <code>Chi.Square</code>-value,
so that a confidence interval for the population noncentral chi-squrae value can be formed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf.limits.nc.chisq(
  Chi.Square = NULL,
  conf.level = 0.95,
  df = NULL,
  alpha.lower = NULL,
  alpha.upper = NULL,
  tol = 1e-09,
  Jumping.Prop = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf.limits.nc.chisq_+3A_chi.square">Chi.Square</code></td>
<td>
<p>the observed chi-square value</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_conf.level">conf.level</code></td>
<td>
<p>the desired degree of confidence for the interval</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_df">df</code></td>
<td>
<p>the degrees of freedom</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_alpha.lower">alpha.lower</code></td>
<td>
<p>Type I error for the lower confidence limit</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_alpha.upper">alpha.upper</code></td>
<td>
<p>Type I error for the upper confidence limit</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_tol">tol</code></td>
<td>
<p>tolerance for iterative convergence</p>
</td></tr>
<tr><td><code id="conf.limits.nc.chisq_+3A_jumping.prop">Jumping.Prop</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentral parameters necessary for confidence interval construction using noncentral chi square-distributions (<code>0 &lt; Jumping.Prop &lt; 1</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the function fails (or if a function relying upon this function fails), adjust the <code>Jumping.Prop</code> (to a smaller value).
</p>


<h3>Value</h3>


<ul>
<li><p>Lower.LimitValue of the distribution with <code>Lower.Limit</code> noncentral value that has at its specified quantile <code>Chi.Square</code>
</p>
</li>
<li><p>Prob.Less.LowerProportion of cases falling below <code>Lower.Limit</code>
</p>
</li>
<li><p>Upper.LimitValue of the distribution with <code>Upper.Limit</code> noncentral value that has at its specified quantile <code>Chi.Square</code>
</p>
</li>
<li><p>Prob.Greater.UpperProportion of cases falling above <code>Upper.Limit</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ken Kelley (University of Notre Dame; <a href="mailto:KKelley@ND.edu">KKelley@ND.edu</a>), Keke Lai (University of California&ndash;Merced)
</p>

<hr>
<h2 id='confidence'>Construct a confidence interval</h2><span id='topic+confidence'></span>

<h3>Description</h3>

<p>Function to construct a confidence interval around an effect size or mean effect size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confidence(
  mean,
  se = NULL,
  df = NULL,
  conf_level = 0.95,
  conf_method = c("t", "norm"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confidence_+3A_mean">mean</code></td>
<td>
<p>Mean effect size (if used in a meta-analysis) or observed effect size (if used on individual statistics).</p>
</td></tr>
<tr><td><code id="confidence_+3A_se">se</code></td>
<td>
<p>Standard error of the statistic.</p>
</td></tr>
<tr><td><code id="confidence_+3A_df">df</code></td>
<td>
<p>Degrees of freedom of the statistic (necessary if using the <em>t</em> distribution).</p>
</td></tr>
<tr><td><code id="confidence_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level that defines the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id="confidence_+3A_conf_method">conf_method</code></td>
<td>
<p>Distribution to be used to compute the width of confidence intervals. Available options are &quot;t&quot; for <em>t</em> distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
<tr><td><code id="confidence_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">CI=mean_{es}\pm quantile\times SE_{es}</code>
</p>



<h3>Value</h3>

<p>A matrix of confidence intervals of the specified width.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>confidence(mean = c(.3, .5), se = c(.15, .2), df = c(100, 200), conf_level = .95, conf_method = "t")
confidence(mean = c(.3, .5), se = c(.15, .2), conf_level = .95, conf_method = "norm")
</code></pre>

<hr>
<h2 id='confidence_r'>Construct a confidence interval for correlations using Fisher's z transformation</h2><span id='topic+confidence_r'></span>

<h3>Description</h3>

<p>Construct a confidence interval for correlations using Fisher's z transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confidence_r(r, n, conf_level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confidence_r_+3A_r">r</code></td>
<td>
<p>A vector of correlations</p>
</td></tr>
<tr><td><code id="confidence_r_+3A_n">n</code></td>
<td>
<p>A vector of sample sizes</p>
</td></tr>
<tr><td><code id="confidence_r_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level that defines the width of the confidence interval (default = .95).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A confidence interval of the specified width (or matrix of confidence intervals)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>confidence_r(r = .3, n = 200, conf_level = .95)
</code></pre>

<hr>
<h2 id='confint'>Confidence interval method for objects of classes deriving from &quot;lm_mat&quot;</h2><span id='topic+confint'></span>

<h3>Description</h3>

<p>Confidence interval method for objects of classes deriving from &quot;lm_mat.&quot;
Returns lower and upper bounds of confidence intervals for regression coefficients.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint_+3A_object">object</code></td>
<td>
<p>Matrix regression object.</p>
</td></tr>
<tr><td><code id="confint_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="confint_+3A_level">level</code></td>
<td>
<p>Confidence level</p>
</td></tr>
<tr><td><code id="confint_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='control_intercor'>Control function to curate intercorrelations to be used in automatic compositing routine</h2><span id='topic+control_intercor'></span>

<h3>Description</h3>

<p>Control function to curate intercorrelations to be used in automatic compositing routine
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_intercor(
  rxyi = NULL,
  n = NULL,
  sample_id = NULL,
  construct_x = NULL,
  construct_y = NULL,
  construct_names = NULL,
  facet_x = NULL,
  facet_y = NULL,
  intercor_vec = NULL,
  intercor_scalar = 0.5,
  dx = NULL,
  dy = NULL,
  p = 0.5,
  partial_intercor = FALSE,
  data = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_intercor_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector or column name of observed correlations.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_n">n</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_sample_id">sample_id</code></td>
<td>
<p>Vector of identification labels for samples/studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_construct_x">construct_x</code>, <code id="control_intercor_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for constructs designated as &quot;X&quot; or &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_construct_names">construct_names</code></td>
<td>
<p>Vector of all construct names to be included in the meta-analysis.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_facet_x">facet_x</code>, <code id="control_intercor_+3A_facet_y">facet_y</code></td>
<td>
<p>Vector of facet names for constructs designated as &quot;X&quot; or &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_intercor_vec">intercor_vec</code></td>
<td>
<p>Named vector of pre-specified intercorrelations among measures of constructs in the meta-analysis.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_intercor_scalar">intercor_scalar</code></td>
<td>
<p>Generic scalar intercorrelation that can stand in for unobserved or unspecified values.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_dx">dx</code>, <code id="control_intercor_+3A_dy">dy</code></td>
<td>
<p><em>d</em> values corresponding to <code>construct_x</code> and <code>construct_y</code>. These values only need to be supplied for cases in which <code>rxyi</code> represents a correlation between two measures of the same construct.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_p">p</code></td>
<td>
<p>Scalar or vector containing the proportions of group membership corresponding to the <em>d</em> values.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_partial_intercor">partial_intercor</code></td>
<td>
<p>For meta-analyses of <em>d</em> values only: Logical scalar, vector, or column corresponding to values in <code>rxyi</code> that determines whether the correlations are to be treated as within-group correlations (i.e., partial correlation controlling for group membership; <code>TRUE</code>) or not (<code>FALSE</code>; default).
Note that this only converts correlation values from the <code>rxyi</code> argument - any values provided in the <code>intercor_vec</code> or <code>intercor_scalar</code> arguments must be total correlations or converted to total correlations using the <code>mix_r_2group()</code> function prior to running <code>control_intercor</code>.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments.</p>
</td></tr>
<tr><td><code id="control_intercor_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of intercorrelations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create a dataset in which constructs correlate with themselves
rxyi &lt;- seq(.1, .5, length.out = 27)
construct_x &lt;- rep(rep(c("X", "Y", "Z"), 3), 3)
construct_y &lt;- c(rep("X", 9), rep("Y", 9), rep("Z", 9))
dat &lt;- data.frame(rxyi = rxyi, 
                  construct_x = construct_x, 
                  construct_y = construct_y, 
                  stringsAsFactors = FALSE)
dat &lt;- rbind(cbind(sample_id = "Sample 1", dat), 
             cbind(sample_id = "Sample 2", dat), 
             cbind(sample_id = "Sample 3", dat))

## Identify some constructs for which intercorrelations are not 
## represented in the data object:
construct_names = c("U", "V", "W")

## Specify some externally determined intercorrelations among measures:
intercor_vec &lt;- c(W = .4, X = .1)

## Specify a generic scalar intercorrelation that can stand in for missing values:
intercor_scalar &lt;- .5

control_intercor(rxyi = rxyi, sample_id = sample_id, 
                 construct_x = construct_x, construct_y = construct_y, 
                 construct_names = construct_names, 
                 intercor_vec = intercor_vec, intercor_scalar = intercor_scalar, data = dat)
</code></pre>

<hr>
<h2 id='control_psychmeta'>Control for <span class="pkg">psychmeta</span> meta-analyses</h2><span id='topic+control_psychmeta'></span>

<h3>Description</h3>

<p>Control for <span class="pkg">psychmeta</span> meta-analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_psychmeta(
  error_type = c("mean", "sample"),
  conf_level = 0.95,
  cred_level = 0.8,
  conf_method = c("t", "norm"),
  cred_method = c("t", "norm"),
  var_unbiased = TRUE,
  pairwise_ads = FALSE,
  moderated_ads = FALSE,
  residual_ads = TRUE,
  check_dependence = TRUE,
  collapse_method = c("composite", "average", "stop"),
  intercor = control_intercor(),
  clean_artifacts = TRUE,
  impute_artifacts = TRUE,
  impute_method = c("bootstrap_mod", "bootstrap_full", "simulate_mod", "simulate_full",
    "wt_mean_mod", "wt_mean_full", "unwt_mean_mod", "unwt_mean_full", "replace_unity",
    "stop"),
  seed = 42,
  use_all_arts = TRUE,
  estimate_pa = FALSE,
  decimals = 2,
  hs_override = FALSE,
  zero_substitute = .Machine$double.eps,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_psychmeta_+3A_error_type">error_type</code></td>
<td>
<p>Method to be used to estimate error variances: &quot;mean&quot; uses the mean effect size to estimate error variances and &quot;sample&quot; uses the sample-specific effect sizes.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_cred_level">cred_level</code></td>
<td>
<p>Credibility level to define the width of the credibility interval (default = .80).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_conf_method">conf_method</code></td>
<td>
<p>Distribution to be used to compute the width of confidence intervals. Available options are &quot;t&quot; for <em>t</em> distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_cred_method">cred_method</code></td>
<td>
<p>Distribution to be used to compute the width of credibility intervals. Available options are &quot;t&quot; for <em>t</em> distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Logical scalar determining whether variances should be unbiased (<code>TRUE</code>) or maximum-likelihood (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_pairwise_ads">pairwise_ads</code></td>
<td>
<p>Logical value that determines whether to compute artifact distributions in a construct-pair-wise fashion (<code>TRUE</code>) or separately by construct (<code>FALSE</code>, default).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_moderated_ads">moderated_ads</code></td>
<td>
<p>Logical value that determines whether to compute artifact distributions separately for each moderator combination (<code>TRUE</code>) or for overall analyses only (<code>FALSE</code>, default).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_residual_ads">residual_ads</code></td>
<td>
<p>Logical argument that determines whether to use residualized variances (<code>TRUE</code>) or observed variances (<code>FALSE</code>) of artifact distributions to estimate <code>sd_rho</code>.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_check_dependence">check_dependence</code></td>
<td>
<p>Logical scalar that determines whether database should be checked for violations of independence (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_collapse_method">collapse_method</code></td>
<td>
<p>Character argument that determines how to collapse dependent studies. Options are &quot;composite&quot; (default), &quot;average,&quot; and &quot;stop.&quot;</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_intercor">intercor</code></td>
<td>
<p>The intercorrelation(s) among variables to be combined into a composite. Can be a scalar, a named vector with element named according to the names of constructs, or output from the <code>control_intercor</code> function. Default scalar value is .5.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_clean_artifacts">clean_artifacts</code></td>
<td>
<p>If <code>TRUE</code>, multiple instances of the same construct (or construct-measure pair, if measure is provided) in the database are compared and reconciled with each other
in the case that any of the matching entries within a study have different artifact values. When impute_method is anything other than &quot;stop&quot;, this method is always implemented to prevent discrepancies among imputed values.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_impute_artifacts">impute_artifacts</code></td>
<td>
<p>If <code>TRUE</code>, artifact imputation will be performed (see <code>impute_method</code> for imputation procedures). Default is <code>FALSE</code> for artifact-distribution meta-analyses and <code>TRUE</code> otherwise.
When imputation is performed, <code>clean_artifacts</code> is treated as <code>TRUE</code> so as to resolve all discrepancies among artifact entries before and after imputation.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_impute_method">impute_method</code></td>
<td>
<p>Method to use for imputing artifacts. Choices are:
</p>

<ul>
<li><p>bootstrap_mod<br /> Select random values from the most specific moderator categories available (default).
</p>
</li>
<li><p>bootstrap_full<br /> Select random values from the full vector of artifacts.
</p>
</li>
<li><p>simulate_mod<br /> Generate random values from the distribution with the mean and variance of observed artifacts from the most specific moderator categories available.
(uses <code>rnorm</code> for u ratios and <code>rbeta</code> for reliability values).
</p>
</li>
<li><p>simulate_full<br /> Generate random values from the distribution with the mean and variance of all observed artifacts (uses <code>rnorm</code> for u ratios and <code>rbeta</code> for reliability values).
</p>
</li>
<li><p>wt_mean_mod<br /> Replace missing values with the sample-size weighted mean of the distribution of artifacts from the most specific moderator categories available (not recommended).
</p>
</li>
<li><p>wt_mean_full<br /> Replace missing values with the sample-size weighted mean of the full distribution of artifacts (not recommended).
</p>
</li>
<li><p>unwt_mean_mod<br /> Replace missing values with the unweighted mean of the distribution of artifacts from the most specific moderator categories available (not recommended).
</p>
</li>
<li><p>unwt_mean_full<br /> Replace missing values with the unweighted mean of the full distribution of artifacts (not recommended).
</p>
</li>
<li><p>replace_unity<br /> Replace missing values with 1 (not recommended).
</p>
</li>
<li><p>stop<br /> Stop evaluations when missing artifacts are encountered.
</p>
</li></ul>

<p>If an imputation method ending in &quot;mod&quot; is selected but no moderators are provided, the &quot;mod&quot; suffix will internally be replaced with &quot;full&quot;.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_seed">seed</code></td>
<td>
<p>Seed value to use for imputing artifacts in a reproducible way. Default value is 42.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_use_all_arts">use_all_arts</code></td>
<td>
<p>Logical scalar that determines whether artifact values from studies without valid effect sizes should be used in artifact distributions (<code>TRUE</code>; default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_estimate_pa">estimate_pa</code></td>
<td>
<p>Logical scalar that determines whether the unrestricted subgroup proportions associated with univariate-range-restricted effect sizes should be estimated by rescaling the range-restricted subgroup proportions as a function of the range-restriction correction (<code>TRUE</code>) or not (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimal places to which interactive artifact distributions should be rounded (default is 2 decimal places).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_hs_override">hs_override</code></td>
<td>
<p>When <code>TRUE</code>, this will override settings for <code>wt_type</code> (will set to &quot;sample_size&quot;), 
<code>error_type</code> (will set to &quot;mean&quot;),
<code>correct_bias</code> (will set to <code>TRUE</code>), 
<code>conf_method</code> (will set to &quot;norm&quot;),
<code>cred_method</code> (will set to &quot;norm&quot;), 
<code>var_unbiased</code> (will set to <code>FALSE</code>), 
<code>residual_ads</code> (will be set to <code>FALSE</code>),
and <code>use_all_arts</code> (will set to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_zero_substitute">zero_substitute</code></td>
<td>
<p>Value to be used as a functionally equivalent substitute for exactly zero effect sizes in individual-correction meta-analyses to facilitate the estimation of corrected error variances. By default, this is set to <code>.Machine$double.eps</code>.</p>
</td></tr>
<tr><td><code id="control_psychmeta_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of control arguments in the package environment.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control_psychmeta()
</code></pre>

<hr>
<h2 id='convert_consistency2reltype'>Convert logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate to a string variable of generic reliability types</h2><span id='topic+convert_consistency2reltype'></span>

<h3>Description</h3>

<p>Convert logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate to a string variable of generic reliability types
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_consistency2reltype(consistency)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_consistency2reltype_+3A_consistency">consistency</code></td>
<td>
<p>Logical internal-consistency indicators.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String variable of generic reliability types.
</p>

<hr>
<h2 id='convert_es'>Convert effect sizes</h2><span id='topic+convert_es'></span>

<h3>Description</h3>

<p>This function converts a variety of effect sizes to correlations, Cohen's <code class="reqn">d</code> values, or common language effect sizes, and calculates sampling error variances and effective sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_es(
  es,
  input_es = c("r", "d", "delta", "g", "t", "p.t", "F", "p.F", "chisq", "p.chisq",
    "or", "lor", "Fisherz", "A", "auc", "cles"),
  output_es = c("r", "d", "A", "auc", "cles"),
  n1 = NULL,
  n2 = NULL,
  df1 = NULL,
  df2 = NULL,
  sd1 = NULL,
  sd2 = NULL,
  tails = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_es_+3A_es">es</code></td>
<td>
<p>Vector of effect sizes to convert.</p>
</td></tr>
<tr><td><code id="convert_es_+3A_input_es">input_es</code></td>
<td>
<p>Scalar. Metric of input effect sizes. Currently supports correlations, Cohen's <code class="reqn">d</code>, independent samples <code class="reqn">t</code> values (or their <code class="reqn">p</code> values), two-group one-way ANOVA <code class="reqn">F</code> values (or their <code class="reqn">p</code> values), 1-df <code class="reqn">\chi^{2}</code> values (or their <code class="reqn">p</code> values), odds ratios, log odds ratios, Fisher <em>z</em>, and the common language effect size (CLES, A, AUC).</p>
</td></tr>
<tr><td><code id="convert_es_+3A_output_es">output_es</code></td>
<td>
<p>Scalar. Metric of output effect sizes. Currently supports correlations, Cohen's <code class="reqn">d</code> values, and common language effect sizes (CLES, A, AUC).</p>
</td></tr>
<tr><td><code id="convert_es_+3A_n1">n1</code></td>
<td>
<p>Vector of total sample sizes or sample sizes of group 1 of the two groups being contrasted.</p>
</td></tr>
<tr><td><code id="convert_es_+3A_n2">n2</code></td>
<td>
<p>Vector of sample sizes of group 2 of the two groups being contrasted.</p>
</td></tr>
<tr><td><code id="convert_es_+3A_df1">df1</code></td>
<td>
<p>Vector of input test statistic degrees of freedom (for <code class="reqn">t</code> and <code class="reqn">\chi^{2}</code>) or between-groups degree of freedom (for <code class="reqn">F</code>).</p>
</td></tr>
<tr><td><code id="convert_es_+3A_df2">df2</code></td>
<td>
<p>Vector of input test statistic within-group degrees of freedom (for <code class="reqn">F</code>).</p>
</td></tr>
<tr><td><code id="convert_es_+3A_sd1">sd1</code></td>
<td>
<p>Vector of pooled (within-group) standard deviations or standard deviations of group 1 of the two groups being contrasted.</p>
</td></tr>
<tr><td><code id="convert_es_+3A_sd2">sd2</code></td>
<td>
<p>Vector of standard deviations of group 2 of the two groups being contrasted.</p>
</td></tr>
<tr><td><code id="convert_es_+3A_tails">tails</code></td>
<td>
<p>Vector of the tails for <code class="reqn">p</code> values when <code>input_es = "p.t"</code>. Can be <code>2</code> (default) or <code>1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of class <code>es</code> with variables:
</p>
<table>
<tr><td><code>r</code>, <code>d</code>, <code>A</code></td>
<td>
<p>The converted effect sizes</p>
</td></tr>
<tr><td><code>n_effective</code></td>
<td>
<p>The effective total sample size</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The total number of cases (original sample size)</p>
</td></tr>
<tr><td><code>n1</code>, <code>n2</code></td>
<td>
<p>If applicable, subgroup sample sizes</p>
</td></tr>
<tr><td><code>var_e</code></td>
<td>
<p>The estimated sampling error variance</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chinn, S. (2000).
A simple method for converting an odds ratio to effect size for use in meta-analysis.
<em>Statistics in Medicine, 19</em>(22), 3127–3131.
doi: <a href="https://doi.org/10.1002/1097-0258(20001130)19:22%3C3127::AID-SIM784%3E3.0.CO;2-M">10.1002/1097-0258(20001130)19:22&lt;3127::AID-SIM784&gt;3.0.CO;2-M</a>
</p>
<p>Lipsey, M. W., &amp; Wilson, D. B. (2001). <em>Practical meta-analysis</em>. Sage.
</p>
<p>Ruscio, J. (2008).
A probability-based measure of effect size: Robustness to base rates and other factors.
<em>Psychological Methods, 13</em>(1), 19–30. doi: <a href="https://doi.org/10.1037/1082-989X.13.1.19">10.1037/1082-989X.13.1.19</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>convert_es(es = 1,  input_es="d", output_es="r", n1=100)
convert_es(es = 1, input_es="d", output_es="r", n1=50, n2 = 50)
convert_es(es = .2, input_es="r", output_es="d",  n1=100, n2=150)
convert_es(es = -1.3, input_es="t", output_es="r", n1 = 100, n2 = 140)
convert_es(es = 10.3, input_es="F", output_es="d", n1 = 100, n2 = 150)
convert_es(es = 1.3, input_es="chisq", output_es="r", n1 = 100, n2 = 100)
convert_es(es = .021, input_es="p.chisq", output_es="d", n1 = 100, n2 = 100)
convert_es(es = 4.37, input_es="or", output_es="r", n1=100, n2=100)
convert_es(es = 4.37, input_es="or", output_es="d", n1=100, n2=100)
convert_es(es = 1.47, input_es="lor", output_es="r", n1=100, n2=100)
convert_es(es = 1.47, input_es="lor", output_es="d", n1=100, n2=100)
</code></pre>

<hr>
<h2 id='convert_ma'>Function to convert meta-analysis of correlations to d values or vice-versa</h2><span id='topic+convert_ma'></span><span id='topic+convert_meta'></span>

<h3>Description</h3>

<p>Takes a meta-analysis class object of <em>d</em> values or correlations (classes <code>r_as_r</code>, <code>d_as_d</code>, <code>r_as_d</code>, and <code>d_as_r</code>; second-order meta-analyses are currently not supported) as an input and uses conversion formulas and Taylor series approximations to convert effect sizes and variance estimates, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_ma(ma_obj, ...)

convert_meta(ma_obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_ma_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A meta-analysis object of class <code>r_as_r</code>, <code>d_as_d</code>, <code>r_as_d</code>, or <code>d_as_r</code></p>
</td></tr>
<tr><td><code id="convert_ma_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula used to convert correlations to <em>d</em> values is:
</p>
<p style="text-align: center;"><code class="reqn">d=\frac{r\sqrt{\frac{1}{p\left(1-p\right)}}}{\sqrt{1-r^{2}}}</code>
</p>

<p>The formula used to convert <em>d</em> values to correlations is:
</p>
<p style="text-align: center;"><code class="reqn">r=\frac{d}{\sqrt{d^{2}+\frac{1}{p\left(1-p\right)}}}</code>
</p>

<p>To approximate the variance of correlations from the variance of <em>d</em> values, the function computes:
</p>
<p style="text-align: center;"><code class="reqn">var_{r}\approx a_{d}^{2}var_{d}</code>
</p>

<p>where <code class="reqn">a_{d}</code> is the first partial derivative of the <em>d</em>-to-<em>r</em> transformation with respect to <em>d</em>:
</p>
<p style="text-align: center;"><code class="reqn">a_{d}=-\frac{1}{\left[d^{2}p\left(1-p\right)-1\right]\sqrt{d^{2}+\frac{1}{p-p^{2}}}}</code>
</p>

<p>To approximate the variance of <em>d</em> values from the variance of correlations, the function computes:
</p>
<p style="text-align: center;"><code class="reqn">var_{d}\approx a_{r}^{2}var_{r}</code>
</p>

<p>where <code class="reqn">a_{r}</code> is the first partial derivative of the <em>r</em>-to-<em>d</em> transformation with respect to <em>r</em>:
</p>
<p style="text-align: center;"><code class="reqn">a_{r}=\frac{\sqrt{\frac{1}{p-p^{2}}}}{\left(1-r^{2}\right)^{1.5}}</code>
</p>



<h3>Value</h3>

<p>A meta-analysis converted to the <em>d</em> value metric (if ma_obj was a meta-analysis in the correlation metric) or converted to the correlation metric (if ma_obj was a meta-analysis in the <em>d</em> value metric).
</p>

<hr>
<h2 id='convert_pq_to_p'>Convert the dichotomous variable variance to a proportion</h2><span id='topic+convert_pq_to_p'></span>

<h3>Description</h3>

<p>Converts the variance of a dichotomous variable (i.e., <code class="reqn">pq</code>) to the proportion of one of the categories in the variable (i.e., <code class="reqn">p</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_pq_to_p(pq)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_pq_to_p_+3A_pq">pq</code></td>
<td>
<p>The variance of a dichotomous variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The proportion of cases in one of the dichotomous groups.
</p>

<hr>
<h2 id='convert_reltype2consistency'>Convert string variable containing reliability type indicators to a logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate</h2><span id='topic+convert_reltype2consistency'></span>

<h3>Description</h3>

<p>Convert string variable containing reliability type indicators to a logical variable indicating whether a reliability value is an internal-consistency estimate or a multiple-administration estimate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_reltype2consistency(rel_type, arg_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_reltype2consistency_+3A_rel_type">rel_type</code></td>
<td>
<p>String vector containing reliability type indicators.</p>
</td></tr>
<tr><td><code id="convert_reltype2consistency_+3A_arg_name">arg_name</code></td>
<td>
<p>Optional name of the arg_name object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical internal-consistency indicators.
</p>

<hr>
<h2 id='convert_sdd_to_sdr'>Convert the SD of d to the SD of r via TSA</h2><span id='topic+convert_sdd_to_sdr'></span>

<h3>Description</h3>

<p>Convert the SD of d to the SD of r via TSA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_sdd_to_sdr(d, sd, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_sdd_to_sdr_+3A_d">d</code></td>
<td>
<p>Standardized mean difference in the d-value metric.</p>
</td></tr>
<tr><td><code id="convert_sdd_to_sdr_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of the d value.</p>
</td></tr>
<tr><td><code id="convert_sdd_to_sdr_+3A_p">p</code></td>
<td>
<p>Proportion of the dichotomous variable involved in the d value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximated standard deviation in the correlation metric.
</p>

<hr>
<h2 id='convert_sdr_to_sdd'>Convert the SD of r to the SD of d via TSA</h2><span id='topic+convert_sdr_to_sdd'></span>

<h3>Description</h3>

<p>Convert the SD of r to the SD of d via TSA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_sdr_to_sdd(r, sd, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_sdr_to_sdd_+3A_r">r</code></td>
<td>
<p>Correlation coefficient.</p>
</td></tr>
<tr><td><code id="convert_sdr_to_sdd_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of the correlation.</p>
</td></tr>
<tr><td><code id="convert_sdr_to_sdd_+3A_p">p</code></td>
<td>
<p>Proportion of the dichotomous variable involved in the correlation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximated standard deviation in the d value metric.
</p>

<hr>
<h2 id='convert_vard_to_varr'>Convert the variance of d to the variance of r via TSA</h2><span id='topic+convert_vard_to_varr'></span>

<h3>Description</h3>

<p>Convert the variance of d to the variance of r via TSA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_vard_to_varr(d, var, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_vard_to_varr_+3A_d">d</code></td>
<td>
<p>Standardized mean difference in the d-value metric.</p>
</td></tr>
<tr><td><code id="convert_vard_to_varr_+3A_var">var</code></td>
<td>
<p>Variance of the d value.</p>
</td></tr>
<tr><td><code id="convert_vard_to_varr_+3A_p">p</code></td>
<td>
<p>Proportion of the dichotomous variable involved in the d value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximated variance in the correlation metric.
</p>

<hr>
<h2 id='convert_varr_to_vard'>Convert the variance of r to the variance of d via TSA</h2><span id='topic+convert_varr_to_vard'></span>

<h3>Description</h3>

<p>Convert the variance of r to the variance of d via TSA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_varr_to_vard(r, var, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_varr_to_vard_+3A_r">r</code></td>
<td>
<p>Correlation coefficient.</p>
</td></tr>
<tr><td><code id="convert_varr_to_vard_+3A_var">var</code></td>
<td>
<p>Variance of the correlation.</p>
</td></tr>
<tr><td><code id="convert_varr_to_vard_+3A_p">p</code></td>
<td>
<p>Proportion of the dichotomous variable involved in the correlation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximated variance in the d value metric.
</p>

<hr>
<h2 id='correct_d'>Correct <code class="reqn">d</code> values for measurement error and/or range restriction</h2><span id='topic+correct_d'></span>

<h3>Description</h3>

<p>This function is a wrapper for the <code><a href="#topic+correct_r">correct_r()</a></code> function to correct <code class="reqn">d</code> values
for statistical and psychometric artifacts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_d(
  correction = c("meas", "uvdrr_g", "uvdrr_y", "uvirr_g", "uvirr_y", "bvdrr", "bvirr"),
  d,
  ryy = 1,
  uy = 1,
  rGg = 1,
  pi = NULL,
  pa = NULL,
  uy_observed = TRUE,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NA,
  sign_rgz = 1,
  sign_ryz = 1,
  n1 = NULL,
  n2 = NA,
  conf_level = 0.95,
  correct_bias = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_d_+3A_correction">correction</code></td>
<td>
<p>Type of correction to be applied. Options are &quot;meas&quot;, &quot;uvdrr_g&quot;, &quot;uvdrr_y&quot;, &quot;uvirr_g&quot;, &quot;uvirr_y&quot;, &quot;bvdrr&quot;, &quot;bvirr&quot;</p>
</td></tr>
<tr><td><code id="correct_d_+3A_d">d</code></td>
<td>
<p>Vector of <code class="reqn">d</code> values.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_ryy">ryy</code></td>
<td>
<p>Vector of reliability coefficients for Y (the continuous variable).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_uy">uy</code></td>
<td>
<p>Vector of u ratios for Y (the continuous variable).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_rgg">rGg</code></td>
<td>
<p>Vector of reliabilities for the group variable (i.e., the correlations between observed group membership and latent group membership).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_pi">pi</code></td>
<td>
<p>Proportion of cases in one of the groups in the observed data (not necessary if <code>n1</code> and <code>n2</code> reflect this proportionality).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_pa">pa</code></td>
<td>
<p>Proportion of cases in one of the groups in the population.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding uy value is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_ryy_restricted">ryy_restricted</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding rxx value is an incumbent reliability (<code>TRUE</code>) or an applicant reliability. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r()</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_sign_rgz">sign_rgz</code></td>
<td>
<p>Vector of signs of the relationships between grouping variables and the selection mechanism.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Vector of signs of the relationships between Y variables and the selection mechanism.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_n1">n1</code></td>
<td>
<p>Optional vector of sample sizes associated with group 1 (or the total sample size, if <code>n2</code> is <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_n2">n2</code></td>
<td>
<p>Optional vector of sample sizes associated with group 2.</p>
</td></tr>
<tr><td><code id="correct_d_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id="correct_d_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in correlations (<code>TRUE</code>) or not (<code>FALSE</code>).
For sporadic corrections (e.g., in mixed artifact-distribution meta-analyses), this should be set to <code>FALSE</code> (the default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame(s) of observed <code class="reqn">d</code> values (<code>dgyi</code>), operational range-restricted <code class="reqn">d</code> values corrected for measurement error in Y only (<code>dgpi</code>), operational range-restricted <code class="reqn">d</code> values corrected for measurement error in the grouping only (<code>dGyi</code>), and range-restricted true-score <code class="reqn">d</code> values (<code>dGpi</code>),
range-corrected observed-score <code class="reqn">d</code> values (<code>dgya</code>), operational range-corrected <code class="reqn">d</code> values corrected for measurement error in Y only (<code>dgpa</code>), operational range-corrected <code class="reqn">d</code> values corrected for measurement error in the grouping only (<code>dGya</code>), and range-corrected true-score <code class="reqn">d</code> values (<code>dGpa</code>).
</p>


<h3>References</h3>

<p>Alexander, R. A., Carson, K. P., Alliger, G. M., &amp; Carr, L. (1987).
Correcting doubly truncated correlations: An improved approximation for
correcting the bivariate normal correlation when truncation has occurred on
both variables. <em>Educational and Psychological Measurement, 47</em>(2), 309–315.
doi: <a href="https://doi.org/10.1177/0013164487472002">10.1177/0013164487472002</a>
</p>
<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006). Implications of direct and
indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612.
doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>
<p>Le, H., Oh, I.-S., Schmidt, F. L., &amp; Wooldridge, C. D. (2016).
Correction for range restriction in meta-analysis revisited:
Improvements and implications for organizational research.
<em>Personnel Psychology, 69</em>(4), 975–1008. doi: <a href="https://doi.org/10.1111/peps.12122">10.1111/peps.12122</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 43–44, 140–141.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Correction for measurement error only
correct_d(correction = "meas", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "meas", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for direct range restriction in the continuous variable
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for direct range restriction in the grouping variable
correct_d(correction = "uvdrr_g", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "uvdrr_g", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for indirect range restriction in the continuous variable
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for indirect range restriction in the grouping variable
correct_d(correction = "uvirr_g", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "uvirr_g", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for indirect range restriction in the continuous variable
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "uvdrr_y", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for direct range restriction in both variables
correct_d(correction = "bvdrr", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "bvdrr", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)

## Correction for indirect range restriction in both variables
correct_d(correction = "bvirr", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = .7, pa = .5)
correct_d(correction = "bvirr", d = .5, ryy = .8, uy = .7,
          rGg = .9, pi = NULL, pa = .5, n1 = 100, n2 = 200)
</code></pre>

<hr>
<h2 id='correct_d_bias'>Correct for small-sample bias in Cohen's <code class="reqn">d</code> values</h2><span id='topic+correct_d_bias'></span>

<h3>Description</h3>

<p>Corrects a vector of Cohen's <code class="reqn">d</code> values for small-sample bias, as Cohen's <code class="reqn">d</code>
has a slight positive bias. The bias-corrected <code class="reqn">d</code> value is often called
Hedges's <code class="reqn">g</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_d_bias(d, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_d_bias_+3A_d">d</code></td>
<td>
<p>Vector of Cohen's d values.</p>
</td></tr>
<tr><td><code id="correct_d_bias_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bias correction is:
</p>
<p style="text-align: center;"><code class="reqn">g = d_{c} = d_{obs} \times J</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">J = \frac{\Gamma(\frac{n - 2}{2})}{\sqrt{\frac{n - 2}{2}} \times \Gamma(\frac{n - 3}{2})}</code>
</p>

<p>and <code class="reqn">d_{obs}</code> is the observed effect size, <code class="reqn">g = d_{c}</code> is the
corrected (unbiased) estimate, <code class="reqn">n</code> is the total sample size, and
<code class="reqn">\Gamma()</code> is the <a href="base.html#topic+Special">gamma function</a>.
</p>
<p>Historically, using the gamma function was computationally intensive, so an
approximation for <code class="reqn">J</code> was used (Borenstein et al., 2009):
</p>
<p style="text-align: center;"><code class="reqn">J = 1 - 3 / (4 * (n - 2) - 1)</code>
</p>

<p>This approximation is no longer necessary with modern computers.
</p>


<h3>Value</h3>

<p>Vector of g values (d values corrected for small-sample bias).
</p>


<h3>References</h3>

<p>Hedges, L. V., &amp; Olkin, I. (1985).
<em>Statistical methods for meta-analysis</em>.
Academic Press. p. 104
</p>
<p>Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. R. (2009).
<em>Introduction to meta-analysis</em>.
Wiley. p. 27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_d_bias(d = .3, n = 30)
correct_d_bias(d = .3, n = 300)
correct_d_bias(d = .3, n = 3000)
</code></pre>

<hr>
<h2 id='correct_glass_bias'>Correct for small-sample bias in Glass' <code class="reqn">\Delta</code> values</h2><span id='topic+correct_glass_bias'></span>

<h3>Description</h3>

<p>Correct for small-sample bias in Glass' <code class="reqn">\Delta</code> values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_glass_bias(delta, nc, ne, use_pooled_sd = rep(FALSE, length(delta)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_glass_bias_+3A_delta">delta</code></td>
<td>
<p>Vector of Glass' <code class="reqn">\Delta</code> values.</p>
</td></tr>
<tr><td><code id="correct_glass_bias_+3A_nc">nc</code></td>
<td>
<p>Vector of control-group sample sizes.</p>
</td></tr>
<tr><td><code id="correct_glass_bias_+3A_ne">ne</code></td>
<td>
<p>Vector of experimental-group sample sizes.</p>
</td></tr>
<tr><td><code id="correct_glass_bias_+3A_use_pooled_sd">use_pooled_sd</code></td>
<td>
<p>Logical vector determining whether the pooled standard deviation was used (<code>TRUE</code>) or not (<code>FALSE</code>; default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bias correction is estimated as:
</p>
<p style="text-align: center;"><code class="reqn">\Delta_{c}=\Delta_{obs}\frac{\Gamma\left(\frac{n_{control}-1}{2}\right)}{\Gamma\left(\frac{n_{control}-1}{2}\right)\Gamma\left(\frac{n_{control}-2}{2}\right)}</code>
</p>

<p>where <code class="reqn">\Delta</code> is the observed effect size, <code class="reqn">\Delta_{c}</code> is the
corrected estimate of <code class="reqn">\Delta</code>,
<code class="reqn">n_{control}</code> is the control-group sample size, and
<code class="reqn">\Gamma()</code> is the <a href="base.html#topic+Special">gamma function</a>.
</p>


<h3>Value</h3>

<p>Vector of d values corrected for small-sample bias.
</p>


<h3>References</h3>

<p>Hedges, L. V. (1981). Distribution theory for Glass’s estimator of effect
size and related estimators. <em>Journal of Educational Statistics, 6</em>(2),
107–128. doi: <a href="https://doi.org/10.2307/1164588">10.2307/1164588</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_glass_bias(delta = .3, nc = 30, ne = 30)
</code></pre>

<hr>
<h2 id='correct_matrix_mvrr'>Multivariate select/correction for covariance matrices</h2><span id='topic+correct_matrix_mvrr'></span>

<h3>Description</h3>

<p>Correct (or select upon) a covariance matrix using the Pearson-Aitken-Lawley multivariate selection theorem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_matrix_mvrr(
  Sigma_i,
  Sigma_xx_a,
  x_col,
  y_col = NULL,
  standardize = FALSE,
  var_names = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_matrix_mvrr_+3A_sigma_i">Sigma_i</code></td>
<td>
<p>The complete range-restricted (unrestricted) covariance matrix to be corrected (selected upon).</p>
</td></tr>
<tr><td><code id="correct_matrix_mvrr_+3A_sigma_xx_a">Sigma_xx_a</code></td>
<td>
<p>The matrix of unrestricted (range-restricted) covariances among of selection variables.</p>
</td></tr>
<tr><td><code id="correct_matrix_mvrr_+3A_x_col">x_col</code></td>
<td>
<p>The row/column indices of the variables in <code>Sigma_i</code> that correspond, in order, to the variables in <code>Sigma_xx_a</code>.</p>
</td></tr>
<tr><td><code id="correct_matrix_mvrr_+3A_y_col">y_col</code></td>
<td>
<p>Optional: The variables in <code>Sigma_i</code> not listed in <code>x_col</code> that are to be manipuated by the multivariate range-restriction formula.</p>
</td></tr>
<tr><td><code id="correct_matrix_mvrr_+3A_standardize">standardize</code></td>
<td>
<p>Should the function's output matrix be returned in standardized form (<code>TRUE</code>) or in unstandardized form (<code>FALSE</code>; the default).</p>
</td></tr>
<tr><td><code id="correct_matrix_mvrr_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of names for the variables in <code>Sigma_i</code>, in order of appearance in the matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix that has been manipulated by the multivariate range-restriction formula.
</p>


<h3>References</h3>

<p>Aitken, A. C. (1934). Note on selection from a multivariate normal population.
<em>Proceedings of the Edinburgh Mathematical Society (Series 2), 4</em>(2), 106–110.
</p>
<p>Lawley, D. N. (1943). A note on Karl Pearson’s selection formulae.
<em>Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences, 62</em>(1), 28–30.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma_i &lt;- reshape_vec2mat(cov = .2, var = .8, order = 4)
Sigma_xx_a &lt;- reshape_vec2mat(cov = .5, order = 2)
correct_matrix_mvrr(Sigma_i = Sigma_i, Sigma_xx_a = Sigma_xx_a, x_col = 1:2, standardize = TRUE)
</code></pre>

<hr>
<h2 id='correct_means_mvrr'>Multivariate select/correction for vectors of means</h2><span id='topic+correct_means_mvrr'></span>

<h3>Description</h3>

<p>Correct (or select upon) a vector of means using principles from the Pearson-Aitken-Lawley multivariate selection theorem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_means_mvrr(
  Sigma,
  means_i = rep(0, ncol(Sigma)),
  means_x_a,
  x_col,
  y_col = NULL,
  var_names = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_means_mvrr_+3A_sigma">Sigma</code></td>
<td>
<p>The complete covariance matrix to be used to manipulate means: This matrix may be entirely unrestricted or entirely range restricted,
as the regression weights estimated from this matrix are expected to be invariant to the effects of selection.</p>
</td></tr>
<tr><td><code id="correct_means_mvrr_+3A_means_i">means_i</code></td>
<td>
<p>The complete range-restricted (unrestricted) vector of means to be corrected (selected upon).</p>
</td></tr>
<tr><td><code id="correct_means_mvrr_+3A_means_x_a">means_x_a</code></td>
<td>
<p>The vector of unrestricted (range-restricted) means of selection variables</p>
</td></tr>
<tr><td><code id="correct_means_mvrr_+3A_x_col">x_col</code></td>
<td>
<p>The row/column indices of the variables in <code>Sigma</code> that correspond, in order, to the variables in means_x_a</p>
</td></tr>
<tr><td><code id="correct_means_mvrr_+3A_y_col">y_col</code></td>
<td>
<p>Optional: The variables in <code>Sigma</code> not listed in <code>x_col</code> that are to be manipuated by the multivariate range-restriction formula.</p>
</td></tr>
<tr><td><code id="correct_means_mvrr_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of names for the variables in <code>Sigma</code>, in order of appearance in the matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of means that has been manipulated by the multivariate range-restriction formula.
</p>


<h3>References</h3>

<p>Aitken, A. C. (1934). Note on selection from a multivariate normal population.
<em>Proceedings of the Edinburgh Mathematical Society (Series 2), 4</em>(2), 106–110.
</p>
<p>Lawley, D. N. (1943). A note on Karl Pearson’s selection formulae.
<em>Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences, 62</em>(1), 28–30.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma &lt;- diag(.5, 4)
Sigma[lower.tri(Sigma)] &lt;- c(.2, .3, .4, .3, .4, .4)
Sigma &lt;- Sigma + t(Sigma)
diag(Sigma) &lt;- 1
correct_means_mvrr(Sigma = Sigma, means_i = c(.3, .3, .1, .1),
means_x_a = c(-.1, -.1), x_col = 1:2)
</code></pre>

<hr>
<h2 id='correct_r'>Correct correlations for range restriction and/or measurement error</h2><span id='topic+correct_r'></span>

<h3>Description</h3>

<p>Corrects Pearson correlations (<code class="reqn">r</code>) for range restriction and/or measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_r(
  correction = c("meas", "uvdrr_x", "uvdrr_y", "uvirr_x", "uvirr_y", "bvdrr", "bvirr"),
  rxyi,
  ux = 1,
  uy = 1,
  rxx = 1,
  ryy = 1,
  ux_observed = TRUE,
  uy_observed = TRUE,
  rxx_restricted = TRUE,
  rxx_type = "alpha",
  k_items_x = NA,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NA,
  sign_rxz = 1,
  sign_ryz = 1,
  n = NULL,
  conf_level = 0.95,
  correct_bias = FALSE,
  zero_substitute = .Machine$double.eps
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_r_+3A_correction">correction</code></td>
<td>
<p>Type of correction to be applied. Options are &quot;meas&quot;, &quot;uvdrr_x&quot;, &quot;uvdrr_y&quot;, &quot;uvirr_x&quot;, &quot;uvirr_y&quot;, &quot;bvdrr&quot;, &quot;bvirr&quot;</p>
</td></tr>
<tr><td><code id="correct_r_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.
<em>NOTE</em>: Beginning in <span class="pkg">psychmeta</span> version 2.5.2, <code>rxyi</code> values of exactly 0 in individual-correction meta-analyses are replaced with a functionally equivalent value via the <code>zero_substitute</code> argument to facilitate the estimation of effective sample sizes.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_ux">ux</code></td>
<td>
<p>Vector of u ratios for X.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_uy">uy</code></td>
<td>
<p>Vector of u ratios for Y.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_rxx">rxx</code></td>
<td>
<p>Vector of reliability coefficients for X.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_ryy">ryy</code></td>
<td>
<p>Vector of reliability coefficients for Y.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding ux value is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding uy value is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding rxx value is an incumbent reliability (<code>TRUE</code>) or an applicant reliability. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_rxx_type">rxx_type</code>, <code id="correct_r_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_k_items_x">k_items_x</code>, <code id="correct_r_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_ryy_restricted">ryy_restricted</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding rxx value is an incumbent reliability (<code>TRUE</code>) or an applicant reliability. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Vector of signs of the relationships between X variables and the selection mechanism.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Vector of signs of the relationships between Y variables and the selection mechanism.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_n">n</code></td>
<td>
<p>Optional vector of sample sizes associated with the rxyi correlations.</p>
</td></tr>
<tr><td><code id="correct_r_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval (default = .95).</p>
</td></tr>
<tr><td><code id="correct_r_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in correlations (<code>TRUE</code>) or not (<code>FALSE</code>).
For sporadic corrections (e.g., in mixed artifact-distribution meta-analyses), this should be set to <code>FALSE</code>, the default).</p>
</td></tr>
<tr><td><code id="correct_r_+3A_zero_substitute">zero_substitute</code></td>
<td>
<p>Value to be used as a functionally equivalent substitute for exactly zero effect sizes to facilitate the estimation of effective sample sizes. By default, this is set to <code>.Machine$double.eps</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The correction for measurement error is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP}=\frac{\rho_{XY}}{\sqrt{\rho_{XX}\rho_{YY}}}</code>
</p>

<p>The correction for univariate direct range restriction is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP_{a}}=\left[\frac{\rho_{XY_{i}}}{u_{X}\sqrt{\rho_{YY_{i}}}\sqrt{\left(\frac{1}{u_{X}^{2}}-1\right)\frac{\rho_{XY_{i}}^{2}}{\rho_{YY_{i}}}+1}}\right]/\sqrt{\rho_{XX_{a}}}</code>
</p>

<p>The correction for univariate indirect range restriction is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP_{a}}=\frac{\rho_{XY_{i}}}{u_{T}\sqrt{\rho_{XX_{i}}\rho_{YY_{i}}}\sqrt{\left(\frac{1}{u_{T}^{2}}-1\right)\frac{\rho_{XY_{i}}^{2}}{\rho_{XX_{i}}\rho_{YY_{i}}}+1}}</code>
</p>

<p>The correction for bivariate direct range restriction is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP_{a}}=\frac{\frac{\rho_{XY_{i}}^{2}-1}{2\rho_{XY_{i}}}u_{X}u_{Y}+\mathrm{sign}\left(\rho_{XY_{i}}\right)\sqrt{\frac{\left(1-\rho_{XY_{i}}^{2}\right)^{2}}{4\rho_{XY_{i}}}u_{X}^{2}u_{Y}^{2}+1}}{\sqrt{\rho_{XX_{a}}\rho_{YY_{a}}}}</code>
</p>

<p>The correction for bivariate indirect range restriction is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP_{a}}=\frac{\rho_{XY_{i}}u_{X}u_{Y}+\lambda\sqrt{\left|1-u_{X}^{2}\right|\left|1-u_{Y}^{2}\right|}}{\sqrt{\rho_{XX_{a}}\rho_{YY_{a}}}}</code>
</p>

<p>where the <code class="reqn">\lambda</code> value allows <code class="reqn">u_{X}</code> and <code class="reqn">u_{Y}</code> to fall on either side of unity so as to function as a two-stage correction for mixed patterns of range restriction and range enhancement. The <code class="reqn">\lambda</code> value is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\lambda=\mathrm{sign}\left[\rho_{ST_{a}}\rho_{SP_{a}}\left(1-u_{X}\right)\left(1-u_{Y}\right)\right]\frac{\mathrm{sign}\left(1-u_{X}\right)\min\left(u_{X},\frac{1}{u_{X}}\right)+\mathrm{sign}\left(1-u_{Y}\right)\min\left(u_{Y},\frac{1}{u_{Y}}\right)}{\min\left(u_{X},\frac{1}{u_{X}}\right)\min\left(u_{Y},\frac{1}{u_{Y}}\right)}</code>
</p>



<h3>Value</h3>

<p>Data frame(s) of observed correlations (<code>rxyi</code>), operational range-restricted correlations corrected for measurement error in Y only (<code>rxpi</code>), operational range-restricted correlations corrected for measurement error in X only (<code>rtyi</code>), and range-restricted true-score correlations (<code>rtpi</code>),
range-corrected observed-score correlations (<code>rxya</code>), operational range-corrected correlations corrected for measurement error in Y only (<code>rxpa</code>), operational range-corrected correlations corrected for measurement error in X only (<code>rtya</code>), and range-corrected true-score correlations (<code>rtpa</code>).
</p>


<h3>References</h3>

<p>Alexander, R. A., Carson, K. P., Alliger, G. M., &amp; Carr, L. (1987).
Correcting doubly truncated correlations: An improved approximation for correcting the bivariate normal correlation when truncation has occurred on both variables.
<em>Educational and Psychological Measurement, 47</em>(2), 309–315. doi: <a href="https://doi.org/10.1177/0013164487472002">10.1177/0013164487472002</a>
</p>
<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006).
Implications of direct and indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612. doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>
<p>Le, H., Oh, I.-S., Schmidt, F. L., &amp; Wooldridge, C. D. (2016).
Correction for range restriction in meta-analysis revisited: Improvements and implications for organizational research.
<em>Personnel Psychology, 69</em>(4), 975–1008. doi: <a href="https://doi.org/10.1111/peps.12122">10.1111/peps.12122</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 43-44, 140–141.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Correction for measurement error only
correct_r(correction = "meas", rxyi = .3, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE)
correct_r(correction = "meas", rxyi = .3, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE, n = 100)

## Correction for direct range restriction in X
correct_r(correction = "uvdrr_x", rxyi = .3, ux = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE)
correct_r(correction = "uvdrr_x", rxyi = .3, ux = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE, n = 100)

## Correction for indirect range restriction in X
correct_r(correction = "uvirr_x", rxyi = .3, ux = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE)
correct_r(correction = "uvirr_x", rxyi = .3, ux = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE, n = 100)

## Correction for direct range restriction in X and Y
correct_r(correction = "bvdrr", rxyi = .3, ux = .8, uy = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE)
correct_r(correction = "bvdrr", rxyi = .3, ux = .8, uy = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE, n = 100)

## Correction for indirect range restriction in X and Y
correct_r(correction = "bvirr", rxyi = .3, ux = .8, uy = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE)
correct_r(correction = "bvirr", rxyi = .3, ux = .8, uy = .8, rxx = .8, ryy = .8,
     ux_observed = TRUE, uy_observed = TRUE, rxx_restricted = TRUE, ryy_restricted = TRUE, n = 100)
</code></pre>

<hr>
<h2 id='correct_r_bias'>Correct correlations for small-sample bias</h2><span id='topic+correct_r_bias'></span>

<h3>Description</h3>

<p>Corrects Pearson correlations (<code class="reqn">r</code>) for small-sample bias
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_r_bias(r, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_r_bias_+3A_r">r</code></td>
<td>
<p>Vector of correlations.</p>
</td></tr>
<tr><td><code id="correct_r_bias_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">r_{c}=\frac{r_{obs}}{\left(\frac{2n-2}{2n-1}\right)}</code>
</p>



<h3>Value</h3>

<p>Vector of correlations corrected for small-sample bias.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 140–141.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_r_bias(r = .3, n = 30)
correct_r_bias(r = .3, n = 300)
correct_r_bias(r = .3, n = 3000)
</code></pre>

<hr>
<h2 id='correct_r_coarseness'>Correct correlations for scale coarseness</h2><span id='topic+correct_r_coarseness'></span>

<h3>Description</h3>

<p>Corrects correlations for scale coarseness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_r_coarseness(
  r,
  kx = NULL,
  ky = NULL,
  n = NULL,
  dist_x = "norm",
  dist_y = "norm",
  bin_value_x = c("median", "mean", "index"),
  bin_value_y = c("median", "mean", "index"),
  width_x = 3,
  width_y = 3,
  lbound_x = NULL,
  ubound_x = NULL,
  lbound_y = NULL,
  ubound_y = NULL,
  index_values_x = NULL,
  index_values_y = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_r_coarseness_+3A_r">r</code></td>
<td>
<p>Observed correlation.</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_kx">kx</code>, <code id="correct_r_coarseness_+3A_ky">ky</code></td>
<td>
<p>Number of scale points used to measure the x and y variables. Set to NULL to treat as continuously measured.</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_n">n</code></td>
<td>
<p>Optional sample size.</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_dist_x">dist_x</code>, <code id="correct_r_coarseness_+3A_dist_y">dist_y</code></td>
<td>
<p>Assumed latent distribution of the x and y variables.</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_bin_value_x">bin_value_x</code>, <code id="correct_r_coarseness_+3A_bin_value_y">bin_value_y</code></td>
<td>
<p>Are the scale points used to measure the of the x and y variables assumed to represent bin medians, means, or index values?</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_width_x">width_x</code>, <code id="correct_r_coarseness_+3A_width_y">width_y</code></td>
<td>
<p>For symmetrically distributed variables, how many standard deviations above/below the latent mean should be be used for the latent variable range to make the correction? (Note: Setting <code>width</code> &gt; 3 produces erratic results.) The latent variable range can alternatively be set using <code>lbound</code> and <code>ubound</code>.</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_lbound_x">lbound_x</code>, <code id="correct_r_coarseness_+3A_lbound_y">lbound_y</code></td>
<td>
<p>What lower bound of the range for the latent x and y variables should be used to make the correction? (Note: For normally distributed variables, setting <code>lbound</code> &lt; -3 produces erratic results.)</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_ubound_x">ubound_x</code>, <code id="correct_r_coarseness_+3A_ubound_y">ubound_y</code></td>
<td>
<p>What upper bound of the range for the latent x and y variables should be used to make the correction? (Note: For normally distributed variables, setting <code>ubound</code> &gt; 3 produces erratic results.)</p>
</td></tr>
<tr><td><code id="correct_r_coarseness_+3A_index_values_x">index_values_x</code>, <code id="correct_r_coarseness_+3A_index_values_y">index_values_y</code></td>
<td>
<p>Optional. If <code>bin_value</code> = &quot;index&quot;, the bin index values. If unspecified, values 1:k are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of correlations corrected for scale coarseness (if <code>n</code> is supplied, corrected error variance and adjusted sample size is also reported).
</p>


<h3>References</h3>

<p>Aguinis, H., Pierce, C. A., &amp; Culpepper, S. A. (2009).
Scale coarseness as a methodological artifact:
Correcting correlation coefficients attenuated from using coarse scales.
<em>Organizational Research Methods, 12</em>(4), 623–652. doi: <a href="https://doi.org/10.1177/1094428108318065">10.1177/1094428108318065</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 287-288.
</p>
<p>Peters, C. C., &amp; Van Voorhis, W. R. (1940).
<em>Statistical procedures and their mathematical bases</em>.
New York, NY: Mcgraw-Hill. doi: <a href="https://doi.org/10.1037/13596-000">10.1037/13596-000</a>. pp. 393–399.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_r_coarseness(r = .35, kx = 5, ky = 4, n = 100)
correct_r_coarseness(r = .35, kx = 5, n = 100)
correct_r_coarseness(r = .35, kx = 5, ky = 4, n = 100, dist_x="unif", dist_y="norm")
</code></pre>

<hr>
<h2 id='correct_r_dich'>Correct correlations for artificial dichotomization of one or both variables</h2><span id='topic+correct_r_dich'></span>

<h3>Description</h3>

<p>Correct correlations for artificial dichotomization of one or both variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_r_dich(r, px = NA, py = NA, n = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_r_dich_+3A_r">r</code></td>
<td>
<p>Vector of correlations attenuated by artificial dichomization.</p>
</td></tr>
<tr><td><code id="correct_r_dich_+3A_px">px</code></td>
<td>
<p>Vector of proportions of the distribution on either side of the split applied to X (set as NA if X is continuous).</p>
</td></tr>
<tr><td><code id="correct_r_dich_+3A_py">py</code></td>
<td>
<p>Vector of proportions of the distribution on either side of the split applied to Y (set as NA if Y is continuous).</p>
</td></tr>
<tr><td><code id="correct_r_dich_+3A_n">n</code></td>
<td>
<p>Optional vector of sample sizes.</p>
</td></tr>
<tr><td><code id="correct_r_dich_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">r_{c}=\frac{r_{obs}}{\left[\frac{\phi\left(p_{X}\right)}{p_{X}\left(1-p_{X}\right)}\right]\left[\frac{\phi\left(p_{Y}\right)}{p_{Y}\left(1-p_{Y}\right)}\right]}</code>
</p>



<h3>Value</h3>

<p>Vector of correlations corrected for artificial dichomization (if <code>n</code> is supplied, corrected error variance and adjusted sample size is also reported).
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 43–44.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_r_dich(r = 0.32, px = .5, py = .5, n = 100)
</code></pre>

<hr>
<h2 id='correct_r_split'>Correct correlations for uneven/unrepresentative splits</h2><span id='topic+correct_r_split'></span>

<h3>Description</h3>

<p>This correction is mathematically equivalent to correcting the correlation for direct range restriction in the split variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct_r_split(r, pi, pa = 0.5, n = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct_r_split_+3A_r">r</code></td>
<td>
<p>Vector of correlations affected by an uneven or unrepresentative split of a dichotomous variable.</p>
</td></tr>
<tr><td><code id="correct_r_split_+3A_pi">pi</code></td>
<td>
<p>Vector of proportions of incumbent/sample cases in one of the categories of the dichotomous variable.</p>
</td></tr>
<tr><td><code id="correct_r_split_+3A_pa">pa</code></td>
<td>
<p>Vector of proportions of applicant/population cases in one of the categories of the dichotomous variable.</p>
</td></tr>
<tr><td><code id="correct_r_split_+3A_n">n</code></td>
<td>
<p>Optional vector of sample sizes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">r_{c}=\frac{r_{obs}}{u\sqrt{\left(\frac{1}{u^{2}}-1\right)r_{obs}^{2}+1}}</code>
</p>

<p>where <code class="reqn">u=\sqrt{\frac{p_{i}(1-p_{i})}{p_{a}(1-p_{a})}}</code>, the ratio of the dichotomous variance in the sample (<code class="reqn">p_{i}</code> is the incumbent/sample proportion in one of the two groups) to the dichotomous variance in the population (<code class="reqn">p_{a}</code> is the applicant/population proportion in one of the two groups).
This correction is identical to the correction for univariate direct range restriction, applied to a dichotomous variable.
</p>


<h3>Value</h3>

<p>Vector of correlations corrected for unrepresentative splits (if <code>n</code> is supplied, corrected error variance and adjusted sample size is also reported).
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 287-288.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>correct_r_split(r = 0.3, pi = .9, pa = .5, n = 100)
</code></pre>

<hr>
<h2 id='create_ad'>Generate an artifact distribution object for use in artifact-distribution meta-analysis programs.</h2><span id='topic+create_ad'></span>

<h3>Description</h3>

<p>This function generates artifact-distribution objects containing either interactive or Taylor series artifact distributions.
Use this to create objects that can be supplied to the <code><a href="#topic+ma_r_ad">ma_r_ad</a></code> and <code><a href="#topic+ma_r_ad">ma_r_ad</a></code> functions to apply psychometric corrections to barebones meta-analysis objects via artifact distribution methods.
</p>
<p>Allows consolidation of observed and estimated artifact information by cross-correcting artifact distributions and forming weighted artifact summaries.
</p>
<p>For u ratios, error variances can be computed for independent samples (i.e., settings in which the unrestricted standard deviation comes from an external study) or
dependent samples (i.e., settings in which the range-restricted standard deviation comes from a sample that represents a subset of the applicant sample that provided the
unrestricted standard deviation). The former circumstance is presumed to be more common, so error variances are computed for independent samples by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ad(
  ad_type = c("tsa", "int"),
  rxxi = NULL,
  n_rxxi = NULL,
  wt_rxxi = n_rxxi,
  rxxi_type = rep("alpha", length(rxxi)),
  k_items_rxxi = rep(NA, length(rxxi)),
  rxxa = NULL,
  n_rxxa = NULL,
  wt_rxxa = n_rxxa,
  rxxa_type = rep("alpha", length(rxxa)),
  k_items_rxxa = rep(NA, length(rxxa)),
  ux = NULL,
  ni_ux = NULL,
  na_ux = NULL,
  wt_ux = ni_ux,
  dep_sds_ux_obs = rep(FALSE, length(ux)),
  ut = NULL,
  ni_ut = NULL,
  na_ut = NULL,
  wt_ut = ni_ut,
  dep_sds_ut_obs = rep(FALSE, length(ut)),
  mean_qxi = NULL,
  var_qxi = NULL,
  k_qxi = NULL,
  mean_n_qxi = NULL,
  qxi_dist_type = rep("alpha", length(mean_qxi)),
  mean_k_items_qxi = rep(NA, length(mean_qxi)),
  mean_rxxi = NULL,
  var_rxxi = NULL,
  k_rxxi = NULL,
  mean_n_rxxi = NULL,
  rxxi_dist_type = rep("alpha", length(mean_rxxi)),
  mean_k_items_rxxi = rep(NA, length(mean_rxxi)),
  mean_qxa = NULL,
  var_qxa = NULL,
  k_qxa = NULL,
  mean_n_qxa = NULL,
  qxa_dist_type = rep("alpha", length(mean_qxa)),
  mean_k_items_qxa = rep(NA, length(mean_qxa)),
  mean_rxxa = NULL,
  var_rxxa = NULL,
  k_rxxa = NULL,
  mean_n_rxxa = NULL,
  rxxa_dist_type = rep("alpha", length(mean_rxxa)),
  mean_k_items_rxxa = rep(NA, length(mean_rxxa)),
  mean_ux = NULL,
  var_ux = NULL,
  k_ux = NULL,
  mean_ni_ux = NULL,
  mean_na_ux = rep(NA, length(mean_ux)),
  dep_sds_ux_spec = rep(FALSE, length(mean_ux)),
  mean_ut = NULL,
  var_ut = NULL,
  k_ut = NULL,
  mean_ni_ut = NULL,
  mean_na_ut = rep(NA, length(mean_ut)),
  dep_sds_ut_spec = rep(FALSE, length(mean_ut)),
  estimate_rxxa = TRUE,
  estimate_rxxi = TRUE,
  estimate_ux = TRUE,
  estimate_ut = TRUE,
  var_unbiased = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ad_+3A_ad_type">ad_type</code></td>
<td>
<p>Type of artifact distribution to be computed: Either &quot;tsa&quot; for Taylor series approximation or &quot;int&quot; for interactive.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_rxxi">rxxi</code></td>
<td>
<p>Vector of incumbent reliability estimates.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_n_rxxi">n_rxxi</code></td>
<td>
<p>Vector of sample sizes associated with the elements of <code>rxxi</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_wt_rxxi">wt_rxxi</code></td>
<td>
<p>Vector of weights associated with the elements of <code>rxxi</code> (by default, sample sizes will be used as weights).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_rxxi_type">rxxi_type</code>, <code id="create_ad_+3A_rxxa_type">rxxa_type</code>, <code id="create_ad_+3A_qxi_dist_type">qxi_dist_type</code>, <code id="create_ad_+3A_rxxi_dist_type">rxxi_dist_type</code>, <code id="create_ad_+3A_qxa_dist_type">qxa_dist_type</code>, <code id="create_ad_+3A_rxxa_dist_type">rxxa_dist_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_items_rxxi">k_items_rxxi</code>, <code id="create_ad_+3A_mean_k_items_qxi">mean_k_items_qxi</code>, <code id="create_ad_+3A_mean_k_items_rxxi">mean_k_items_rxxi</code>, <code id="create_ad_+3A_k_items_rxxa">k_items_rxxa</code>, <code id="create_ad_+3A_mean_k_items_qxa">mean_k_items_qxa</code>, <code id="create_ad_+3A_mean_k_items_rxxa">mean_k_items_rxxa</code></td>
<td>
<p>Numeric vector of the number of items in each scale (or mean number of items, for pre-specified distributions).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_rxxa">rxxa</code></td>
<td>
<p>Vector of applicant reliability estimates.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_n_rxxa">n_rxxa</code></td>
<td>
<p>Vector of sample sizes associated with the elements of <code>rxxa</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_wt_rxxa">wt_rxxa</code></td>
<td>
<p>Vector of weights associated with the elements of <code>rxxa</code> (by default, sample sizes will be used as weights).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_ni_ux">ni_ux</code></td>
<td>
<p>Vector of incumbent sample sizes associated with the elements of <code>ux</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_na_ux">na_ux</code></td>
<td>
<p>Vector of applicant sample sizes that can be used in estimating the sampling error of supplied ux values. <code>NULL</code> by default.
Only used when ni_ux is not NULL. If supplied, must be either a scalar or the same length as <code>ni_ux</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_wt_ux">wt_ux</code></td>
<td>
<p>Vector of weights associated with the elements of <code>ux</code> (by default, sample sizes will be used as weights).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_dep_sds_ux_obs">dep_sds_ux_obs</code></td>
<td>
<p>Logical scalar or vector determining whether supplied ux values were computed using dependent samples (<code>TRUE</code>) or independent samples (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_ut">ut</code></td>
<td>
<p>Vector of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_ni_ut">ni_ut</code></td>
<td>
<p>Vector of incumbent sample sizes associated with the elements of <code>ut</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_na_ut">na_ut</code></td>
<td>
<p>Vector of applicant sample sizes that can be used in estimating the sampling error of supplied ut values. <code>NULL</code> by default.
Only used when ni_ut is not NULL. If supplied, must be either a scalar or the same length as <code>ni_ut</code>.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_wt_ut">wt_ut</code></td>
<td>
<p>Vector of weights associated with the elements of <code>ut</code> (by default, sample sizes will be used as weights).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_dep_sds_ut_obs">dep_sds_ut_obs</code></td>
<td>
<p>Logical scalar or vector determining whether supplied ut values were computed using dependent samples (<code>TRUE</code>) or independent samples (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_qxi">mean_qxi</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of incumbent square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_qxi">var_qxi</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of incumbent square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_qxi">k_qxi</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of incumbent square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_n_qxi">mean_n_qxi</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes of externally computed distributions of incumbent square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_rxxi">mean_rxxi</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of incumbent reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_rxxi">var_rxxi</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of incumbent reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_rxxi">k_rxxi</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of incumbent reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_n_rxxi">mean_n_rxxi</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes of externally computed distributions of incumbent reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_qxa">mean_qxa</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of applicant square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_qxa">var_qxa</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of applicant square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_qxa">k_qxa</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of applicant square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_n_qxa">mean_n_qxa</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes of externally computed distributions of applicant square-root reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_rxxa">mean_rxxa</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of applicant reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_rxxa">var_rxxa</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of applicant reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_rxxa">k_rxxa</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of applicant reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_n_rxxa">mean_n_rxxa</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes of externally computed distributions of applicant reliabilities.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_ux">mean_ux</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_ux">var_ux</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_ux">k_ux</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_ni_ux">mean_ni_ux</code></td>
<td>
<p>Vector that can be used to supply the mean incumbent sample sizes of externally computed distributions of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_na_ux">mean_na_ux</code></td>
<td>
<p>Vector or scalar that can be used to supply the mean applicant sample size(s) of externally computed distributions of observed-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_dep_sds_ux_spec">dep_sds_ux_spec</code></td>
<td>
<p>Logical scalar or vector determining whether externally computed ux distributions were computed using dependent samples (<code>TRUE</code>) or independent samples (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_ut">mean_ut</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_ut">var_ut</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_k_ut">k_ut</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_ni_ut">mean_ni_ut</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes for of externally computed distributions of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_mean_na_ut">mean_na_ut</code></td>
<td>
<p>Vector or scalar that can be used to supply the mean applicant sample size(s) of externally computed distributions of true-score u ratios.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_dep_sds_ut_spec">dep_sds_ut_spec</code></td>
<td>
<p>Logical scalar or vector determining whether externally computed ut distributions were computed using dependent samples (<code>TRUE</code>) or independent samples (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_estimate_rxxa">estimate_rxxa</code></td>
<td>
<p>Logical argument to estimate rxxa values from other artifacts (<code>TRUE</code>) or to only used supplied rxxa values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_estimate_rxxi">estimate_rxxi</code></td>
<td>
<p>Logical argument to estimate rxxi values from other artifacts (<code>TRUE</code>) or to only used supplied rxxi values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_estimate_ux">estimate_ux</code></td>
<td>
<p>Logical argument to estimate ux values from other artifacts (<code>TRUE</code>) or to only used supplied ux values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_estimate_ut">estimate_ut</code></td>
<td>
<p>Logical argument to estimate ut values from other artifacts (<code>TRUE</code>) or to only used supplied ut values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Logical scalar determining whether variance should be unbiased (<code>TRUE</code>) or maximum-likelihood (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_+3A_...">...</code></td>
<td>
<p>Further arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Artifact distribution object (matrix of artifact-distribution means and variances) for use artifact-distribution meta-analyses.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example computed using observed values only:
create_ad(ad_type = "tsa", rxxa = c(.9, .8), n_rxxa = c(50, 150),
              rxxi = c(.8, .7), n_rxxi = c(50, 150),
              ux = c(.9, .8), ni_ux = c(50, 150))

create_ad(ad_type = "int", rxxa = c(.9, .8), n_rxxa = c(50, 150),
              rxxi = c(.8, .7), n_rxxi = c(50, 150),
              ux = c(.9, .8), ni_ux = c(50, 150))

## Example computed using all possible input arguments (arbitrary values):
rxxa &lt;- rxxi &lt;- ux &lt;- ut &lt;- c(.7, .8)
n_rxxa &lt;- n_rxxi &lt;- ni_ux &lt;- ni_ut &lt;- c(50, 100)
na_ux &lt;- na_ut &lt;- c(200, 200)
mean_qxa &lt;- mean_qxi &lt;- mean_ux &lt;- mean_ut &lt;- mean_rxxi &lt;- mean_rxxa &lt;- c(.7, .8)
var_qxa &lt;- var_qxi &lt;- var_ux &lt;- var_ut &lt;- var_rxxi &lt;- var_rxxa &lt;- c(.1, .05)
k_qxa &lt;- k_qxi &lt;- k_ux &lt;- k_ut &lt;- k_rxxa &lt;- k_rxxi &lt;- 2
mean_n_qxa &lt;- mean_n_qxi &lt;- mean_ni_ux &lt;- mean_ni_ut &lt;- mean_n_rxxa &lt;- mean_n_rxxi &lt;- c(100, 100)
dep_sds_ux_obs &lt;- dep_sds_ux_spec &lt;- dep_sds_ut_obs &lt;- dep_sds_ut_spec &lt;- FALSE
mean_na_ux &lt;- mean_na_ut &lt;- c(200, 200)

wt_rxxa &lt;- n_rxxa
wt_rxxi &lt;- n_rxxi
wt_ux &lt;- ni_ux
wt_ut &lt;- ni_ut

estimate_rxxa &lt;- TRUE
estimate_rxxi &lt;- TRUE
estimate_ux &lt;- TRUE
estimate_ut &lt;- TRUE
var_unbiased &lt;- TRUE

create_ad(rxxa = rxxa, n_rxxa = n_rxxa, wt_rxxa = wt_rxxa,
              mean_qxa = mean_qxa, var_qxa = var_qxa,
              k_qxa = k_qxa, mean_n_qxa = mean_n_qxa,
              mean_rxxa = mean_rxxa, var_rxxa = var_rxxa,
              k_rxxa = k_rxxa, mean_n_rxxa = mean_n_rxxa,

              rxxi = rxxi, n_rxxi = n_rxxi, wt_rxxi = wt_rxxi,
              mean_qxi = mean_qxi, var_qxi = var_qxi,
              k_qxi = k_qxi, mean_n_qxi = mean_n_qxi,
              mean_rxxi = mean_rxxi, var_rxxi = var_rxxi,
              k_rxxi = k_rxxi, mean_n_rxxi = mean_n_rxxi,

              ux = ux, ni_ux = ni_ux, na_ux = na_ux, wt_ux = wt_ux,
              dep_sds_ux_obs = dep_sds_ux_obs,
              mean_ux = mean_ux, var_ux = var_ux, k_ux =
               k_ux, mean_ni_ux = mean_ni_ux,
              mean_na_ux = mean_na_ux, dep_sds_ux_spec = dep_sds_ux_spec,

              ut = ut, ni_ut = ni_ut, na_ut = na_ut, wt_ut = wt_ut,
              dep_sds_ut_obs = dep_sds_ut_obs,
              mean_ut = mean_ut, var_ut = var_ut,
              k_ut = k_ut, mean_ni_ut = mean_ni_ut,
              mean_na_ut = mean_na_ut, dep_sds_ut_spec = dep_sds_ut_spec,

              estimate_rxxa = estimate_rxxa, estimate_rxxi = estimate_rxxi,
              estimate_ux = estimate_ux, estimate_ut = estimate_ut, var_unbiased = var_unbiased)
</code></pre>

<hr>
<h2 id='create_ad_array'>Create an array of all possible combinations of artifact values from 2-4 artifact distributions</h2><span id='topic+create_ad_array'></span>

<h3>Description</h3>

<p>Creates a fully crossed multidimensional array of artifacts and weights (with 1 to 4 dimensions of artifact values) for use in interactive artifact-distribution meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ad_array(ad_list, name_vec = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ad_array_+3A_ad_list">ad_list</code></td>
<td>
<p>List of artifact distribution tables (i.e., objects produced by the create_ad function).</p>
</td></tr>
<tr><td><code id="create_ad_array_+3A_name_vec">name_vec</code></td>
<td>
<p>Optional vector of artifact names that correspond to the tables in ad_list; if NULL, artifact names are taken from the names of list objects in ad_list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of all possible combinations of artifact values with the weight associated with each combination of artifacts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create_ad_array(ad_list = list(.create_ad_int(art_vec = c(.8, .8, .9),
#                                         wt_vec = c(100, 200, 100), decimals = 2),
#                               .create_ad_int(art_vec = c(.8, .8, .9),
#                                         wt_vec = c(100, 200, 100), decimals = 2)),
#                name_vec = c("q_x", "q_y"))
#create_ad_array(ad_list = list(q_x = .create_ad_int(art_vec = c(.8, .8, .9),
#                                               wt_vec = c(100, 200, 100), decimals = 2),
#                               q_y = .create_ad_int(art_vec = c(.8, .8, .9),
#                                               wt_vec = c(100, 200, 100), decimals = 2)))

</code></pre>

<hr>
<h2 id='create_ad_group'>Generate an artifact distribution object for a dichotomous grouping variable.</h2><span id='topic+create_ad_group'></span>

<h3>Description</h3>

<p>This function generates artifact-distribution objects containing either interactive or Taylor series artifact distributions for dichotomous group-membership variables.
Use this to create objects that can be supplied to the <code>ma_r_ad</code> and <code>ma_d_ad</code> functions to apply psychometric corrections to barebones meta-analysis objects via artifact distribution methods.
</p>
<p>Allows consolidation of observed and estimated artifact information by cross-correcting artifact distributions and forming weighted artifact summaries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ad_group(
  ad_type = c("tsa", "int"),
  rGg = NULL,
  n_rGg = NULL,
  wt_rGg = n_rGg,
  pi = NULL,
  pa = NULL,
  n_pi = NULL,
  n_pa = NULL,
  wt_p = n_pi,
  mean_rGg = NULL,
  var_rGg = NULL,
  k_rGg = NULL,
  mean_n_rGg = NULL,
  var_unbiased = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ad_group_+3A_ad_type">ad_type</code></td>
<td>
<p>Type of artifact distribution to be computed: Either &quot;tsa&quot; for Taylor series approximation or &quot;int&quot; for interactive.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_rgg">rGg</code></td>
<td>
<p>Vector of incumbent reliability estimates.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_n_rgg">n_rGg</code></td>
<td>
<p>Vector of sample sizes associated with the elements of <code>rGg.</code></p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_wt_rgg">wt_rGg</code></td>
<td>
<p>Vector of weights associated with the elements of <code>rGg</code> (by default, sample sizes will be used as weights if provided).</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_pi">pi</code></td>
<td>
<p>Vector of incumbent/sample proportions of members in one of the two groups being compared (one or both of <code>pi</code>/<code>pa</code> can be vectors - if both are vectors, they must be of equal length).</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_pa">pa</code></td>
<td>
<p>Vector of applicant/population proportions of members in one of the two groups being compared (one or both of <code>pi</code>/<code>pa</code> can be vectors - if both are vectors, they must be of equal length).</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_n_pi">n_pi</code></td>
<td>
<p>Vector of sample sizes associated with the elements in <code>pi</code>.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_n_pa">n_pa</code></td>
<td>
<p>Vector of sample sizes associated with the elements in <code>pa</code>.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_wt_p">wt_p</code></td>
<td>
<p>Vector of weights associated with the collective element pairs in <code>pi</code> and pa.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_mean_rgg">mean_rGg</code></td>
<td>
<p>Vector that can be used to supply the means of externally computed distributions of correlations between observed and latent group membership.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_var_rgg">var_rGg</code></td>
<td>
<p>Vector that can be used to supply the variances of externally computed distributions of correlations between observed and latent group membership.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_k_rgg">k_rGg</code></td>
<td>
<p>Vector that can be used to supply the number of studies included in externally computed distributions of correlations between observed and latent group membership.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_mean_n_rgg">mean_n_rGg</code></td>
<td>
<p>Vector that can be used to supply the mean sample sizes of externally computed distributions of correlations between observed and latent group membership.</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Logical scalar determining whether variance should be unbiased (<code>TRUE</code>) or maximum-likelihood (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_group_+3A_...">...</code></td>
<td>
<p>Further arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Artifact distribution object (matrix of artifact-distribution means and variances) for use in artifact-distribution meta-analyses.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example artifact distribution for a dichotomous grouping variable:
create_ad_group(rGg = c(.8, .9, .95), n_rGg = c(100, 200, 250),
                mean_rGg = .9, var_rGg = .05,
                k_rGg = 5, mean_n_rGg = 100,
                pi = c(.6, .55, .3), pa = .5, n_pi = c(100, 200, 250), n_pa = c(300, 300, 300),
                var_unbiased = TRUE)
                
create_ad_group(ad_type = "int", rGg = c(.8, .9, .95), n_rGg = c(100, 200, 250),
                mean_rGg = .9, var_rGg = .05,
                k_rGg = 5, mean_n_rGg = 100,
                pi = c(.6, .55, .3), pa = .5, n_pi = c(100, 200, 250), n_pa = c(300, 300, 300),
                var_unbiased = TRUE)
</code></pre>

<hr>
<h2 id='create_ad_tibble'>Create a tibble of artifact distributions by construct</h2><span id='topic+create_ad_tibble'></span><span id='topic+create_ad_list'></span>

<h3>Description</h3>

<p>Create a tibble of artifact distributions by construct
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ad_tibble(
  ad_type = c("tsa", "int"),
  n = NULL,
  sample_id = NULL,
  construct_x = NULL,
  facet_x = NULL,
  measure_x = NULL,
  construct_y = NULL,
  facet_y = NULL,
  measure_y = NULL,
  rxx = NULL,
  rxx_restricted = TRUE,
  rxx_type = "alpha",
  k_items_x = NA,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NA,
  ux = NULL,
  ux_observed = TRUE,
  uy = NULL,
  uy_observed = TRUE,
  estimate_rxxa = TRUE,
  estimate_rxxi = TRUE,
  estimate_ux = TRUE,
  estimate_ut = TRUE,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  construct_order = NULL,
  supplemental_ads = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)

create_ad_list(
  ad_type = c("tsa", "int"),
  n = NULL,
  sample_id = NULL,
  construct_x = NULL,
  facet_x = NULL,
  measure_x = NULL,
  construct_y = NULL,
  facet_y = NULL,
  measure_y = NULL,
  rxx = NULL,
  rxx_restricted = TRUE,
  rxx_type = "alpha",
  k_items_x = NA,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NA,
  ux = NULL,
  ux_observed = TRUE,
  uy = NULL,
  uy_observed = TRUE,
  estimate_rxxa = TRUE,
  estimate_rxxi = TRUE,
  estimate_ux = TRUE,
  estimate_ut = TRUE,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  construct_order = NULL,
  supplemental_ads = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ad_tibble_+3A_ad_type">ad_type</code></td>
<td>
<p>Type of artifact distributions to be computed: Either &quot;tsa&quot; for Taylor series approximation or &quot;int&quot; for interactive.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_n">n</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_sample_id">sample_id</code></td>
<td>
<p>Optional vector of identification labels for samples/studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_construct_x">construct_x</code>, <code id="create_ad_tibble_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for constructs initially designated as &quot;X&quot; or &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_facet_x">facet_x</code>, <code id="create_ad_tibble_+3A_facet_y">facet_y</code></td>
<td>
<p>Vector of facet names for constructs initially designated as &quot;X&quot; or &quot;Y&quot;.
Facet names &quot;global&quot;, &quot;overall&quot;, and &quot;total&quot; are reserved to indicate observations that represent effect sizes that have already been composited or that represent construct-level measurements rather than facet-level measurements.
To avoid double-compositing, any observation with one of these reserved names will only be eligible for auto-compositing with other such observations and will not be combined with narrow facets.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_measure_x">measure_x</code>, <code id="create_ad_tibble_+3A_measure_y">measure_y</code></td>
<td>
<p>Vector of names for measures associated with constructs initially designated as &quot;X&quot; or &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_rxx">rxx</code></td>
<td>
<p>Vector or column name of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector or column name determining whether each element of rxx is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_rxx_type">rxx_type</code>, <code id="create_ad_tibble_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied. See documentation of <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable values.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_k_items_x">k_items_x</code>, <code id="create_ad_tibble_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_ryy">ryy</code></td>
<td>
<p>Vector or column name of reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_ryy_restricted">ryy_restricted</code></td>
<td>
<p>Logical vector or column name determining whether each element of ryy is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_ux">ux</code></td>
<td>
<p>Vector or column name of u ratios for X.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector or column name determining whether each element of ux is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_uy">uy</code></td>
<td>
<p>Vector or column name of u ratios for Y.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector or column name determining whether each element of uy is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_estimate_rxxa">estimate_rxxa</code></td>
<td>
<p>Logical argument to estimate rxxa values from other artifacts (<code>TRUE</code>) or to only used supplied rxxa values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_estimate_rxxi">estimate_rxxi</code></td>
<td>
<p>Logical argument to estimate rxxi values from other artifacts (<code>TRUE</code>) or to only used supplied rxxi values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_estimate_ux">estimate_ux</code></td>
<td>
<p>Logical argument to estimate ux values from other artifacts (<code>TRUE</code>) or to only used supplied ux values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_estimate_ut">estimate_ut</code></td>
<td>
<p>Logical argument to estimate ut values from other artifacts (<code>TRUE</code>) or to only used supplied ut values (<code>FALSE</code>). <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_moderators">moderators</code></td>
<td>
<p>Matrix or column names of moderator variables to be used in the meta-analysis (can be a vector in the case of one moderator).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Logical scalar or vector identifying whether variables in the <code>moderators</code> argument are categorical variables (<code>TRUE</code>) or continuous variables (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis: &quot;none&quot; means that no moderators are to be used, &quot;simple&quot; means that moderators are to be examined one at a time, and
&quot;hierarchical&quot; means that all possible combinations and subsets of moderators are to be examined.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which variables should be arranged, with variables listed earlier in the vector being preferred for designation as X.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_supplemental_ads">supplemental_ads</code></td>
<td>
<p>Named list (named according to the constructs included in the meta-analysis) of supplemental artifact distribution information from studies not included in the meta-analysis. This is a list of lists, where the elements of a list associated with a construct are named like the arguments of the <code>create_ad()</code> function.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="create_ad_tibble_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble of artifact distributions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Examples to create Taylor series artifact distributions:
# Overall artifact distributions (not pairwise, not moderated)
create_ad_tibble(ad_type = "tsa",
                 n = n, rxx = rxxi, ryy = ryyi,
                 construct_x = x_name, construct_y = y_name,
                 sample_id = sample_id, moderators = moderator,
                 data = data_r_meas_multi,
                 control = control_psychmeta(pairwise_ads = FALSE,
                                             moderated_ads = FALSE))

# Overall artifact distributions by moderator combination
create_ad_tibble(ad_type = "tsa",
                 n = n, rxx = rxxi, ryy = ryyi,
                 construct_x = x_name, construct_y = y_name,
                 sample_id = sample_id, moderators = moderator,
                 data = data_r_meas_multi,
                 control = control_psychmeta(pairwise_ads = FALSE,
                                             moderated_ads = TRUE))

# Pairwise artifact distributions (not moderated)
create_ad_tibble(ad_type = "tsa",
                 n = n, rxx = rxxi, ryy = ryyi,
                 construct_x = x_name, construct_y = y_name,
                 sample_id = sample_id, moderators = moderator,
                 data = data_r_meas_multi,
                 control = control_psychmeta(pairwise_ads = TRUE,
                                               moderated_ads = FALSE))

# Pairwise artifact distributions by moderator combination
create_ad_tibble(ad_type = "tsa",
                 n = n, rxx = rxxi, ryy = ryyi,
                 construct_x = x_name, construct_y = y_name,
                 sample_id = sample_id, moderators = moderator,
                 data = data_r_meas_multi,
                 control = control_psychmeta(pairwise_ads = TRUE,
                                             moderated_ads = TRUE))
</code></pre>

<hr>
<h2 id='credibility'>Construct a credibility interval</h2><span id='topic+credibility'></span>

<h3>Description</h3>

<p>Function to construct a credibility interval around a mean effect size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>credibility(mean, sd, k = NULL, cred_level = 0.8, cred_method = c("t", "norm"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="credibility_+3A_mean">mean</code></td>
<td>
<p>Mean effect size.</p>
</td></tr>
<tr><td><code id="credibility_+3A_sd">sd</code></td>
<td>
<p>Residual/true standard deviation of effect sizes, after accounting for variance from artifacts.</p>
</td></tr>
<tr><td><code id="credibility_+3A_k">k</code></td>
<td>
<p>Number of studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="credibility_+3A_cred_level">cred_level</code></td>
<td>
<p>Credibility level that defines the width of the credibility interval (default = .80).</p>
</td></tr>
<tr><td><code id="credibility_+3A_cred_method">cred_method</code></td>
<td>
<p>Distribution to be used to compute the width of credibility intervals. Available options are &quot;t&quot; for <em>t</em> distribution or &quot;norm&quot; for normal distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">CR=mean_{es}\pm quantile\times SD_{es}</code>
</p>



<h3>Value</h3>

<p>A matrix of credibility intervals of the specified width.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>credibility(mean = .3, sd = .15, cred_level = .8, cred_method = "norm")
credibility(mean = .3, sd = .15, cred_level = .8, k = 10)
credibility(mean = c(.3, .5), sd = c(.15, .2), cred_level = .8, k = 10)
</code></pre>

<hr>
<h2 id='data_d_bb_multi'>Hypothetical <em>d</em> value dataset simulated with sampling error only</h2><span id='topic+data_d_bb_multi'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses of multiple variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_d_bb_multi)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_d_bb_multi)
</code></pre>

<hr>
<h2 id='data_d_meas_multi'>Hypothetical <em>d</em> value dataset simulated to satisfy the assumptions of the correction for measurement error only in multiple constructs</h2><span id='topic+data_d_meas_multi'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses correcting for measurement error in multiple variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_d_meas_multi)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_d_meas_multi)
</code></pre>

<hr>
<h2 id='data_r_bvdrr'>Hypothetical dataset simulated to satisfy the assumptions of the bivariate correction for direct range restriction</h2><span id='topic+data_r_bvdrr'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses of bivariate direct range restriction.
Note that the BVDRR correction is only an approximation of the appropriate range-restriction correction and tends to have a noticeable positive bias when applied in meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_bvdrr)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_bvdrr)
</code></pre>

<hr>
<h2 id='data_r_bvirr'>Hypothetical dataset simulated to satisfy the assumptions of the bivariate correction for indirect range restriction</h2><span id='topic+data_r_bvirr'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses of bivariate indirect range restriction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_bvirr)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_bvirr)
</code></pre>

<hr>
<h2 id='data_r_gonzalezmule_2014'>Meta-analysis of OCB correlations with other constructs</h2><span id='topic+data_r_gonzalezmule_2014'></span>

<h3>Description</h3>

<p>Data set to demonstrate corrections for univariate range restriction and measurement error using
individual corrections or artifact distributions.
NOTE: This is an updated version of the data set reported in the Gonzalez-Mulé, Mount, an Oh (2014) article that was obtained from the first author.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_gonzalezmule_2014)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>Gonzalez-Mulé, E., Mount, M. K., &amp; Oh, I.-S. (2014).
A meta-analysis of the relationship between general mental ability and nontask performance.
<em>Journal of Applied Psychology, 99</em>(6), 1222–1243. doi: <a href="https://doi.org/10.1037/a0037547">10.1037/a0037547</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_gonzalezmule_2014)
</code></pre>

<hr>
<h2 id='data_r_mcdaniel_1994'>Artifact-distribution meta-analysis of the validity of interviews</h2><span id='topic+data_r_mcdaniel_1994'></span>

<h3>Description</h3>

<p>Data set to demonstrate corrections for univariate range restriction and criterion measurement error using artifact distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_mcdaniel_1994)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>McDaniel, M. A., Whetzel, D. L., Schmidt, F. L., &amp; Maurer, S. D. (1994).
The validity of employment interviews: A comprehensive review and meta-analysis.
<em>Journal of Applied Psychology, 79</em>(4), 599–616. doi: <a href="https://doi.org/10.1037/0021-9010.79.4.599">10.1037/0021-9010.79.4.599</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_mcdaniel_1994)
</code></pre>

<hr>
<h2 id='data_r_mcleod_2007'>Bare-bones meta-analysis of parenting and childhood depression</h2><span id='topic+data_r_mcleod_2007'></span>

<h3>Description</h3>

<p>Data set to demonstrate bare-bones meta-analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_mcleod_2007)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>McLeod, B. D., Weisz, J. R., &amp; Wood, J. J., (2007).
Examining the association between parenting and childhood depression:  A meta-analysis.
<em>Clinical Psychology Review, 27</em>(8), 986–1003. doi: <a href="https://doi.org/10.1016/j.cpr.2007.03.001">10.1016/j.cpr.2007.03.001</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_mcleod_2007)
</code></pre>

<hr>
<h2 id='data_r_meas'>Hypothetical dataset simulated to satisfy the assumptions of the correction for measurement error only</h2><span id='topic+data_r_meas'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses correcting for measurement error in two variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_meas)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_meas)
</code></pre>

<hr>
<h2 id='data_r_meas_multi'>Hypothetical correlation dataset simulated to satisfy the assumptions of the correction for measurement error only in multiple constructs</h2><span id='topic+data_r_meas_multi'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses correcting for measurement error in multiple variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_meas_multi)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_meas_multi)
</code></pre>

<hr>
<h2 id='data_r_oh_2009'>Second order meta-analysis of operational validities of big five personality measures across East Asian countries</h2><span id='topic+data_r_oh_2009'></span>

<h3>Description</h3>

<p>Example of a second-order meta-analysis of correlations corrected using artifact-distribution methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_oh_2009)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>Oh, I. -S. (2009).
<em>The Five-Factor Model of personality and job performance in East Asia: A cross-cultural validity generalization study</em>.
(Doctoral dissertation) Iowa City, IA: University of Iowa. <a href="https://www.proquest.com/dissertations/docview/304903943/">https://www.proquest.com/dissertations/docview/304903943/</a>
</p>
<p>Schmidt, F. L., &amp; Oh, I.-S. (2013).
Methods for second order meta-analysis and illustrative applications.
<em>Organizational Behavior and Human Decision Processes, 121</em>(2), 204–218. doi: <a href="https://doi.org/10.1016/j.obhdp.2013.03.002">10.1016/j.obhdp.2013.03.002</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_oh_2009)
</code></pre>

<hr>
<h2 id='data_r_roth_2015'>Artifact-distribution meta-analysis of the correlation between school grades and cognitive ability</h2><span id='topic+data_r_roth_2015'></span>

<h3>Description</h3>

<p>Data set to demonstrate corrections for univariate range restriction and cognitive ability measurement error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_roth_2015)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>Roth, B., Becker, N., Romeyke, S., Schäfer, S., Domnick, F., &amp; Spinath, F. M. (2015).
Intelligence and school grades: A meta-analysis.
<em>Intelligence, 53</em>, 118–137. doi: <a href="https://doi.org/10.1016/j.intell.2015.09.002">10.1016/j.intell.2015.09.002</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_roth_2015)
</code></pre>

<hr>
<h2 id='data_r_uvdrr'>Hypothetical dataset simulated to satisfy the assumptions of the univariate correction for direct range restriction</h2><span id='topic+data_r_uvdrr'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses correcting for univariate direct range restriction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_uvdrr)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_uvdrr)
</code></pre>

<hr>
<h2 id='data_r_uvirr'>Hypothetical dataset simulated to satisfy the assumptions of the univariate correction for indirect range restriction</h2><span id='topic+data_r_uvirr'></span>

<h3>Description</h3>

<p>Data set for use in example meta-analyses correcting for univariate indirect range restriction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data_r_uvirr)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_r_uvirr)
</code></pre>

<hr>
<h2 id='estimate_artifacts'>Estimation of applicant and incumbent reliabilities and of true- and observed-score u ratios</h2><span id='topic+estimate_artifacts'></span><span id='topic+estimate_rxxa'></span><span id='topic+estimate_rxxi'></span><span id='topic+estimate_ut'></span><span id='topic+estimate_ux'></span><span id='topic+estimate_ryya'></span><span id='topic+estimate_ryyi'></span><span id='topic+estimate_uy'></span><span id='topic+estimate_up'></span><span id='topic+estimate_rxxa_u'></span><span id='topic+estimate_rxxi_u'></span>

<h3>Description</h3>

<p>Functions to estimate the values of artifacts from other artifacts. These functions allow for reliability estimates to be corrected/attenuated for range restriction and allow
u ratios to be converted between observed-score and true-score metrics. Some functions also allow for the extrapolation of an artifact from other available information.
</p>
<p>Available functions include:
</p>

<ul>
<li><p>estimate_rxxa<br /> Estimate the applicant reliability of variable X from X's incumbent reliability value and X's observed-score or true-score u ratio.
</p>
</li>
<li><p>estimate_rxxa_u<br /> Estimate the applicant reliability of variable X from X's observed-score and true-score u ratios.
</p>
</li>
<li><p>estimate_rxxi<br /> Estimate the incumbent reliability of variable X from X's applicant reliability value and X's observed-score or true-score u ratio.
</p>
</li>
<li><p>estimate_rxxi_u<br /> Estimate the incumbent reliability of variable X from X's observed-score and true-score u ratios.
</p>
</li>
<li><p>estimate_ux<br /> Estimate the true-score u ratio for variable X from X's reliability coefficient and X's observed-score u ratio.
</p>
</li>
<li><p>estimate_uy<br /> Estimate the observed-score u ratio for variable X from X's reliability coefficient and X's true-score u ratio.
</p>
</li>
<li><p>estimate_ryya<br /> Estimate the applicant reliability of variable Y from Y's incumbent reliability value, Y's correlation with X, and X's u ratio.
</p>
</li>
<li><p>estimate_ryyi<br /> Estimate the incumbent reliability of variable Y from Y's applicant reliability value, Y's correlation with X, and X's u ratio.
</p>
</li>
<li><p>estimate_uy<br /> Estimate the observed-score u ratio for variable Y from Y's applicant and incumbent reliability coefficients.
</p>
</li>
<li><p>estimate_up<br /> Estimate the true-score u ratio for variable Y from Y's applicant and incumbent reliability coefficients.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_rxxa(
  rxxi,
  ux,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  rxxi_type = "alpha"
)

estimate_rxxi(
  rxxa,
  ux,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  rxxa_type = "alpha"
)

estimate_ut(ux, rxx, rxx_restricted = TRUE)

estimate_ux(ut, rxx, rxx_restricted = TRUE)

estimate_ryya(ryyi, rxyi, ux)

estimate_ryyi(ryya, rxyi, ux)

estimate_uy(ryyi, ryya, indirect_rr = TRUE, ryy_type = "alpha")

estimate_up(ryyi, ryya)

estimate_rxxa_u(ux, ut)

estimate_rxxi_u(ux, ut)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_artifacts_+3A_rxxi">rxxi</code></td>
<td>
<p>Vector of incumbent reliability estimates for X.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X (if used in the context of estimating a reliability value, a true-score u ratio may be supplied by setting ux_observed to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector determining whether each element of ux is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_indirect_rr">indirect_rr</code></td>
<td>
<p>Logical vector determining whether each reliability value is associated with indirect range restriction (<code>TRUE</code>) or direct range restriction (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_rxxi_type">rxxi_type</code>, <code id="estimate_artifacts_+3A_rxxa_type">rxxa_type</code>, <code id="estimate_artifacts_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_rxxa">rxxa</code></td>
<td>
<p>Vector of applicant reliability estimates for X.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_rxx">rxx</code></td>
<td>
<p>Vector of reliability estimates for X (used in the context of estimating ux and ut - specify that reliability is an incumbent value by setting rxx_restricted to <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector determining whether each element of rxx is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_ut">ut</code></td>
<td>
<p>Vector of true-score u ratios for X.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_ryyi">ryyi</code></td>
<td>
<p>Vector of incumbent reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed-score incumbent correlations between X and Y.</p>
</td></tr>
<tr><td><code id="estimate_artifacts_+3A_ryya">ryya</code></td>
<td>
<p>Vector of applicant reliability estimates for Y.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>#### Formulas to estimate rxxa ####
</p>
<p>Formulas for indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX_{a}}=1-u_{X}^{2}\left(1-\rho_{XX_{i}}\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">\rho_{XX_{a}}=\frac{\rho_{XX_{i}}}{\rho_{XX_{i}}+u_{T}^{2}-\rho_{XX_{i}}u_{T}^{2}}</code>
</p>

<p>Formula for direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX_{a}}=\frac{\rho_{XX_{i}}}{u_{X}^{2}\left[1+\rho_{XX_{i}}\left(\frac{1}{u_{X}^{2}}-1\right)\right]}</code>
</p>

<p>#### Formulas to estimate rxxi ####
</p>
<p>Formulas for indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX_{i}}=1-\frac{1-\rho_{XX_{a}}}{u_{X}^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">\rho_{XX_{i}}=1-\frac{1-\rho_{XX_{a}}}{\rho_{XX_{a}}\left[u_{T}^{2}-\left(1-\frac{1}{\rho_{XX_{a}}}\right)\right]}</code>
</p>

<p>Formula for direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX_{i}}=\frac{\rho_{XX_{i}}u_{X}^{2}}{1+\rho_{XX_{i}}\left(u_{X}^{2}-1\right)}</code>
</p>

<p>#### Formulas to estimate ut ####
</p>
<p style="text-align: center;"><code class="reqn">u_{T}=\sqrt{\frac{\rho_{XX_{i}}u_{X}^{2}}{1+\rho_{XX_{i}}u_{X}^{2}-u_{X}^{2}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">u_{T}=\sqrt{\frac{u_{X}^{2}-\left(1-\rho_{XX_{a}}\right)}{\rho_{XX_{a}}}}</code>
</p>

<p>#### Formulas to estimate ux ####
</p>
<p style="text-align: center;"><code class="reqn">u_{X}=\sqrt{\frac{u_{T}^{2}}{\rho_{XX_{i}}\left(1+\frac{u_{T}^{2}}{\rho_{XX_{i}}}-u_{T}^{2}\right)}}</code>
</p>

<p style="text-align: center;"><code class="reqn">u_{X}=\sqrt{\rho_{XX_{a}}\left[u_{T}^{2}-\left(1-\frac{1}{\rho_{XX_{a}}}\right)\right]}</code>
</p>

<p>#### Formula to estimate ryya ####
</p>
<p style="text-align: center;"><code class="reqn">\rho_{YY_{a}}=1-\frac{1-\rho_{YY_{i}}}{1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)}</code>
</p>

<p>#### Formula to estimate ryyi
</p>
<p style="text-align: center;"><code class="reqn">\rho_{YY_{i}}=1-\left(1-\rho_{YY_{a}}\right)\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]</code>
</p>

<p>#### Formula to estimate uy ####
</p>
<p style="text-align: center;"><code class="reqn">u_{Y}=\sqrt{\frac{1-\rho_{YY_{a}}}{1-\rho_{YY_{i}}}}</code>
</p>

<p>#### Formula to estimate up ####
</p>
<p style="text-align: center;"><code class="reqn">u_{P}=\sqrt{\frac{\frac{1-\rho_{YY_{a}}}{1-\rho_{YY_{i}}}-\left(1-\rho_{YY_{a}}\right)}{\rho_{YY_{a}}}}</code>
</p>



<h3>Value</h3>

<p>A vector of estimated artifact values.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a> p. 127.
</p>
<p>Le, H., &amp; Schmidt, F. L. (2006).
Correcting for indirect range restriction in meta-analysis: Testing a new meta-analytic procedure.
<em>Psychological Methods, 11</em>(4), 416–438. doi: <a href="https://doi.org/10.1037/1082-989X.11.4.416">10.1037/1082-989X.11.4.416</a>
</p>
<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006).
Implications of direct and indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612. doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>
<p>Le, H., Oh, I.-S., Schmidt, F. L., &amp; Wooldridge, C. D. (2016).
Correction for range restriction in meta-analysis revisited: Improvements and implications for organizational research.
<em>Personnel Psychology, 69</em>(4), 975–1008. doi: <a href="https://doi.org/10.1111/peps.12122">10.1111/peps.12122</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_rxxa(rxxi = .8, ux = .8, ux_observed = TRUE)
estimate_rxxi(rxxa = .8, ux = .8, ux_observed = TRUE)
estimate_ut(ux = .8, rxx = .8, rxx_restricted = TRUE)
estimate_ux(ut = .8, rxx = .8, rxx_restricted = TRUE)
estimate_ryya(ryyi = .8, rxyi = .3, ux = .8)
estimate_ryyi(ryya = .8, rxyi = .3, ux = .8)
estimate_uy(ryyi = c(.5, .7), ryya = c(.7, .8))
estimate_up(ryyi = c(.5, .7), ryya = c(.7, .8))
estimate_rxxa_u(ux = c(.7, .8), ut = c(.65, .75))
estimate_rxxi_u(ux = c(.7, .8), ut = c(.65, .75))
</code></pre>

<hr>
<h2 id='estimate_length_sb'>Inverse Spearman-Brown formula to estimate the amount by which a measure would have to be lengthened or shorted to achieve a desired level of reliability</h2><span id='topic+estimate_length_sb'></span>

<h3>Description</h3>

<p>This function implements the inverse of the Spearman-Brown prophecy formula and answers the question: &quot;How much would I have to increase (do decrease) the length of this measure
to obtain a desired reliability level given the current reliability of the measure?&quot; The result of the function is the multiplier by which the length of the original measure should be adjusted.
The formula implemented here assumes that all items added to (or subtracted from) the measure will be parallel forms of the original items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_length_sb(rel_initial, rel_desired)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_length_sb_+3A_rel_initial">rel_initial</code></td>
<td>
<p>Initial reliability of a measure.</p>
</td></tr>
<tr><td><code id="estimate_length_sb_+3A_rel_desired">rel_desired</code></td>
<td>
<p>Desired reliability of a lengthened or shortened measure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is computed as:
</p>
<p style="text-align: center;"><code class="reqn">k^{*}=\frac{\rho_{XX}^{*}(\rho_{XX}-1)}{(\rho_{XX}^{*}-1)\rho_{XX}}</code>
</p>

<p>where <code class="reqn">\rho_{XX}</code> is the inital reliability, <code class="reqn">\rho_{XX}^{*}</code> is the predicted reliability of a measure with a different length, and <code class="reqn">k^{*}</code> is the number of times the measure would have to be lengthened to obtain a reliability equal to <code class="reqn">\rho_{XX}^{*}</code>.
</p>


<h3>Value</h3>

<p>The estimated number of times by which the number of items in the initial measure would have to be multiplied to achieve the desired reliability.
</p>


<h3>References</h3>

<p>Ghiselli, E. E., Campbell, J. P., &amp; Zedeck, S. (1981).
<em>Measurement theory for the behavioral sciences</em>.
San Francisco, CA: Freeman. p. 236.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Estimated k to achieve a reliability of .8 from a measure with an initial reliability of .7
estimate_length_sb(rel_initial = .7, rel_desired = .8)

## Estimated k to achieve a reliability of .8 from a measure with an initial reliability of .9
estimate_length_sb(rel_initial = .9, rel_desired = .8)
</code></pre>

<hr>
<h2 id='estimate_matrix_prods'>Estimate covariance matrices and mean vectors containing product terms</h2><span id='topic+estimate_matrix_prods'></span>

<h3>Description</h3>

<p>Estimate covariance matrices and mean vectors containing product terms
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_matrix_prods(sigma_mat, mu_vec, prod_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_matrix_prods_+3A_sigma_mat">sigma_mat</code></td>
<td>
<p>Covariance parameter matrix.</p>
</td></tr>
<tr><td><code id="estimate_matrix_prods_+3A_mu_vec">mu_vec</code></td>
<td>
<p>Mean parameter matrix.</p>
</td></tr>
<tr><td><code id="estimate_matrix_prods_+3A_prod_list">prod_list</code></td>
<td>
<p>List of 2-element vectors containing the names of variables in <code>sigma_mat</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Augmented covariance matrix and mean vector containing product terms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Establish mean and covariance parameters
mu_vec &lt;- 1:4
sigma_mat &lt;- reshape_vec2mat(c(.1, .2, .3, .4, .5, .6), var_names = LETTERS[1:4])
names(mu_vec) &lt;- colnames(sigma_mat)

## Define a list of variables to be used in estimating products:
prod_list &lt;- list(c("A", "B"),
                  c("A", "C"),
                  c("A", "D"),
                  c("B", "C"),
                  c("B", "D"),
                  c("C", "D"))

## Generate data for the purposes of comparison:
set.seed(1)
dat &lt;- data.frame(MASS::mvrnorm(100000, mu = mu_vec, Sigma = sigma_mat, empirical = TRUE))

## Create product terms in simulated data:
for(i in 1:length(prod_list))
     dat[,paste(prod_list[[i]], collapse = "_x_")] &lt;-
     dat[,prod_list[[i]][1]] * dat[,prod_list[[i]][2]]

## Analytically estimate product variables and compare to simulated data:
estimate_matrix_prods(sigma_mat = sigma_mat, mu_vec = mu_vec, prod_list = prod_list)
round(cov(dat), 2)
round(apply(dat, 2, mean), 2)
</code></pre>

<hr>
<h2 id='estimate_prod'>Estimation of statistics computed from products of random, normal variables</h2><span id='topic+estimate_prod'></span><span id='topic+estimate_mean_prod'></span><span id='topic+estimate_var_prod'></span><span id='topic+estimate_cov_prods'></span><span id='topic+estimate_cor_prods'></span>

<h3>Description</h3>

<p>This family of functions computes univariate descriptive statistics for the products of two variables denoted as &quot;x&quot; and &quot;y&quot; (e.g., mean(x * y) or var(x * y)) and
the covariance between the products of &quot;x&quot; and &quot;y&quot; and of &quot;u&quot; and &quot;v&quot; (e.g., cov(x * y, u * v) or cor(x * y, u * v)). These functions presume all variables are random normal variables.
</p>
<p>Available functions include:
</p>

<ul>
<li><p>estimate_mean_prod<br /> Estimate the mean of the product of two variables: x * y.
</p>
</li>
<li><p>estimate_var_prod<br /> Estimate the variance of the product of two variables: x * y.
</p>
</li>
<li><p>estimate_cov_prods<br /> Estimate the covariance between the products of two pairs of variables: x * y and u * v.
</p>
</li>
<li><p>estimate_cor_prods<br /> Estimate the correlation between the products of two pairs of variables: x * y and u * v.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_mean_prod(mu_x, mu_y, cov_xy)

estimate_var_prod(mu_x, mu_y, var_x, var_y, cov_xy)

estimate_cov_prods(mu_x, mu_y, mu_u, mu_v, cov_xu, cov_xv, cov_yu, cov_yv)

estimate_cor_prods(
  mu_x,
  mu_y,
  mu_u,
  mu_v,
  var_x,
  var_y,
  var_u,
  var_v,
  cov_xu,
  cov_xv,
  cov_yu,
  cov_yv,
  cov_xy,
  cov_uv
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_prod_+3A_mu_x">mu_x</code></td>
<td>
<p>Expected value of variable x.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_mu_y">mu_y</code></td>
<td>
<p>Expected value of variable y.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_xy">cov_xy</code></td>
<td>
<p>Covariance between x and y.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_var_x">var_x</code></td>
<td>
<p>Variance of variable x.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_var_y">var_y</code></td>
<td>
<p>Variance of variable y.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_mu_u">mu_u</code></td>
<td>
<p>Expected value of variable u.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_mu_v">mu_v</code></td>
<td>
<p>Expected value of variable v.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_xu">cov_xu</code></td>
<td>
<p>Covariance between x and u.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_xv">cov_xv</code></td>
<td>
<p>Covariance between x and v.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_yu">cov_yu</code></td>
<td>
<p>Covariance between y and u.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_yv">cov_yv</code></td>
<td>
<p>Covariance between y and v.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_var_u">var_u</code></td>
<td>
<p>Variance of variable u.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_var_v">var_v</code></td>
<td>
<p>Variance of variable v.</p>
</td></tr>
<tr><td><code id="estimate_prod_+3A_cov_uv">cov_uv</code></td>
<td>
<p>Covariance between u and v.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An estimated statistic computed from the products of random, normal variables.
</p>


<h3>References</h3>

<p>Bohrnstedt, G. W., &amp; Goldberger, A. S. (1969). On the exact covariance of products of random variables.
<em>Journal of the American Statistical Association, 64</em>(328), 1439. doi: <a href="https://doi.org/10.2307/2286081">10.2307/2286081</a>
</p>
<p>Goodman, L. A. (1960). On the exact variance of products.
<em>Journal of the American Statistical Association, 55</em>(292), 708. doi: <a href="https://doi.org/10.2307/2281592">10.2307/2281592</a>
</p>

<hr>
<h2 id='estimate_q_dist'>Estimate descriptive statistics of square-root reliabilities</h2><span id='topic+estimate_q_dist'></span>

<h3>Description</h3>

<p>Estimate descriptive statistics of square-root reliabilities from descriptive statistics of reliabilities via Taylor series approximation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_q_dist(mean_rel, var_rel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_q_dist_+3A_mean_rel">mean_rel</code></td>
<td>
<p>Mean reliability value.</p>
</td></tr>
<tr><td><code id="estimate_q_dist_+3A_var_rel">var_rel</code></td>
<td>
<p>Variance of reliability values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">var_{q_{X}}=\frac{var_{\rho_{XX}}}{4q_{X}^{2}}</code>
</p>



<h3>Value</h3>

<p>The estimated mean and variance of a distribution of square-root reliability values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_q_dist(mean_rel = .8, var_rel = .15)
</code></pre>

<hr>
<h2 id='estimate_rel_dist'>Estimate descriptive statistics of reliabilities</h2><span id='topic+estimate_rel_dist'></span>

<h3>Description</h3>

<p>Estimate descriptive statistics of reliabilities from descriptive statistics of square-root reliabilities via Taylor series approximation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_rel_dist(mean_q, var_q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_rel_dist_+3A_mean_q">mean_q</code></td>
<td>
<p>Mean square-root reliability value.</p>
</td></tr>
<tr><td><code id="estimate_rel_dist_+3A_var_q">var_q</code></td>
<td>
<p>Variance of square-root reliability values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">var_{\rho_{XX}}=4q_{X}^{2}var_{\rho_{XX}}</code>
</p>



<h3>Value</h3>

<p>The estimated mean and variance of a distribution of reliability values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_rel_dist(mean_q = .9, var_q = .05)
</code></pre>

<hr>
<h2 id='estimate_rel_sb'>Spearman-Brown prophecy formula to estimate the reliability of a lengthened measure</h2><span id='topic+estimate_rel_sb'></span>

<h3>Description</h3>

<p>This function implements the Spearman-Brown prophecy formula for estimating the reliability of a lengthened (or shortened) measure.
The formula implemented here assumes that all items added to (or subtracted from) the measure will be parallel forms of the original items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_rel_sb(rel_initial, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_rel_sb_+3A_rel_initial">rel_initial</code></td>
<td>
<p>Initial reliability of a measure.</p>
</td></tr>
<tr><td><code id="estimate_rel_sb_+3A_k">k</code></td>
<td>
<p>The number of times by which the measure should be lengthened (if k &gt; 1) or shortened (if k &lt; 1), assuming that all new items are parallel forms of initial items.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XX}^{*}=\frac{k\rho_{XX}}{1+(k-1)\rho_{XX}}</code>
</p>

<p>where <code class="reqn">\rho_{XX}</code> is the initial reliability, <em>k</em> is the multiplier by which the measure is to be lengthened (or shortened), and <code class="reqn">\rho_{XX}^{*}</code> is the predicted reliability of a measure with a different length.
</p>


<h3>Value</h3>

<p>The estimated reliability of the lengthened (or shortened) measure.
</p>


<h3>References</h3>

<p>Ghiselli, E. E., Campbell, J. P., &amp; Zedeck, S. (1981).
<em>Measurement theory for the behavioral sciences</em>.
San Francisco, CA: Freeman. p. 232.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Double the length of a measure with an initial reliability of .7
estimate_rel_sb(rel_initial = .7, k = 2)

## Halve the length of a measure with an initial reliability of .9
estimate_rel_sb(rel_initial = .9, k = .5)
</code></pre>

<hr>
<h2 id='estimate_u'>Estimate u ratios from available artifact information</h2><span id='topic+estimate_u'></span>

<h3>Description</h3>

<p>Uses information about standard deviations, reliability estimates, and selection ratios to estimate u ratios.
Selection ratios are only used to estimate u when no other information is available, but estimates of u computed from SDs and reliabilities will be averaged to reduce error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_u(
  measure_id = NULL,
  sdi = NULL,
  sda = NULL,
  rxxi = NULL,
  rxxa = NULL,
  item_ki = NULL,
  item_ka = NULL,
  n = NULL,
  meani = NULL,
  sr = NULL,
  rxya_est = NULL,
  data = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_u_+3A_measure_id">measure_id</code></td>
<td>
<p>Vector of measure identifiers.</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_sdi">sdi</code></td>
<td>
<p>Scalar or vector containing restricted standard deviation(s).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_sda">sda</code></td>
<td>
<p>Scalar or vector containing unrestricted standard deviation(s).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_rxxi">rxxi</code></td>
<td>
<p>Scalar or vector containing restricted reliability coefficient(s).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_rxxa">rxxa</code></td>
<td>
<p>Scalar or vector containing unrestricted reliability coefficient(s).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_item_ki">item_ki</code></td>
<td>
<p>Scalar or vector containing the number of items used in measures within samples.</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_item_ka">item_ka</code></td>
<td>
<p>Scalar or vector indicating the number of items toward which reliability estimates should be adjusted using the Spearman-Brown formula.</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_meani">meani</code></td>
<td>
<p>Vector of sample means.</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_sr">sr</code></td>
<td>
<p>Vector of selection ratios (used only when no other useable u-ratio inputs are available).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_rxya_est">rxya_est</code></td>
<td>
<p>Vector of estimated unrestricted correlations between the selection mechanism and the variable of interest (used only when <code>sr</code> is used).</p>
</td></tr>
<tr><td><code id="estimate_u_+3A_data">data</code></td>
<td>
<p>Optional data frame containing any or all information for use in other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of estimated u ratios.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sdi &lt;- c(1.4, 1.2, 1.3, 1.4)
sda &lt;- 2
rxxi &lt;- c(.6, .7, .75, .8)
rxxa &lt;- c(.9, .95, .8, .9)
item_ki &lt;- c(12, 12, 12, NA)
item_ka &lt;- NULL
n &lt;- c(200, 200, 200, 200)
meani &lt;- c(2, 1, 2, 3)
sr &lt;- c(.5, .6, .7, .4)
rxya_est &lt;- .5

## Estimate u from standard deviations only:
estimate_u(sdi = sdi, sda = sda)

## Estimate u from incumbent standard deviations and the
## mixture standard deviation:
estimate_u(sdi = sdi, sda = "mixture", meani = meani, n = n)

## Estimate u from reliability information:
estimate_u(rxxi = rxxi, rxxa = rxxa)

## Estimate u from both standard deviations and reliabilities:
estimate_u(sdi = sdi, sda = sda, rxxi = rxxi, rxxa = rxxa,
           item_ki = item_ki, item_ka = item_ka, n = n,
           meani = meani, sr = sr, rxya_est = rxya_est)

estimate_u(sdi = sdi, sda = "average", rxxi = rxxi, rxxa = "average",
           item_ki = item_ki, item_ka = item_ka, n = n, meani = meani)

## Estimate u from selection ratios as direct range restriction:
estimate_u(sr = sr)

## Estimate u from selection ratios as indirect range restriction:
estimate_u(sr = sr, rxya_est = rxya_est)
</code></pre>

<hr>
<h2 id='estimate_var_artifacts'>Taylor series approximations for the variances of estimates artifact distributions.</h2><span id='topic+estimate_var_artifacts'></span><span id='topic+estimate_var_qxi'></span><span id='topic+estimate_var_qxa'></span><span id='topic+estimate_var_rxxi'></span><span id='topic+estimate_var_rxxa'></span><span id='topic+estimate_var_ut'></span><span id='topic+estimate_var_ux'></span><span id='topic+estimate_var_ryya'></span><span id='topic+estimate_var_qya'></span><span id='topic+estimate_var_qyi'></span><span id='topic+estimate_var_ryyi'></span>

<h3>Description</h3>

<p>Taylor series approximations to estimate the variances of artifacts that have been estimated from other artifacts.
These functions are implemented internally in the <code><a href="#topic+create_ad">create_ad</a></code> function and related functions, but are useful as general tools for manipulating artifact distributions.
</p>
<p>Available functions include:
</p>

<ul>
<li><p>estimate_var_qxi<br /> Estimate the variance of a qxi distribution from a qxa distribution and a distribution of u ratios.
</p>
</li>
<li><p>estimate_var_rxxi<br /> Estimate the variance of an rxxi distribution from an rxxa distribution and a distribution of u ratios.
</p>
</li>
<li><p>estimate_var_qxa<br /> Estimate the variance of a qxa distribution from a qxi distribution and a distribution of u ratios.
</p>
</li>
<li><p>estimate_var_rxxa<br /> Estimate the variance of an rxxa distribution from an rxxi distribution and a distribution of u ratios.
</p>
</li>
<li><p>estimate_var_ut<br /> Estimate the variance of a true-score u ratio distribution from an observed-score u ratio distribution and a reliability distribution.
</p>
</li>
<li><p>estimate_var_ux<br /> Estimate the variance of an observed-score u ratio distribution from a true-score u ratio distribution and a reliability distribution.
</p>
</li>
<li><p>estimate_var_qyi<br /> Estimate the variance of a qyi distribution from the following distributions: qya, rxyi, and ux.
</p>
</li>
<li><p>estimate_var_ryyi<br /> Estimate the variance of an ryyi distribution from the following distributions: ryya, rxyi, and ux.
</p>
</li>
<li><p>estimate_var_qya<br /> Estimate the variance of a qya distribution from the following distributions: qyi, rxyi, and ux.
</p>
</li>
<li><p>estimate_var_ryya<br /> Estimate the variance of an ryya distribution from the following distributions: ryyi, rxyi, and ux.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_var_qxi(
  qxa,
  var_qxa = 0,
  ux,
  var_ux = 0,
  cor_qxa_ux = 0,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  qxa_type = "alpha"
)

estimate_var_qxa(
  qxi,
  var_qxi = 0,
  ux,
  var_ux = 0,
  cor_qxi_ux = 0,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  qxi_type = "alpha"
)

estimate_var_rxxi(
  rxxa,
  var_rxxa = 0,
  ux,
  var_ux = 0,
  cor_rxxa_ux = 0,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  rxxa_type = "alpha"
)

estimate_var_rxxa(
  rxxi,
  var_rxxi = 0,
  ux,
  var_ux = 0,
  cor_rxxi_ux = 0,
  ux_observed = TRUE,
  indirect_rr = TRUE,
  rxxi_type = "alpha"
)

estimate_var_ut(
  rxx,
  var_rxx = 0,
  ux,
  var_ux = 0,
  cor_rxx_ux = 0,
  rxx_restricted = TRUE,
  rxx_as_qx = FALSE
)

estimate_var_ux(
  rxx,
  var_rxx = 0,
  ut,
  var_ut = 0,
  cor_rxx_ut = 0,
  rxx_restricted = TRUE,
  rxx_as_qx = FALSE
)

estimate_var_ryya(
  ryyi,
  var_ryyi = 0,
  rxyi,
  var_rxyi = 0,
  ux,
  var_ux = 0,
  cor_ryyi_rxyi = 0,
  cor_ryyi_ux = 0,
  cor_rxyi_ux = 0
)

estimate_var_qya(
  qyi,
  var_qyi = 0,
  rxyi,
  var_rxyi = 0,
  ux,
  var_ux = 0,
  cor_qyi_rxyi = 0,
  cor_qyi_ux = 0,
  cor_rxyi_ux = 0
)

estimate_var_qyi(
  qya,
  var_qya = 0,
  rxyi,
  var_rxyi = 0,
  ux,
  var_ux = 0,
  cor_qya_rxyi = 0,
  cor_qya_ux = 0,
  cor_rxyi_ux = 0
)

estimate_var_ryyi(
  ryya,
  var_ryya = 0,
  rxyi,
  var_rxyi = 0,
  ux,
  var_ux = 0,
  cor_ryya_rxyi = 0,
  cor_ryya_ux = 0,
  cor_rxyi_ux = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_var_artifacts_+3A_qxa">qxa</code></td>
<td>
<p>Square-root of applicant reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_qxa">var_qxa</code></td>
<td>
<p>Variance of square-root of applicant reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_ux">ux</code></td>
<td>
<p>Observed-score u ratio.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_ux">var_ux</code></td>
<td>
<p>Variance of observed-score u ratio.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qxa_ux">cor_qxa_ux</code></td>
<td>
<p>Correlation between qxa and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector determining whether u ratios are observed-score u ratios (<code>TRUE</code>) or true-score u ratios (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_indirect_rr">indirect_rr</code></td>
<td>
<p>Logical vector determining whether reliability values are associated with indirect range restriction (<code>TRUE</code>) or direct range restriction (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_qxi">qxi</code></td>
<td>
<p>Square-root of incumbent reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_qxi">var_qxi</code></td>
<td>
<p>Variance of square-root of incumbent reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qxi_ux">cor_qxi_ux</code></td>
<td>
<p>Correlation between qxi and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxxa">rxxa</code></td>
<td>
<p>Incumbent reliability value.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_rxxa">var_rxxa</code></td>
<td>
<p>Variance of incumbent reliability values.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_rxxa_ux">cor_rxxa_ux</code></td>
<td>
<p>Correlation between rxxa and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxxi">rxxi</code></td>
<td>
<p>Incumbent reliability value.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_rxxi">var_rxxi</code></td>
<td>
<p>Variance of incumbent reliability values.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_rxxi_ux">cor_rxxi_ux</code></td>
<td>
<p>Correlation between rxxi and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxxi_type">rxxi_type</code>, <code id="estimate_var_artifacts_+3A_rxxa_type">rxxa_type</code>, <code id="estimate_var_artifacts_+3A_qxi_type">qxi_type</code>, <code id="estimate_var_artifacts_+3A_qxa_type">qxa_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxx">rxx</code></td>
<td>
<p>Generic argument for a reliability estimate (whether this is a reliability or the square root of a reliability is clarified by the <code>rxx_as_qx</code> argument).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_rxx">var_rxx</code></td>
<td>
<p>Generic argument for the variance of reliability estimates (whether this pertains to reliabilities or the square roots of reliabilities is clarified by the <code>rxx_as_qx</code> argument).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_rxx_ux">cor_rxx_ux</code></td>
<td>
<p>Correlation between rxx and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector determining whether reliability estimates were incumbent reliabilities (<code>TRUE</code>) or applicant reliabilities (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxx_as_qx">rxx_as_qx</code></td>
<td>
<p>Logical vector determining whether the reliability estimates were reliabilities (<code>TRUE</code>) or square-roots of reliabilities (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_ut">ut</code></td>
<td>
<p>True-score u ratio.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_ut">var_ut</code></td>
<td>
<p>Variance of true-score u ratio.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_rxx_ut">cor_rxx_ut</code></td>
<td>
<p>Correlation between rxx and ut.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_ryyi">ryyi</code></td>
<td>
<p>Incumbent reliability value.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_ryyi">var_ryyi</code></td>
<td>
<p>Variance of incumbent reliability values.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_rxyi">rxyi</code></td>
<td>
<p>Incumbent correlation between X and Y.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_rxyi">var_rxyi</code></td>
<td>
<p>Variance of incumbent correlations.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_ryyi_rxyi">cor_ryyi_rxyi</code></td>
<td>
<p>Correlation between ryyi and rxyi.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_ryyi_ux">cor_ryyi_ux</code></td>
<td>
<p>Correlation between ryyi and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_rxyi_ux">cor_rxyi_ux</code></td>
<td>
<p>Correlation between rxyi and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_qyi">qyi</code></td>
<td>
<p>Square-root of incumbent reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_qyi">var_qyi</code></td>
<td>
<p>Variance of square-root of incumbent reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qyi_rxyi">cor_qyi_rxyi</code></td>
<td>
<p>Correlation between qyi and rxyi.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qyi_ux">cor_qyi_ux</code></td>
<td>
<p>Correlation between qyi and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_qya">qya</code></td>
<td>
<p>Square-root of applicant reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_qya">var_qya</code></td>
<td>
<p>Variance of square-root of applicant reliability estimate.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qya_rxyi">cor_qya_rxyi</code></td>
<td>
<p>Correlation between qya and rxyi.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_qya_ux">cor_qya_ux</code></td>
<td>
<p>Correlation between qya and ux.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_ryya">ryya</code></td>
<td>
<p>Applicant reliability value.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_var_ryya">var_ryya</code></td>
<td>
<p>Variance of applicant reliability values.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_ryya_rxyi">cor_ryya_rxyi</code></td>
<td>
<p>Correlation between ryya and rxyi.</p>
</td></tr>
<tr><td><code id="estimate_var_artifacts_+3A_cor_ryya_ux">cor_ryya_ux</code></td>
<td>
<p>Correlation between ryya and ux.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>#### Partial derivatives to estimate the variance of qxa using ux ####
</p>
<p>Indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{(q_{X_{i}}^{2}-1)u_{X}}{\sqrt{(q_{X_{i}}^{2}-1)u_{X}^{2}+1}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{i}}}=\frac{q_{X_{i}}^{2}u_{X}^{2}}{\sqrt{(q_{X_{i}}^{2}-1)u_{X}^{2}+1}}</code>
</p>

<p>Direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{q_{X_{i}}^{2}(q_{X_{i}}^{2}-1)u_{X}}{\sqrt{-\frac{q_{X_{i}}^{2}}{q_{X_{i}}^{2}(u_{X}^{2}-1)-u_{X}^{2}}}(q_{X_{i}}^{2}(u_{X}^{2}-1)-u_{X}^{2})^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{i}}}=\frac{q_{X_{i}}u_{X}^{2}}{\sqrt{-\frac{q_{X_{i}}^{2}}{q_{X_{i}}^{2}(u_{X}^{2}-1)-u_{X}^{2}}}(q_{X_{i}}^{2}(u_{X}^{2}-1)-u_{X}^{2})^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of rxxa using ux ####
</p>
<p>Indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=2\left(\rho_{XX_{i}}-1\right)u_{X}</code>
</p>

<p style="text-align: center;"><code class="reqn">\rho_{XX_{i}}: b_{\rho_{XX_{i}}}=u_{X}^{2}</code>
</p>

<p>Direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{2(\rho_{XX_{i}}-1)\rho_{XX_{i}}u_{X}}{(-\rho_{XX_{i}}u_{X}^{2}+\rho_{XX_{i}}+u_{X}^{2})^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{i}}}=\frac{u_{X}^{2}}{(-\rho_{XX_{i}}u_{X}^{2}+\rho_{XX_{i}}+u_{X}^{2})^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of rxxa using ut ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{2(\rho_{XX_{i}}-1)*\rho_{XX_{i}}u_{T}}{(-\rho_{XX_{i}}u_{T}^{2}+\rho_{XX_{i}}+u_{T}^{2})^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{i}}}=\frac{u_{T}^{2}}{(-\rho_{XX_{i}}u_{T}^{2}+\rho_{XX_{i}}+u_{T}^{2})^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of qxa using ut ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{q_{X_{i}}^{2}(q_{X_{i}}^{2}-1)u_{T}}{\sqrt{\frac{-q_{X_{i}}^{2}}{q_{X_{i}}^{2}*(u_{T}^{2}-1)-u_{T}^{2}}}(q_{X_{i}}^{2}(u_{T}^{2}-1)-u_{T}^{2})^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{i}}}=\frac{q_{X_{i}}u_{T}^{2}}{\sqrt{\frac{q_{X_{i}}^{2}}{u_{T}^{2}-q_{X_{i}}^{2}(u_{T}^{2}-1)}}(u_{T}^{2}-q_{X_{i}}^{2}(u_{T}^{2}-1))^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of qxi using ux ####
</p>
<p>Indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{1-qxa^{2}}{u_{X}^{3}\sqrt{\frac{q_{X_{a}}^{2}+u_{X}^{2}-1}{u_{X}^{2}}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{a}}}=\frac{q_{X_{a}}}{u_{X}^{2}\sqrt{\frac{q_{X_{a}-1}^{2}}{u_{X}^{2}}+1}}</code>
</p>

<p>Direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=-\frac{q_{X_{a}}^{2}(q_{X_{a}}^{2}-1)u_{X}}{\sqrt{\frac{q_{X_{a}}^{2}u_{X}^{2}}{q_{X_{a}}^{2}(u_{X}^{2}-1)+1}}(q_{X_{a}}^{2}(u_{X}^{2}-1)+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{a}}}=\frac{q_{X_{a}}u_{X}^{2}}{\sqrt{\frac{q_{X_{a}}^{2}u_{X}^{2}}{q_{X_{a}}^{2}(u_{X}^{2}-1)+1}}(q_{X_{a}}^{2}(u_{X}^{2}-1)+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of rxxi using ux ####
</p>
<p>Indirect range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{2-2\rho_{XX_{a}}}{u_{X}^{3}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{a}}}=\frac{1}{u_{X}^{2}}</code>
</p>

<p>Direct range restriction:
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=-\frac{2(\rho_{XX_{a}}-1)\rho_{XX_{a}}u_{X}}{(\rho_{XX_{a}}(u_{X}^{2}-1)+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{a}}}=\frac{u_{X}^{2}}{(\rho_{XX_{a}}(u_{X}^{2}-1)+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of rxxi using ut ####
</p>
<p style="text-align: center;"><code class="reqn">u_{T}: b_{u_{T}}=-\frac{2(\rho_{XX_{a}}-1)\rho_{XX_{a}}u_{T}}{(\rho_{XX_{a}}(u_{T}^{2}-1)+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{a}}}=\frac{u_{T}^{2}}{(\rho_{XX_{a}}(u_{T}^{2}-1)+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of qxi using ut ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=-\frac{(q_{X_{a}}-1)q_{X_{a}}^{2}(q_{X_{a}}+1)u_{T}}{\sqrt{\frac{q_{X_{a}}^{2}u_{T}^{2}}{q_{X_{a}}^{2}u_{T}^{2}-q_{X_{a}}^{2}+1}}(q_{X_{a}}^{2}u_{T}^{2}-q_{X_{a}}^{2}+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{a}}}=\frac{q_{X_{a}}u_{T}^{2}}{\sqrt{\frac{q_{X_{a}}^{2}u_{T}^{2}}{q_{X_{a}}^{2}u_{T}^{2}-q_{X_{a}}^{2}+1}}(q_{X_{a}}^{2}u_{T}^{2}-q_{X_{a}}^{2}+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ut using qxi ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{q_{X_{i}}^{2}u_{X}}{\sqrt{\frac{q_{X_{i}}^{2}u_{X}^{2}}{(q_{X_{i}}^{2}-1)u_{X}^{2}+1}}((q_{X_{i}}^{2}-1)u_{X}^{2}+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{i}}}=-\frac{u_{X}^{2}(u_{X}^{2}-1)}{\sqrt{\frac{q_{X_{i}}^{2}u_{X}^{2}}{(q_{X_{i}}^{2}-1)u_{X}^{2}+1}}((q_{X_{i}}^{2}-1)u_{X}^{2}+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ut using rxxi ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{\rho_{XX_{i}}u_{X}}{\sqrt{\frac{\rho_{XX_{i}}u_{X}^{2}}{(\rho_{XX_{i}}-1)u_{X}^{2}+1}}((\rho_{XX_{i}}-1)u_{X}^{2}+1)^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{i}}}=-\frac{u_{X}^{2}(u_{X}^{2}-1)}{2\sqrt{\frac{\rho_{XX_{i}}u_{X}^{2}}{(\rho_{XX_{i}}-1)u_{X}^{2}+1}}((\rho_{XX_{i}}-1)u_{X}^{2}+1)^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ut using qxa ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{u_{X}}{q_{X_{a}}^{2}\sqrt{\frac{q_{X_{a}}^{2}+u_{X}^{2}-1}{q_{X_{a}}^{2}}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{a}}}=\frac{1-u_{X}^{2}}{q_{X_{a}}^{3}\sqrt{\frac{q_{X_{a}}^{2}+u_{X}^{2}-1}{q_{X_{a}}^{2}}}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ut using rxxa ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{u_{X}}{\rho_{XX_{a}}\sqrt{\frac{\rho_{XX_{a}}+u_{X}^{2}-1}{\rho_{XX_{a}}}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{a}}}=\frac{1-u_{X}^{2}}{2\rho_{XX_{a}}^{2}\sqrt{\frac{\rho_{XX_{a}}+u_{X}^{2}-1}{\rho_{XX_{a}}}}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ux using qxi ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{q_{X_{i}}^{2}u_{T}}{\sqrt{\frac{u_{T}^{2}}{u_{T}^{2}-q_{X_{i}}^{2}(u_{T}^{2}-1)}}(u_{T}^{2}-q_{X_{i}}^{2}(u_{T}^{2}-1))^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{i}}}=\frac{q_{X_{i}}(u_{T}^{2}-1)\left(\frac{u_{T}^{2}}{u_{T}^{2}-q_{X_{i}}^{2}(u_{T}^{2}-1)}\right)^{1.5}}{u_{T}^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ux using rxxi ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{\rho_{XX_{i}}u_{T}}{\sqrt{\frac{u_{T}^{2}}{-\rho_{XX_{i}}u_{T}^{2}+\rho_{XX_{i}}+u_{T}^{2}}}(-\rho_{XX_{i}}u_{T}^{2}+\rho_{XX_{i}}+u_{T}^{2})^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{i}}}=\frac{(u_{T}^{2}-1)\left(\frac{u_{T}^{2}}{-\rho_{XX_{i}}u_{T}^{2}+\rho_{XX_{i}}+u_{T}^{2}}\right)^{1.5}}{2u_{T}^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ux using qxa ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{q_{X_{a}}^{2}u_{T}}{\sqrt{q_{X_{a}}^{2}(u_{T}^{2}-1)+1}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{q_{X_{a}}}=\frac{q_{X_{a}}(u_{T}-1)}{\sqrt{q_{X_{a}}^{2}(u_{T}^{2}-1)+1}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ux using rxxa ####
</p>
<p style="text-align: center;"><code class="reqn">b_{u_{T}}=\frac{\rho_{XX_{a}}u_{T}}{\sqrt{\rho_{XX_{a}}(u_{T}^{2}-1)+1}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XX_{a}}}=\frac{u_{T}^{2}-1}{2\sqrt{\rho_{XX_{a}}(u_{T}^{2}-1)+1}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ryya ####
</p>
<p style="text-align: center;"><code class="reqn">b_{\rho_{YY_{i}}}=\frac{1}{\rho_{XY_{i}}^{2}\left(\frac{1}{u_{X}^{2}}-1\right)+1}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{2(\rho_{YY_{i}}-1)\rho_{XY_{i}}^{2}u_{X}}{(u_{X}^{2}-\rho_{XY_{i}}^{2}(u_{X}^{2}-1))^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XY_{i}}}=\frac{2(\rho_{YY_{i}}-1)\rho_{XY_{i}}u_{X}^{2}(u_{X}^{2}-1)}{(u_{X}^{2}-\rho_{XY_{i}}^{2}(u_{X}^{2}-1))^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of qya ####
</p>
<p style="text-align: center;"><code class="reqn">b_{q_{Y_{i}}}=\frac{q_{Y_{i}}}{\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]\sqrt{1-\frac{1-q_{Y_{i}}^{2}}{1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{u_{X}}=-\frac{(1-q_{Y_{i}}^{2})\rho_{XY_{i}}^{2}}{u_{X}^{3}\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]\sqrt{1-\frac{1-q_{Y_{i}}^{2}}{1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XY_{i}}}=-\frac{(1-q_{Y_{i}}^{2})\rho_{XY_{i}}\left(1-\frac{1}{u_{X}^{2}}\right)}{\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]\sqrt{1-\frac{1-q_{Y_{i}}^{2}}{1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)}}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of ryyi ####
</p>
<p style="text-align: center;"><code class="reqn">\rho_{YY_{a}}: b_{\rho_{YY_{a}}}=\rho_{XY_{i}}^{2}\left(\frac{1}{u_{X}^{2}}-1\right)+1</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{u_{X}}=-\frac{2(\rho_{YY_{a}}-1)\rho_{XY_{i}}^{2}}{u_{X}^{3}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XY_{i}}}=-\frac{2(\rho_{YY_{a}}-1)\rho_{XY_{i}}(u_{X}^{2}-1)}{u_{X}^{2}}</code>
</p>

<p>#### Partial derivatives to estimate the variance of qyi ####
</p>
<p style="text-align: center;"><code class="reqn">b_{q_{Y_{a}}}=\frac{q_{Y_{a}}\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]}{\sqrt{1-\left(1-q_{Y_{a}}\right)\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{u_{X}}=\frac{(1-q_{Y_{a}}^{2})\rho_{XY_{i}}\left(1-\frac{1}{u_{X}^{2}}\right)}{\sqrt{1-\left(1-q_{Y_{a}}\right)\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{\rho_{XY_{i}}}=\frac{(1-q_{Y_{a}}^{2})\rho_{XY_{i}}^{2}}{u_{X}^{3}\sqrt{1-\left(1-q_{Y_{a}}\right)\left[1-\rho_{XY_{i}}^{2}\left(1-\frac{1}{u_{X}^{2}}\right)\right]}}</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>estimate_var_qxi(qxa = c(.8, .85, .9, .95), var_qxa = c(.02, .03, .04, .05),
                 ux = .8, var_ux = 0,
                 ux_observed = c(TRUE, TRUE, FALSE, FALSE),
                 indirect_rr = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_qxa(qxi = c(.8, .85, .9, .95), var_qxi = c(.02, .03, .04, .05),
                 ux = .8, var_ux = 0,
                 ux_observed = c(TRUE, TRUE, FALSE, FALSE),
                 indirect_rr = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_rxxi(rxxa = c(.8, .85, .9, .95),
                  var_rxxa = c(.02, .03, .04, .05), ux = .8, var_ux = 0,
                 ux_observed = c(TRUE, TRUE, FALSE, FALSE),
                 indirect_rr = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_rxxa(rxxi = c(.8, .85, .9, .95), var_rxxi = c(.02, .03, .04, .05),
                  ux = .8, var_ux = 0,
                 ux_observed = c(TRUE, TRUE, FALSE, FALSE),
                 indirect_rr = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_ut(rxx = c(.8, .85, .9, .95), var_rxx = 0,
                ux = c(.8, .8, .9, .9), var_ux = c(.02, .03, .04, .05),
                 rxx_restricted = c(TRUE, TRUE, FALSE, FALSE),
                rxx_as_qx = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_ux(rxx = c(.8, .85, .9, .95), var_rxx = 0,
                ut = c(.8, .8, .9, .9), var_ut = c(.02, .03, .04, .05),
                 rxx_restricted = c(TRUE, TRUE, FALSE, FALSE),
                rxx_as_qx = c(TRUE, FALSE, TRUE, FALSE))
estimate_var_ryya(ryyi = .9, var_ryyi = .04, rxyi = .4, var_rxyi = 0, ux = .8, var_ux = 0)
estimate_var_ryya(ryyi = .9, var_ryyi = .04, rxyi = .4, var_rxyi = 0, ux = .8, var_ux = 0)
estimate_var_qyi(qya = .9, var_qya = .04, rxyi = .4, var_rxyi = 0, ux = .8, var_ux = 0)
estimate_var_ryyi(ryya = .9, var_ryya = .04, rxyi = .4, var_rxyi = 0, ux = .8, var_ux = 0)
</code></pre>

<hr>
<h2 id='estimate_var_rho_int'>Non-linear estimate of variance of <code class="reqn">\rho</code> corrected for psychometric artifacts using numeric integration</h2><span id='topic+estimate_var_rho_int'></span><span id='topic+estimate_var_rho_int_meas'></span><span id='topic+estimate_var_rho_int_uvdrr'></span><span id='topic+estimate_var_rho_int_uvirr'></span><span id='topic+estimate_var_rho_int_bvirr'></span><span id='topic+estimate_var_rho_int_bvdrr'></span><span id='topic+estimate_var_rho_int_rb'></span>

<h3>Description</h3>

<p>Functions to estimate the variance of <code class="reqn">\rho</code> corrected for psychometric artifacts. These functions integrate over the residual distribution of correlations from an interactive artifact-distribution meta-analysis to non-linearly estimate the variance of <code class="reqn">\rho</code>.
</p>
<p>Available functions include:
</p>

<ul>
<li><p><code>estimate_var_rho_int_meas</code><br /> Variance of <code class="reqn">\rho</code> corrected for measurement error only
</p>
</li>
<li><p><code>estimate_var_rho_int_uvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate direct range restriction (i.e., Case II) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_int_bvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate direct range restriction and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_int_uvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate indirect range restriction (i.e., Case IV) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_int_bvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate indirect range restriction (i.e., Case V) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_int_rb</code><br /> Variance of <code class="reqn">\rho</code> corrected using Raju and Burke's correction for direct range restriction and measurement error
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_var_rho_int_meas(mean_qx, mean_qy, var_res)

estimate_var_rho_int_uvdrr(
  mean_rxyi,
  mean_rtpa,
  mean_qxa,
  mean_qyi,
  mean_ux,
  var_res
)

estimate_var_rho_int_uvirr(
  mean_rxyi,
  mean_rtpa,
  mean_qxi,
  mean_qyi,
  mean_ut,
  var_res
)

estimate_var_rho_int_bvirr(mean_qxa, mean_qya, mean_ux, mean_uy, var_res)

estimate_var_rho_int_bvdrr(
  mean_rxyi,
  mean_rtpa,
  mean_qxa,
  mean_qya,
  mean_ux,
  mean_uy,
  var_res
)

estimate_var_rho_int_rb(
  mean_rxyi,
  mean_rtpa,
  mean_qx,
  mean_qy,
  mean_ux,
  var_res
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qx">mean_qx</code></td>
<td>
<p>Mean square root of reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qy">mean_qy</code></td>
<td>
<p>Mean square root of reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_var_res">var_res</code></td>
<td>
<p>Residual variance from an interative artifact distribution (i.e., variance of observed correlations minus predicted error variance and predicted artifact variance).</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_rxyi">mean_rxyi</code></td>
<td>
<p>Mean observed correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_rtpa">mean_rtpa</code></td>
<td>
<p>Mean corrected correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qxa">mean_qxa</code></td>
<td>
<p>Mean square root of unrestricted reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qyi">mean_qyi</code></td>
<td>
<p>Mean square root of restricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_ux">mean_ux</code></td>
<td>
<p>Mean observed-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qxi">mean_qxi</code></td>
<td>
<p>Mean square root of restricted reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_ut">mean_ut</code></td>
<td>
<p>Mean true-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_qya">mean_qya</code></td>
<td>
<p>Mean square root of unrestricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_int_+3A_mean_uy">mean_uy</code></td>
<td>
<p>Mean observed-score u ratio for Y.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of non-linear estimates of the variance of rho.
</p>


<h3>Notes</h3>

<p><code>estimate_var_rho_int_meas</code> and <code>estimate_var_rho_int_bvirr</code> do not make use of numeric integration because they are linear functions.
</p>


<h3>References</h3>

<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425–438. doi: <a href="https://doi.org/10.1037/0021-9010.79.3.425">10.1037/0021-9010.79.3.425</a>
</p>

<hr>
<h2 id='estimate_var_rho_tsa'>Taylor Series Approximation of variance of <code class="reqn">\rho</code> corrected for psychometric artifacts</h2><span id='topic+estimate_var_rho_tsa'></span><span id='topic+estimate_var_rho_tsa_meas'></span><span id='topic+estimate_var_rho_tsa_uvdrr'></span><span id='topic+estimate_var_rho_tsa_bvdrr'></span><span id='topic+estimate_var_rho_tsa_uvirr'></span><span id='topic+estimate_var_rho_tsa_bvirr'></span><span id='topic+estimate_var_rho_tsa_rb1'></span><span id='topic+estimate_var_rho_tsa_rb2'></span>

<h3>Description</h3>

<p>Functions to estimate the variance of <code class="reqn">\rho</code> corrected for psychometric artifacts.
These functions use Taylor series approximations (i.e., the delta method) to estimate the variance in observed effect sizes predictable from the variance in artifact distributions based on the partial derivatives.
</p>
<p>The available Taylor-series functions include:
</p>

<ul>
<li><p><code>estimate_var_rho_tsa_meas</code><br /> Variance of <code class="reqn">\rho</code> corrected for measurement error only
</p>
</li>
<li><p><code>estimate_var_rho_tsa_uvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate direct range restriction (i.e., Case II) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_tsa_bvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate direct range restriction and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_tsa_uvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate indirect range restriction (i.e., Case IV) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_tsa_bvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate indirect range restriction (i.e., Case V) and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_tsa_rb1</code><br /> Variance of <code class="reqn">\rho</code> corrected using Raju and Burke's TSA1 correction for direct range restriction and measurement error
</p>
</li>
<li><p><code>estimate_var_rho_tsa_rb2</code><br /> Variance of <code class="reqn">\rho</code> corrected using Raju and Burke's TSA2 correction for direct range restriction and measurement error. Note that a typographical error in Raju and Burke's article has been corrected in this function so as to compute appropriate partial derivatives.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_var_rho_tsa_meas(
  mean_rtp,
  var_rxy,
  var_e,
  mean_qx = 1,
  var_qx = 0,
  mean_qy = 1,
  var_qy = 0,
  ...
)

estimate_var_rho_tsa_uvdrr(
  mean_rtpa,
  var_rxyi,
  var_e,
  mean_ux = 1,
  var_ux = 0,
  mean_qxa = 1,
  var_qxa = 0,
  mean_qyi = 1,
  var_qyi = 0,
  ...
)

estimate_var_rho_tsa_bvdrr(
  mean_rtpa,
  var_rxyi,
  var_e = 0,
  mean_ux = 1,
  var_ux = 0,
  mean_uy = 1,
  var_uy = 0,
  mean_qxa = 1,
  var_qxa = 0,
  mean_qya = 1,
  var_qya = 0,
  ...
)

estimate_var_rho_tsa_uvirr(
  mean_rtpa,
  var_rxyi,
  var_e,
  mean_ut = 1,
  var_ut = 0,
  mean_qxa = 1,
  var_qxa = 0,
  mean_qyi = 1,
  var_qyi = 0,
  ...
)

estimate_var_rho_tsa_bvirr(
  mean_rtpa,
  var_rxyi,
  var_e = 0,
  mean_ux = 1,
  var_ux = 0,
  mean_uy = 1,
  var_uy = 0,
  mean_qxa = 1,
  var_qxa = 0,
  mean_qya = 1,
  var_qya = 0,
  sign_rxz = 1,
  sign_ryz = 1,
  ...
)

estimate_var_rho_tsa_rb1(
  mean_rtpa,
  var_rxyi,
  var_e,
  mean_ux = 1,
  var_ux = 0,
  mean_rxx = 1,
  var_rxx = 0,
  mean_ryy = 1,
  var_ryy = 0,
  ...
)

estimate_var_rho_tsa_rb2(
  mean_rtpa,
  var_rxyi,
  var_e,
  mean_ux = 1,
  var_ux = 0,
  mean_qx = 1,
  var_qx = 0,
  mean_qy = 1,
  var_qy = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_rtp">mean_rtp</code></td>
<td>
<p>Mean corrected correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_rxy">var_rxy</code></td>
<td>
<p>Variance of observed correlations.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_e">var_e</code></td>
<td>
<p>Error variance of observed correlations</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_qx">mean_qx</code></td>
<td>
<p>Mean square root of reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_qx">var_qx</code></td>
<td>
<p>Variance of square roots of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_qy">mean_qy</code></td>
<td>
<p>Mean square root of reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_qy">var_qy</code></td>
<td>
<p>Variance of square roots of reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_rtpa">mean_rtpa</code></td>
<td>
<p>Mean corrected correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_rxyi">var_rxyi</code></td>
<td>
<p>Variance of observed correlations.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_ux">mean_ux</code></td>
<td>
<p>Mean observed-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_ux">var_ux</code></td>
<td>
<p>Variance of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_qxa">mean_qxa</code></td>
<td>
<p>Mean square root of unrestricted reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_qxa">var_qxa</code></td>
<td>
<p>Variance of square roots of unrestricted reliability estimates for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_qyi">mean_qyi</code></td>
<td>
<p>Mean square root of restricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_qyi">var_qyi</code></td>
<td>
<p>Variance of square roots of restricted reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_uy">mean_uy</code></td>
<td>
<p>Mean observed-score u ratio for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_uy">var_uy</code></td>
<td>
<p>Variance of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_qya">mean_qya</code></td>
<td>
<p>Mean square root of unrestricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_qya">var_qya</code></td>
<td>
<p>Variance of square roots of unrestricted reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_ut">mean_ut</code></td>
<td>
<p>Mean true-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_ut">var_ut</code></td>
<td>
<p>Variance of true-score u ratios for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_rxx">mean_rxx</code></td>
<td>
<p>Mean reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_rxx">var_rxx</code></td>
<td>
<p>Variance of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_mean_ryy">mean_ryy</code></td>
<td>
<p>Mean reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_rho_tsa_+3A_var_ryy">var_ryy</code></td>
<td>
<p>Variance of reliability estimates for Y.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>######## Measurement error only ########
</p>
<p>The attenuation formula for measurement error is
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY}=\rho_{TP}q_{X}q_{Y}</code>
</p>

<p>where <code class="reqn">\rho_{XY}</code> is an observed correlation, <code class="reqn">\rho_{TP}</code> is a true-score correlation, and <code class="reqn">q_{X}</code> and <code class="reqn">q_{Y}</code> are the square roots of reliability coefficients for X and Y, respectively.
</p>
<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP}} \approx \left[var_{r_{XY}}-var_{e}-\left(b_{1}^{2}var_{q_{X}}+b_{2}^{2}var_{q_{Y}}\right)\right]/b_{3}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, and <code class="reqn">b_{3}</code> are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">q_{X}</code>, <code class="reqn">q_{Y}</code>, and <code class="reqn">\rho_{TP}</code>, respectively.
The first-order partial derivatives of the attenuation formula are:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{XY}}{\partial q_{X}}=\rho_{TP}q_{Y}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{XY}}{\partial q_{Y}}=\rho_{TP}q_{X}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{XY}}{\partial\rho_{TP}}=q_{X}q_{Y}</code>
</p>

<p>######## Univariate direct range restriction (UVDRR; i.e., Case II) ########
</p>
<p>The UVDRR attenuation procedure may be represented as
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{\rho_{TP_{a}}q_{Y_{i}}q_{X_{a}}u_{X}}{\sqrt{\rho_{TP_{a}}^{2}q_{X_{a}}^{2}\left(u_{X}^{2}-1\right)+1}}</code>
</p>

<p>The attenuation formula can also be represented as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\rho_{TP_{a}}q_{Y_{i}}q_{X_{a}}u_{X}A</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">A=\frac{1}{\sqrt{\rho_{TP_{a}}^{2}q_{X_{a}}^{2}\left(u_{X}^{2}-1\right)+1}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(b_{1}^{2}var_{q_{X_{a}}}+b_{2}^{2}var_{q_{Y_{i}}}+b_{3}^{2}var_{u_{X}}\right)\right]/b_{4}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, <code class="reqn">b_{3}</code>, and <code class="reqn">b_{4}</code> are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{i}}</code>, <code class="reqn">u_{X}</code>, and <code class="reqn">\rho_{TP_{a}}</code>, respectively.
The first-order partial derivatives of the attenuation formula are:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{XY_{i}}}{\partial q_{X_{a}}}=\rho_{TP_{a}}q_{Y_{i}}u_{X}A^{3}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{XY_{i}}}{\partial q_{Y_{i}}}=\frac{\rho_{XY_{i}}}{q_{Y_{i}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{XY_{i}}}{\partial u_{X}}=-\rho_{TP_{a}}q_{Y_{i}}q_{X_{a}}\left(\rho_{TP_{a}}^{2}q_{X_{a}}^{2}-1\right)A^{3}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{4}=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=q_{Y_{i}}q_{X_{a}}u_{X}A^{3}</code>
</p>

<p>######## Univariate indirect range restriction (UVIRR; i.e., Case IV) ########
</p>
<p>Under univariate indirect range restriction, the attenuation formula yielding <code class="reqn">\rho_{XY_{i}}</code> is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{u_{T}q_{X_{a}}}{\sqrt{u_{T}^{2}q_{X_{a}}^{2}+1-q_{X_{a}}^{2}}}\frac{u_{T}\rho_{TP_{a}}}{\sqrt{u_{T}^{2}\rho_{TP_{a}}^{2}+1-\rho_{TP_{a}}^{2}}}</code>
</p>

<p>The attenuation formula can also be represented as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=q_{X_{a}}q_{Y_{i}}\rho_{TP_{a}}u_{T}^{2}AB</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">A=\frac{1}{\sqrt{u_{T}^{2}q_{X_{a}}^{2}+1-q_{X_{a}}^{2}}}</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">B=\frac{1}{\sqrt{u_{T}^{2}\rho_{TP_{a}}^{2}+1-\rho_{TP_{a}}^{2}}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(b_{1}^{2}var_{q_{X_{a}}}+b_{2}^{2}var_{q_{Y_{i}}}+b_{3}^{2}var_{u_{T}}\right)\right]/b_{4}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, <code class="reqn">b_{3}</code>, and <code class="reqn">b_{4}</code> are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{i}}</code>, <code class="reqn">u_{T}</code>, and <code class="reqn">\rho_{TP_{a}}</code>, respectively.
The first-order partial derivatives of the attenuation formula are:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{XY_{i}}}{\partial q_{X_{a}}}=\frac{\rho_{XY_{i}}}{q_{X_{a}}}-\rho_{XY_{i}}q_{X_{a}}B^{2}\left(u_{T}^{2}-1\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{XY_{i}}}{\partial q_{Y_{i}}}=\frac{\rho_{XY_{i}}}{q_{Y_{i}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{XY_{i}}}{\partial u_{T}}=\frac{2\rho_{XY_{i}}}{u_{T}}-\rho_{XY_{i}}u_{T}q_{X_{a}}^{2}B^{2}-\rho_{XY_{i}}u_{T}\rho_{TP_{a}}^{2}A^{2}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{4}=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=\frac{\rho_{XY_{i}}}{\rho_{TP_{a}}}-\rho_{XY_{i}}\rho_{TP_{a}}A^{2}\left(u_{T}^{2}-1\right)</code>
</p>

<p>######## Bivariate direct range restriction (BVDRR) ########
</p>
<p>Under bivariate direct range restriction, the attenuation formula yielding <code class="reqn">\rho_{XY_{i}}</code> is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{A+\rho_{TP_{a}}^{2}q_{X_{a}}q_{Y_{a}}-\frac{1}{q_{X_{a}}q_{Y_{a}}}}{2\rho_{TP_{a}}u_{X}u_{Y}}</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">A=\sqrt{\left(\frac{1}{q_{X_{a}}q_{Y_{a}}}-\rho_{TP_{a}}^{2}q_{X_{a}}q_{Y_{a}}\right)^{2}+4\rho_{TP_{a}}u_{X}^{2}u_{Y}^{2}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(b_{1}^{2}var_{q_{X_{a}}}+b_{2}^{2}var_{q_{Y_{i}}}+b_{3}^{2}var_{u_{X}}+b_{4}^{2}var_{u_{Y}}\right)\right]/b_{5}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, <code class="reqn">b_{3}</code>, <code class="reqn">b_{4}</code>, and <code class="reqn">b_{5}</code> are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{a}}</code>, <code class="reqn">u_{X}</code>, <code class="reqn">u_{Y}</code>, and <code class="reqn">\rho_{TP_{a}}</code>, respectively.
First, we define terms to simplify the computation of partial derivatives:
</p>
<p style="text-align: center;"><code class="reqn">B=\left(\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}+q_{X_{a}}q_{Y_{a}}A-1\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">C=2\rho_{TP_{a}}q_{X_{a}}^{2}q_{Y_{a}}^{2}u_{X}u_{Y}A</code>
</p>

<p>The first-order partial derivatives of the attenuation formula are:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{XY_{i}}}{\partial q_{X_{a}}}=\frac{\left(\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}+1\right)B}{q_{X_{a}}C}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{XY_{i}}}{\partial q_{Y_{i}}}=\frac{\left(\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}+1\right)B}{q_{Y_{a}}C}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{XY_{i}}}{\partial u_{X}}=-\frac{\left(\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}-1\right)\left(\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}+1\right)B}{u_{X}C}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{4}=\frac{\partial\rho_{XY_{i}}}{\partial u_{Y}}=-\frac{\left(\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}-1\right)\left(\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}+1\right)B}{u_{Y}C}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{5}=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=\frac{\left(\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}+1\right)B}{\rho_{TP_{a}}C}</code>
</p>

<p>######## Bivariate indirect range restriction (BVIRR; i.e., Case V) ########
</p>
<p>Under bivariate indirect range restriction, the attenuation formula yielding <code class="reqn">\rho_{XY_{i}}</code> is:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}-\lambda\sqrt{\left|1-u_{X}^{2}\right|\left|1-u_{Y}^{2}\right|}}{u_{X}u_{Y}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(b_{1}^{2}var_{q_{X_{a}}}+b_{2}^{2}var_{q_{Y_{i}}}+b_{3}^{2}var_{u_{X}}+b_{4}^{2}var_{u_{Y}}\right)\right]/b_{5}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, <code class="reqn">b_{3}</code>, <code class="reqn">b_{4}</code>, and <code class="reqn">b_{5}</code> are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{a}}</code>, <code class="reqn">u_{X}</code>, <code class="reqn">u_{Y}</code>, and <code class="reqn">\rho_{TP_{a}}</code>, respectively.
First, we define terms to simplify the computation of partial derivatives:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{XY_{i}}}{\partial q_{X_{a}}}=\frac{\rho_{TP_{a}}q_{Y_{a}}}{u_{X}u_{Y}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{XY_{i}}}{\partial q_{Y_{i}}}=\frac{\rho_{TP_{a}}q_{X_{a}}}{u_{X}u_{Y}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{XY_{i}}}{\partial u_{X}}=\frac{\lambda\left(1-u_{X}^{2}\right)\sqrt{\left|1-u_{Y}^{2}\right|}}{u_{Y}\left|1-u_{X}^{2}\right|^{1.5}}-\frac{\rho_{XY_{i}}}{u_{X}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{4}=\frac{\partial\rho_{XY_{i}}}{\partial u_{Y}}=\frac{\lambda\left(1-u_{Y}^{2}\right)\sqrt{\left|1-u_{X}^{2}\right|}}{u_{X}\left|1-u_{Y}^{2}\right|^{1.5}}-\frac{\rho_{XY_{i}}}{u_{Y}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{5}=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=\frac{q_{X_{a}}q_{Y_{a}}}{u_{X}u_{Y}}</code>
</p>

<p>######## Raju and Burke's TSA1 procedure ########
</p>
<p>Raju and Burke's attenuation formula may be represented as
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{\rho_{TP_{a}}u_{X}\sqrt{\rho_{XX_{a}}\rho_{YY_{a}}}}{\sqrt{\rho_{TP_{a}}^{2}\rho_{XX_{a}}\rho_{YY_{a}}u_{X}^{2}-\rho_{TP_{a}}^{2}\rho_{XX_{a}}\rho_{YY_{a}}+1}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(B^{2}var_{\rho_{YY_{a}}}+C^{2}var_{\rho_{XX_{a}}}+D^{2}var_{u_{X}}\right)\right]/A^{2}</code>
</p>

<p>where A, B, C, and D are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">\rho_{TP_{a}}</code>, <code class="reqn">\rho_{XX_{a}}</code>, <code class="reqn">\rho_{YY_{a}}</code>, and <code class="reqn">u_{X}</code>, respectively.
The first-order partial derivatives of the attenuation formula are:
</p>
<p style="text-align: center;"><code class="reqn">A=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=\frac{\rho_{XY_{i}}}{\rho_{TP_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{\rho_{TP_{a}}u_{X}^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">B=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{YY_{a}}}=\frac{1}{2}\left(\frac{\rho_{XY_{i}}}{\rho_{YY_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{\rho_{YY_{a}}u_{X}^{2}}\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">C=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{XX_{a}}}=\frac{1}{2}\left(\frac{\rho_{XY_{i}}}{\rho_{XX_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{\rho_{XX_{a}}u_{X}^{2}}\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">D=\frac{\partial\rho_{XY_{i}}}{\partial u_{X}}=\frac{\rho_{XY_{i}}-\rho_{XY_{i}}^{3}}{u_{X}}</code>
</p>

<p>######## Raju and Burke's TSA2 procedure ########
</p>
<p>Raju and Burke's attenuation formula may be represented as
</p>
<p style="text-align: center;"><code class="reqn">\rho_{XY_{i}}=\frac{\rho_{TP_{a}}q_{X_{a}}q_{Y_{a}}u_{X}}{\sqrt{\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}u_{X}^{2}-\rho_{TP_{a}}^{2}q_{X_{a}}^{2}q_{Y_{a}}^{2}+1}}</code>
</p>

<p>The Taylor series approximation of the variance of <code class="reqn">\rho_{TP_{a}}</code> can be computed using the following linear equation,
</p>
<p style="text-align: center;"><code class="reqn">var_{\rho_{TP_{a}}}	\approx	\left[var_{r_{XY_{i}}}-var_{e}-\left(F^{2}var_{q_{Y_{a}}}+G^{2}var_{q_{X_{a}}}+H^{2}var_{u_{X}}\right)\right]/E^{2}</code>
</p>

<p>where E, F, G, and H are first-order partial derivatives of the attenuation formula with respect to <code class="reqn">\rho_{TP_{a}}</code>, <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{a}}</code>, and <code class="reqn">u_{X}</code>, respectively.
The first-order partial derivatives of the attenuation formula (with typographic errors in the original article corrected) are:
</p>
<p style="text-align: center;"><code class="reqn">E=\frac{\partial\rho_{XY_{i}}}{\partial\rho_{TP_{a}}}=\frac{\rho_{XY_{i}}}{\rho_{TP_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{\rho_{TP_{a}}u_{X}^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">F=\frac{\partial\rho_{XY_{i}}}{\partial q_{Y_{a}}}=\frac{\rho_{XY_{i}}}{q_{Y_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{q_{Y_{a}}u_{X}^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">G=\frac{\partial\rho_{XY_{i}}}{\partial q_{X_{a}}}=\frac{\rho_{XY_{i}}}{q_{X_{a}}}+\frac{\rho_{XY_{i}\left(1-u_{X}^{2}\right)}^{3}}{q_{X_{a}}u_{X}^{2}}</code>
</p>

<p style="text-align: center;"><code class="reqn">H=\frac{\partial\rho_{XY_{i}}}{\partial u_{X}}=\frac{\rho_{XY_{i}}-\rho_{XY_{i}}^{3}}{u_{X}}</code>
</p>



<h3>Value</h3>

<p>Vector of meta-analytic variances estimated via Taylor series approximation.
</p>


<h3>Notes</h3>

<p>A typographical error in Raju and Burke's article has been corrected in <code>estimate_var_rho_tsa_rb2</code> so as to compute appropriate partial derivatives.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006).
Implications of direct and indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612. doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_var_rho_tsa_meas(mean_rtp = .5, var_rxy = .02, var_e = .01,
                 mean_qx = .8, var_qx = .005,
                 mean_qy = .8, var_qy = .005)
estimate_var_rho_tsa_uvdrr(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                  mean_ux = .8, var_ux = .005,
                  mean_qxa = .8, var_qxa = .005,
                  mean_qyi = .8, var_qyi = .005)
estimate_var_rho_tsa_bvdrr(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                  mean_ux = .8, var_ux = .005,
                  mean_uy = .8, var_uy = .005,
                  mean_qxa = .8, var_qxa = .005,
                  mean_qya = .8, var_qya = .005)
estimate_var_rho_tsa_uvirr(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                  mean_ut = .8, var_ut = .005,
                  mean_qxa = .8, var_qxa = .005,
                  mean_qyi = .8, var_qyi = .005)
estimate_var_rho_tsa_bvirr(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                  mean_ux = .8, var_ux = .005,
                  mean_uy = .8, var_uy = .005,
                  mean_qxa = .8, var_qxa = .005,
                  mean_qya = .8, var_qya = .005,
                  sign_rxz = 1, sign_ryz = 1)
estimate_var_rho_tsa_rb1(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                mean_ux = .8, var_ux = .005,
                mean_rxx = .8, var_rxx = .005,
                mean_ryy = .8, var_ryy = .005)
estimate_var_rho_tsa_rb2(mean_rtpa = .5, var_rxyi = .02, var_e = .01,
                mean_ux = .8, var_ux = .005,
                mean_qx = .8, var_qx = .005,
                mean_qy = .8, var_qy = .005)
</code></pre>

<hr>
<h2 id='estimate_var_tsa'>Taylor Series Approximation of effect-size variances corrected for psychometric artifacts</h2><span id='topic+estimate_var_tsa'></span><span id='topic+estimate_var_tsa_meas'></span><span id='topic+estimate_var_tsa_uvdrr'></span><span id='topic+estimate_var_tsa_bvdrr'></span><span id='topic+estimate_var_tsa_uvirr'></span><span id='topic+estimate_var_tsa_bvirr'></span><span id='topic+estimate_var_tsa_rb1'></span><span id='topic+estimate_var_tsa_rb2'></span>

<h3>Description</h3>

<p>Functions to estimate the variances corrected for psychometric artifacts.
These functions use Taylor series approximations (i.e., the delta method) to estimate the corrected variance of an effect-size distribution.
</p>
<p>The available Taylor-series functions include:
</p>

<ul>
<li><p><code>estimate_var_tsa_meas</code><br /> Variance of <code class="reqn">\rho</code> corrected for measurement error only
</p>
</li>
<li><p><code>estimate_var_tsa_uvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate direct range restriction (i.e., Case II) and measurement error
</p>
</li>
<li><p><code>estimate_var_tsa_bvdrr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate direct range restriction and measurement error
</p>
</li>
<li><p><code>estimate_var_tsa_uvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for univariate indirect range restriction (i.e., Case IV) and measurement error
</p>
</li>
<li><p><code>estimate_var_tsa_bvirr</code><br /> Variance of <code class="reqn">\rho</code> corrected for bivariate indirect range restriction (i.e., Case V) and measurement error
</p>
</li>
<li><p><code>estimate_var_tsa_rb1</code><br /> Variance of <code class="reqn">\rho</code> corrected using Raju and Burke's TSA1 correction for direct range restriction and measurement error
</p>
</li>
<li><p><code>estimate_var_tsa_rb2</code><br /> Variance of <code class="reqn">\rho</code> corrected using Raju and Burke's TSA2 correction for direct range restriction and measurement error. Note that a typographical error in Raju and Burke's article has been corrected in this function so as to compute appropriate partial derivatives.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_var_tsa_meas(mean_rtp, var = 0, mean_qx = 1, mean_qy = 1, ...)

estimate_var_tsa_uvdrr(
  mean_rtpa,
  var = 0,
  mean_ux = 1,
  mean_qxa = 1,
  mean_qyi = 1,
  ...
)

estimate_var_tsa_bvdrr(
  mean_rtpa,
  var = 0,
  mean_ux = 1,
  mean_uy = 1,
  mean_qxa = 1,
  mean_qya = 1,
  ...
)

estimate_var_tsa_uvirr(
  mean_rtpa,
  var = 0,
  mean_ut = 1,
  mean_qxa = 1,
  mean_qyi = 1,
  ...
)

estimate_var_tsa_bvirr(
  mean_rtpa,
  var = 0,
  mean_ux = 1,
  mean_uy = 1,
  mean_qxa = 1,
  mean_qya = 1,
  sign_rxz = 1,
  sign_ryz = 1,
  ...
)

estimate_var_tsa_rb1(
  mean_rtpa,
  var = 0,
  mean_ux = 1,
  mean_rxx = 1,
  mean_ryy = 1,
  ...
)

estimate_var_tsa_rb2(
  mean_rtpa,
  var = 0,
  mean_ux = 1,
  mean_qx = 1,
  mean_qy = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_var_tsa_+3A_mean_rtp">mean_rtp</code></td>
<td>
<p>Mean corrected correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_var">var</code></td>
<td>
<p>Variance to be corrected for artifacts.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_qx">mean_qx</code></td>
<td>
<p>Mean square root of reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_qy">mean_qy</code></td>
<td>
<p>Mean square root of reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_rtpa">mean_rtpa</code></td>
<td>
<p>Mean corrected correlation.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_ux">mean_ux</code></td>
<td>
<p>Mean observed-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_qxa">mean_qxa</code></td>
<td>
<p>Mean square root of unrestricted reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_qyi">mean_qyi</code></td>
<td>
<p>Mean square root of restricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_uy">mean_uy</code></td>
<td>
<p>Mean observed-score u ratio for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_qya">mean_qya</code></td>
<td>
<p>Mean square root of unrestricted reliability for Y.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_ut">mean_ut</code></td>
<td>
<p>Mean true-score u ratio for X.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_rxx">mean_rxx</code></td>
<td>
<p>Mean reliability for X.</p>
</td></tr>
<tr><td><code id="estimate_var_tsa_+3A_mean_ryy">mean_ryy</code></td>
<td>
<p>Mean reliability for Y.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of variances corrected for mean artifacts via Taylor series approximation.
</p>


<h3>Notes</h3>

<p>A typographical error in Raju and Burke's article has been corrected in <code><a href="#topic+estimate_var_tsa_rb2">estimate_var_tsa_rb2()</a></code> so as to compute appropriate partial derivatives.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006).
Implications of direct and indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612. doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_var_tsa_meas(mean_rtp = .5, var = .02,
                 mean_qx = .8,
                 mean_qy = .8)
estimate_var_tsa_uvdrr(mean_rtpa = .5, var = .02,
                  mean_ux = .8,
                  mean_qxa = .8,
                  mean_qyi = .8)
estimate_var_tsa_bvdrr(mean_rtpa = .5, var = .02,
                  mean_ux = .8,
                  mean_uy = .8,
                  mean_qxa = .8,
                  mean_qya = .8)
estimate_var_tsa_uvirr(mean_rtpa = .5, var = .02,
                  mean_ut = .8,
                  mean_qxa = .8,
                  mean_qyi = .8)
estimate_var_tsa_bvirr(mean_rtpa = .5, var = .02,
                  mean_ux = .8,
                  mean_uy = .8,
                  mean_qxa = .8,
                  mean_qya = .8,
                  sign_rxz = 1, sign_ryz = 1)
estimate_var_tsa_rb1(mean_rtpa = .5, var = .02,
                mean_ux = .8,
                mean_rxx = .8,
                mean_ryy = .8)
estimate_var_tsa_rb2(mean_rtpa = .5, var = .02,
                mean_ux = .8,
                mean_qx = .8,
                mean_qy = .8)
</code></pre>

<hr>
<h2 id='filter_listnonnull'>Filter a list to remove NULL entries</h2><span id='topic+filter_listnonnull'></span>

<h3>Description</h3>

<p>Filter a list to remove NULL entries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_listnonnull(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_listnonnull_+3A_x">x</code></td>
<td>
<p>A list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with NULL entries removed.
</p>

<hr>
<h2 id='filter_ma'>Filter meta-analyses</h2><span id='topic+filter_ma'></span><span id='topic+filter_meta'></span>

<h3>Description</h3>

<p>Filter <code>psychmeta</code> meta-analysis objects based on specified criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_ma(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

filter_meta(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_ma_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A psychmeta meta-analysis object.</p>
</td></tr>
<tr><td><code id="filter_ma_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract? Can be either <code>"all"</code> to extract all meta-analyses in the object (default) or a list containing one or more of the following arguments:
</p>

<ul>
<li><p>construct: A list or vector of construct names to search for.
</p>
</li>
<li><p>construct_pair: A list of vectors of construct pairs to search for. <br />
(e.g., <code>list(c("X", "Y"), c("X", "Z"))</code> ).
</p>
</li>
<li><p>pair_id: A list or vector of numeric construct pair IDs (unique construct-pair indices).
</p>
</li>
<li><p>analysis_id: A list or vector of numeric analysis IDs (unique analysis indexes).
</p>
</li>
<li><p>k_min: A numeric value specifying the minimum <code>k</code> for extracted meta-analyses.
</p>
</li>
<li><p>N_min: A numeric value specifying the minimum <code>N</code> for extracted meta-analyses.
</p>
</li></ul>
</td></tr>
<tr><td><code id="filter_ma_+3A_match">match</code></td>
<td>
<p>Should extracted meta-analyses match all (default) or any of the criteria given in <code>analyses</code>?</p>
</td></tr>
<tr><td><code id="filter_ma_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="filter_ma_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>psychmeta</code> meta-analysis object with analyses matching the specified criteria.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id, citekey = NULL,
               moderators = moderator, data = data_r_meas_multi,
               impute_artifacts = FALSE, clean_artifacts = FALSE)
ma_obj &lt;- ma_r_ad(ma_obj, correct_rr_x = FALSE, correct_rr_y = FALSE)

filter_ma(ma_obj, analyses="all")
filter_ma(ma_obj, analyses=list(construct="X"), match="all")
filter_ma(ma_obj, analyses=list(construct="X", k_min=21), match="any")
filter_ma(ma_obj, analyses=list(construct="X", k_min=21), match="all")
</code></pre>

<hr>
<h2 id='filter_r'>Filter to detect and remove impossible values in vectors of correlations and sample sizes.</h2><span id='topic+filter_r'></span>

<h3>Description</h3>

<p>Filter to detect and remove impossible values in vectors of correlations and sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_r(r_vec, n_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_r_+3A_r_vec">r_vec</code></td>
<td>
<p>Vector of correlations.</p>
</td></tr>
<tr><td><code id="filter_r_+3A_n_vec">n_vec</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of filtered correlations and sample sizes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## filter_r(r_vec = c(-.3, .5, 1.1), n_vec = c(100, 100, 100))
## filter_r(r_vec = c(-.3, .5, .8), n_vec = c(Inf, 100, 100))
## filter_r(r_vec = c(-.3, .5, .8), n_vec = c(2, 100, 100))
</code></pre>

<hr>
<h2 id='filter_r_bar'>Filter to detect and remove impossible values in vectors of meta-analytic mean correlations and numbers of studies</h2><span id='topic+filter_r_bar'></span>

<h3>Description</h3>

<p>Filter to detect and remove impossible values in vectors of meta-analytic mean correlations and numbers of studies
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_r_bar(r_bar_vec, k_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_r_bar_+3A_r_bar_vec">r_bar_vec</code></td>
<td>
<p>Vector of meta-analytic mean correlations.</p>
</td></tr>
<tr><td><code id="filter_r_bar_+3A_k_vec">k_vec</code></td>
<td>
<p>Vector of numbers of studies (k values).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of filtered correlations and numbers of studies.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## filter_r_bar(r_bar_vec = c(-.3, .5, 1.1), k_vec = c(100, 100, 100))
## filter_r_bar(r_bar_vec = c(-.3, .5, .8), k_vec = c(Inf, 100, 100))
## filter_r_bar(r_bar_vec = c(-.3, .5, .8), k_vec = c(2, 100, 100))
</code></pre>

<hr>
<h2 id='filter_rel'>Filter to remove impossible values from vectors of reliabilities and corresponding weights.</h2><span id='topic+filter_rel'></span>

<h3>Description</h3>

<p>Filter to remove impossible values from vectors of reliabilities and corresponding weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_rel(rel_vec, wt_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_rel_+3A_rel_vec">rel_vec</code></td>
<td>
<p>Vector of reliability estimates.</p>
</td></tr>
<tr><td><code id="filter_rel_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Vector of weights corresponding to the elements of rel_vec.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of filtered reliabilities and weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## filter_rel(rel_vec = c(0, .8), wt_vec = c(80, 100))
## filter_rel(rel_vec = c(.7, .8), wt_vec = c(-80, 100))
</code></pre>

<hr>
<h2 id='filter_u'>Filter to remove impossible values from vectors of u ratios and corresponding weights.</h2><span id='topic+filter_u'></span>

<h3>Description</h3>

<p>Filter to remove impossible values from vectors of u ratios and corresponding weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_u(u_vec, wt_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_u_+3A_u_vec">u_vec</code></td>
<td>
<p>Vector of u ratios.</p>
</td></tr>
<tr><td><code id="filter_u_+3A_wt_vec">wt_vec</code></td>
<td>
<p>Vector of weights corresponding to the elements of u_vec</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of filtered u ratios and weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## filter_u(u_vec = c(0, .8), wt_vec = c(80, 100))
## filter_u(u_vec = c(.7, .8), wt_vec = c(-80, 100))
</code></pre>

<hr>
<h2 id='format_long'>Create long-format datasets in simulate_r_database</h2><span id='topic+format_long'></span>

<h3>Description</h3>

<p>Create long-format datasets in simulate_r_database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_long(
  x,
  param,
  sample_id,
  var_names,
  show_applicant,
  decimals = 2,
  noalpha = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_long_+3A_x">x</code></td>
<td>
<p>Simulation list</p>
</td></tr>
<tr><td><code id="format_long_+3A_param">param</code></td>
<td>
<p>Is simulation data parameter data (TRUE) or sample data (FALSE)?</p>
</td></tr>
<tr><td><code id="format_long_+3A_sample_id">sample_id</code></td>
<td>
<p>What is the ID associated with the sample?</p>
</td></tr>
<tr><td><code id="format_long_+3A_var_names">var_names</code></td>
<td>
<p>Variables to pull from simulation list</p>
</td></tr>
<tr><td><code id="format_long_+3A_show_applicant">show_applicant</code></td>
<td>
<p>Should applicant data be shown for sample statistics (TRUE) or suppressed (FALSE)?</p>
</td></tr>
<tr><td><code id="format_long_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimals to which statistical results (not parameters) should be rounded. Rounding to 2 decimal places best captures the precision of data available from published primary research.</p>
</td></tr>
<tr><td><code id="format_long_+3A_noalpha">noalpha</code></td>
<td>
<p>Logical scalar indicating whether or not alpha is in the database.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of results
</p>

<hr>
<h2 id='format_num'>Format numbers for presentation</h2><span id='topic+format_num'></span>

<h3>Description</h3>

<p>A function to format numbers and logical values as characters for display purposes.
Includes control over formatting of decimal digits, leading zeros, sign characters,
and characters to replace logical, NA, NaN, and Inf values. Factors are converted
to strings. Strings are returned verbatim.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_num(x, digits = 2L, decimal.mark = getOption("OutDec"),
              leading0 = "conditional", drop0integer = FALSE,
              neg.sign = "\u2212", pos.sign = "figure",
              big.mark = "\u202F", big.interval = 3L,
              small.mark = "\u202F", small.interval = 3L,
              na.mark = "\u2014", lgl.mark = c("+", "\u2212"),
              inf.mark = c("+\u221E", "\u2212\u221E") )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_num_+3A_x">x</code></td>
<td>
<p>A vector, matrix, or data.frame of numbers to format</p>
</td></tr>
<tr><td><code id="format_num_+3A_digits">digits</code></td>
<td>
<p>The number of decimal digits desired (used strictly; default: 2)</p>
</td></tr>
<tr><td><code id="format_num_+3A_decimal.mark">decimal.mark</code></td>
<td>
<p>The character to use for the decimal point (defaults to locale default: <code>getOption("OutDec")</code>)</p>
</td></tr>
<tr><td><code id="format_num_+3A_leading0">leading0</code></td>
<td>
<p>How to print leading zeros on decimals. Can be logical to print (<code>TRUE</code>) or suppress (<code>FALSE</code>) leading zeros or a character string to subsitute for leading zeros. If <code>"conditional"</code> (default), leading zeros are shown if a column contains any absolute values greater than 1 and suppressed otherwise. If <code>"figure"</code>, leading zeros are replaced with a figure space (<a href="https://unicode-table.com/en/2007/"><code>U+2007</code></a>) if a column contains any absolute values greater than 1 and suppressed otherwise. If <code>"figure_html"</code>, the same as <code>"figure"</code>, but using the HTML entity for figure space (useful for Windows users in some locales).</p>
</td></tr>
<tr><td><code id="format_num_+3A_drop0integer">drop0integer</code></td>
<td>
<p>Logical. Should trailing decimal zeros be dropped for integers?</p>
</td></tr>
<tr><td><code id="format_num_+3A_neg.sign">neg.sign</code></td>
<td>
<p>Character to use as negative sign. Defaults to minus-sign (<a href="https://unicode-table.com/en/2212/"><code>U+2212</code></a>).</p>
</td></tr>
<tr><td><code id="format_num_+3A_pos.sign">pos.sign</code></td>
<td>
<p>Character to use as positive sign. Set to <code>FALSE</code> to suppress. If <code>"figure"</code> (default), the positive sign is a figure-space (<a href="https://unicode-table.com/en/2007/"><code>U+2007</code></a>) if a column contains any negative numbers and suppressed otherwise. If <code>"figure_html"</code>, the same as <code>"figure"</code>, but using the HTML entity for figure space (useful for Windows users in some locales).</p>
</td></tr>
<tr><td><code id="format_num_+3A_big.mark">big.mark</code></td>
<td>
<p>Character to mark between each <code>big.interval</code> digits <em>before</em> the decimal point. Set to <code>FALSE</code> to suppress. Defaults to the SI/ISO 31-0 standard-recommened thin-spaces (<a href="https://unicode-table.com/en/202F/"><code>U+202F</code></a>).</p>
</td></tr>
<tr><td><code id="format_num_+3A_big.interval">big.interval</code></td>
<td>
<p>See <code>big.mark</code> above; defaults to 3.</p>
</td></tr>
<tr><td><code id="format_num_+3A_small.mark">small.mark</code></td>
<td>
<p>Character to mark between each <code>small.interval</code> digits <em>after</em> the decimal point. Set to <code>FALSE</code> to suppress. Defaults to the SI/ISO 31-0 standard-recommened thin-spaces (<a href="https://unicode-table.com/en/202F/"><code>U+202F</code></a>).</p>
</td></tr>
<tr><td><code id="format_num_+3A_small.interval">small.interval</code></td>
<td>
<p>See <code>small.mark</code> above; defaults to 3.</p>
</td></tr>
<tr><td><code id="format_num_+3A_na.mark">na.mark</code></td>
<td>
<p>Character to replace <code>NA</code> and <code>NaN</code> values. Defaults to em-dash (<a href="https://unicode-table.com/en/2014/"><code>U+2014</code></a>))</p>
</td></tr>
<tr><td><code id="format_num_+3A_lgl.mark">lgl.mark</code></td>
<td>
<p>A length 2 vector containing characters to replace <code>TRUE</code> and <code>FALSE</code>. Defaults to c(&quot;+&quot;, &quot;<a href="https://unicode-table.com/en/2212/"><code>U+2212</code></a>&quot;).</p>
</td></tr>
<tr><td><code id="format_num_+3A_inf.mark">inf.mark</code></td>
<td>
<p>A length 2 vector containing characters to replace <code>Inf</code> and <code>-Inf</code>. Defaults to c(&quot;+<a href="https://unicode-table.com/en/221E/"><code>U+221e</code></a>&quot;, &quot;<a href="https://unicode-table.com/en/2212/"><code>U+2212</code></a><a href="https://unicode-table.com/en/221E/"><code>U+221e</code></a>&quot;).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># format_num() converts numeric values to characters with the specified formatting options.
# By default, thousands digit groups are separated by thin spaces, negative signs are replaced
# with minus signs, and positive signs and leading zeros are replaced with figure spaces
# (which have the same width as numbers and minus signs). These options ensure that all
# results will align neatly in columns when tabled.
format_num(x = c(10000, 1000, 2.41, -1.20, 0.41, -0.20))

# By default, format_num() uses your computer locale's default decimal mark as
# the decimal point. To force the usage of "." instead (e.g., for submission to
# a U.S. journal), set decimal.mark = ".":
format_num(x = .41, decimal.mark = ".")

# By default, format_num() separates groups of large digits using thin spaces.
# This is following the international standard for scientific communication (SI/ISO 31-0),
# which advises against using "." or "," to seprate digits because doing so can lead
# to confusion for human and computer readers because "." and "," are also used
# as decimal marks in various countries. If you prefer to use commmas to separate
# large digit groups, set big.mark = ",":
format_num(x = 10000, big.mark = ",")
</code></pre>

<hr>
<h2 id='format_wide'>Create wide-format datasets in simulate_r_database</h2><span id='topic+format_wide'></span>

<h3>Description</h3>

<p>Create wide-format datasets in simulate_r_database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_wide(
  x,
  param,
  sample_id,
  var_names,
  show_applicant,
  decimals = 2,
  noalpha = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_wide_+3A_x">x</code></td>
<td>
<p>Simulation list</p>
</td></tr>
<tr><td><code id="format_wide_+3A_param">param</code></td>
<td>
<p>Is simulation data parameter data (TRUE) or sample data (FALSE)?</p>
</td></tr>
<tr><td><code id="format_wide_+3A_sample_id">sample_id</code></td>
<td>
<p>What is the ID associated with the sample?</p>
</td></tr>
<tr><td><code id="format_wide_+3A_var_names">var_names</code></td>
<td>
<p>Variables to pull from simulation list</p>
</td></tr>
<tr><td><code id="format_wide_+3A_show_applicant">show_applicant</code></td>
<td>
<p>Should applicant data be shown for sample statistics (TRUE) or suppressed (FALSE)?</p>
</td></tr>
<tr><td><code id="format_wide_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimals to which statistical results (not parameters) should be rounded. Rounding to 2 decimal places best captures the precision of data available from published primary research.</p>
</td></tr>
<tr><td><code id="format_wide_+3A_noalpha">noalpha</code></td>
<td>
<p>Logical scalar indicating whether or not alpha is in the database.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of results
</p>

<hr>
<h2 id='generate_bib'>Generate a list of references included in meta-analyses</h2><span id='topic+generate_bib'></span>

<h3>Description</h3>

<p>This function generates a list of studies contributing to a meta-analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_bib(
  ma_obj = NULL,
  bib = NULL,
  title.bib = NULL,
  style = "apa",
  additional_citekeys = NULL,
  file = NULL,
  output_dir = getwd(),
  output_format = c("word", "html", "pdf", "text", "odt", "rmd", "biblatex",
    "citekeys"),
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  save_build_files = FALSE,
  header = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_bib_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A psychmeta meta-analysis object with <code>citekeys</code> supplied.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_bib">bib</code></td>
<td>
<p>A BibTeX file containing the citekeys for the meta-analyses.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_title.bib">title.bib</code></td>
<td>
<p>The title to give to the bibliography. If <code>NULL</code>, defaults to &quot;Sources Contributing to Meta-Analyses&quot;</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_style">style</code></td>
<td>
<p>What style should references be formatted in? Can be a file path or URL for a <a href="https://github.com/citation-style-language/styles">CSL citation style</a> or the style ID for any style available from the <a href="https://www.zotero.org/styles">Zotero Style Repository</a>. Defaults to APA style. (Retrieving a style by ID requires an internet connection. If unavailable, references will be rendered in Chicago style.).</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_additional_citekeys">additional_citekeys</code></td>
<td>
<p>Additional citekeys to include in the reference list.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_file">file</code></td>
<td>
<p>The filename or filepath for the output file. If <code>NULL</code>, function will output directly to the R console (if <code>output_format</code> is <code>"text"</code>, a tibble with basic citation information; if <code>"citekeys"</code>, the citekeys for included sources; otherwise, code to generate the bibliography in an RMarkdown document).</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_output_dir">output_dir</code></td>
<td>
<p>The filepath for the output file. Defaults to the current working directory.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_output_format">output_format</code></td>
<td>
<p>The format of the output reference list. Available options are Word (default), HTML, PDF (requires LaTeX to be installed), ODT, or Rmarkdown, plain text, and BibLaTeX. Returning only the item citekeys is also possible. You can also specify the full name of another RMarkdown <a href="rmarkdown.html#topic+output_format">output_format</a>.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract references for? See <code><a href="#topic+filter_ma">filter_ma()</a></code> for details.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_match">match</code></td>
<td>
<p>Match <code>"all"</code> or <code>"any"</code> of the filter criteria? See <code><a href="#topic+filter_ma">filter_ma()</a></code> for details.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_save_build_files">save_build_files</code></td>
<td>
<p>Should the BibTeX and RMarkdown files used to generate the bibliography be saved (default: <code>FALSE</code>; always <code>TRUE</code> if file is <code>NULL</code>)?</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_header">header</code></td>
<td>
<p>A list of YAML header parameters to pass to <code><a href="rmarkdown.html#topic+render">rmarkdown::render()</a></code>.</p>
</td></tr>
<tr><td><code id="generate_bib_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="rmarkdown.html#topic+render">rmarkdown::render()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing a tibble of bibtex reference data. Additionally, a reference list formatted in the requested style and output_format is exported (or printed if file is &quot;console&quot;).
</p>


<h3>See Also</h3>

<p>Other output functions: 
<code><a href="#topic+metabulate_rmd_helper">metabulate_rmd_helper</a>()</code>,
<code><a href="#topic+metabulate">metabulate</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Run a meta-analysis using ma_r() and include a citekey argument to provide
## citation information for each source contributing to the meta-analyses.
ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               moderators = moderator, citekey = citekey, data = data_r_meas_multi)

## Next, use generate_bib() to generate the bibliography for the retained studies.
## The bib argument is the BibTeX or BibLaTeX .bib file containing the full
## reference information for each of the citekeys included in the meta-analysis database.
generate_bib(ma_obj, bib = system.file("templates/sample_bibliography.bib", package="psychmeta"),
             file = "sample bibliography", output_dir = tempdir(), output_format = "word")

## End(Not run)
</code></pre>

<hr>
<h2 id='generate_directory'>Generate a system of folders from a file path to a new directory</h2><span id='topic+generate_directory'></span>

<h3>Description</h3>

<p>This function is intended to be helpful in simulations when directories need to be created and named according to values that are used or created within the simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_directory(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_directory_+3A_path">path</code></td>
<td>
<p>The path to the directory to be created</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Creates a system of folders to a new directory.
</p>

<hr>
<h2 id='get_stuff'>Extract results from a psychmeta meta-analysis object</h2><span id='topic+get_stuff'></span><span id='topic+get_escalc'></span><span id='topic+get_metafor'></span><span id='topic+get_metatab'></span><span id='topic+get_ad'></span><span id='topic+get_followup'></span><span id='topic+get_heterogeneity'></span><span id='topic+get_leave1out'></span><span id='topic+get_cumulative'></span><span id='topic+get_bootstrap'></span><span id='topic+get_metareg'></span><span id='topic+get_matrix'></span><span id='topic+get_plots'></span>

<h3>Description</h3>

<p>Functions to extract specific results from a meta-analysis tibble.
This family of functions harvests information from meta-analysis objects and returns it as lists or tibbles that are easily navigable.
</p>
<p>Available functions include:
</p>

<ul>
<li><p><code>get_stuff</code><br /> Wrapper function for all other &quot;get_&quot; functions.
</p>
</li>
<li><p><code>get_metatab</code><br /> Retrieve list of meta-analytic tables.
</p>
</li>
<li><p><code>get_ad</code><br /> Retrieve list of artifact-distribution objects or a summary table of artifact descriptive statistics.
</p>
</li>
<li><p><code>get_plots</code><br /> Retrieve list of meta-analytic plots.
</p>
</li>
<li><p><code>get_escalc</code><br /> Retrieve list of escalc objects (i.e., effect-size data) for use with <span class="pkg">metafor</span>.
</p>
</li>
<li><p><code>get_metafor</code><br /> Alias for <code>get_escalc</code>.
</p>
</li>
<li><p><code>get_followup</code><br /> Retrieve list of follow-up analyses.
</p>
</li>
<li><p><code>get_leave1out</code><br /> Retrieve list of leave-one-out meta-analyses (special case of <code>get_followup</code>).
</p>
</li>
<li><p><code>get_cumulative</code><br /> Retrieve list of cumulative meta-analyses (special case of <code>get_followup</code>).
</p>
</li>
<li><p><code>get_bootstrap</code><br /> Retrieve list of bootstrap meta-analyses (special case of <code>get_followup</code>).
</p>
</li>
<li><p><code>get_metareg</code><br /> Retrieve list of meta-regression analyses (special case of <code>get_followup</code>).
</p>
</li>
<li><p><code>get_heterogeneity</code><br /> Retrieve list of heterogeneity analyses (special case of <code>get_followup</code>).
</p>
</li>
<li><p><code>get_matrix</code><br /> Retrieve a tibble of matrices summarizing the relationships among constructs (only applicable to meta-analyses with multiple constructs).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>get_stuff(
  ma_obj,
  what = c("metatab", "escalc", "metafor", "ad", "followup", "heterogeneity",
    "leave1out", "cumulative", "bootstrap", "metareg", "matrix", "plots"),
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ma_method = c("bb", "ic", "ad"),
  correction_type = c("ts", "vgx", "vgy"),
  moderators = FALSE,
  as_ad_obj = TRUE,
  inputs_only = FALSE,
  ad_type = c("tsa", "int"),
  follow_up = c("heterogeneity", "leave1out", "cumulative", "bootstrap", "metareg"),
  plot_types = c("funnel", "forest", "leave1out", "cumulative"),
  ...
)

get_escalc(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  moderators = TRUE,
  ...
)

get_metafor(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  moderators = TRUE,
  ...
)

get_metatab(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ma_method = c("bb", "ic", "ad"),
  correction_type = c("ts", "vgx", "vgy"),
  ...
)

get_ad(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ma_method = c("ad", "ic"),
  ad_type = c("tsa", "int"),
  as_ad_obj = FALSE,
  inputs_only = FALSE,
  ...
)

get_followup(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  follow_up = c("heterogeneity", "leave1out", "cumulative", "bootstrap", "metareg"),
  ...
)

get_heterogeneity(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_leave1out(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_cumulative(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_bootstrap(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_metareg(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_matrix(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ...
)

get_plots(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  plot_types = c("funnel", "forest", "leave1out", "cumulative"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stuff_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A psychmeta meta-analysis object.</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_what">what</code></td>
<td>
<p>For the <code>get_stuff()</code> function only: Character scalar telling <code>get_stuff()</code> what to get.
All suffixes from functions in the &quot;get_&quot; family can be passed as arguments to <code>what</code>:
&quot;metatab&quot;, &quot;escalc&quot;, &quot;metafor&quot;, &quot;ad&quot;, &quot;followup&quot;, &quot;heterogeneity&quot;, &quot;leave1out&quot;, &quot;cumulative&quot;, &quot;bootstrap&quot;, &quot;metareg&quot;, &quot;matrix&quot;, &quot;plots&quot;</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract? Can be either <code>"all"</code> to extract references for all meta-analyses in the object (default) or a list containing one or more of the following arguments:
</p>

<ul>
<li><p>construct: A list or vector of construct names to search for.
</p>
</li>
<li><p>construct_pair: A list of vectors of construct pairs to search for. <br />
(e.g., <code>list(c("X", "Y"), c("X", "Z"))</code>).
</p>
</li>
<li><p>pair_id: A list or vector of numeric construct pair IDs (unique construct-pair indices).
</p>
</li>
<li><p>analysis_id: A list or vector of numeric analysis IDs (unique analysis indexes).
</p>
</li>
<li><p>k_min: A numeric value specifying the minimum <code>k</code> for extracted meta-analyses.
</p>
</li>
<li><p>N_min: A numeric value specifying the minimum <code>N</code> for extracted meta-analyses.
</p>
</li></ul>
</td></tr>
<tr><td><code id="get_stuff_+3A_match">match</code></td>
<td>
<p>Should extracted meta-analyses match all (default) or any of the criteria given in <code>analyses</code>?</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_ma_method">ma_method</code></td>
<td>
<p>Meta-analytic methods to be included. Valid options are: &quot;bb&quot;, &quot;ic&quot;, and &quot;ad&quot;</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_correction_type">correction_type</code></td>
<td>
<p>Types of meta-analytic corrections to be incldued. Valid options are: &quot;ts&quot;, &quot;vgx&quot;, and &quot;vgy&quot;</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_moderators">moderators</code></td>
<td>
<p>Logical scalar that determines whether moderator variables should be included in escalc objects (<code>TRUE</code>; default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_as_ad_obj">as_ad_obj</code></td>
<td>
<p>Logical scalar that determines whether artifact information should be returned as artifact-distribution objects (<code>TRUE</code>) or a summary table of artifact-distribution descriptive statistics (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_inputs_only">inputs_only</code></td>
<td>
<p>Used only if <code>as_ad_obj = TRUE</code>: Logical scalar that determines whether artifact information should be returned as summaries of the raw input values (<code>TRUE</code>) or artifact values that may have been cross-corrected for range restriction and measurement error (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_ad_type">ad_type</code></td>
<td>
<p>Used only if <code>ma_method</code> = &quot;ic&quot;: Character value(s) indicating whether Taylor-series approximation artifact distributions (&quot;tsa&quot;) and/or interactive artifact distributions (&quot;int&quot;) should be retrieved.</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_follow_up">follow_up</code></td>
<td>
<p>Vector of follow-up analysis names (options are: &quot;heterogeneity&quot;, &quot;leave1out&quot;, &quot;cumulative&quot;, &quot;bootstrap&quot;, &quot;metareg&quot;).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_plot_types">plot_types</code></td>
<td>
<p>Vector of plot types (options are: &quot;funnel&quot;, &quot;forest&quot;, &quot;leave1out&quot;, &quot;cumulative&quot;; multiple allowed).</p>
</td></tr>
<tr><td><code id="get_stuff_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Selected set of results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Run meta-analysis:
ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name,
               sample_id = sample_id, citekey = NULL,
               moderators = moderator, data = data_r_meas_multi,
               impute_artifacts = FALSE, clean_artifacts = FALSE)
ma_obj &lt;- ma_r_ad(ma_obj, correct_rr_x = FALSE, correct_rr_y = FALSE)

## Run additional analyses:
ma_obj &lt;- heterogeneity(ma_obj)
ma_obj &lt;- sensitivity(ma_obj, boot_iter = 10, boot_ci_type = "norm")
ma_obj &lt;- metareg(ma_obj)
ma_obj &lt;- plot_funnel(ma_obj)
ma_obj &lt;- plot_forest(ma_obj)

## View summary:
summary(ma_obj)

## Extract selected analyses:
get_metatab(ma_obj)
get_matrix(ma_obj)
get_escalc(ma_obj)
get_bootstrap(ma_obj)
get_cumulative(ma_obj)
get_leave1out(ma_obj)
get_heterogeneity(ma_obj)
get_metareg(ma_obj)
get_plots(ma_obj)
get_ad(ma_obj, ma_method = "ic", as_ad_obj = TRUE)
get_ad(ma_obj, ma_method = "ic", as_ad_obj = FALSE)

## Same extractions as above, but using get_stuff() and the "what" argument:
get_stuff(ma_obj, what = "metatab")
get_stuff(ma_obj, what = "matrix")
get_stuff(ma_obj, what = "escalc")
get_stuff(ma_obj, what = "bootstrap")
get_stuff(ma_obj, what = "cumulative")
get_stuff(ma_obj, what = "leave1out")
get_stuff(ma_obj, what = "heterogeneity")
get_stuff(ma_obj, what = "metareg")
get_stuff(ma_obj, what = "plots")
get_stuff(ma_obj, what = "ad", ma_method = "ic", as_ad_obj = TRUE)
get_stuff(ma_obj, what = "ad", ma_method = "ic", as_ad_obj = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='heterogeneity'>Supplemental heterogeneity statistics for meta-analyses</h2><span id='topic+heterogeneity'></span>

<h3>Description</h3>

<p>This function computes a variety of supplemental statistics for meta-analyses.
The statistics here are included for interested users.
It is strongly recommended that heterogeneity in meta-analysis be interpreted using the <code class="reqn">SD_{res}</code>, <code class="reqn">SD_{\rho}</code>, and <code class="reqn">SD_{\delta}</code> statistics, along with corresponding credibility intervals, which are reported in the default <code>ma_obj</code> output (Wiernik et al., 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heterogeneity(
  ma_obj,
  es_failsafe = NULL,
  conf_level = attributes(ma_obj)$inputs$conf_level,
  var_res_ci_method = c("profile_var_es", "profile_Q", "normal_logQ"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="heterogeneity_+3A_ma_obj">ma_obj</code></td>
<td>
<p>Meta-analysis object.</p>
</td></tr>
<tr><td><code id="heterogeneity_+3A_es_failsafe">es_failsafe</code></td>
<td>
<p>Failsafe effect-size value for file-drawer analyses.</p>
</td></tr>
<tr><td><code id="heterogeneity_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of confidence intervals (default is <code>conf_level</code> specified in <code>ma_obj</code>).</p>
</td></tr>
<tr><td><code id="heterogeneity_+3A_var_res_ci_method">var_res_ci_method</code></td>
<td>
<p>Which method to use to estimate the limits. Options are <code>profile_var_es</code> for a profile-likelihood interval assuming <code class="reqn">\sigma^{2}_{es} ~ \chi^{2}(k-1)</code>, <code>profile_Q</code> for a profile-likelihood interval assuming <code class="reqn">Q ~ \chi^{2}(k-1, \lambda)</code>, <code class="reqn">\lambda = \sum_{i=1}^{k}{w_i(\theta - \bar{\theta})^{2}}</code>, and <code>normal_logQ</code> for a delta method assuming log(Q) follows a standard normal distribution.</p>
</td></tr>
<tr><td><code id="heterogeneity_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ma_obj with heterogeneity statistics added. Included statistics include:
</p>
<table>
<tr><td><code>es_type</code></td>
<td>
<p>The effect size metric used.</p>
</td></tr>
<tr><td><code>percent_var_accounted</code></td>
<td>
<p>Percent variance accounted for statistics (by sampling error, by other artifacts, and total). These statistics are widely reported, but not recommended, as they tend to be misinterpreted as suggesting only a small portion of the observed variance is accounted for by sampling error and other artifacts (Schmidt, 2010; Schmidt &amp; Hunter, 2015, p. 15, 425). The square roots of these values are more interpretable and appropriate indices of the relations between observed effect sizes and statistical artifacts (see <code>cor(es, perturbations)</code>).</p>
</td></tr>
<tr><td><code>cor(es</code>, <code>perturbations)</code></td>
<td>
<p>The correlation between observed effect sizes and statistical artifacts in each sample (with sampling error, with other artifacts, and with artifacts in total), computed as <code class="reqn">\sqrt{percent\;var\;accounted}</code>. These indices are more interpretable and appropriate indices of the relations between observed effect sizes and statistical artifacts than <code>percent_var_accounted</code>.</p>
</td></tr>
<tr><td><code>rel_es_obs</code></td>
<td>
<p><code class="reqn">1-\frac{var_{pre}}{var_{es}}</code>, the reliability of observed effect size differences as indicators of true effect sizes differences in the sampled studies. This value is useful for correcting correlations between moderators and effect sizes in meta-regression.</p>
</td></tr>
<tr><td><code>H_squared</code></td>
<td>
<p>The ratio of the observed effect size variance to the predicted (error) variance. Also the square root of <code>Q</code> divided by its degrees of freedom.</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>The ratio of the observed effect size standard deviation to the predicted (error) standard deviation.</p>
</td></tr>
<tr><td><code>I_squared</code></td>
<td>
<p>The estimated percent variance not accounted for by sampling error or other artifacts (attributable to moderators and uncorrected artifacts). This statistic is simply <code>rel_es_obs</code> expressed as a percentage rather than a decimal.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Cochran's <code class="reqn">\chi^{2}</code> statistic. Significance tests using this statistic are strongly discouraged; heterogeneity should instead be determined by examining the width of the credibility interval and the practical differences between effect sizes contained within it (Wiernik et al., 2017). This value is not accurate when artifact distribution methods are used for corrections.</p>
</td></tr>
<tr><td><code>tau_squared</code></td>
<td>
<p><code class="reqn">\tau^{2}</code>, an estimator of the random effects variance component (analogous to the Hunter-Schmidt <code class="reqn">SD_{res}^{2}</code>, <code class="reqn">SD_{\rho}^{2}</code>, or <code class="reqn">SD_{\delta}^{2}</code> statistics), with its confidence interval. This value is not accurate when artifact distribution methods are used for corrections.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p><code class="reqn">\sqrt{\tau^{2}}</code>, analogous to the Hunter-Schmidt <code class="reqn">SD_{res}</code>, <code class="reqn">SD_{\rho}</code>, and <code class="reqn">SD_{\delta}</code> statistics, with its confidence interval. This value is not accurate when artifact distribution methods are used for corrections.</p>
</td></tr>
<tr><td><code>Q_r</code>, <code>H_r_squared</code>, <code>H_r</code>, <code>I_r_squared</code>, <code>tau_r_squared</code>, <code>tau_r</code></td>
<td>
<p>Outlier-robust versions of these statistics, computed based on absolute deviations from the weighted <em>mean</em> effect size (see Lin et al., 2017). These values are not accurate when artifact distribution methods are used for corrections.</p>
</td></tr>
<tr><td><code>Q_m</code>, <code>H_m_squared</code>, <code>H_m</code>, <code>I_m_squared</code>, <code>tau_m_squared</code>, <code>tau_m</code></td>
<td>
<p>Outlier-robust versions of these statistics, computed based on absolute deviations from the weighted <em>median</em> effect size (see Lin et al., 2017). These values are not accurate when artifact distribution methods are used for corrections.</p>
</td></tr>
<tr><td><code>file_drawer</code></td>
<td>
<p>Fail-safe <code class="reqn">N</code> and <code class="reqn">k</code> statistics (file-drawer analyses). These statistics should not be used to evaluate publication bias, as they counterintuitively suggest <em>less</em> when publication bias is strong (Becker, 2005). However, in the absence of publication bias, they can be used as an index of second-order sampling error (how likely is a mean effect to reduce to the specified value with additional studies?). The confidence interval around the mean effect can be used more directly for the same purpose.</p>
</td></tr>
</table>
<p>Results are reported using computation methods described by Schmidt and Hunter.
For barebones and indivdiual-correction meta-analyses, results are also
reported using computation methods described by DerSimonian and Laird,
outlier-robust computation methods, and, if weights from <span class="pkg">metafor</span>
are used, heterogeneity results from <span class="pkg">metafor</span>.
</p>


<h3>References</h3>

<p>Becker, B. J. (2005).
Failsafe <em>N</em> or file-drawer number.
In H. R. Rothstein, A. J. Sutton, &amp; M. Borenstein (Eds.),
<em>Publication bias in meta-analysis: Prevention, assessment and adjustments</em> (pp. 111–125).
Wiley. doi: <a href="https://doi.org/10.1002/0470870168.ch7">10.1002/0470870168.ch7</a>
</p>
<p>Higgins, J. P. T., &amp; Thompson, S. G. (2002).
Quantifying heterogeneity in a meta-analysis.
<em>Statistics in Medicine, 21</em>(11), 1539–1558. doi: <a href="https://doi.org/10.1002/sim.1186">10.1002/sim.1186</a>
</p>
<p>Lin, L., Chu, H., &amp; Hodges, J. S. (2017).
Alternative measures of between-study heterogeneity in meta-analysis: Reducing the impact of outlying studies.
<em>Biometrics, 73</em>(1), 156–166. doi: <a href="https://doi.org/10.1111/biom.12543">10.1111/biom.12543</a>
</p>
<p>Schmidt, F. (2010).
Detecting and correcting the lies that data tell.
<em>Perspectives on Psychological Science, 5</em>(3), 233–242. doi: <a href="https://doi.org/10.1177/1745691610369339">10.1177/1745691610369339</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 15, 414, 426, 533–534.
</p>
<p>Wiernik, B. M., Kostal, J. W., Wilmot, M. P., Dilchert, S., &amp; Ones, D. S. (2017).
Empirical benchmarks for interpreting effect size variability in meta-analysis.
<em>Industrial and Organizational Psychology, 10</em>(3). doi: <a href="https://doi.org/10.1017/iop.2017.44">10.1017/iop.2017.44</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Correlations
ma_obj &lt;- ma_r_ic(rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi, ux = ux,
                  correct_rr_y = FALSE, data = data_r_uvirr)
ma_obj &lt;- ma_r_ad(ma_obj, correct_rr_y = FALSE)
ma_obj &lt;- heterogeneity(ma_obj = ma_obj)
ma_obj$heterogeneity[[1]]$barebones
ma_obj$heterogeneity[[1]]$individual_correction$true_score
ma_obj$heterogeneity[[1]]$artifact_distribution$true_score

## d values
ma_obj &lt;- ma_d_ic(d = d, n1 = n1, n2 = n2, ryy = ryyi,
                  data = data_d_meas_multi)
ma_obj &lt;- ma_d_ad(ma_obj)
ma_obj &lt;- heterogeneity(ma_obj = ma_obj)
ma_obj$heterogeneity[[1]]$barebones
ma_obj$heterogeneity[[1]]$individual_correction$latentGroup_latentY
ma_obj$heterogeneity[[1]]$artifact_distribution$latentGroup_latentY
</code></pre>

<hr>
<h2 id='impute_artifacts'>Impute missing and impossible artifact values</h2><span id='topic+impute_artifacts'></span>

<h3>Description</h3>

<p>Impute missing and impossible artifact values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_artifacts(
  sample_id = NULL,
  construct_id = NULL,
  measure_id = NULL,
  art_vec,
  cat_moderator_matrix,
  impute_method = "bootstrap_mod",
  art_type = "rel",
  n_vec = rep(1, length(art_vec))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_artifacts_+3A_sample_id">sample_id</code></td>
<td>
<p>Study ID value.</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_construct_id">construct_id</code></td>
<td>
<p>Construct name or other designator.</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_measure_id">measure_id</code></td>
<td>
<p>Measure name or other designator.</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_art_vec">art_vec</code></td>
<td>
<p>Vector of artifact values</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_cat_moderator_matrix">cat_moderator_matrix</code></td>
<td>
<p>Matrix of categorical moderators</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_impute_method">impute_method</code></td>
<td>
<p>Method to use for imputing artifacts. Choices are:
</p>

<ul>
<li><p> bootstrap_mod = select random values from the most specific moderator categories available.
</p>
</li>
<li><p> bootstrap_full = select random values from the full vector of artifacts.
</p>
</li>
<li><p> simulate_mod = generate random values from the distribution with the mean and variance of observed artifacts from the most specific moderator categories available
(uses rnorm for u ratios and rbeta for reliability values).
</p>
</li>
<li><p> simulate_full = generate random values from the distribution with the mean and variance of all observed artifacts (uses rnorm for u ratios and rbeta for reliability values).
</p>
</li>
<li><p> wt_mean_mod = replace missing values with the sample-size weighted mean of the distribution of artifacts from the most specific moderator categories available.
</p>
</li>
<li><p> wt_mean_full = replace missing values with the sample-size weighted mean of the full distribution of artifacts.
</p>
</li>
<li><p> unwt_mean_mod = replace missing values with the unweighted mean of the distribution of artifacts from the most specific moderator categories available.
</p>
</li>
<li><p> unwt_mean_full = replace missing values with the unweighted mean of the full distribution of artifacts.
</p>
</li>
<li><p> replace_unity = replace missing values with 1 (not recommended)
</p>
</li>
<li><p> stop = stop evaluations when missing artifacts are encountered
</p>
</li></ul>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_art_type">art_type</code></td>
<td>
<p>Type of artifacts to be imputed: &quot;rel&quot; for reliabilities and &quot;u&quot; for u ratios.</p>
</td></tr>
<tr><td><code id="impute_artifacts_+3A_n_vec">n_vec</code></td>
<td>
<p>Vector of sample sizes associated with the elements of art_vec.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of artifacts that includes imputed values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># art_vec &lt;- c(.6, .7, NA, .8, .9, NA)
# cat_moderator_matrix &lt;- matrix(c(rep(1, 3), rep(2, 3)))
# art_type &lt;- "rel"
# n_vec &lt;- c(50, 200, 100, 50, 200, 100)
#
# ## Compute unweighted means
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "unwt_mean_full", art_type = art_type, n_vec = n_vec)
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "unwt_mean_mod", art_type = art_type, n_vec = n_vec)
#
# ## Compute weighted means
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "wt_mean_full", art_type = art_type, n_vec = n_vec)
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "wt_mean_mod", art_type = art_type, n_vec = n_vec)
#
# ## Simulate from distribution with the mean and variance of the observed artifacts
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "simulate_full", art_type = art_type, n_vec = n_vec)
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "simulate_mod", art_type = art_type, n_vec = n_vec)
#
# ## Sample random values from the observed distribution of artifacts
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "bootstrap_mod", art_type = art_type, n_vec = n_vec)
# impute_artifacts(art_vec = art_vec, cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "bootstrap_full", art_type = art_type, n_vec = n_vec)
#
# ## If all values are missing from a moderator category, the program will run
# ## full-data imputation on the remaining missing values:
# impute_artifacts(art_vec = c(NA, NA, NA, .7, .8, .9), cat_moderator_matrix = cat_moderator_matrix,
#                 impute_method = "bootstrap_mod", art_type = art_type, n_vec = n_vec)
</code></pre>

<hr>
<h2 id='interval_warning'>Warning message for the widths of uncertainty intervals</h2><span id='topic+interval_warning'></span>

<h3>Description</h3>

<p>Warning message for the widths of uncertainty intervals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interval_warning(interval, interval_name = NULL, default)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interval_warning_+3A_interval">interval</code></td>
<td>
<p>Width for interval</p>
</td></tr>
<tr><td><code id="interval_warning_+3A_interval_name">interval_name</code></td>
<td>
<p>Name of interval</p>
</td></tr>
<tr><td><code id="interval_warning_+3A_default">default</code></td>
<td>
<p>Default value for interval</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Warning if length of 'interval' takes on impossible values and the revised value of 'interval'.
</p>

<hr>
<h2 id='limits_tau'>Confidence limits of tau</h2><span id='topic+limits_tau'></span>

<h3>Description</h3>

<p>Note that this interval does not incorporate uncertainty in artifact estimates,
so the interval will be somewhat conservative when applied to individual-correction or
artifact-distribution meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>limits_tau(
  var_es,
  var_pre,
  k,
  method = c("profile_var_es", "profile_Q", "normal_logQ"),
  conf_level = 0.95,
  var_unbiased = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="limits_tau_+3A_var_es">var_es</code></td>
<td>
<p>The observed variance of effect sizes.</p>
</td></tr>
<tr><td><code id="limits_tau_+3A_var_pre">var_pre</code></td>
<td>
<p>The predicted variance of effect sizes due to artifacts.</p>
</td></tr>
<tr><td><code id="limits_tau_+3A_k">k</code></td>
<td>
<p>The number of studies in a meta-analysis.</p>
</td></tr>
<tr><td><code id="limits_tau_+3A_method">method</code></td>
<td>
<p>Which method to use to estimate the limits. Options are <code>profile_var_es</code> for a profile-likelihood interval assuming <code class="reqn">\sigma^{2}_es ~ \chi^{2}(k-1)</code>, <code>profile_Q</code> for a profile-likelihood interval assuming <code class="reqn">Q ~ \chi^{2}(k-1, \lambda)</code>, <code class="reqn">\lambda = \sum_{i=1}{k}{w_i(\theta - \bar{\theta})^{2}}</code>, and <code>normal_logQ</code> for a delta method assuming log(Q) follows a standard normal distribution.</p>
</td></tr>
<tr><td><code id="limits_tau_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level.</p>
</td></tr>
<tr><td><code id="limits_tau_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Are variances computed using the unbiased (<code>TRUE</code>) or maximum likelihood (<code>FALSE</code>) estimator?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The confidence limits of tau
</p>


<h3>Examples</h3>

<pre><code class='language-R'>limits_tau(var_es = 0.008372902, var_pre = 0.004778935, k = 20)
</code></pre>

<hr>
<h2 id='limits_tau2'>Confidence limits of tau-squared</h2><span id='topic+limits_tau2'></span>

<h3>Description</h3>

<p>Note that this interval does not incorporate uncertainty in artifact estimates,
so the interval will be somewhat conservative when applied to individual-correction or
artifact-distribution meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>limits_tau2(
  var_es,
  var_pre,
  k,
  method = c("profile_var_es", "profile_Q", "normal_logQ"),
  conf_level = 0.95,
  var_unbiased = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="limits_tau2_+3A_var_es">var_es</code></td>
<td>
<p>The observed variance of effect sizes.</p>
</td></tr>
<tr><td><code id="limits_tau2_+3A_var_pre">var_pre</code></td>
<td>
<p>The predicted variance of effect sizes due to artifacts.</p>
</td></tr>
<tr><td><code id="limits_tau2_+3A_k">k</code></td>
<td>
<p>The number of studies in a meta-analysis.</p>
</td></tr>
<tr><td><code id="limits_tau2_+3A_method">method</code></td>
<td>
<p>Which method to use to estimate the limits. Options are <code>profile_var_es</code> for a profile-likelihood interval assuming <code class="reqn">\sigma^{2}_es ~ \chi^{2}(k-1)</code>, <code>profile_Q</code> for a profile-likelihood interval assuming <code class="reqn">Q ~ \chi^{2}(k-1, \lambda)</code>, <code class="reqn">\lambda = \sum_{i=1}{k}{w_i(\theta - \bar{\theta})^{2}}</code>, and <code>normal_logQ</code> for a delta method assuming log(Q) follows a standard normal distribution.</p>
</td></tr>
<tr><td><code id="limits_tau2_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level.</p>
</td></tr>
<tr><td><code id="limits_tau2_+3A_var_unbiased">var_unbiased</code></td>
<td>
<p>Are variances computed using the unbiased (<code>TRUE</code>) or maximum likelihood (<code>FALSE</code>) estimator?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The confidence limits of tau-squared
</p>


<h3>Examples</h3>

<pre><code class='language-R'>limits_tau2(var_es = 0.008372902, var_pre = 0.004778935, k = 20)
</code></pre>

<hr>
<h2 id='lm_mat'>Compute linear regression models and generate &quot;lm&quot; objects from covariance matrices.</h2><span id='topic+lm_mat'></span><span id='topic+matrixreg'></span><span id='topic+matreg'></span><span id='topic+lm_matrix'></span>

<h3>Description</h3>

<p>Compute linear regression models and generate &quot;lm&quot; objects from covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm_mat(
  formula,
  cov_mat,
  mean_vec = rep(0, ncol(cov_mat)),
  n = Inf,
  se_beta_method = c("lm", "normal"),
  ...
)

matrixreg(
  formula,
  cov_mat,
  mean_vec = rep(0, ncol(cov_mat)),
  n = Inf,
  se_beta_method = c("lm", "normal"),
  ...
)

matreg(
  formula,
  cov_mat,
  mean_vec = rep(0, ncol(cov_mat)),
  n = Inf,
  se_beta_method = c("lm", "normal"),
  ...
)

lm_matrix(
  formula,
  cov_mat,
  mean_vec = rep(0, ncol(cov_mat)),
  n = Inf,
  se_beta_method = c("lm", "normal"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm_mat_+3A_formula">formula</code></td>
<td>
<p>Regression formula with a single outcome variable on the left-hand side and one or more predictor variables on the right-hand side (e.g., Y ~ X1 + X2).</p>
</td></tr>
<tr><td><code id="lm_mat_+3A_cov_mat">cov_mat</code></td>
<td>
<p>Covariance matrix containing the variables to be used in the regression.</p>
</td></tr>
<tr><td><code id="lm_mat_+3A_mean_vec">mean_vec</code></td>
<td>
<p>Vector of means corresponding to the variables in <code>cov_mat</code>.</p>
</td></tr>
<tr><td><code id="lm_mat_+3A_n">n</code></td>
<td>
<p>Sample size to be used in significance testing</p>
</td></tr>
<tr><td><code id="lm_mat_+3A_se_beta_method">se_beta_method</code></td>
<td>
<p>Method to use to estimate the standard errors of standardized regression (beta) coefficients.
Current options include &quot;lm&quot; (estimate standard errors using conventional regression formulas) and &quot;normal&quot; (use the Jones-Waller normal-theory approach from the <code>fungible::seBeta()</code> and <code>fungible::seBetaCor()</code> functions)</p>
</td></tr>
<tr><td><code id="lm_mat_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with the class &quot;lm_mat&quot; that can be used with summary, print, predict, and anova methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data
S &lt;- reshape_vec2mat(cov = c(.3 * 2 * 3,
                             .4 * 2 * 4,
                             .5 * 3 * 4),
                     var = c(2, 3, 4)^2,
                     var_names = c("X", "Y", "Z"))
mean_vec &lt;- setNames(c(1, 2, 3), colnames(S))
dat &lt;- data.frame(MASS::mvrnorm(n = 100, mu = mean_vec,
                                Sigma = S, empirical = TRUE))

## Compute regression models with lm
lm_out1 &lt;- lm(Y ~ X, data = dat)
lm_out2 &lt;- lm(Y ~ X + Z, data = dat)

## Compute regression models with lm_mat
matreg_out1 &lt;- lm_mat(formula = Y ~ X, cov_mat = S, mean_vec = mean_vec, n = nrow(dat))
matreg_out2 &lt;- lm_mat(formula = Y ~ X + Z, cov_mat = S, mean_vec = mean_vec, n = nrow(dat))

## Compare results of lm and lm_mat with one predictor
lm_out1
matreg_out1

## Compare summaries of lm and lm_mat with one predictor
summary(lm_out1)
summary(matreg_out1)

## Compare results of lm and lm_mat with two predictors
lm_out2
matreg_out2

## Compare summaries of lm and lm_mat with two predictors
summary(lm_out2)
summary(matreg_out2)

## Compare predictions made with lm and lm_mat
predict(object = matreg_out1, newdata = data.frame(X = 1:5))
predict(object = summary(matreg_out1), newdata = data.frame(X = 1:5))
predict(lm_out1, newdata = data.frame(X = 1:5))

## Compare predictions made with lm and lm_mat (with confidence intervals)
predict(object = matreg_out1, newdata = data.frame(X = 1:5),
        se.fit = TRUE, interval = "confidence")
predict(lm_out1, newdata = data.frame(X = 1:5),
        se.fit = TRUE, interval = "confidence")

## Compare predictions made with lm and lm_mat (with prediction intervals)
predict(object = matreg_out1, newdata = data.frame(X = 1:5),
        se.fit = TRUE, interval = "prediction")
predict(lm_out1, newdata = data.frame(X = 1:5),
        se.fit = TRUE, interval = "prediction")

## Compare model comparisons computed using lm and lm_mat objects
anova(lm_out1, lm_out2)
anova(matreg_out1, matreg_out2)

## Model comparisons can be run on lm_mat summaries, too:
anova(summary(matreg_out1), summary(matreg_out2))
## Or summaries and raw models can be mixed:
anova(matreg_out1, summary(matreg_out2))
anova(summary(matreg_out1), matreg_out2)


## Compare confidence intervals computed using lm and lm_mat objects
confint(object = lm_out1)
confint(object = matreg_out1)
confint(object = summary(matreg_out1))

confint(object = lm_out2)
confint(object = matreg_out2)
confint(object = summary(matreg_out2))
</code></pre>

<hr>
<h2 id='ma_d'>Meta-analysis of <em>d</em> values</h2><span id='topic+ma_d'></span><span id='topic+ma_d_ad'></span><span id='topic+ma_d_bb'></span><span id='topic+ma_d_barebones'></span><span id='topic+ma_d_ic'></span>

<h3>Description</h3>

<p>The <code>ma_r_bb</code>, <code>ma_r_ic</code>, and <code>ma_r_ad</code> functions implement bare-bones, individual-correction, and artifact-distribution correction methods for <em>d</em> values, respectively.
The <code>ma_d</code> function is the master function for meta-analyses of <em>d</em> values - it facilitates the computation of bare-bones, artifact-distribution, and individual-correction meta-analyses of correlations for any number of group-wise contrasts and any number of dependent variables.
When artifact-distribution meta-analyses are performed, <code>ma_d</code> will automatically extract the artifact information from a database and organize it into the requested type of artifact distribution object (i.e., either Taylor series or interactive artifact distributions).
<code>ma_d</code> is also equipped with the capability to clean databases containing inconsistently recorded artifact data, impute missing artifacts (when individual-correction meta-analyses are requested), and remove dependency among samples by forming composites or averaging effect sizes and artifacts.
The automatic compositing features in <code>ma_d</code> are employed when <code>sample_id</code>s and/or construct names are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_d(
  d,
  n1,
  n2 = NULL,
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  treat_as_r = FALSE,
  ma_method = c("bb", "ic", "ad"),
  ad_type = c("tsa", "int"),
  correction_method = "auto",
  group_id = NULL,
  group1 = NULL,
  group2 = NULL,
  group_order = NULL,
  construct_y = NULL,
  facet_y = NULL,
  measure_y = NULL,
  construct_order = NULL,
  wt_type = c("n_effective", "sample_size", "inv_var_mean", "inv_var_sample", "DL",
    "HE", "HS", "SJ", "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  correct_rel = NULL,
  correct_rGg = FALSE,
  correct_ryy = TRUE,
  correct_rr = NULL,
  correct_rr_g = TRUE,
  correct_rr_y = TRUE,
  indirect_rr = NULL,
  indirect_rr_g = TRUE,
  indirect_rr_y = TRUE,
  rGg = NULL,
  pi = NULL,
  pa = NULL,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NULL,
  uy = NULL,
  uy_observed = TRUE,
  sign_rz = NULL,
  sign_rgz = 1,
  sign_ryz = 1,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  supplemental_ads = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)

ma_d_ad(
  ma_obj,
  ad_obj_g = NULL,
  ad_obj_y = NULL,
  correction_method = "auto",
  use_ic_ads = c("tsa", "int"),
  correct_rGg = FALSE,
  correct_ryy = TRUE,
  correct_rr_g = TRUE,
  correct_rr_y = TRUE,
  indirect_rr_g = TRUE,
  indirect_rr_y = TRUE,
  sign_rgz = 1,
  sign_ryz = 1,
  control = control_psychmeta(),
  ...
)

ma_d_bb(
  d,
  n1,
  n2 = rep(NA, length(d)),
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  wt_type = c("n_effective", "sample_size", "inv_var_mean", "inv_var_sample", "DL",
    "HE", "HS", "SJ", "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  data = NULL,
  control = control_psychmeta(),
  ...
)

ma_d_ic(
  d,
  n1,
  n2 = NULL,
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  treat_as_r = FALSE,
  wt_type = c("n_effective", "sample_size", "inv_var_mean", "inv_var_sample", "DL",
    "HE", "HS", "SJ", "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  correct_rGg = FALSE,
  correct_ryy = TRUE,
  correct_rr_g = FALSE,
  correct_rr_y = TRUE,
  indirect_rr_g = TRUE,
  indirect_rr_y = TRUE,
  rGg = NULL,
  pi = NULL,
  pa = NULL,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NULL,
  uy = NULL,
  uy_observed = TRUE,
  sign_rgz = 1,
  sign_ryz = 1,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  supplemental_ads_y = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_d_+3A_d">d</code></td>
<td>
<p>Vector or column name of observed <em>d</em> values.
<em>NOTE</em>: Beginning in <span class="pkg">psychmeta</span> version 2.5.2, <code>d</code> values of exactly 0 in individual-correction meta-analyses are replaced with a functionally equivalent value (in the correlation metric) via the <code>zero_substitute</code> argument for <code><a href="#topic+control_psychmeta">control_psychmeta</a></code> to facilitate the estimation of corrected error variances.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_n1">n1</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_n2">n2</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_n_adj">n_adj</code></td>
<td>
<p>Optional: Vector or column name of sample sizes adjusted for sporadic artifact corrections.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_sample_id">sample_id</code></td>
<td>
<p>Optional vector of identification labels for samples/studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_treat_as_r">treat_as_r</code></td>
<td>
<p>Logical scalar determining whether <em>d</em> values are to be meta-analyzed as <em>d</em> values (<code>FALSE</code>; default) or whether they should be meta-analyzed as correlations and have the final results converted to the <em>d</em> metric (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ma_method">ma_method</code></td>
<td>
<p>Method to be used to compute the meta-analysis: &quot;bb&quot; (barebones), &quot;ic&quot; (individual correction), or &quot;ad&quot; (artifact distribution).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ad_type">ad_type</code></td>
<td>
<p>For when ma_method is &quot;ad&quot;, specifies the type of artifact distribution to use: &quot;int&quot; or &quot;tsa&quot;.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correction_method">correction_method</code></td>
<td>
<p>Character scalar or a matrix with <code>group_id</code> levels as row names and <code>construct_y</code> levels as column names.
When ma_method is &quot;ad&quot;, select one of the following methods for correcting artifacts: &quot;auto&quot;, &quot;meas&quot;, &quot;uvdrr&quot;, &quot;uvirr&quot;, &quot;bvdrr&quot;, &quot;bvirr&quot;,
&quot;rbOrig&quot;, &quot;rb1Orig&quot;, &quot;rb2Orig&quot;, &quot;rbAdj&quot;, &quot;rb1Adj&quot;, and &quot;rb2Adj&quot;.
(note: &quot;rb1Orig&quot;, &quot;rb2Orig&quot;, &quot;rb1Adj&quot;, and &quot;rb2Adj&quot; can only be used when Taylor series artifact distributions are provided and &quot;rbOrig&quot; and &quot;rbAdj&quot; can only
be used when interactive artifact distributions are provided). See &quot;Details&quot; of <code><a href="#topic+ma_d_ad">ma_d_ad</a></code> for descriptions of the available methods.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_group_id">group_id</code></td>
<td>
<p>Vector of group comparison IDs (e.g., Treatment1-Control, Treatment2-Control).
The <code>group_id</code> argument supersedes the <code>group1</code> and <code>group2</code> arguments.
If <code>group_id</code> is not <code>NULL</code>, the values supplied to the <code>group_order</code> argument must correspond to <code>group_id</code> values.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_group1">group1</code>, <code id="ma_d_+3A_group2">group2</code></td>
<td>
<p>Vector of group identification labels (e.g., Treatment1, Treatment2, Control)</p>
</td></tr>
<tr><td><code id="ma_d_+3A_group_order">group_order</code></td>
<td>
<p>Optional vector indicating the order in which (1) <code>group1</code> and <code>group2</code> values or (2) <code>group_ids</code> should be arranged.
If <code>group_order</code> is <code>NULL</code>, the order of group pairings will be determined internally using alpha-numeric ordering.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for construct designated as &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_facet_y">facet_y</code></td>
<td>
<p>Vector of facet names for constructs designated as &quot;Y&quot;.
Facet names &quot;global&quot;, &quot;overall&quot;, and &quot;total&quot; are reserved to indicate observations that represent effect sizes that have already been composited or that represent construct-level measurements rather than facet-level measurements.
To avoid double-compositing, any observation with one of these reserved names will only be eligible for auto-compositing with other such observations and will not be combined with narrow facets.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_measure_y">measure_y</code></td>
<td>
<p>Vector of names for measures associated with constructs designated as &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which Y variables should be arranged.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_wt_type">wt_type</code></td>
<td>
<p>Type of weight to use in the meta-analysis: options are &quot;n_effective&quot; (effective sample size), &quot;sample_size&quot;, &quot;inv_var_mean&quot; (inverse variance computed using mean effect size), and
&quot;inv_var_sample&quot; (inverse variance computed using sample-specific effect sizes). Supported options borrowed from metafor are &quot;DL&quot;, &quot;HE&quot;, &quot;HS&quot;, &quot;SJ&quot;, &quot;ML&quot;, &quot;REML&quot;, &quot;EB&quot;, and &quot;PM&quot;
(see <span class="pkg">metafor</span> documentation for details about the <span class="pkg">metafor</span> methods).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical scalar that determines whether to correct correlations for small-sample bias (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_rel">correct_rel</code></td>
<td>
<p>Optional named vector that supersedes <code>correct_rGg</code> and <code>correct_ryy</code>. Names should correspond to construct names in <code>group_id</code> and <code>construct_y</code> to determine which constructs should be corrected for unreliability.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_rgg">correct_rGg</code></td>
<td>
<p>Logical scalar or vector that determines whether to correct the grouping variable variable for measurement error (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_ryy">correct_ryy</code></td>
<td>
<p>Logical scalar or vector that determines whether to correct the Y variable for measurement error (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_rr">correct_rr</code></td>
<td>
<p>Optional named vector that supersedes <code>correct_rr_g</code> and <code>correct_rr_y</code>. Names should correspond to construct names in <code>group_id</code> and <code>construct_y</code> to determine which constructs should be corrected for range restriction.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_rr_g">correct_rr_g</code></td>
<td>
<p>Logical scalar or vector or column name determining whether each <em>d</em> value should be corrected for range restriction in the grouping variable (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_correct_rr_y">correct_rr_y</code></td>
<td>
<p>Logical scalar or vector or column name determining whether each <em>d</em> should be corrected for range restriction in Y (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_indirect_rr">indirect_rr</code></td>
<td>
<p>Optional named vector that supersedes <code>indirect_rr_g</code> and <code>indirect_rr_y</code>. Names should correspond to construct names in <code>group_id</code> and <code>construct_y</code> to determine which constructs should be corrected for indirect range restriction.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_indirect_rr_g">indirect_rr_g</code></td>
<td>
<p>Logical vector or column name determining whether each <em>d</em> should be corrected for indirect range restriction in the grouping variable (<code>TRUE</code>) or not (<code>FALSE</code>).
Superseded in evaluation by <code>correct_rr_g</code> (i.e., if <code>correct_rr_g</code> == <code>FALSE</code>, the value supplied for <code>indirect_rr_g</code> is disregarded).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_indirect_rr_y">indirect_rr_y</code></td>
<td>
<p>Logical vector or column name determining whether each <em>d</em> should be corrected for indirect range restriction in Y (<code>TRUE</code>) or not (<code>FALSE</code>).
Superseded in evaluation by <code>correct_rr_y</code> (i.e., if <code>correct_rr_y</code> == <code>FALSE</code>, the value supplied for <code>indirect_rr_y</code> is disregarded).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_rgg">rGg</code></td>
<td>
<p>Vector or column name of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_pi">pi</code></td>
<td>
<p>Scalar or vector containing the restricted-group proportions of group membership. If a vector, it must either (1) have as many elements as there are <em>d</em> values or (2) be named so as to match with levels of the <code>group_id</code> argument.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_pa">pa</code></td>
<td>
<p>Scalar or vector containing the unrestricted-group proportions of group membership (default = .5). If a vector, it must either (1) have as many elements as there are <em>d</em> values or (2) be named so as to match with levels of the <code>group_id</code> argument.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ryy">ryy</code></td>
<td>
<p>Vector or column name of reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ryy_restricted">ryy_restricted</code></td>
<td>
<p>Logical vector or column name determining whether each element of <code>ryy</code> is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_uy">uy</code></td>
<td>
<p>Vector or column name of u ratios for Y.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector or column name determining whether each element of <code>uy</code> is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_sign_rz">sign_rz</code></td>
<td>
<p>Optional named vector that supersedes <code>sign_rgz</code> and <code>sign_ryz</code>. Names should correspond to construct names in <code>group_id</code> and <code>construct_y</code> to determine the sign of each construct's relationship with the selection mechanism.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_sign_rgz">sign_rgz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism (for use with bvirr corrections only).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism (for use with bvirr corrections only).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_moderators">moderators</code></td>
<td>
<p>Matrix or column names of moderator variables to be used in the meta-analysis (can be a vector in the case of one moderator).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Logical scalar or vector identifying whether variables in the <code>moderators</code> argument are categorical variables (<code>TRUE</code>) or continuous variables (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis: &quot;none&quot; means that no moderators are to be used, &quot;simple&quot; means that moderators are to be examined one at a time,
&quot;hierarchical&quot; means that all possible combinations and subsets of moderators are to be examined, and &quot;all&quot; means that simple and hierarchical moderator analyses are to be performed.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_supplemental_ads">supplemental_ads</code></td>
<td>
<p>Named list (named according to the constructs included in the meta-analysis) of supplemental artifact distribution information from studies not included in the meta-analysis. This is a list of lists, where the elements of a list associated with a construct are named like the arguments of the <code>create_ad()</code> function.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments and/or moderators.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ma_obj">ma_obj</code></td>
<td>
<p>For <code>ma_d_ad</code> only: Meta-analysis object of correlations or <em>d</em> values (regardless of input metric, output metric will be <em>d</em>).</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ad_obj_g">ad_obj_g</code></td>
<td>
<p>For <code>ma_d_ad</code> only: Artifact-distribution object for the grouping variable (output of the <code>link{create_ad}</code> or <code>link{create_ad_group}</code> functions).
If ma_obj is of the class <code>ma_master</code> (i.e., the output of <code><a href="#topic+ma_r">ma_r</a></code> or <code><a href="#topic+ma_d">ma_d</a></code>), the object supplied for
<code>ad_obj_g</code> must be a named list of artifact distributions with names.
corresponding to the &quot;X&quot; constructs in the meta-analyses contained within <code>ma_obj</code>.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_ad_obj_y">ad_obj_y</code></td>
<td>
<p>For <code>ma_d_ad</code> only: AArtifact-distribution object for the Y variable (output of the <code>create_ad</code> function).
If ma_obj is of the class <code>ma_master</code>, the object supplied for <code>ad_obj_y</code> must be a named list of artifact distributions with names
corresponding to the &quot;Y&quot; constructs in the meta-analyses contained within <code>ma_obj</code>.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_use_ic_ads">use_ic_ads</code></td>
<td>
<p>For <code>ma_d_ad</code> only: Determines whether artifact distributions should be extracted from the individual correction results in <code>ma_obj</code>.
Only evaluated when <code>ad_obj_g</code> or <code>ad_obj_y</code> is NULL and <code>ma_obj</code> does not contain individual correction results.
Use one of the following commands: <code>tsa</code> to use the Taylor series method or <code>int</code> to use the interactive method.</p>
</td></tr>
<tr><td><code id="ma_d_+3A_supplemental_ads_y">supplemental_ads_y</code></td>
<td>
<p>For <code>ma_d_ic</code> only: List supplemental artifact distribution information from studies not included in the meta-analysis. The elements of this list are named like the arguments of the <code>create_ad()</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The options for <code>correction_method</code> are:
</p>

<ul>
<li><p>&quot;auto&quot;<br /> Automatic selection of the most appropriate correction procedure, based on the available artifacts and the logical arguments provided to the function. (default)
</p>
</li>
<li><p>&quot;meas&quot;<br /> Correction for measurement error only.
</p>
</li>
<li><p>&quot;uvdrr&quot;<br /> Correction for univariate direct range restriction (i.e., Case II). The choice of which variable to correct for range restriction is made using the <code>correct_rr_x</code> and <code>correct_rr_y</code> arguments.
</p>
</li>
<li><p>&quot;uvirr&quot;<br /> Correction for univariate indirect range restriction (i.e., Case IV). The choice of which variable to correct for range restriction is made using the <code>correct_rr_x</code> and <code>correct_rr_y</code> arguments.
</p>
</li>
<li><p>&quot;bvdrr&quot;<br /> Correction for bivariate direct range restriction. Use with caution: This correction is an approximation only and is known to have a positive bias.
</p>
</li>
<li><p>&quot;bvirr&quot;<br /> Correction for bivariate indirect range restriction (i.e., Case V).
</p>
</li>
<li><p>&quot;rbOrig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied interactively. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rbAdj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied interactively. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb1Orig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA1 method. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb1Adj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA1 method. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb2Orig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA2 method. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb2Adj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA2 method. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li></ul>



<h3>Value</h3>

<p>A nested tabular object of the class &quot;ma_psychmeta&quot;.
Components of output tables for bare-bones meta-analyses:
</p>

<ul>
<li><p><code>Pair_ID</code><br /> Unique identification number for each construct-contrast pairing.
</p>
</li>
<li><p><code>group_contrast</code><br /> Name of the variable analyzed as the group-contrast variable.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_d</code><br /> Mean observed <em>d</em> value.
</p>
</li>
<li><p><code>var_d</code><br /> Weighted variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed <em>d</em> values after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_d</code><br /> Square root of <code>var_r</code>.
</p>
</li>
<li><p><code>se_d</code><br /> Standard error of <code>mean_d</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_d</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_d</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_d</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_d</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>

<p>Components of output tables for individual-correction meta-analyses:
</p>

<ul>
<li><p><code>pair_id</code><br /> Unique identification number for each construct-contrast pairing.
</p>
</li>
<li><p><code>group_contrast</code><br /> Name of the variable analyzed as the group-contrast variable.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_d</code><br /> Mean observed <em>d</em> value.
</p>
</li>
<li><p><code>var_d</code><br /> Weighted variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed <em>d</em> values after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_d</code><br /> Square root of <code>var_r</code>.
</p>
</li>
<li><p><code>se_d</code><br /> Standard error of <code>mean_d</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>mean_delta</code><br /> Mean artifact-corrected <em>d</em> value.
</p>
</li>
<li><p><code>var_d_c</code><br /> Variance of artifact-corrected <em>d</em> values.
</p>
</li>
<li><p><code>var_e_c</code><br /> Predicted sampling-error variance of artifact-corrected <em>d</em> values.
</p>
</li>
<li><p><code>var_delta</code><br /> Variance of artifact-corrected <em>d</em> values after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_d_c</code><br /> Square root of <code>var_r_c</code>.
</p>
</li>
<li><p><code>se_d_c</code><br /> Standard error of <code>mean_delta</code>.
</p>
</li>
<li><p><code>sd_e_c</code><br /> Square root of <code>var_e_c</code>.
</p>
</li>
<li><p><code>sd_delta</code><br /> Square root of <code>var_delta</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_delta</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_delta</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_delta</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_delta</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>

<p>Components of output tables for artifact-distribution meta-analyses:
</p>

<ul>
<li><p><code>pair_id</code><br /> Unique identification number for each construct-contrast pairing.
</p>
</li>
<li><p><code>group_contrast</code><br /> Name of the variable analyzed as the group-contrast variable.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_d</code><br /> Mean observed <em>d</em> value.
</p>
</li>
<li><p><code>var_d</code><br /> Weighted variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed <em>d</em> values.
</p>
</li>
<li><p><code>var_art</code><br /> Amount of variance in observed <em>d</em> values that is attributable to measurement-error and range-restriction artifacts.
</p>
</li>
<li><p><code>var_pre</code><br /> Total predicted artifactual variance (i.e., the sum of <code>var_e</code> and <code>var_art</code>).
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed <em>d</em> values after removing predicted sampling-error variance and predicted artifact variance.
</p>
</li>
<li><p><code>sd_d</code><br /> Square root of <code>var_d</code>.
</p>
</li>
<li><p><code>se_d</code><br /> Standard error of <code>mean_d</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_art</code><br /> Square root of <code>var_art</code>.
</p>
</li>
<li><p><code>sd_pre</code><br /> Square root of <code>var_pre</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>mean_delta</code><br /> Mean artifact-corrected <em>d</em> value.
</p>
</li>
<li><p><code>var_d</code><br /> Weighted variance of observed <em>d</em> values corrected to the metric of delta.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed <em>d</em> values corrected to the metric of delta.
</p>
</li>
<li><p><code>var_art</code><br /> Amount of variance in observed <em>d</em> values that is attributable to measurement-error and range-restriction artifacts corrected to the metric of delta.
</p>
</li>
<li><p><code>var_pre</code><br /> Total predicted artifactual variance (i.e., the sum of <code>var_e</code> and <code>var_art</code>) corrected to the metric of delta.
</p>
</li>
<li><p><code>var_delta</code><br /> Variance of artifact-corrected <em>d</em> values after removing predicted sampling-error variance and predicted artifact variance.
</p>
</li>
<li><p><code>sd_d</code><br /> Square root of <code>var_d</code> corrected to the metric of delta.
</p>
</li>
<li><p><code>se_d</code><br /> Standard error of <code>mean_d</code> corrected to the metric of delta.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code> corrected to the metric of delta.
</p>
</li>
<li><p><code>sd_art</code><br /> Square root of <code>var_art</code> corrected to the metric of delta.
</p>
</li>
<li><p><code>sd_pre</code><br /> Square root of <code>var_pre</code> corrected to the metric of delta.
</p>
</li>
<li><p><code>sd_delta</code><br /> Square root of <code>var_delta</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_delta</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_delta</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_delta</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_delta</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>



<h3>Note</h3>

<p>The difference between &quot;rb&quot; methods with the &quot;orig&quot; and &quot;adj&quot; suffixes is that the original does not account for the impact of range restriction on criterion reliabilities, whereas
the adjusted procedure attempts to estimate the applicant reliability information for the criterion. The &quot;rb&quot; procedures are included for posterity: We strongly recommend using
the &quot;uvdrr&quot; procedure to appropriately correct for univariate range restriction.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.)</em>.
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425.
</p>
<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Demonstration of ma_d ###
## The 'ma_d' function can compute multi-construct bare-bones meta-analyses:
ma_d(d = d, n1 = n1, n2 = n2, construct_y = construct, data = data_d_meas_multi)

## It can also perform multiple individual-correction meta-analyses:
ma_d(ma_method = "ic", d = d, n1 = n1, n2 = n2, ryy = ryyi,
     construct_y = construct, data = data_d_meas_multi)

## And 'ma_d' can also curate artifact distributions and compute multiple
## artifact-distribution meta-analyses:
ma_d(ma_method = "ad", d = d, n1 = n1, n2 = n2,
     ryy = ryyi, correct_rr_y = FALSE,
     construct_y = construct, data = data_d_meas_multi)


### Demonstration of ma_d_bb ###
## Example meta-analyses using simulated data:
ma_d_bb(d = d, n1 = n1, n2 = n2,
        data = data_d_meas_multi[data_d_meas_multi$construct == "Y",])
ma_d_bb(d = d, n1 = n1, n2 = n2,
        data = data_d_meas_multi[data_d_meas_multi$construct == "Z",])


### Demonstration of ma_d_ic ###
## Example meta-analyses using simulated data:
ma_d_ic(d = d, n1 = n1, n2 = n2, ryy = ryyi, correct_rr_y = FALSE,
        data = data_d_meas_multi[data_d_meas_multi$construct == "Y",])
ma_d_ic(d = d, n1 = n1, n2 = n2, ryy = ryyi, correct_rr_y = FALSE,
        data = data_d_meas_multi[data_d_meas_multi$construct == "Z",])
</code></pre>

<hr>
<h2 id='ma_d_order2'>Second-order meta-analysis function for <em>d</em> values</h2><span id='topic+ma_d_order2'></span>

<h3>Description</h3>

<p>This function computes second-order meta-analysis function for <em>d</em> values. It supports second-order analyses of bare-bones, artifact-distribution, and individual-correction meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_d_order2(
  k,
  N = NULL,
  d = NULL,
  delta = NULL,
  var_d = NULL,
  var_d_c = NULL,
  ma_type = c("bb", "ic", "ad"),
  sample_id = NULL,
  citekey = NULL,
  moderators = NULL,
  moderator_type = "simple",
  construct_x = NULL,
  construct_y = NULL,
  construct_order = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_d_order2_+3A_k">k</code></td>
<td>
<p>Vector or column name of meta-analyses' k values.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_n">N</code></td>
<td>
<p>Vector or column name of meta-analyses' total sample sizes (optional).</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_d">d</code></td>
<td>
<p>Vector or column name of mean observed <em>d</em> values.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_delta">delta</code></td>
<td>
<p>Vector or column name of mean corrected <em>d</em> values.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_var_d">var_d</code></td>
<td>
<p>Vector or column name of observed variances of observed <em>d</em> values.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_var_d_c">var_d_c</code></td>
<td>
<p>Vector or column name of observed variances of corrected <em>d</em> values.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_ma_type">ma_type</code></td>
<td>
<p>Type of meta-analyses being analyzed: &quot;bb&quot; (barebones), &quot;ic&quot; (individual correction), or &quot;ad&quot; (artifact distribution).</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_sample_id">sample_id</code></td>
<td>
<p>Vector or column name of study ID labels.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_moderators">moderators</code></td>
<td>
<p>Matrix or column names of moderator variables to be used in the meta-analysis (can be a vector in the case of one moderator).</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis (&quot;none&quot;, &quot;simple&quot;, or &quot;hierarchical&quot;).</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_construct_x">construct_x</code></td>
<td>
<p>Vector or column name of construct names for X.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector or column name of construct names for Y.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which variables should be arranged, with variables listed earlier in the vector being preferred for designation as X.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments and/or moderators.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="ma_d_order2_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A nested tabular object of the class &quot;ma_psychmeta&quot;.
</p>

<hr>
<h2 id='ma_generic'>Bare-bones meta-analysis of generic effect sizes</h2><span id='topic+ma_generic'></span>

<h3>Description</h3>

<p>This function computes bare-bones meta-analyses of any effect size using user-supplied effect error variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_generic(
  es,
  n,
  var_e,
  sample_id = NULL,
  citekey = NULL,
  construct_x = NULL,
  construct_y = NULL,
  group1 = NULL,
  group2 = NULL,
  wt_type = c("sample_size", "inv_var", "DL", "HE", "HS", "SJ", "ML", "REML", "EB",
    "PM"),
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  data = NULL,
  control = control_psychmeta(),
  weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_generic_+3A_es">es</code></td>
<td>
<p>Vector or column name of observed effect sizes.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_n">n</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_var_e">var_e</code></td>
<td>
<p>Vector or column name of error variances.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_sample_id">sample_id</code></td>
<td>
<p>Optional vector of identification labels for samples/studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).
When <code>TRUE</code>, program will use sample-size weights, error variances estimated from the mean effect size, maximum likelihood variances, and normal-distribution confidence and credibility intervals.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_construct_x">construct_x</code>, <code id="ma_generic_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for constructs designated as &quot;X&quot; and as &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_group1">group1</code>, <code id="ma_generic_+3A_group2">group2</code></td>
<td>
<p>Vector of groups' names associated with effect sizes that represent pairwise contrasts.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_wt_type">wt_type</code></td>
<td>
<p>Type of weight to use in the meta-analysis: native options are &quot;sample_size&quot; and &quot;inv_var&quot; (inverse error variance).
Supported options borrowed from metafor are &quot;DL&quot;, &quot;HE&quot;, &quot;HS&quot;, &quot;SJ&quot;, &quot;ML&quot;, &quot;REML&quot;, &quot;EB&quot;, and &quot;PM&quot;
(see metafor documentation for details about the metafor methods).</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_moderators">moderators</code></td>
<td>
<p>Matrix of moderator variables to be used in the meta-analysis (can be a vector in the case of one moderator).</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Logical scalar or vector identifying whether variables in the <code>moderators</code> argument are categorical variables (<code>TRUE</code>) or continuous variables (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis (&quot;none&quot;, &quot;simple&quot;, or &quot;hierarchical&quot;).</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments and/or moderators.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_weights">weights</code></td>
<td>
<p>Optional vector of weights to be used. When <code>weights</code> is non-NULL, these weights override the argument supplied to <code>wt_type</code>.</p>
</td></tr>
<tr><td><code id="ma_generic_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A nested tabular object of the class &quot;ma_psychmeta&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>es &lt;- c(.3, .5, .8)
n &lt;- c(100, 200, 150)
var_e &lt;- 1 / n
ma_obj &lt;- ma_generic(es = es, n = n, var_e = var_e)
ma_obj
summary(ma_obj)
</code></pre>

<hr>
<h2 id='ma_r'>Meta-analysis of correlations</h2><span id='topic+ma_r'></span><span id='topic+ma_r_ad'></span><span id='topic+ma_r_bb'></span><span id='topic+ma_r_barebones'></span><span id='topic+ma_r_ic'></span>

<h3>Description</h3>

<p>The <code>ma_r_bb</code>, <code>ma_r_ic</code>, and <code>ma_r_ad</code> functions implement bare-bones, individual-correction, and artifact-distribution correction methods for correlations, respectively.
The <code>ma_r</code> function is the master function for meta-analyses of correlations - it facilitates the computation of bare-bones, artifact-distribution, and individual-correction meta-analyses of correlations for any number of construct pairs.
When artifact-distribution meta-analyses are performed, <code>ma_r</code> will automatically extract the artifact information from a database and organize it into the requested type of artifact distribution object (i.e., either Taylor series or interactive artifact distributions).
<code>ma_r</code> is also equipped with the capability to clean databases containing inconsistently recorded artifact data, impute missing artifacts (when individual-correction meta-analyses are requested), and remove dependency among samples by forming composites or averaging effect sizes and artifacts.
The automatic compositing features in <code>ma_r</code> are employed when <code>sample_id</code>s and/or construct names are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r(
  rxyi,
  n,
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  ma_method = c("bb", "ic", "ad"),
  ad_type = c("tsa", "int"),
  correction_method = "auto",
  construct_x = NULL,
  construct_y = NULL,
  facet_x = NULL,
  facet_y = NULL,
  measure_x = NULL,
  measure_y = NULL,
  construct_order = NULL,
  wt_type = c("sample_size", "inv_var_mean", "inv_var_sample", "DL", "HE", "HS", "SJ",
    "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  correct_rel = NULL,
  correct_rxx = TRUE,
  correct_ryy = TRUE,
  correct_rr = NULL,
  correct_rr_x = TRUE,
  correct_rr_y = TRUE,
  indirect_rr = NULL,
  indirect_rr_x = TRUE,
  indirect_rr_y = TRUE,
  rxx = NULL,
  rxx_restricted = TRUE,
  rxx_type = "alpha",
  k_items_x = NULL,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NULL,
  ux = NULL,
  ux_observed = TRUE,
  uy = NULL,
  uy_observed = TRUE,
  sign_rz = NULL,
  sign_rxz = 1,
  sign_ryz = 1,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  supplemental_ads = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)

ma_r_ad(
  ma_obj,
  ad_obj_x = NULL,
  ad_obj_y = NULL,
  correction_method = "auto",
  use_ic_ads = c("tsa", "int"),
  correct_rxx = TRUE,
  correct_ryy = TRUE,
  correct_rr_x = TRUE,
  correct_rr_y = TRUE,
  indirect_rr_x = TRUE,
  indirect_rr_y = TRUE,
  sign_rxz = 1,
  sign_ryz = 1,
  control = control_psychmeta(),
  ...
)

ma_r_bb(
  r,
  n,
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  wt_type = c("sample_size", "inv_var_mean", "inv_var_sample", "DL", "HE", "HS", "SJ",
    "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  data = NULL,
  control = control_psychmeta(),
  ...
)

ma_r_ic(
  rxyi,
  n,
  n_adj = NULL,
  sample_id = NULL,
  citekey = NULL,
  wt_type = c("sample_size", "inv_var_mean", "inv_var_sample", "DL", "HE", "HS", "SJ",
    "ML", "REML", "EB", "PM"),
  correct_bias = TRUE,
  correct_rxx = TRUE,
  correct_ryy = TRUE,
  correct_rr_x = TRUE,
  correct_rr_y = TRUE,
  indirect_rr_x = TRUE,
  indirect_rr_y = TRUE,
  rxx = NULL,
  rxx_restricted = TRUE,
  rxx_type = "alpha",
  k_items_x = NULL,
  ryy = NULL,
  ryy_restricted = TRUE,
  ryy_type = "alpha",
  k_items_y = NULL,
  ux = NULL,
  ux_observed = TRUE,
  uy = NULL,
  uy_observed = TRUE,
  sign_rxz = 1,
  sign_ryz = 1,
  moderators = NULL,
  cat_moderators = TRUE,
  moderator_type = c("simple", "hierarchical", "none"),
  supplemental_ads_x = NULL,
  supplemental_ads_y = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_+3A_rxyi">rxyi</code>, <code id="ma_r_+3A_r">r</code></td>
<td>
<p>Vector or column name of observed correlations. The <code>r</code> argument is used with the <code>ma_r_bb</code> (i.e., the barebones function) function and the <code>rxyi</code> argument is used with <code>ma_r</code> and <code>ma_r_ic</code> (i.e., the function in which corrections are applied).
<em>NOTE</em>: Beginning in <span class="pkg">psychmeta</span> version 2.5.2, <code>rxyi</code> values of exactly 0 in individual-correction meta-analyses are replaced with a functionally equivalent value via the <code>zero_substitute</code> argument for <code><a href="#topic+control_psychmeta">control_psychmeta</a></code> to facilitate the estimation of corrected error variances.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_n">n</code></td>
<td>
<p>Vector or column name of sample sizes.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_n_adj">n_adj</code></td>
<td>
<p>Optional: Vector or column name of sample sizes adjusted for sporadic artifact corrections.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_sample_id">sample_id</code></td>
<td>
<p>Optional vector of identification labels for samples/studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ma_method">ma_method</code></td>
<td>
<p>Method to be used to compute the meta-analysis: &quot;bb&quot; (barebones), &quot;ic&quot; (individual correction), or &quot;ad&quot; (artifact distribution).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ad_type">ad_type</code></td>
<td>
<p>For when ma_method is &quot;ad&quot;. Dpecifies the type of artifact distribution to use: &quot;int&quot; or &quot;tsa&quot;.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correction_method">correction_method</code></td>
<td>
<p>For when ma_method is &quot;ad&quot;. Character scalar or a square matrix with the collective levels of <code>construct_x</code> and <code>construct_y</code> as row names and column names.
Select one of the following methods for correcting artifacts: &quot;auto&quot;, &quot;meas&quot;, &quot;uvdrr&quot;, &quot;uvirr&quot;, &quot;bvdrr&quot;, &quot;bvirr&quot;,
&quot;rbOrig&quot;, &quot;rb1Orig&quot;, &quot;rb2Orig&quot;, &quot;rbAdj&quot;, &quot;rb1Adj&quot;, and &quot;rb2Adj&quot;.
(note: &quot;rb1Orig&quot;, &quot;rb2Orig&quot;, &quot;rb1Adj&quot;, and &quot;rb2Adj&quot; can only be used when Taylor series artifact distributions are provided and &quot;rbOrig&quot; and &quot;rbAdj&quot; can only
be used when interative artifact distributions are provided). See &quot;Details&quot; of <code><a href="#topic+ma_r_ad">ma_r_ad</a></code> for descriptions of the available methods.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_construct_x">construct_x</code>, <code id="ma_r_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for constructs initially designated as &quot;X&quot; or as &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_facet_x">facet_x</code>, <code id="ma_r_+3A_facet_y">facet_y</code></td>
<td>
<p>Vector of facet names for constructs initially designated as &quot;X&quot; or as &quot;Y&quot;.
Facet names &quot;global&quot;, &quot;overall&quot;, and &quot;total&quot; are reserved to indicate observations that represent effect sizes that have already been composited or that represent construct-level measurements rather than facet-level measurements.
To avoid double-compositing, any observation with one of these reserved names will only be eligible for auto-compositing with other such observations and will not be combined with narrow facets.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_measure_x">measure_x</code>, <code id="ma_r_+3A_measure_y">measure_y</code></td>
<td>
<p>Vector of names for measures associated with constructs initially designated as &quot;X&quot; or as &quot;Y&quot;.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which variables should be arranged, with variables listed earlier in the vector being preferred for designation as X.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_wt_type">wt_type</code></td>
<td>
<p>Type of weight to use in the meta-analysis: options are &quot;sample_size&quot;, &quot;inv_var_mean&quot; (inverse variance computed using mean effect size), and
&quot;inv_var_sample&quot; (inverse variance computed using sample-specific effect sizes). Supported options borrowed from metafor are &quot;DL&quot;, &quot;HE&quot;, &quot;HS&quot;, &quot;SJ&quot;, &quot;ML&quot;, &quot;REML&quot;, &quot;EB&quot;, and &quot;PM&quot;
(see <span class="pkg">metafor</span> documentation for details about the <span class="pkg">metafor</span> methods).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical scalar that determines whether to correct correlations for small-sample bias (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_rel">correct_rel</code></td>
<td>
<p>Optional named vector that supersedes <code>correct_rxx</code> and <code>correct_ryy</code>. Names should correspond to construct names in <code>construct_x</code> and <code>construct_y</code> to determine which constructs should be corrected for unreliability.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_rxx">correct_rxx</code>, <code id="ma_r_+3A_correct_ryy">correct_ryy</code></td>
<td>
<p>Logical scalar or vector that determines whether to correct the X or Y variable for measurement error (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_rr">correct_rr</code></td>
<td>
<p>Optional named vector that supersedes <code>correct_rr_x</code> and <code>correct_rr_y</code>. Names should correspond to construct names in <code>construct_x</code> and <code>construct_y</code> to determine which constructs should be corrected for range restriction.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_rr_x">correct_rr_x</code></td>
<td>
<p>Logical scalar, logical vector, or column name determining whether each correlation in <code>rxyi</code> should be corrected for range restriction in X (<code>TRUE</code>) or not (<code>FALSE</code>). If using artifact distribution methods, this must be a scalar value.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_correct_rr_y">correct_rr_y</code></td>
<td>
<p>Logical scalar, logical vector, or column name determining whether each correlation in <code>rxyi</code> should be corrected for range restriction in Y (<code>TRUE</code>) or not (<code>FALSE</code>). If using artifact distribution methods, this must be a scalar value.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_indirect_rr">indirect_rr</code></td>
<td>
<p>Optional named vector that supersedes <code>indirect_rr_x</code> and <code>indirect_rr_y</code>. Names should correspond to construct names in <code>construct_x</code> and <code>construct_y</code> to determine which constructs should be corrected for indirect range restriction.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_indirect_rr_x">indirect_rr_x</code></td>
<td>
<p>Logical vector or column name determining whether each correlation in <code>rxyi</code> should be corrected for indirect range restriction in X (<code>TRUE</code>) or not (<code>FALSE</code>).
Superseded in evaluation by <code>correct_rr_x</code> (i.e., if <code>correct_rr_x</code> == <code>FALSE</code>, the value supplied for <code>indirect_rr_x</code> is disregarded).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_indirect_rr_y">indirect_rr_y</code></td>
<td>
<p>Logical vector or column name determining whether each correlation in <code>rxyi</code> should be corrected for indirect range restriction in Y (<code>TRUE</code>) or not (<code>FALSE</code>).
Superseded in evaluation by <code>correct_rr_y</code> (i.e., if <code>correct_rr_y</code> == <code>FALSE</code>, the value supplied for <code>indirect_rr_y</code> is disregarded).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_rxx">rxx</code></td>
<td>
<p>Vector or column name of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_rxx_restricted">rxx_restricted</code></td>
<td>
<p>Logical vector or column name determining whether each element of <code>rxx</code> is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_rxx_type">rxx_type</code>, <code id="ma_r_+3A_ryy_type">ryy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied. Acceptable reliability types are:
</p>

<ul>
<li><p>internal_consistency<br /> A generic designation for internal-consistency reliability estimates derived from responses to a single test administration.
</p>
</li>
<li><p>multiple_administrations<br /> A generic designation for reliability estimates derived from multiple administrations of a test.
</p>
</li>
<li><p>alpha<br /> Coefficient alpha.
</p>
</li>
<li><p>lambda<br /> Generic designation for a Guttman's lambda coefficient.
</p>
</li>
<li><p>lambda1<br /> Guttman's lambda 1 coefficient.
</p>
</li>
<li><p>lambda2<br /> Guttman's lambda 2 coefficient.
</p>
</li>
<li><p>lambda3<br /> Guttman's lambda 3 coefficient.
</p>
</li>
<li><p>lambda4<br /> Guttman's lambda 4 coefficient.
</p>
</li>
<li><p>lambda5<br /> Guttman's lambda 5 coefficient.
</p>
</li>
<li><p>lambda6<br /> Guttman's lambda 6 coefficient.
</p>
</li>
<li><p>omega<br /> Omega coefficient indicating the proportion variance in a variable accounted for by modeled latent factors.
</p>
</li>
<li><p>icc<br /> Intraclass correlation coefficient.
</p>
</li>
<li><p>interrater_r<br /> Inter-rater correlation coefficient.
</p>
</li>
<li><p>interrater_r_sb<br /> Inter-rater correlation coefficient, stepped up with the Spearman-Brown formula.
</p>
</li>
<li><p>splithalf<br /> Split-half reliability coefficient.
</p>
</li>
<li><p>splithalf_sb<br /> Split-half reliability coefficient, corrected toward the full test length with the Spearman-Brown formula.
</p>
</li>
<li><p>retest<br /> Test-retest reliability coefficient.
</p>
</li>
<li><p>parallel<br /> Parallel-forms reliability coefficient with tests taken during the same testing session.
</p>
</li>
<li><p>alternate<br /> Alternate-forms reliability coefficient with tests taken during the same testing session.
</p>
</li>
<li><p>parallel_delayed<br /> Parallel-forms reliability coefficient with tests taken during separate testing sessions with a time delay in between.
</p>
</li>
<li><p>alternate_delayed<br /> Alternate-forms reliability coefficient with tests taken during separate testing sessions with a time delay in between.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ma_r_+3A_k_items_x">k_items_x</code>, <code id="ma_r_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ryy">ryy</code></td>
<td>
<p>Vector or column name of reliability estimates for Y.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ryy_restricted">ryy_restricted</code></td>
<td>
<p>Logical vector or column name determining whether each element of coderyy is an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ux">ux</code></td>
<td>
<p>Vector or column name of u ratios for X.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector or column name determining whether each element of ux is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_uy">uy</code></td>
<td>
<p>Vector or column name of u ratios for Y.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector or column name determining whether each element of uy is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_sign_rz">sign_rz</code></td>
<td>
<p>Optional named vector that supersedes <code>sign_rxz</code> and <code>sign_ryz</code>. Names should correspond to construct names in <code>construct_x</code> and <code>construct_y</code> to determine the sign of each construct's relationship with the selection mechanism.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism (for use with bvirr corrections only).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism (for use with bvirr corrections only).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_moderators">moderators</code></td>
<td>
<p>Either (1) a vector of column names in <code>data</code> of moderator variables to be used in the meta-analysis (names can be quoted or unquoted), or (2) a vector, data frame, or matrix containing moderator variables.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Either (1) A character vector listing the variable names in <code>moderators</code> that are categorical, or (2) a logical scalar or vector identifying whether each variable in <code>moderators</code> is categorical (<code>TRUE</code>) or continuous (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis: &quot;none&quot; means that no moderators are to be used, &quot;simple&quot; means that moderators are to be examined one at a time, and
&quot;hierarchical&quot; means that all possible combinations and subsets of moderators are to be examined.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_supplemental_ads">supplemental_ads</code></td>
<td>
<p>For <code>ma_r</code> only: Named list (named according to the constructs included in the meta-analysis) of supplemental artifact distribution information from studies not included in the meta-analysis. This is a list of lists, where the elements of a list associated with a construct are named like the arguments of the <code>create_ad()</code> function.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments and/or moderators.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ma_obj">ma_obj</code></td>
<td>
<p>For <code>ma_r_ad</code> only: Meta-analysis object of correlations or <em>d</em> values (regardless of input metric, output metric will be <em>r</em>).</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ad_obj_x">ad_obj_x</code></td>
<td>
<p>For <code>ma_r_ad</code> only: Artifact-distribution object for the X variable (output of the <code><a href="#topic+create_ad">create_ad</a></code> function).
If ma_obj is of the class <code>ma_master</code> (i.e,. the output of <code><a href="#topic+ma_r">ma_r</a></code> or <code><a href="#topic+ma_d">ma_d</a></code>), the object supplied for
<code>ad_obj_x</code> must be a named list of artifact distributions with names corresponding to the &quot;X&quot; constructs in the meta-analyses contained within <code>ma_obj</code>.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_ad_obj_y">ad_obj_y</code></td>
<td>
<p>For <code>ma_r_ad</code> only: Artifact-distribution object for the Y variable (output of the <code>create_ad</code> function).
If ma_obj is of the class <code>ma_master</code>, the object supplied for <code>ad_obj_y</code> must be a named list of artifact distributions with names
corresponding to the &quot;Y&quot; constructs in the meta-analyses contained within <code>ma_obj</code>.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_use_ic_ads">use_ic_ads</code></td>
<td>
<p>For <code>ma_r_ad</code> only: Determines whether artifact distributions should be extracted from the individual correction results in <code>ma_obj</code>.
Only evaluated when <code>ad_obj_x</code> or <code>ad_obj_y</code> is NULL and <code>ma_obj</code> does not contain individual correction results.
Use one of the following commands: <code>tsa</code> to use the Taylor series method or <code>int</code> to use the interactive method.</p>
</td></tr>
<tr><td><code id="ma_r_+3A_supplemental_ads_x">supplemental_ads_x</code>, <code id="ma_r_+3A_supplemental_ads_y">supplemental_ads_y</code></td>
<td>
<p>For <code>ma_r_ic</code> only: List supplemental artifact distribution information from studies not included in the meta-analysis. The elements of this list  are named like the arguments of the <code>create_ad()</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The options for <code>correction_method</code> are:
</p>

<ul>
<li><p>&quot;auto&quot;<br /> Automatic selection of the most appropriate correction procedure, based on the available artifacts and the logical arguments provided to the function. (default)
</p>
</li>
<li><p>&quot;meas&quot;<br /> Correction for measurement error only.
</p>
</li>
<li><p>&quot;uvdrr&quot;<br /> Correction for univariate direct range restriction (i.e., Case II). The choice of which variable to correct for range restriction is made using the <code>correct_rr_x</code> and <code>correct_rr_y</code> arguments.
</p>
</li>
<li><p>&quot;uvirr&quot;<br /> Correction for univariate indirect range restriction (i.e., Case IV). The choice of which variable to correct for range restriction is made using the <code>correct_rr_x</code> and <code>correct_rr_y</code> arguments.
</p>
</li>
<li><p>&quot;bvdrr&quot;<br /> Correction for bivariate direct range restriction. Use with caution: This correction is an approximation only and is known to have a positive bias.
</p>
</li>
<li><p>&quot;bvirr&quot;<br /> Correction for bivariate indirect range restriction (i.e., Case V).
</p>
</li>
<li><p>&quot;rbOrig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied interactively. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rbAdj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied interactively. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb1Orig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA1 method. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb1Adj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA1 method. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb2Orig&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA2 method. We recommend using &quot;uvdrr&quot; instead.
</p>
</li>
<li><p>&quot;rb2Adj&quot;<br /> Not recommended: Raju and Burke's version of the correction for direct range restriction, applied using their TSA2 method. Adjusted to account for range restriction in the reliability of the Y variable. We recommend using &quot;uvdrr&quot; instead.
</p>
</li></ul>



<h3>Value</h3>

<p>A nested tabular object of the class &quot;ma_psychmeta&quot;.
Components of output tables for bare-bones meta-analyses:
</p>

<ul>
<li><p><code>pair_id</code><br /> Unique identification number for each construct pairing.
</p>
</li>
<li><p><code>construct_x</code><br /> Name of the variable analyzed as construct X.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_r</code><br /> Mean observed correlation.
</p>
</li>
<li><p><code>var_r</code><br /> Weighted variance of observed correlations.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed correlations.
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed correlations after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_r</code><br /> Square root of <code>var_r</code>.
</p>
</li>
<li><p><code>se_r</code><br /> Standard error of <code>mean_r</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_r</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_r</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_r</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_r</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>

<p>Components of output tables for individual-correction meta-analyses:
</p>

<ul>
<li><p><code>pair_id</code><br /> Unique identification number for each construct pairing.
</p>
</li>
<li><p><code>construct_x</code><br /> Name of the variable analyzed as construct X.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_r</code><br /> Mean observed correlation.
</p>
</li>
<li><p><code>var_r</code><br /> Weighted variance of observed correlations.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed correlations.
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed correlations after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_r</code><br /> Square root of <code>var_r</code>.
</p>
</li>
<li><p><code>se_r</code><br /> Standard error of <code>mean_r</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>mean_rho</code><br /> Mean artifact-corrected correlation.
</p>
</li>
<li><p><code>var_r_c</code><br /> Variance of artifact-corrected correlations.
</p>
</li>
<li><p><code>var_e_c</code><br /> Predicted sampling-error variance of artifact-corrected correlations.
</p>
</li>
<li><p><code>var_rho</code><br /> Variance of artifact-corrected correlations after removing predicted sampling-error variance.
</p>
</li>
<li><p><code>sd_r_c</code><br /> Square root of <code>var_r_c</code>.
</p>
</li>
<li><p><code>se_r_c</code><br /> Standard error of <code>mean_rho</code>.
</p>
</li>
<li><p><code>sd_e_c</code><br /> Square root of <code>var_e_c</code>.
</p>
</li>
<li><p><code>sd_rho</code><br /> Square root of <code>var_rho</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_rho</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_rho</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_rho</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_rho</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>

<p>Components of output tables for artifact-distribution meta-analyses:
</p>

<ul>
<li><p><code>pair_id</code><br /> Unique identification number for each construct pairing.
</p>
</li>
<li><p><code>construct_x</code><br /> Name of the variable analyzed as construct X.
</p>
</li>
<li><p><code>construct_y</code><br /> Name of the variable analyzed as construct Y.
</p>
</li>
<li><p><code>analysis_id</code><br /> Unique identification number for each analysis.
</p>
</li>
<li><p><code>analysis_type</code><br /> Type of moderator analyses: Overall, Simple Moderator, or Hierarchical Moderator.
</p>
</li>
<li><p><code>k</code><br /> Number of effect sizes meta-analyzed.
</p>
</li>
<li><p><code>N</code><br /> Total sample size of all effect sizes in the meta-analysis.
</p>
</li>
<li><p><code>mean_r</code><br /> Mean observed correlation.
</p>
</li>
<li><p><code>var_r</code><br /> Weighted variance of observed correlations.
</p>
</li>
<li><p><code>var_e</code><br /> Predicted sampling-error variance of observed correlations.
</p>
</li>
<li><p><code>var_art</code><br /> Amount of variance in observed correlations that is attributable to measurement-error and range-restriction artifacts.
</p>
</li>
<li><p><code>var_pre</code><br /> Total predicted artifactual variance (i.e., the sum of <code>var_e</code> and <code>var_art</code>).
</p>
</li>
<li><p><code>var_res</code><br /> Variance of observed correlations after removing predicted sampling-error variance and predicted artifact variance.
</p>
</li>
<li><p><code>sd_r</code><br /> Square root of <code>var_r</code>.
</p>
</li>
<li><p><code>se_r</code><br /> Standard error of <code>mean_r</code>.
</p>
</li>
<li><p><code>sd_e</code><br /> Square root of <code>var_e</code>.
</p>
</li>
<li><p><code>sd_art</code><br /> Square root of <code>var_art</code>.
</p>
</li>
<li><p><code>sd_pre</code><br /> Square root of <code>var_pre</code>.
</p>
</li>
<li><p><code>sd_res</code><br /> Square root of <code>var_res</code>.
</p>
</li>
<li><p><code>mean_rho</code><br /> Mean artifact-corrected correlation.
</p>
</li>
<li><p><code>var_r_c</code><br /> Weighted variance of observed correlations corrected to the metric of rho.
</p>
</li>
<li><p><code>var_e_c</code><br /> Predicted sampling-error variance of observed correlations corrected to the metric of rho.
</p>
</li>
<li><p><code>var_art_c</code><br /> Amount of variance in observed correlations that is attributable to measurement-error and range-restriction artifacts corrected to the metric of rho.
</p>
</li>
<li><p><code>var_pre_c</code><br /> Total predicted artifactual variance (i.e., the sum of <code>var_e</code> and <code>var_art</code>) corrected to the metric of rho.
</p>
</li>
<li><p><code>var_rho</code><br /> Variance of artifact-corrected correlations after removing predicted sampling-error variance and predicted artifact variance.
</p>
</li>
<li><p><code>sd_r_c</code><br /> Square root of <code>var_r</code> corrected to the metric of rho.
</p>
</li>
<li><p><code>se_r_c</code><br /> Standard error of <code>mean_r</code> corrected to the metric of rho.
</p>
</li>
<li><p><code>sd_e_c</code><br /> Square root of <code>var_e</code> corrected to the metric of rho.
</p>
</li>
<li><p><code>sd_art_c</code><br /> Square root of <code>var_art</code> corrected to the metric of rho.
</p>
</li>
<li><p><code>sd_pre_c</code><br /> Square root of <code>var_pre</code> corrected to the metric of rho.
</p>
</li>
<li><p><code>sd_rho</code><br /> Square root of <code>var_rho</code>.
</p>
</li>
<li><p><code>CI_LL_XX</code><br /> Lower limit of the confidence interval around <code>mean_rho</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CI_UL_XX</code><br /> Upper limit of the confidence interval around <code>mean_rho</code>, where &quot;XX&quot; represents the confidence level as a percentage.
</p>
</li>
<li><p><code>CR_LL_XX</code><br /> Lower limit of the credibility interval around <code>mean_rho</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li>
<li><p><code>CR_UL_XX</code><br /> Upper limit of the credibility interval around <code>mean_rho</code>, where &quot;XX&quot; represents the credibility level as a percentage.
</p>
</li></ul>



<h3>Note</h3>

<p>The difference between &quot;rb&quot; methods with the &quot;orig&quot; and &quot;adj&quot; suffixes is that the original does not account for the impact of range restriction on criterion reliabilities, whereas
the adjusted procedure attempts to estimate the applicant reliability information for the criterion. The &quot;rb&quot; procedures are included for posterity: We strongly recommend using
the &quot;uvdrr&quot; procedure to appropriately correct for univariate range restriction.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425–438. doi: <a href="https://doi.org/10.1037/0021-9010.79.3.425">10.1037/0021-9010.79.3.425</a>
</p>
<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983).
Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## The 'ma_r' function can compute multi-construct bare-bones meta-analyses:
ma_obj &lt;- ma_r(rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
     construct_x = x_name, construct_y = y_name, sample_id = sample_id,
     moderators = moderator, data = data_r_meas_multi)
summary(ma_obj)

## It can also perform multiple individual-correction meta-analyses:
ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               moderators = moderator, data = data_r_meas_multi)
summary(ma_obj)
ma_obj$meta_tables[[1]]$individual_correction$true_score

## And 'ma_r' can also curate artifact distributions and compute multiple
## artifact-distribution meta-analyses:
ma_obj &lt;- ma_r(ma_method = "ad", ad_type = "int", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               correct_rr_x = FALSE, correct_rr_y = FALSE,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               clean_artifacts = FALSE, impute_artifacts = FALSE,
               moderators = moderator, data = data_r_meas_multi)
summary(ma_obj)
ma_obj$meta_tables[[1]]$artifact_distribution$true_score

## Even if no studies in the database provide artifact information,
## pre-specified artifact distributions from previous meta-analyses
## can still be used! (These results should match the previous example.)
ma_obj &lt;- ma_r(ma_method = "ad", rxyi = rxyi, n = n,
               correct_rr_x = FALSE, correct_rr_y = FALSE,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               clean_artifacts = FALSE, impute_artifacts = FALSE,
               moderators = moderator, data = data_r_meas_multi,
               supplemental_ads =
                    list(X = list(mean_qxi = 0.8927818, var_qxi = 0.0008095520, k_qxi = 40,
                                  mean_n_qxi = 11927 / 40, qxi_dist_type = "alpha"),
                         Y = list(mean_qxi = 0.8941266, var_qxi = 0.0009367234, k_qxi = 40,
                                  mean_n_qxi = 11927 / 40, qxi_dist_type = "alpha"),
                         Z = list(mean_qxi = 0.8962108, var_qxi = 0.0007840593, k_qxi = 40,
                                  mean_n_qxi = 11927 / 40, qxi_dist_type = "alpha")))
summary(ma_obj)
ma_obj$meta_tables[[1]]$artifact_distribution$true_score

## Artifact information may also be supplied by passing "ad_obj" class objects with the
## "supplemental_ads" argument.
## Create a list of artifact-distribution objects:
ad_list &lt;- create_ad_list(n = n, rxx = rxxi, ryy = ryyi,
                          construct_x = x_name, construct_y = y_name,
                          sample_id = sample_id,
                          data = data_r_meas_multi)
ad_list &lt;- setNames(ad_list$ad_x, ad_list$construct_x)

## Run the artifact-distribution meta-analysis:
ma_obj &lt;- ma_r(ma_method = "ad", rxyi = rxyi, n = n,
               correct_rr_x = FALSE, correct_rr_y = FALSE,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               clean_artifacts = FALSE, impute_artifacts = FALSE,
               moderators = moderator, data = data_r_meas_multi,
               supplemental_ads = ad_list)
summary(ma_obj)
ma_obj$meta_tables[[1]]$artifact_distribution$true_score


## Artifact information from studies not included in the meta-analysis can also be used to make
## corrections. Passing artifact information with the 'supplemental_ads' argument allows for
## additional artifact values and/or means and variances of artifacts to be used.
## The 'supplemental_ads' analysis below gives the same results as the prior meta-analysis.
x_ids &lt;- c(data_r_meas_multi$x_name, data_r_meas_multi$y_name) == "X"
rxxi &lt;- c(data_r_meas_multi$rxxi, data_r_meas_multi$ryyi)[x_ids]
n_rxxi = c(data_r_meas_multi$n, data_r_meas_multi$n)[x_ids]

y_ids &lt;- c(data_r_meas_multi$x_name, data_r_meas_multi$y_name) == "Y"
ryyi &lt;- c(data_r_meas_multi$rxxi, data_r_meas_multi$ryyi)[y_ids]
n_ryyi = c(data_r_meas_multi$n, data_r_meas_multi$n)[y_ids]

z_ids &lt;- c(data_r_meas_multi$x_name, data_r_meas_multi$y_name) == "Z"
rzzi &lt;- c(data_r_meas_multi$rxxi, data_r_meas_multi$ryyi)[z_ids]
n_rzzi = c(data_r_meas_multi$n, data_r_meas_multi$n)[z_ids]

ma_obj &lt;- ma_r(ma_method = "ad", rxyi = rxyi, n = n,
               correct_rr_x = FALSE, correct_rr_y = FALSE,
               construct_x = x_name, construct_y = y_name,
               moderators = moderator, sample_id = sample_id, data = data_r_meas_multi,
               supplemental_ads = list(X = list(rxxi = rxxi, n_rxxi = n_rxxi, wt_rxxi = n_rxxi),
                                       Y = list(rxxi = ryyi, n_rxxi = n_ryyi, wt_rxxi = n_ryyi),
                                       Z = list(rxxi = rzzi, n_rxxi = n_rzzi, wt_rxxi = n_rzzi)))
summary(ma_obj)
ma_obj$meta_tables[[1]]$artifact_distribution$true_score

## If 'use_all_arts' is set to TRUE, artifacts from studies without valid correlations
## will be used to inform artifact distributions. Below, correlations and artifacts
## are provided by non-overlapping sets of studies.
dat1 &lt;- dat2 &lt;- data_r_meas_multi
dat1$rxxi &lt;- dat1$ryyi &lt;- NA
dat2$rxyi &lt;- NA
dat2$sample_id &lt;- dat2$sample_id + 40
dat &lt;- rbind(dat1, dat2)
ma_obj &lt;- ma_r(ma_method = "ad", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               correct_rr_x = FALSE, correct_rr_y = FALSE,
               construct_x = x_name, construct_y = y_name,
               sample_id = sample_id, moderators = moderator,
               use_all_arts = TRUE, data = dat)
summary(ma_obj)
ma_obj$meta_tables[[1]]$artifact_distribution$true_score



### Demonstration of ma_r_bb ###
## Example analysis using data from Gonzalez-Mule et al. (2014):

## Not correcting for bias and using normal distributions to compute uncertainty intervals
## allows for exact replication of the results reported in the text:
ma_r_bb(r = rxyi, n = n, correct_bias = FALSE, conf_method = "norm", cred_method = "norm",
               data = data_r_gonzalezmule_2014)

## Using hs_override = TRUE allows one to easily implement the traditional Hunter-Schmidt method:
ma_r_bb(r = rxyi, n = n, hs_override = TRUE, data = data_r_gonzalezmule_2014)

## With hs_override = FALSE, the program defaults will compute unbiased variances and use
## t-distributions to estimate confidence and credibility intervals - these settings make
## a noticeable difference for small studies like the textbook example:
ma_r_bb(r = rxyi, n = n, hs_override = FALSE, data = data_r_gonzalezmule_2014)



### Demonstration of ma_r_ic ###
## Simulated example satisfying the assumptions of the Case IV
## range-restriction correction (parameter values: mean_rho = .3, sd_rho = .15):
ma_r_ic(rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi, ux = ux, data = data_r_uvirr)

## Simulated example satisfying the assumptions of the Case V
## range-restriction correction
ma_r_ic(rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
        rxx_type = "parallel", ryy_type = "parallel",
        ux = ux, uy = uy, data = data_r_bvirr)

## Published example from Gonzalez-Mule et al. (2014)
ma_r_ic(rxyi = rxyi, n = n, hs_override = TRUE, data = data_r_gonzalezmule_2014,
        rxx = rxxi, ryy = ryyi, ux = ux, indirect_rr_x = TRUE,
        moderators = c("Rating source", "Published", "Type", "Complexity"))



### Demonstration of ma_r_ad ###
## Compute barebones meta-analysis
ma_obj &lt;- ma_r_bb(r = rxyi, n = n, correct_bias = FALSE,
                           conf_method = "norm", cred_method = "norm", data = data_r_mcdaniel_1994)

## Construct artifact distribution for X
ad_obj_x &lt;- create_ad(ad_type = "tsa", mean_rxxi = data_r_mcdaniel_1994$Mrxxi[1],
                      var_rxxi = data_r_mcdaniel_1994$SDrxxi[1]^.5,
                      ux = data_r_mcdaniel_1994$ux,
                      wt_ux = data_r_mcdaniel_1994$`ux frequency`)

## Construct artifact distribution for Y
ad_obj_y &lt;- create_ad(ad_type = "tsa", rxxi = data_r_mcdaniel_1994$ryyi,
                      wt_rxxi = data_r_mcdaniel_1994$`ryyi frequency`)

## Compute artifact-distribution meta-analysis, correcting for measurement error only
ma_r_ad(ma_obj = ma_obj, ad_obj_x = ad_obj_x, ad_obj_y = ad_obj_y, correction_method = "meas")

## Compute artifact-distribution meta-analysis, correcting for univariate direct range restriction
ma_r_ad(ma_obj = ma_obj, ad_obj_x = ad_obj_x, ad_obj_y = ad_obj_y, correction_method = "uvdrr",
        correct_rr_y = FALSE, indirect_rr_x = FALSE)


# The results of ma_r() can also be corrected using artifact distributions
ma_obj &lt;- ma_r(ma_method = "bb", rxyi = rxyi, n = n,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               moderators = moderator, data = data_r_meas_multi)

# The create_ad_list function can be used to generate batches of artifact-distribution objects.
# Here is an example in which one distribution is created per construct.
ad_tibble &lt;- create_ad_list(n = n, rxx = rxxi, ryy = ryyi,
                            construct_x = x_name, construct_y = y_name,
                            sample_id = sample_id,
                            data = data_r_meas_multi)
# Passing that collection of distributions to ma_r_ad() corrects 'ma_obj' for artifacts:
ma_obj_tibble &lt;- ma_r_ad(ma_obj = ma_obj,
                         ad_obj_x = ad_tibble, ad_obj_y = ad_tibble)
summary(ma_obj_tibble)
ma_obj_tibble$meta_tables[[1]]$artifact_distribution$true_score


# The same outcomes as the previous example can be achieved by passing a named list of
# artifact information, with each element bearing the name of a construct:
ad_list &lt;- setNames(ad_tibble$ad_x, ad_tibble$construct_x)
ma_obj_list &lt;- ma_r_ad(ma_obj = ma_obj,
                       ad_obj_x = ad_list, ad_obj_y = ad_list)
summary(ma_obj_list)
ma_obj_list$meta_tables[[1]]$artifact_distribution$true_score


# It is also possible to construct artifact distributions in a pairwise fashion.
# For example, if correlations between X and Y and between X and Z are being analyzed,
# X will get a different distribution for its relationships with Y than with Z.
# These pairwise distributions are based only on artifact data from specific construct pairs.
ad_tibble_pair &lt;- create_ad_list(n = n, rxx = rxxi, ryy = ryyi,
                                 construct_x = x_name, construct_y = y_name,
                                 sample_id = sample_id,
                                 control = control_psychmeta(pairwise_ads = TRUE),
                                 data = data_r_meas_multi)
# Passing these pairwise distributions to ma_r_ad() corrects 'ma_obj' for artifacts:
ma_obj_pair &lt;- ma_r_ad(ma_obj = ma_obj,
                       ad_obj_x = ad_tibble_pair, ad_obj_y = ad_tibble_pair)
summary(ma_obj_pair)
ma_obj_pair$meta_tables[[1]]$artifact_distribution$true_score


# Sometimes moderators have important influcnces on artifact distributions as well as
# distributions of effect sizes. When this occurs, moderated artifact distributions
# can be created to make more appropriate corrections.
ad_tibble_mod &lt;- create_ad_list(n = n, rxx = rxxi, ryy = ryyi,
                                construct_x = x_name, construct_y = y_name,
                                sample_id = sample_id,
                                control = control_psychmeta(moderated_ads = TRUE),
                                moderators = moderator,
                                data = data_r_meas_multi)
# Passing these moderated distributions to ma_r_ad() corrects 'ma_obj' for artifacts:
ma_obj_mod &lt;- ma_r_ad(ma_obj = ma_obj,
                      ad_obj_x = ad_tibble_mod, ad_obj_y = ad_tibble_mod)
summary(ma_obj_mod)
ma_obj_mod$meta_tables[[1]]$artifact_distribution$true_score


# It is also possible to create pairwise moderated artifact distributions.
ad_tibble_pairmod &lt;- create_ad_list(n = n, rxx = rxxi, ryy = ryyi,
                                    construct_x = x_name, construct_y = y_name,
                                    sample_id = sample_id,
                                    control = control_psychmeta(moderated_ads = TRUE,
                                                                pairwise_ads = TRUE),
                                    moderators = moderator,
                                    data = data_r_meas_multi)
# Passing these pairwise moderated distributions to ma_r_ad() corrects 'ma_obj' for artifacts:
ma_obj_pairmod &lt;- ma_r_ad(ma_obj = ma_obj,
                          ad_obj_x = ad_tibble_pairmod, ad_obj_y = ad_tibble_pairmod)
summary(ma_obj_pairmod)
ma_obj_pairmod$meta_tables[[1]]$artifact_distribution$true_score


# For even more control over which artifact distributions are used in corrections, you can supply
# un-named list of distributions in which the order of distributions corresponds to the order of
# meta-analyses in ma_obj. It is important for the elements to be un-named, as the absence of names
# and the length of the list are the two ways in which ma_r_ad() validates the lists.
ad_list_pairmod_x &lt;- ad_tibble_pairmod$ad_x
ad_list_pairmod_y &lt;- ad_tibble_pairmod$ad_y
# Passing these lists of distributions to ma_r_ad() corrects 'ma_obj' for artifacts:
ma_obj_pairmodlist &lt;- ma_r_ad(ma_obj = ma_obj,
                              ad_obj_x = ad_list_pairmod_x, ad_obj_y = ad_list_pairmod_y)
summary(ma_obj_pairmodlist)
ma_obj_pairmodlist$meta_tables[[1]]$artifact_distribution$true_score

## End(Not run)
</code></pre>

<hr>
<h2 id='ma_r_ad.int_bvdrr'>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</h2><span id='topic+ma_r_ad.int_bvdrr'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_bvdrr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_bvdrr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.int_bvirr'>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</h2><span id='topic+ma_r_ad.int_bvirr'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_bvirr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_bvirr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.int_meas'>Interactive artifact-distribution meta-analysis correcting for measurement error</h2><span id='topic+ma_r_ad.int_meas'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_meas(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_meas_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.)</em>.
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>

<hr>
<h2 id='ma_r_ad.int_none'>Null artifact distribution result: No corrections performed</h2><span id='topic+ma_r_ad.int_none'></span><span id='topic+ma_r_ad.tsa_none'></span>

<h3>Description</h3>

<p>Null artifact distribution result: No corrections performed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_none(x)

ma_r_ad.tsa_none(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_none_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.int_rbAdj'>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</h2><span id='topic+ma_r_ad.int_rbAdj'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_rbAdj(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_rbAdj_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425–438. doi: <a href="https://doi.org/10.1037/0021-9010.79.3.425">10.1037/0021-9010.79.3.425</a>
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983).
Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.int_rbOrig'>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</h2><span id='topic+ma_r_ad.int_rbOrig'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_rbOrig(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_rbOrig_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.)</em>.
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425.
</p>
<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.int_uvdrr'>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</h2><span id='topic+ma_r_ad.int_uvdrr'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_uvdrr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_uvdrr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425–438. doi: <a href="https://doi.org/10.1037/0021-9010.79.3.425">10.1037/0021-9010.79.3.425</a>
</p>

<hr>
<h2 id='ma_r_ad.int_uvirr'>Interactive artifact-distribution meta-analysis correcting for Case IV indirect range restriction and measurement error</h2><span id='topic+ma_r_ad.int_uvirr'></span>

<h3>Description</h3>

<p>Interactive artifact-distribution meta-analysis correcting for Case IV indirect range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.int_uvirr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.int_uvirr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Le, H., &amp; Schmidt, F. L. (2006).
Correcting for indirect range restriction in meta-analysis: Testing a new meta-analytic procedure.
<em>Psychological Methods, 11</em>(4), 416–438. doi: <a href="https://doi.org/10.1037/1082-989X.11.4.416">10.1037/1082-989X.11.4.416</a>
</p>
<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. Chapter 4.
</p>
<p>Law, K. S., Schmidt, F. L., &amp; Hunter, J. E. (1994).
Nonlinearity of range corrections in meta-analysis: Test of an improved procedure.
<em>Journal of Applied Psychology, 79</em>(3), 425–438. doi: <a href="https://doi.org/10.1037/0021-9010.79.3.425">10.1037/0021-9010.79.3.425</a>
</p>

<hr>
<h2 id='ma_r_ad.tsa_bvdrr'>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_bvdrr'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_bvdrr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_bvdrr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.tsa_bvirr'>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_bvirr'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Case V indirect range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_bvirr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_bvirr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.tsa_meas'>Taylor series approximation artifact-distribution meta-analysis correcting for measurement error</h2><span id='topic+ma_r_ad.tsa_meas'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_meas(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_meas_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.tsa_rb1Adj'>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_rb1Adj'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_rb1Adj(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_rb1Adj_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of artifact-distribution meta-analysis results to be returned to the ma_r_ad function.
</p>


<h3>References</h3>

<p>Raju, N. S., &amp; Burke, M. J. (1983).
Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.tsa_rb1Orig'>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_rb1Orig'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 1 direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_rb1Orig(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_rb1Orig_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of artifact-distribution meta-analysis results to be returned to the ma_r_ad function.
</p>


<h3>References</h3>

<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.tsa_rb2Adj'>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_rb2Adj'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_rb2Adj(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_rb2Adj_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of artifact-distribution meta-analysis results to be returned to the ma_r_ad function.
</p>


<h3>References</h3>

<p>Raju, N. S., &amp; Burke, M. J. (1983).
Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382–395. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.tsa_rb2Orig'>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_rb2Orig'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Raju and Burke's case 2 direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_rb2Orig(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_rb2Orig_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of artifact-distribution meta-analysis results to be returned to the ma_r_ad function.
</p>


<h3>References</h3>

<p>Raju, N. S., &amp; Burke, M. J. (1983). Two new procedures for studying validity generalization.
<em>Journal of Applied Psychology, 68</em>(3), 382. doi: <a href="https://doi.org/10.1037/0021-9010.68.3.382">10.1037/0021-9010.68.3.382</a>
</p>

<hr>
<h2 id='ma_r_ad.tsa_uvdrr'>Taylor series approximation artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_uvdrr'></span>

<h3>Description</h3>

<p>Implements a newly derived TSA model based on the Case II formula for direct range restriction to estimate sd_rho.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_uvdrr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_uvdrr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>

<hr>
<h2 id='ma_r_ad.tsa_uvirr'>Taylor series approximation artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error</h2><span id='topic+ma_r_ad.tsa_uvirr'></span>

<h3>Description</h3>

<p>Taylor series approximation artifact-distribution meta-analysis correcting for Case II direct range restriction and measurement error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_ad.tsa_uvirr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_ad.tsa_uvirr_+3A_x">x</code></td>
<td>
<p>List of bare-bones meta-analytic data, artifact-distribution objects for X and Y, and other meta-analysis options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A meta-analysis class object containing all results.
</p>


<h3>References</h3>

<p>Hunter, J. E., Schmidt, F. L., &amp; Le, H. (2006).
Implications of direct and indirect range restriction for meta-analysis methods and findings.
<em>Journal of Applied Psychology, 91</em>(3), 594–612. doi: <a href="https://doi.org/10.1037/0021-9010.91.3.594">10.1037/0021-9010.91.3.594</a>
</p>

<hr>
<h2 id='ma_r_order2'>Second-order meta-analysis function for correlations</h2><span id='topic+ma_r_order2'></span>

<h3>Description</h3>

<p>This function computes second-order meta-analysis function for correlations. It supports second-order analyses of bare-bones, artifact-distribution, and individual-correction meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_r_order2(
  k,
  N = NULL,
  r = NULL,
  rho = NULL,
  var_r = NULL,
  var_r_c = NULL,
  ma_type = c("bb", "ic", "ad"),
  sample_id = NULL,
  citekey = NULL,
  moderators = NULL,
  moderator_type = "simple",
  construct_x = NULL,
  construct_y = NULL,
  construct_order = NULL,
  data = NULL,
  control = control_psychmeta(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_r_order2_+3A_k">k</code></td>
<td>
<p>Vector or column name of meta-analyses' k values.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_n">N</code></td>
<td>
<p>Vector or column name of meta-analyses' total sample sizes (optional).</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_r">r</code></td>
<td>
<p>Vector or column name of mean observed correlations.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_rho">rho</code></td>
<td>
<p>Vector or column name of mean corrected correlations.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_var_r">var_r</code></td>
<td>
<p>Vector or column name of observed variances of observed correlations.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_var_r_c">var_r_c</code></td>
<td>
<p>Vector or column name of observed variances of corrected correlations.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_ma_type">ma_type</code></td>
<td>
<p>Type of meta-analyses being analyzed: &quot;bb&quot; (barebones), &quot;ic&quot; (individual correction), or &quot;ad&quot; (artifact distribution).</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_sample_id">sample_id</code></td>
<td>
<p>Vector or column name of study ID labels.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_moderators">moderators</code></td>
<td>
<p>Matrix or column names of moderator variables to be used in the meta-analysis (can be a vector in the case of one moderator).</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis (&quot;none&quot;, &quot;simple&quot;, or &quot;hierarchical&quot;).</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_construct_x">construct_x</code></td>
<td>
<p>Vector or column name of construct names for X.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector or column name of construct names for Y.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which variables should be arranged, with variables listed earlier in the vector being preferred for designation as X.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_data">data</code></td>
<td>
<p>Data frame containing columns whose names may be provided as arguments to vector arguments and/or moderators.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_control">control</code></td>
<td>
<p>Output from the <code>control_psychmeta()</code> function or a list of arguments controlled by the <code>control_psychmeta()</code> function. Ellipsis arguments will be screened for internal inclusion in <code>control</code>.</p>
</td></tr>
<tr><td><code id="ma_r_order2_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to functions called within the meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A nested tabular object of the class &quot;ma_psychmeta&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Analysis of the validity of conscientiousness as a predictor of job performance in East Asia
out &lt;- ma_r_order2(k = k, r = r_bar_i, rho = rho_bar_i, var_r = var_r,
                   var_r_c = NULL, ma_type = c("bb", "ad"),
                   sample_id = NULL, moderators = NULL,
                   construct_x = NULL, construct_y = NULL,
                   data = dplyr::filter(data_r_oh_2009, Predictor == "Conscientiousness"))
summary(out)

## Analysis of the validity of the Big Five traits as predictors of job performance in East Asia
out &lt;- ma_r_order2(k = k, r = r_bar_i, rho = rho_bar_i, var_r = var_r,
                   var_r_c = NULL, ma_type = c("bb", "ad"),
                   sample_id = NULL, moderators = NULL, construct_x = Predictor,
                   data = data_r_oh_2009)
summary(out)

## Analysis of the average validity of the Big Five traits as predictors of
## job performance by Eastern Asian country
out &lt;- ma_r_order2(k = k, r = r_bar_i, rho = rho_bar_i, var_r = var_r,
                   var_r_c = NULL, ma_type = c("bb", "ad"),
                   sample_id = NULL, moderators = "Country", data = data_r_oh_2009)
summary(out)
</code></pre>

<hr>
<h2 id='ma_wrapper'>Wrapper function to compute meta-analytic results for all analyses.</h2><span id='topic+ma_wrapper'></span>

<h3>Description</h3>

<p>Wrapper function to compute meta-analytic results for all analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma_wrapper(
  es_data,
  es_type = "r",
  ma_type = "bb",
  ma_fun,
  moderator_matrix = NULL,
  moderator_type = "all",
  cat_moderators = TRUE,
  construct_x = NULL,
  construct_y = NULL,
  ma_arg_list,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ma_wrapper_+3A_es_data">es_data</code></td>
<td>
<p>Matrix of effect-size data.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_es_type">es_type</code></td>
<td>
<p>Effect-size type (e.g., &quot;r&quot; or &quot;d&quot;)</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_ma_type">ma_type</code></td>
<td>
<p>The meta-analysis type: &quot;bb&quot; or &quot;individual_correction.&quot;</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_ma_fun">ma_fun</code></td>
<td>
<p>Meta-analysis function to be used in computing meta-analytic results.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_moderator_matrix">moderator_matrix</code></td>
<td>
<p>Matrix (or vector) of moderator variables.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis: &quot;none&quot; means that no moderators are to be used, &quot;simple&quot; means that moderators are to be examined one at a time,
&quot;hierarchical&quot; means that all possible combinations and subsets of moderators are to be examined, and &quot;all&quot; means that simple and hierarchical moderator analyses are to be performed.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Logical vector identifying whether each variable in the moderator_matrix is a categorical variable (TRUE) or a continuous variable (FALSE).</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_construct_x">construct_x</code></td>
<td>
<p>Vector of construct names for construct X.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for construct Y.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_ma_arg_list">ma_arg_list</code></td>
<td>
<p>List of arguments to be passed to the meta-analysis function.</p>
</td></tr>
<tr><td><code id="ma_wrapper_+3A_...">...</code></td>
<td>
<p>Further arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of meta-analytic results.
</p>

<hr>
<h2 id='manage_arglength'>Check the length of x against the length of y and replicate z if necessary</h2><span id='topic+manage_arglength'></span>

<h3>Description</h3>

<p>Check the length of x against the length of y and replicate z if necessary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manage_arglength(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manage_arglength_+3A_x">x</code></td>
<td>
<p>Argument to be checked.</p>
</td></tr>
<tr><td><code id="manage_arglength_+3A_y">y</code></td>
<td>
<p>Argument against which <code>x</code> should be checked.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Error message or vector of values
</p>

<hr>
<h2 id='merge_simdat_d'>Merge multiple &quot;simdat_d_database&quot; class objects</h2><span id='topic+merge_simdat_d'></span>

<h3>Description</h3>

<p>This function allows for multiple simulated databases from <code><a href="#topic+simulate_d_database">simulate_d_database</a></code> to be merged together into a single database. Merged databases will be assigned moderator variable codes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_simdat_d(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_simdat_d_+3A_...">...</code></td>
<td>
<p>Collection of objects created by the &quot;simulate_d_database&quot; function. Simply enter the database objects as <code>merge_simdat_d</code>(data_obj1, data_obj2, data_obj_3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A merged database of class <code>simdat_d</code>
</p>

<hr>
<h2 id='merge_simdat_r'>Merge multiple &quot;simdat_r_database&quot; class objects</h2><span id='topic+merge_simdat_r'></span>

<h3>Description</h3>

<p>This function allows for multiple simulated databases from <code><a href="#topic+simulate_r_database">simulate_r_database</a></code> to be merged together into a single database. Merged databases will be assigned moderator variable codes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_simdat_r(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_simdat_r_+3A_...">...</code></td>
<td>
<p>Collection of objects created by the &quot;simulate_r_database&quot; function. Simply enter the database objects as <code>merge_simdat_r</code>(data_obj1, data_obj2, data_obj_3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A merged database of class <code>simdat_r_database</code>
</p>

<hr>
<h2 id='metabulate'>Write a summary table of meta-analytic results</h2><span id='topic+metabulate'></span>

<h3>Description</h3>

<p>Write a summary table of meta-analytic results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metabulate(
  ma_obj,
  file = NULL,
  output_dir = getwd(),
  output_format = c("word", "html", "pdf", "odt", "text", "rmd"),
  show_msd = TRUE,
  show_conf = TRUE,
  show_cred = TRUE,
  show_se = FALSE,
  show_var = FALSE,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  ma_method = "ad",
  correction_type = "ts",
  collapse_construct_labels = TRUE,
  bold_headers = TRUE,
  digits = 2L,
  decimal.mark = getOption("OutDec"),
  leading0 = "conditional",
  drop0integer = FALSE,
  neg.sign = "&amp;minus;",
  pos.sign = "figure_html",
  big.mark = "&amp;#8239;",
  big.interval = 3L,
  small.mark = "&amp;#8239;",
  small.interval = 3L,
  na.mark = "&amp;mdash;",
  lgl.mark = c("+", "&amp;minus;"),
  inf.mark = c("+&amp;infin;", "&amp;minus;&amp;infin;"),
  conf_format = "brackets",
  cred_format = "brackets",
  symbol_es = "ES",
  caption = "Results of meta-analyses",
  header = NULL,
  verbose = FALSE,
  unicode = NULL,
  bib = NULL,
  title.bib = NULL,
  style = "apa",
  additional_citekeys = NULL,
  save_build_files = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metabulate_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A psychmeta meta-analysis object.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_file">file</code></td>
<td>
<p>The filename (optionally with a subfolder path) for the output file. If <code>NULL</code>, the function will output directly to the R console (also useful if you want to include psychmeta results in a larger RMarkdown document).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_output_dir">output_dir</code></td>
<td>
<p>The filepath for the output directory/folder. Defaults to the current working directory.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_output_format">output_format</code></td>
<td>
<p>The format of the output tables. Available options are Word (default), HTML, PDF (requires LaTeX and the <code>unicode-math</code> LaTeX package to be installed), ODT, rmd (Rmarkdown), and text (plain text). You can also specify the full name of another RMarkdown <code><a href="rmarkdown.html#topic+output_format">output_format</a></code>.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_show_msd">show_msd</code></td>
<td>
<p>Logical. Should means and standard deviations of effect sizes be shown (default <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="metabulate_+3A_show_conf">show_conf</code></td>
<td>
<p>Logical. Should confidence intervals be shown (default: <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_show_cred">show_cred</code></td>
<td>
<p>Logical. Should credibility intervals be shown (default: <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_show_se">show_se</code></td>
<td>
<p>Logical Should standard errors be shown (default: <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_show_var">show_var</code></td>
<td>
<p>Logical. Should variances be shown (default: <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract references for? See <code><a href="#topic+filter_ma">filter_ma</a></code> for details.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_match">match</code></td>
<td>
<p>Match <code>all</code> or <code>any</code> of the filter criteria? See <code><a href="#topic+filter_ma">filter_ma</a></code> for details.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_ma_method">ma_method</code></td>
<td>
<p>Meta-analytic methods to be included. Valid options are: <code>"ad"</code>, <code>"ic"</code>, and <code>"bb"</code>. Multiple methods are permitted. By default, results are given for one method with order of priority: 1. <code>"ad"</code>, 2. <code>"ic"</code>, 3. <code>"bb"</code>.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_correction_type">correction_type</code></td>
<td>
<p>Type of meta-analytic corrections to be incldued. Valid options are: &quot;ts&quot; (default), &quot;vgx&quot;, and &quot;vgy&quot;. Multiple options are permitted.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_collapse_construct_labels">collapse_construct_labels</code></td>
<td>
<p>Should the construct labels for construct pairs with multiple rows of results be simplified so that only the first occurence of each set of construct names is shown (<code>TRUE</code>; default) or should construct labels be shown for each row of the table (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_bold_headers">bold_headers</code></td>
<td>
<p>Logical. Should column headers be bolded (default: <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_digits">digits</code>, <code id="metabulate_+3A_decimal.mark">decimal.mark</code>, <code id="metabulate_+3A_leading0">leading0</code>, <code id="metabulate_+3A_drop0integer">drop0integer</code>, <code id="metabulate_+3A_neg.sign">neg.sign</code>, <code id="metabulate_+3A_pos.sign">pos.sign</code>, <code id="metabulate_+3A_big.mark">big.mark</code>, <code id="metabulate_+3A_big.interval">big.interval</code>, <code id="metabulate_+3A_small.mark">small.mark</code>, <code id="metabulate_+3A_small.interval">small.interval</code>, <code id="metabulate_+3A_na.mark">na.mark</code>, <code id="metabulate_+3A_lgl.mark">lgl.mark</code>, <code id="metabulate_+3A_inf.mark">inf.mark</code></td>
<td>
<p>Number formatting arguments. See <code><a href="#topic+format_num">format_num</a></code> for details.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_conf_format">conf_format</code></td>
<td>
<p>How should confidence intervals be formatted? Options are:
</p>

<ul>
<li><p><code>parentheses</code>: Bounds are enclosed in parentheses and separated by a comma: (LO, UP).
</p>
</li>
<li><p><code>brackets</code>: Bounds are enclosed in square brackets and separated by a comma: [LO, UP].
</p>
</li>
<li><p><code>columns</code>: Bounds are shown in individual columns.
</p>
</li></ul>
</td></tr>
<tr><td><code id="metabulate_+3A_cred_format">cred_format</code></td>
<td>
<p>How should credility intervals be formatted? Options are the same as for <code>conf_format</code> above.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_symbol_es">symbol_es</code></td>
<td>
<p>For meta-analyses of generic (non-r, non-d) effect sizes, the symbol used for the effect sizes (default: <code>symbol_es = "ES"</code>).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_caption">caption</code></td>
<td>
<p>Caption to print before tables. Either a character scalar or a named character vector with names corresponding to combinations of <code>ma_method</code> and <code>correction_type</code> (i.e., <code>bb</code>, <code>ic_ts</code>, <code>ad_vgx</code>, etc.).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_header">header</code></td>
<td>
<p>A list of YAML header parameters to pass to <code><a href="rmarkdown.html#topic+render">render</a></code>.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_verbose">verbose</code></td>
<td>
<p>Logical. Should detailed SD and variance components be shown (default: <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_unicode">unicode</code></td>
<td>
<p>Logical. If <code>output_format</code> is &quot;text&quot;, should UTF-8 characters be used (defaults to system default).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_bib">bib</code></td>
<td>
<p>A BibTeX file containing the citekeys for the meta-analyses. If provided and file is not <code>NULL</code>, a bibliography will be included with the meta-analysis table. See <code><a href="#topic+generate_bib">generate_bib</a></code> for additional arguments controlling the bibliography.</p>
</td></tr>
<tr><td><code id="metabulate_+3A_title.bib">title.bib</code></td>
<td>
<p>The title to give to the bibliography (see <code>bib</code> above). If <code>NULL</code>, defaults to &quot;Sources Contributing to Meta-Analyses&quot;</p>
</td></tr>
<tr><td><code id="metabulate_+3A_style">style</code></td>
<td>
<p>What style should the bibliography (see <code>bib</code> above) be formatted in? Can be a file path or URL for a <a href="https://github.com/citation-style-language/styles">CSL citation style</a> or the style ID for any style available from the <a href="https://www.zotero.org/styles">Zotero Style Repository</a>). Defaults to APA style. (Retrieving a style by ID requires an internet connection. If unavailable, references will be rendered in Chicago style.).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_additional_citekeys">additional_citekeys</code></td>
<td>
<p>Additional citekeys to include in the reference list (see <code>bib</code> above).</p>
</td></tr>
<tr><td><code id="metabulate_+3A_save_build_files">save_build_files</code></td>
<td>
<p>Should the RMarkdown and BibLaTeX (if any) files used to generate the output be saved (default: <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="metabulate_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="rmarkdown.html#topic+render">render</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of meta-analysis results <code><a href="tibble.html#topic+tibble">tibble</a></code>s with &quot;caption&quot; and &quot;footnote&quot; attributes.
</p>
<p>If <code>file</code> is specified, formatted tables and bibliographies are exported in the requested <code>output_format</code>.
</p>
<p>Formatted tables of meta-analytic output.
</p>


<h3>See Also</h3>

<p>Other output functions: 
<code><a href="#topic+generate_bib">generate_bib</a>()</code>,
<code><a href="#topic+metabulate_rmd_helper">metabulate_rmd_helper</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Create a results table for meta-analysis of correlations and output to Word:
ma_r_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
                 construct_x = x_name, construct_y = y_name,
                 moderators = moderator, data = data_r_meas_multi)
metabulate(ma_obj = ma_r_obj, file = "meta tables correlations",
           output_format = "word", output_dir = tempdir())

## Output to PDF:
metabulate(ma_obj = ma_r_obj, file = "meta tables correlations",
           output_format = "pdf", output_dir = tempdir())

## Output to ODT (LibreOffice):
metabulate(ma_obj = ma_r_obj, file = "meta tables correlations",
           output_format = "odt", output_dir = tempdir())


## To produce Markdown tables to include inline in an RMarkdown report,
## leave file == NULL and output_format to anything but "text":
ma_table &lt;- metabulate(ma_obj = ma_r_obj, file = NULL, output_format = "rmd")

## Use the metabulate_rmd_helper() function to ensure all symbols render properly.
Insert the following code as 'as-is' output:
metabulate_rmd_helper()

## Then, add the formatted table to your document using your preferred table
## formatting functions:

#### Using just the 'knitr' package, include the following as 'as-is' output:
knitr::kable(ma_table[[1]], caption = attr(ma_table[[1]], "caption"))
cat("\n", attr(ma_table[[1]], "footnote"))

#### Using 'knitr' plus the 'kableExtra' package:
knitr::kable(ma_table[[1]], "latex", booktabs = TRUE,
                  caption = attr(ma_table[[1]], "caption")) %&gt;%
       kableExtra::kable_styling(latex_options = c("striped", "hold_position")) %&gt;%
       kableExtra::footnote(general = attr(ma_table[[1]], "footnote")

# !!! Note: On Windows, R currently can only handle Unicode characters if kables
# are printed at top-level (e.g., not in an if() statement, in a for() loop,
# or in lapply() or map() ). To correctly print Unicode metabulate tables, call
# kable() as a top-level function (as above).


## Create output table for meta-analysis of d values:
ma_d_obj &lt;- ma_d(ma_method = "ic", d = d, n1 = n1, n2 = n2, ryy = ryyi,
                 construct_y = construct, data = data_d_meas_multi)
ma_d_obj &lt;- ma_d_ad(ma_obj = ma_d_obj, correct_rr_g = FALSE, correct_rr_y = FALSE)
metabulate(ma_obj = ma_d_obj, file = "meta tables d values", output_dir = tempdir())

## Create output table for meta-analysis of generic effect sizes:
dat &lt;- data.frame(es = data_r_meas_multi$rxyi,
                  n = data_r_meas_multi$n,
                  var_e = (1 - data_r_meas_multi$rxyi^2)^2 / (data_r_meas_multi$n - 1))
ma_obj &lt;- ma_generic(es = es, n = n, var_e = var_e, data = dat)
metabulate(ma_obj = ma_obj, file = "meta tables generic es", output_dir = tempdir())

## End(Not run)
</code></pre>

<hr>
<h2 id='metabulate_rmd_helper'>Add metabulate equation commands and LaTeX dependencies</h2><span id='topic+metabulate_rmd_helper'></span>

<h3>Description</h3>

<p><code><a href="#topic+metabulate">metabulate</a></code> requires several lines of code to correctly render meta-analysis results table column headings and footnotes. If <code>metabulate</code> is used to render files directly, these are added to the internal RMarkdown document. If you use <code>metabulate</code> output in a larger RMarkdown document, use this function to automatically add the necessary lines of code based on your chosen output format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metabulate_rmd_helper(latex = TRUE, html = TRUE, word_proc = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metabulate_rmd_helper_+3A_latex">latex</code></td>
<td>
<p>Should required commands be included when converting to PDF, LaTeX, and related formats?</p>
</td></tr>
<tr><td><code id="metabulate_rmd_helper_+3A_html">html</code></td>
<td>
<p>Should required commands be included when converting to HTML and related formats?</p>
</td></tr>
<tr><td><code id="metabulate_rmd_helper_+3A_word_proc">word_proc</code></td>
<td>
<p>Should required commands be included when converting to Word, ODT, and related formats?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Requested commands are printed to the console.
</p>


<h3>PDF and LaTeX output</h3>

<p>If <code>latex</code> is <code>TRUE</code> and you render to PDF, LaTeX, or other output
formats requiring LaTeX (e.g., <code>beamer_presentation</code>, see <code><a href="knitr.html#topic+output_type">knitr::is_latex_output</a></code>),
a YAML metadata block with a <code>header-includes</code> argument calling the required
<code>unicode-math</code> LaTeX package is printed.
</p>
<p>An RMarkdown file can only include one <code>header-includes</code> metadata entry. If
your document already has one, set <code>latex</code> to <code>FALSE</code> and manually add
add the <code>unicode-math</code> package to your LaTeX header instead.
</p>
<p>(Note that <code>header-includes</code> is generally discouraged in favor of adding
an <code>include</code> argument to specific output formats, see
<a href="https://bookdown.org/yihui/rmarkdown/pdf-document.html#includes">https://bookdown.org/yihui/rmarkdown/pdf-document.html#includes</a>.)
</p>


<h3>HTML output</h3>

<p>If <code>html</code> is <code>TRUE</code> and you render to HTML (or related formats, see
<code><a href="knitr.html#topic+output_type">knitr::is_html_output</a></code>, the following LaTeX
math commands are defined:
</p>

<ul>
<li><p><code>symit</code>
</p>
</li>
<li><p><code>symup</code>
</p>
</li>
<li><p><code>symbfit</code>
</p>
</li>
<li><p><code>symbfup</code>
</p>
</li></ul>

<p>If you define your own LaTeX or MathJax macros for these commands, set
<code>html</code> to <code>FALSE</code>.
</p>


<h3>Microsoft Office and LibreOffice output</h3>

<p>If <code>word_proc</code> is <code>TRUE</code> and you render to Word or ODT (or related
formats such as PowerPoint), the following LaTeX math commands are defined:
</p>

<ul>
<li><p><code>symit</code>
</p>
</li>
<li><p><code>symup</code>
</p>
</li>
<li><p><code>symbfit</code>
</p>
</li>
<li><p><code>symbfup</code>
</p>
</li></ul>

<p>If you define your own LaTeX, Office, or OpenDocument macros for these commands,
set <code>word_proc</code> to <code>FALSE</code>.
</p>


<h3>See Also</h3>

<p>Other output functions: 
<code><a href="#topic+generate_bib">generate_bib</a>()</code>,
<code><a href="#topic+metabulate">metabulate</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Include this line as 'asis' output in your RMarkdown document:
metabulate_rmd_helper()

## If you've already included \usepackage{unicode-math} in your RMarkdown header
## for PDF (and related formats) header, set latex to FALSE:
metabulate_rmd_helper(latex = FALSE)
</code></pre>

<hr>
<h2 id='metareg'>Compute meta-regressions</h2><span id='topic+metareg'></span>

<h3>Description</h3>

<p>This function is a wrapper for <span class="pkg">metafor</span>'s <code>rma</code> function that computes meta-regressions for all bare-bones and individual-correction meta-analyses within an object.
It makes use of both categorical and continuous moderator information stored in the meta-analysis object and allows for interaction effects to be included in the regression model.
Output from this function will be added to the meta-analysis object in a list called <code>follow_up_analyses</code>.
If using this function with a multi-construct meta-analysis object from <code><a href="#topic+ma_r">ma_r</a></code> or <code><a href="#topic+ma_d">ma_d</a></code>, note that the <code>follow_up_analyses</code> list is appended to the meta-analysis object belonging
to a specific construct pair within the <code>construct_pairs</code> list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metareg(ma_obj, formula_list = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metareg_+3A_ma_obj">ma_obj</code></td>
<td>
<p>Meta-analysis object.</p>
</td></tr>
<tr><td><code id="metareg_+3A_formula_list">formula_list</code></td>
<td>
<p>Optional list of regression formulas to evaluate.
NOTE: If there are spaces in your moderator names, replace them with underscores (i.e., &quot;_&quot;) so that the formula(s) will perform properly.
The function will remove spaces in the data, you only have to account for this in <code>formula_list</code> when you supply your own formula(s).</p>
</td></tr>
<tr><td><code id="metareg_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ma_obj with meta-regression results added (see ma_obj$follow_up_analyses$metareg).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Meta-analyze the data from Gonzalez-Mule et al. (2014)
## Note: These are corrected data and we have confirmed with the author that
## these results are accurate:
ma_obj &lt;- ma_r_ic(rxyi = rxyi, n = n, hs_override = TRUE, data = data_r_gonzalezmule_2014,
                  rxx = rxxi, ryy = ryyi, ux = ux, indirect_rr_x = TRUE,
                  correct_rr_x = TRUE, moderators = Complexity)

## Pass the meta-analysis object to the meta-regression function:
ma_obj &lt;- metareg(ma_obj)

## Examine the meta-regression results for the bare-bones and corrected data:
ma_obj$metareg[[1]]$barebones$`Main Effects`
ma_obj$metareg[[1]]$individual_correction$true_score$`Main Effects`


## Meta-analyze simulated d-value data
dat &lt;- data_d_meas_multi
## Simulate a random moderator
set.seed(100)
dat$moderator &lt;- sample(1:2, nrow(dat), replace = TRUE)
ma_obj &lt;- ma_d(ma_method = "ic", d = d, n1 = n1, n2 = n2, ryy = ryyi,
               construct_y = construct, sample_id = sample_id,
               moderators = moderator, data = dat)

## Pass the meta-analysis object to the meta-regression function:
ma_obj &lt;- metareg(ma_obj)

## Examine the meta-regression results for the bare-bones and corrected data:
ma_obj$metareg[[1]]$barebones$`Main Effects`
ma_obj$metareg[[1]]$individual_correction$latentGroup_latentY$`Main Effects`
</code></pre>

<hr>
<h2 id='mix_dist'>Descriptive statistics for a mixture distribution</h2><span id='topic+mix_dist'></span>

<h3>Description</h3>

<p>Compute descriptive statistics for a mixture distribution. This function returns the grand mean, the pooled sample variance (mean square within), variance of sample means (mean square between), portions of the total variance that are within and between groups, and mixture (total sample) variance of the mixture sample data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mix_dist(mean_vec, var_vec, n_vec, unbiased = TRUE, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mix_dist_+3A_mean_vec">mean_vec</code></td>
<td>
<p>Vector of sample means.</p>
</td></tr>
<tr><td><code id="mix_dist_+3A_var_vec">var_vec</code></td>
<td>
<p>Vector of sample variances.</p>
</td></tr>
<tr><td><code id="mix_dist_+3A_n_vec">n_vec</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="mix_dist_+3A_unbiased">unbiased</code></td>
<td>
<p>Logical scalar determining whether variance should be unbiased (TRUE; default) or maximum-likelihood (FALSE).</p>
</td></tr>
<tr><td><code id="mix_dist_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical scalar determining whether to remove missing values prior to computing output (TRUE) or not (FALSE; default)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The grand mean of a mixture distribution is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\mu=\frac{\Sigma_{i=1}^{k}\bar{x}_{i}n_{i}}{\Sigma_{i=1}^{k}n_{i}}</code>
</p>

<p>where <code class="reqn">\mu</code> is the grand mean, <code class="reqn">\bar{x}_{i}</code> represents the sample means, and <code class="reqn">n_{i}</code> represents the sample sizes.
</p>
<p>Maximum-likelihood mixture variances are computed as:
</p>
<p style="text-align: center;"><code class="reqn">var_{pooled_{ML}}=MSW_{ML}=\frac{\Sigma_{i=1}^{k}\left(\bar{x}_{i}-\mu\right)n_{i}}{\Sigma_{i=1}^{k}n_{i}}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{means_{ML}}=MSB_{ML}=\frac{\Sigma_{i=1}^{k}\left(\bar{x}_{i}-\mu\right)n_{i}}{k}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{BG_{ML}}=\frac{\Sigma_{i=1}^{k}\left(\bar{x}_{i}-\mu\right)n_{i}}{\Sigma_{i=1}^{k}n_{i}}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{WG_{ML}}=\frac{\Sigma_{i=1}^{k}v_{i}n_{i}}{\Sigma_{i=1}^{k}n_{i}}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{mix_{ML}}=var_{BG_{ML}}+var_{WG_{ML}}</code>
</p>

<p>where <code class="reqn">v_{i}</code> represents the sample variances.
</p>
<p>Unbiased mixture variances are computed as:
</p>
<p style="text-align: center;"><code class="reqn">var_{pooled_{Unbiased}}=MSW_{Unbiased}=\frac{\Sigma_{i=1}^{k}v_{i}\left(n_{i}-1\right)}{\left(\Sigma_{i=1}^{k}n_{i}\right)-k}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{means_{Unbiased}}=MSB_{Unbiased}=\frac{\Sigma_{i=1}^{k}\left(\bar{x}_{i}-\mu\right)n_{i}}{k-1}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{BG_{Unbiased}}=\frac{\Sigma_{i=1}^{k}\left(\bar{x}_{i}-\mu\right)n_{i}}{\left(\Sigma_{i=1}^{k}n_{i}\right)-1}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{WG_{Unbiased}}=\frac{\Sigma_{i=1}^{k}v_{i}\left(n_{i}-1\right)}{\left(\Sigma_{i=1}^{k}n_{i}\right)-1}</code>
</p>

<p style="text-align: center;"><code class="reqn">var_{mix_{Unbiased}}=var_{BG_{Unbiased}}+var_{WG_{Unbiased}}</code>
</p>



<h3>Value</h3>

<p>The mean, pooled sample (within-sample) variance, variance of sample means (between-groups), and mixture (total sample) variance of the mixture sample data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mix_dist(mean_vec = c(-.5, 0, .5), var_vec = c(.9, 1, 1.1), n_vec = c(100, 100, 100))
</code></pre>

<hr>
<h2 id='mix_matrix'>Estimate mixture covariance matrix from within-group covariance matrices</h2><span id='topic+mix_matrix'></span>

<h3>Description</h3>

<p>Estimate mixture covariance matrix from within-group covariance matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mix_matrix(
  sigma_list,
  mu_mat,
  p_vec,
  N = Inf,
  group_names = NULL,
  var_names = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mix_matrix_+3A_sigma_list">sigma_list</code></td>
<td>
<p>List of covariance matrices.</p>
</td></tr>
<tr><td><code id="mix_matrix_+3A_mu_mat">mu_mat</code></td>
<td>
<p>Matrix of mean parameters, with groups on the rows and variables on the columns.</p>
</td></tr>
<tr><td><code id="mix_matrix_+3A_p_vec">p_vec</code></td>
<td>
<p>Vector of proportion of cases in each group.</p>
</td></tr>
<tr><td><code id="mix_matrix_+3A_n">N</code></td>
<td>
<p>Optional total sample size across all groups (used to compute unbiased covariance estimates).</p>
</td></tr>
<tr><td><code id="mix_matrix_+3A_group_names">group_names</code></td>
<td>
<p>Optional vector of group names.</p>
</td></tr>
<tr><td><code id="mix_matrix_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of mixture covariances and means.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>out &lt;- unmix_matrix(sigma_mat = reshape_vec2mat(.5, order = 2),
                    mu_mat = rbind(c(0, 0), c(.5, 1)),
                    p_vec =  c(.3, .7), N = 100)

mix_matrix(sigma_list = out$cov_group_unbiased,
           mu_mat = out$means_raw[-3,],
           p_vec = out$p_group, N = out$N)
</code></pre>

<hr>
<h2 id='mix_r_2group'>Estimate the mixture correlation for two groups</h2><span id='topic+mix_r_2group'></span>

<h3>Description</h3>

<p>Estimate the mixture correlation for two groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mix_r_2group(rxy, dx, dy, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mix_r_2group_+3A_rxy">rxy</code></td>
<td>
<p>Average within-group correlation</p>
</td></tr>
<tr><td><code id="mix_r_2group_+3A_dx">dx</code></td>
<td>
<p>Standardized mean difference between groups on X.</p>
</td></tr>
<tr><td><code id="mix_r_2group_+3A_dy">dy</code></td>
<td>
<p>Standardized mean difference between groups on Y.</p>
</td></tr>
<tr><td><code id="mix_r_2group_+3A_p">p</code></td>
<td>
<p>Proportion of cases in one of the two groups.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The average within-group correlation is estimated as:
</p>
<p style="text-align: center;"><code class="reqn">\rho_{xy_{WG}}=\rho_{xy_{Mix}}\sqrt{\left(d_{x}^{2}p(1-p)+1\right)\left(d_{y}^{2}p(1-p)+1\right)}-\sqrt{d_{x}^{2}d_{y}^{2}p^{2}(1-p)^{2}}</code>
</p>

<p>where <code class="reqn">\rho_{xy_{WG}}</code> is the average within-group correlation, <code class="reqn">\rho_{xy_{Mix}}</code> is the overall mixture correlation,
<code class="reqn">d_{x}</code> is the standardized mean difference between groups on X, <code class="reqn">d_{y}</code> is the standardized mean difference between groups on Y, and
<em>p</em> is the proportion of cases in one of the two groups.
</p>


<h3>Value</h3>

<p>A vector of two-group mixture correlations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mix_r_2group(rxy = .375, dx = 1, dy = 1, p = .5)
</code></pre>

<hr>
<h2 id='organize_database'>Organize a database of multi-construct or moderated information</h2><span id='topic+organize_database'></span>

<h3>Description</h3>

<p>Organize a database of multi-construct or moderated information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>organize_database(
  es_data,
  sample_id = NULL,
  citekey = NULL,
  construct_x = NULL,
  construct_y = NULL,
  facet_x = NULL,
  facet_y = NULL,
  measure_x = NULL,
  measure_y = NULL,
  data_x = NULL,
  data_y = NULL,
  moderators = NULL,
  use_as_x = NULL,
  use_as_y = NULL,
  construct_order = NULL,
  cat_moderators = TRUE,
  moderator_levels = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="organize_database_+3A_es_data">es_data</code></td>
<td>
<p>Matrix of effect-size data to be used in meta-analyses.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_sample_id">sample_id</code></td>
<td>
<p>Optional vector of identification labels for studies in the meta-analysis.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_citekey">citekey</code></td>
<td>
<p>Optional vector of bibliographic citation keys for samples/studies in the meta-analysis (if multiple citekeys pertain to a given effect size, combine them into a single string entry with comma delimiters (e.g., &quot;citkey1,citekey2&quot;).</p>
</td></tr>
<tr><td><code id="organize_database_+3A_construct_x">construct_x</code></td>
<td>
<p>Vector of construct names for construct initially designated as X.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for construct initially designated as Y.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_facet_x">facet_x</code></td>
<td>
<p>Vector of facet names for construct initially designated as X.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_facet_y">facet_y</code></td>
<td>
<p>Vector of facet names for construct initially designated as Y.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_data_x">data_x</code></td>
<td>
<p>Additional data (e.g., artifact information) specific to the variables originally designated as X.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_data_y">data_y</code></td>
<td>
<p>Additional data (e.g., artifact information) specific to the variables originally designated as Y.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_moderators">moderators</code></td>
<td>
<p>Matrix, dataframe, or vector of moderators.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_use_as_x">use_as_x</code></td>
<td>
<p>Vector of construct names to be categorized as X constructs - cannot overlap with the contents of 'use_as_y'.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_use_as_y">use_as_y</code></td>
<td>
<p>Vector of construct names to be categorized as Y constructs - cannot overlap with the contents of 'use_as_x'.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_construct_order">construct_order</code></td>
<td>
<p>Vector indicating the order in which variables should be arranged, with variables listed earlier in the vector being preferred for designation as X.</p>
</td></tr>
<tr><td><code id="organize_database_+3A_cat_moderators">cat_moderators</code></td>
<td>
<p>Logical vector identifying whether each variable in moderators is a categorical variable (TRUE) or a continuous variable (FALSE).</p>
</td></tr>
<tr><td><code id="organize_database_+3A_moderator_levels">moderator_levels</code></td>
<td>
<p>Optional list of factor levels to be applied to the categorical moderators.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A reorganized list of study data
</p>

<hr>
<h2 id='organize_moderators'>Organization of moderator data for use in meta-analyses</h2><span id='topic+organize_moderators'></span>

<h3>Description</h3>

<p>Organization of moderator data for use in meta-analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>organize_moderators(
  cat_moderator_matrix,
  es_data,
  construct_x = NULL,
  construct_y = NULL,
  construct_order = NULL,
  moderator_levels = NULL,
  moderator_type = "hierarchical",
  cat_moderator_names = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="organize_moderators_+3A_cat_moderator_matrix">cat_moderator_matrix</code></td>
<td>
<p>Matrix (or vector) of categorical moderator variables.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_es_data">es_data</code></td>
<td>
<p>Matrix of effect-size data to be used in meta-analyses.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_construct_x">construct_x</code></td>
<td>
<p>Vector of construct names for construct X.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_construct_y">construct_y</code></td>
<td>
<p>Vector of construct names for construct Y.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_construct_order">construct_order</code></td>
<td>
<p>The order in which constructs are to be arranged.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_moderator_levels">moderator_levels</code></td>
<td>
<p>List containing the factor levels of categorical moderator variables.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_moderator_type">moderator_type</code></td>
<td>
<p>Type of moderator analysis: &quot;none&quot; means that no moderators are to be used, &quot;simple&quot; means that moderators are to be examined one at a time,
&quot;hierarchical&quot; means that all possible combinations and subsets of moderators are to be examined.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_cat_moderator_names">cat_moderator_names</code></td>
<td>
<p>Vector of names for the variables in cat_moderator_matrix.</p>
</td></tr>
<tr><td><code id="organize_moderators_+3A_...">...</code></td>
<td>
<p>Further arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing (1) the full matrix of moderators and effect-size data for use in meta-analyses and (2) the names of the moderator variables.
</p>

<hr>
<h2 id='plot_forest'>Create forest plots</h2><span id='topic+plot_forest'></span>

<h3>Description</h3>

<p>Create forest plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_forest(
  ma_obj,
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  show_filtered = FALSE,
  ma_facetname = "Summary",
  facet_levels = NULL,
  conf_level = NULL,
  conf_method = NULL,
  x_limits = NULL,
  x_breaks = NULL,
  x_lab = NULL,
  y_lab = "Reference"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_forest_+3A_ma_obj">ma_obj</code></td>
<td>
<p>Meta-analysis object.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract? Can be either <code>"all"</code> to extract references for all meta-analyses in the object (default) or a list containing arguments for <code><a href="#topic+filter_ma">filter_ma()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_match">match</code></td>
<td>
<p>Should extracted meta-analyses match all (default) or any of the criteria given in <code>analyses</code>?</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_show_filtered">show_filtered</code></td>
<td>
<p>Logical scalar that determines whether the meta-analysis object given in the output should be the modified input object (<code>FALSE</code>, default) or the filtered object (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_ma_facetname">ma_facetname</code></td>
<td>
<p>Label to use for meta-analysis results in the <code><a href="ggplot2.html#topic+facet_grid">ggplot2::facet_grid()</a></code> function.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_facet_levels">facet_levels</code></td>
<td>
<p>Order in which moderator levels should be displayed.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to define the width of the confidence interval. If <code>NULL</code> (default), uses the level set when <code>ma_obj</code> was estimated.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_conf_method">conf_method</code></td>
<td>
<p>Distribution to be used to compute confidence intervals (either <code>"t"</code> for <em>t</em> distribution or <code>"norm"</code> for normal distribution). If <code>NULL</code> (default), uses the method set when <code>ma_obj</code> was estimated.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_x_limits">x_limits</code></td>
<td>
<p>Span of the X values to be plotted.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_x_breaks">x_breaks</code></td>
<td>
<p>Breaks for the X values to be plotted.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_x_lab">x_lab</code></td>
<td>
<p>Label to use for the X axis.</p>
</td></tr>
<tr><td><code id="plot_forest_+3A_y_lab">y_lab</code></td>
<td>
<p>Label to use for the Y axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of forest plots.
</p>


<h3>Author(s)</h3>

<p>Based on code by John Sakaluk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               moderators = moderator, data = data_r_meas_multi)
plot_forest(ma_obj = ma_obj)
plot_forest(ma_obj = ma_obj, analyses = list(pair_id = 2))
plot_forest(ma_obj = ma_obj, analyses = list(pair_id = 1), show_filtered = TRUE)

## d values
ma_obj &lt;- ma_d(ma_method = "ic", d = d, n1 = n1, n2 = n2, ryy = ryyi,
               construct_y = construct, sample_id = sample_id,
               data = data_d_meas_multi)
plot_forest(ma_obj = ma_obj)
plot_forest(ma_obj = ma_obj, analyses = list(pair_id = 2))
plot_forest(ma_obj = ma_obj, analyses = list(pair_id = 1, analysis_id = 1), show_filtered = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_funnel'>Create funnel plots</h2><span id='topic+plot_funnel'></span><span id='topic+plot_cefp'></span>

<h3>Description</h3>

<p>This function creates funnel plots for meta-analyses (plots of effect size versus standard error).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_funnel(
  ma_obj,
  se_type = c("auto", "mean", "sample"),
  label_es = NULL,
  conf_level = c(0.95, 0.99),
  conf_linetype = c("dashed", "dotted"),
  conf_fill = NA,
  conf_alpha = 1,
  null_effect = NA,
  null_conf_level = c(0.9, 0.95, 0.99),
  null_conf_linetype = c("solid", "dashed", "dotted"),
  null_conf_fill = "black",
  null_conf_alpha = c(0.1, 0.2, 0.4),
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  show_filtered = FALSE
)

plot_cefp(
  ma_obj,
  se_type = "sample",
  label_es = NULL,
  conf_level = NA,
  conf_linetype = NA,
  conf_fill = NA,
  conf_alpha = 1,
  null_effect = NULL,
  null_conf_level = c(0.9, 0.95, 0.99),
  null_conf_linetype = c("solid", "dashed", "dotted"),
  null_conf_fill = "black",
  null_conf_alpha = c(0, 0.2, 0.4),
  analyses = "all",
  match = c("all", "any"),
  case_sensitive = TRUE,
  show_filtered = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_funnel_+3A_ma_obj">ma_obj</code></td>
<td>
<p>Meta-analysis object.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_se_type">se_type</code></td>
<td>
<p>Method to calculate standard errors (y-axis). Options are <code>"auto"</code> (default) to use the same method as used to estimate the meta-analysis models, <code>"mean"</code> to calculate SEs using the mean effect size and indivdiual sample sizes, or '&quot;sample&quot;&ldquo; to use the SE calculated using the sample effect sizes and sample sizes.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_label_es">label_es</code></td>
<td>
<p>Label for effect size (x-axis). Defaults to &quot;Correlation (<em>r</em>)&quot; for correlation meta-analyses, &quot;Cohen's <em>d</em> (Hedges's <em>g</em>)&quot; for d value meta-analyses, and &quot;Effect size&quot; for generic meta-analyses.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence regions levels to be plotted (default: .95, .99).</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_conf_linetype">conf_linetype</code></td>
<td>
<p>Line types for confidence region boundaries. Length should be either 1 or equal to the length of conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_conf_fill">conf_fill</code></td>
<td>
<p>Colors for confidence regions. Set to <code>NA</code> for transparent. Length should be either 1 or equal to to the length of conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_conf_alpha">conf_alpha</code></td>
<td>
<p>Transparency level for confidence regions. Length should be either 1 or equal to to the length of conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_null_effect">null_effect</code></td>
<td>
<p>Null effect to be plotted for contour-enhanced funnel plots. If <code>NA</code>, not shown. If <code>NULL</code>, set to the null value for the effect size metric (0 for correlations and d values).</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_null_conf_level">null_conf_level</code></td>
<td>
<p>Null-effect confidence regions levels to be plotted (default: .90, .95, .99).</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_null_conf_linetype">null_conf_linetype</code></td>
<td>
<p>Line types for null-effect confidence region boundaries. Length should be either 1 or equal to the length of null_conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_null_conf_fill">null_conf_fill</code></td>
<td>
<p>Colors for null-effect confidence regions. Set to <code>NA</code> for transparent. Length should be either 1 or equal to the length of null_conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_null_conf_alpha">null_conf_alpha</code></td>
<td>
<p>Transparency level for null-effect confidence regions. Length should be either 1 or equal to the length of null_conf_level.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_analyses">analyses</code></td>
<td>
<p>Which analyses to extract? Can be either <code>"all"</code> to extract references for all meta-analyses in the object (default) or a list containing arguments for <a href="#topic+filter_ma">filter_ma</a>.</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_match">match</code></td>
<td>
<p>Should extracted meta-analyses match all (default) or any of the criteria given in <code>analyses</code>?</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Logical scalar that determines whether character values supplied in <code>analyses</code> should be treated as case sensitive (<code>TRUE</code>, default) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="plot_funnel_+3A_show_filtered">show_filtered</code></td>
<td>
<p>Logical scalar that determines whether the meta-analysis object given in the output should be the modified input object (<code>FALSE</code>, default) or the filtered object (<code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both traditional funnel plots and contour-enhanced funnel plots are provided.
Contour-enhanced funnel plots show comparison regions for varying null-hypothesis significance test levels and can be useful for detecting publication bias.
</p>


<h3>Value</h3>

<p>A list of funnel plots.
</p>


<h3>Author(s)</h3>

<p>Based on code by John Sakaluk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Correlations
ma_obj &lt;- ma_r(ma_method = "ic", rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi,
               construct_x = x_name, construct_y = y_name, sample_id = sample_id,
               moderators = moderator, data = data_r_meas_multi)
plot_funnel(ma_obj = ma_obj)
plot_funnel(ma_obj = ma_obj, analyses = list(pair_id = 2))
plot_funnel(ma_obj = ma_obj, analyses = list(pair_id = 1, analysis_id = 1), show_filtered = TRUE)

## d values
ma_obj &lt;- ma_d(ma_method = "ic", d = d, n1 = n1, n2 = n2, ryy = ryyi,
               construct_y = construct, sample_id = sample_id,
               data = data_d_meas_multi)
plot_funnel(ma_obj = ma_obj)
plot_funnel(ma_obj = ma_obj, analyses = list(pair_id = 2))
plot_funnel(ma_obj = ma_obj, analyses = list(pair_id = 1, analysis_id = 1), show_filtered = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict'>Prediction method for objects of classes deriving from &quot;lm_mat&quot;</h2><span id='topic+predict'></span>

<h3>Description</h3>

<p>Prediction method for objects of classes deriving from &quot;lm_mat&quot;
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>Object of class inheriting from &quot;lm_mat&quot;</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame in which to look for variables with which to predict. If omitted, the fitted values are used.</p>
</td></tr>
<tr><td><code id="predict_+3A_se.fit">se.fit</code></td>
<td>
<p>A switch indicating if standard errors are required.</p>
</td></tr>
<tr><td><code id="predict_+3A_df">df</code></td>
<td>
<p>Degrees of freedom for scale.</p>
</td></tr>
<tr><td><code id="predict_+3A_interval">interval</code></td>
<td>
<p>Type of interval calculation. Can be abbreviated.</p>
</td></tr>
<tr><td><code id="predict_+3A_level">level</code></td>
<td>
<p>Tolerance/confidence level.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An set of predicted values
</p>

<hr>
<h2 id='print'>Print methods for <strong><code>psychmeta</code></strong></h2><span id='topic+print'></span>

<h3>Description</h3>

<p>Print methods for <strong><code>psychmeta</code></strong> output objects with classes exported from <strong><code>psychmeta</code></strong>.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_+3A_x">x</code></td>
<td>
<p>Object to be printed (object is used to select a method).</p>
</td></tr>
<tr><td><code id="print_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
<tr><td><code id="print_+3A_digits">digits</code></td>
<td>
<p>Number of digits to which results should be rounded.</p>
</td></tr>
<tr><td><code id="print_+3A_ma_methods">ma_methods</code></td>
<td>
<p>Meta-analytic methods to be included. Valid options are: &quot;bb&quot;, &quot;ic&quot;, and &quot;ad&quot;</p>
</td></tr>
<tr><td><code id="print_+3A_correction_types">correction_types</code></td>
<td>
<p>Types of meta-analytic corrections to be included Valid options are: &quot;ts&quot;, &quot;vgx&quot;, and &quot;vgy&quot;</p>
</td></tr>
<tr><td><code id="print_+3A_verbose">verbose</code></td>
<td>
<p>Logical scalar that determines whether printed object should contain verbose information (e.g., non-standard columns of meta-analytic output; <code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="print_+3A_n">n</code></td>
<td>
<p>For <code>print.ma_psychmeta()</code> and <code>print.ad_tibble()</code>, number of rows to print for tibble. Defaults to all rows. See <code><a href="tibble.html#topic+formatting">tibble::print.tbl()</a></code> for details.</p>
</td></tr>
<tr><td><code id="print_+3A_width">width</code></td>
<td>
<p>For <code>print.ma_psychmeta()</code> and <code>print.ad_tibble()</code>, width of text output to generate for tibble. See <code><a href="tibble.html#topic+formatting">tibble::print.tbl()</a></code> for details.</p>
</td></tr>
<tr><td><code id="print_+3A_n_extra">n_extra</code></td>
<td>
<p>For <code>print.ma_psychmeta()</code> and <code>print.ad_tibble()</code>, number of extra columns to print abbreviated information for, if the width is too small for the entire meta-analysis tibble. See <code><a href="tibble.html#topic+formatting">tibble::print.tbl()</a></code> for details.</p>
</td></tr>
<tr><td><code id="print_+3A_symbolic.cor">symbolic.cor</code></td>
<td>
<p>For <code>print.lm_mat()</code>, Logical. If <code>TRUE</code>, print the correlations in a symbolic form (see <code><a href="stats.html#topic+symnum">stats::symnum()</a></code>) rather than as numbers.</p>
</td></tr>
<tr><td><code id="print_+3A_signif.stars">signif.stars</code></td>
<td>
<p>For <code>print.lm_mat()</code>, Logical. If <code>TRUE</code>, ‘significance stars’ are printed for each coefficient.</p>
</td></tr>
</table>

<hr>
<h2 id='psychmeta-package'><span class="pkg">psychmeta</span>: Psychometric meta-analysis toolkit</h2><span id='topic+psychmeta'></span><span id='topic+psychmeta-package'></span>

<h3>Description</h3>

<p>Overview of the <span class="pkg">psychmeta</span> package.
</p>


<h3>Details</h3>

<p>The <span class="pkg">psychmeta</span> package provides tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Currently, <span class="pkg">psychmeta</span> supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and <em>d</em> values.
Please refer to the overview tutorial vignette for an introduction to <span class="pkg">psychmeta</span>'s functions and workflows.
</p>


<h3>Running a meta-analysis</h3>

<p>The main functions for conducting meta-analyses in <span class="pkg">psychmeta</span> are <code><a href="#topic+ma_r">ma_r</a></code> for correlations and <code><a href="#topic+ma_d">ma_d</a></code> for <em>d</em> values. These functions take meta-analytic dataframes including effect sizes and sample sizes (and, optionally, study labels, moderators, construct and measure labels, and psychometric artifact information) and return the full results of psychometric meta-analyses for all of the specified variable pairs. Examples of correctly formatted meta-analytic datasets for ma functions are <code><a href="#topic+data_r_roth_2015">data_r_roth_2015</a></code>, <code><a href="#topic+data_r_gonzalezmule_2014">data_r_gonzalezmule_2014</a></code>, and <code><a href="#topic+data_r_mcdaniel_1994">data_r_mcdaniel_1994</a></code>. Individual parts of the meta-analysis process can also be run separately; these functions are described in detail below.
</p>


<h3>Preparing a database for meta-analysis</h3>

<p>The <code><a href="#topic+convert_es">convert_es</a></code> function can be used to convert a variety of effect sizes to either correlations or <em>d</em> values. Sporadic psychometric artifacts, such as artificial dichotomization or uneven splits for a <em>truly</em> dichotomous variable, can be individually corrected using <code><a href="#topic+correct_r">correct_r</a></code> and <code><a href="#topic+correct_d">correct_d</a></code>. These functions can also be used to compute confidence intervals for observed, converted, and corrected effect sizes. 'Wide' meta-analytic coding sheets can be reformatted to the 'long' data frames used by <span class="pkg">psychmeta</span> with <code><a href="#topic+reshape_wide2long">reshape_wide2long</a></code>. A correlation matrix and accompanying vectors of information can be similarly reformatted using <code><a href="#topic+reshape_mat2dat">reshape_mat2dat</a></code>.
</p>


<h3>Meta-analytic models</h3>

<p><span class="pkg">psychmeta</span> can compute barebones meta-analyses (no corrections for psychometric artifacts), as well as models correcting for measurement error in one or both variables, univariate direct (Case II) range restriction, univariate indirect (Case IV) range restriction, bivariate direct range restriction, bivariate indirect (Case V) range restriction, and multivariate range restriction. Artifacts can be corrected individually or using artifact distributions. Artifact distribution corrections can be applied using either Schmidt and Hunter's (2015) interactive method or Taylor series approximation models. Meta-analyses can be computed using various weights, including sample size (default for correlations), inverse variance (computed using either sample or mean effect size; error based on mean effect size is the default for <em>d</em> values), and weight methods imported from <span class="pkg">metafor</span>.
</p>


<h3>Preparing artifact distributions meta-analyses</h3>

<p>For individual-corrections meta-analyses, reliability and range restriction (u) values should be supplied in the same data frame as the effect sizes and sample sizes. Missing artifact data can be imputed using either bootstrap or other imputation methods. For artifact distribution meta-analyses, artifact distributions can be created automatically by <code><a href="#topic+ma_r">ma_r</a></code> or <code><a href="#topic+ma_d">ma_d</a></code> or manually by the <code><a href="#topic+create_ad">create_ad</a></code> family of functions.
</p>


<h3>Moderator analyses</h3>

<p>Subgroup moderator analyses are run by supplying a moderator matrix to the <code><a href="#topic+ma_r">ma_r</a></code> or <code><a href="#topic+ma_d">ma_d</a></code> families of functions. Both simple and fully hierarchical moderation can be computed. Subgroup moderator analysis results are shown by passing an <code>ma_obj</code> to <code>print</code>(). Meta-regression analyses can be run using <code><a href="#topic+metareg">metareg</a></code>.
</p>


<h3>Reporting results and supplemental analyses</h3>

<p>Meta-analysis results can be viewed by passing an ma object to <code><a href="#topic+summary">summary</a></code>. Bootstrap confidence intervals, leave one out analyses, and other sensitivity analyses are available in <code><a href="#topic+sensitivity">sensitivity</a></code>. Supplemental heterogeneity statistics (e.g., <code class="reqn">Q</code>, <code class="reqn">I^{2}</code>) can be computed using <code><a href="#topic+heterogeneity">heterogeneity</a></code>. Meta-analytic results can be converted between the <code class="reqn">r</code> and <code class="reqn">d</code> metrics using <code><a href="#topic+convert_ma">convert_ma</a></code>. Each <code>ma_obj</code> contains a <span class="pkg">metafor</span> <code>escalc</code> object in <code>ma$...$escalc</code> that can be passed to <span class="pkg">metafor</span>'s functions for plotting, publication/availability bias, and other supplemental analyses. Second-order meta-analyses of correlations can be computed using <code><a href="#topic+ma_r_order2">ma_r_order2</a></code>. Example second-order meta-analysis datasets from Schmidt and Oh (2013) are available.
Tables of meta-analytic results can be written as markdown, Word, HTML, or PDF files using the <code><a href="#topic+metabulate">metabulate</a></code> function, which exports near publication-quality tables that will typically require only minor customization by the user.
</p>


<h3>Simulating psychometric meta-analyses</h3>

<p><span class="pkg">psychmeta</span> can be used to run Monte Carlo simulations for different meta-analytic models. <code><a href="#topic+simulate_r_sample">simulate_r_sample</a></code> and <code><a href="#topic+simulate_d_sample">simulate_d_sample</a></code> simulate samples of correlations and <em>d</em> values, respectively, with measurement error and/or range restriction artifacts. <code><a href="#topic+simulate_r_database">simulate_r_database</a></code> and <code><a href="#topic+simulate_d_database">simulate_d_database</a></code> can be used to simulate full meta-analytic databases of sample correlations and <em>d</em> values, respecitively, with artifacts. Example datasets fitting different meta-analytic models simulated using these functions are available (<code><a href="#topic+data_r_meas">data_r_meas</a></code>, <code><a href="#topic+data_r_uvdrr">data_r_uvdrr</a></code>, <code><a href="#topic+data_r_uvirr">data_r_uvirr</a></code>, <code><a href="#topic+data_r_bvdrr">data_r_bvdrr</a></code>, <code><a href="#topic+data_r_bvirr">data_r_bvirr</a></code>, <code><a href="#topic+data_r_meas_multi">data_r_meas_multi</a></code>, and <code><a href="#topic+data_d_meas_multi">data_d_meas_multi</a></code>). Additional simulation functions are also available.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jeffrey A. Dahlke <a href="mailto:jdahlke@humrro.org">jdahlke@humrro.org</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Brenton M. Wiernik <a href="mailto:brenton@psychmeta.com">brenton@psychmeta.com</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Wesley Gardiner (Unit tests) [contributor]
</p>
</li>
<li><p> Michael T. Brannick (Testing) [contributor]
</p>
</li>
<li><p> Jack Kostal (Code for reshape_mat2dat function) [contributor]
</p>
</li>
<li><p> Sean Potter (Testing; Code for cumulative and leave1out plots) [contributor]
</p>
</li>
<li><p> John Sakaluk (Code for funnel and forest plots) [contributor]
</p>
</li>
<li><p> Yuejia (Mandy) Teng (Testing) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li><p> Report bugs at <a href="https://github.com/psychmeta/psychmeta/issues">https://github.com/psychmeta/psychmeta/issues</a>
</p>
</li></ul>


<hr>
<h2 id='reattribute'>Copy class and attributes from the original version of an object to a modified version.</h2><span id='topic+reattribute'></span>

<h3>Description</h3>

<p>Copy class and attributes from the original version of an object to a modified version.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reattribute(x, result)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reattribute_+3A_x">x</code></td>
<td>
<p>The original object, which has a class/attributes to copy</p>
</td></tr>
<tr><td><code id="reattribute_+3A_result">result</code></td>
<td>
<p>The modified object, which is / might be missing the class/attributes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>result</code>, now with class/attributes restored.
</p>

<hr>
<h2 id='record_fyis'>Summary of FYI messages generated within a function</h2><span id='topic+record_fyis'></span>

<h3>Description</h3>

<p>Summary of FYI messages generated within a function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>record_fyis(
  es_metric = "r",
  fyi_messages = NULL,
  neg_var_res = 0,
  neg_var_rtpa = 0,
  neg_var_rxpa = 0,
  neg_var_rtya = 0,
  neg_var_r_order2 = 0,
  neg_var_rho_ic_order2 = 0,
  neg_var_rho_ad_order2 = 0,
  neg_var_d_order2 = 0,
  neg_var_delta_ic_order2 = 0,
  neg_var_delta_ad_order2 = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="record_fyis_+3A_es_metric">es_metric</code></td>
<td>
<p>Effect-size metric (&quot;r&quot; or &quot;d&quot;).</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_fyi_messages">fyi_messages</code></td>
<td>
<p>Vector of assorted FYI messages accumulated during function.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_res">neg_var_res</code></td>
<td>
<p>Number of negative residual variances</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_rtpa">neg_var_rtpa</code></td>
<td>
<p>Number of negative true-score variances.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_rxpa">neg_var_rxpa</code></td>
<td>
<p>Number of negative validity generalization variances (X as predictor).</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_rtya">neg_var_rtya</code></td>
<td>
<p>Number of negative validity generalization variances (Y as predictor).</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_r_order2">neg_var_r_order2</code></td>
<td>
<p>Variance of mean r from second-order bare-bones meta-analysis.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_rho_ic_order2">neg_var_rho_ic_order2</code></td>
<td>
<p>Variance of mean r from second-order individual-correction meta-analysis.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_rho_ad_order2">neg_var_rho_ad_order2</code></td>
<td>
<p>Variance of mean r from second-order artifact-distribution meta-analysis.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_d_order2">neg_var_d_order2</code></td>
<td>
<p>Variance of mean d from second-order bare-bones meta-analysis.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_delta_ic_order2">neg_var_delta_ic_order2</code></td>
<td>
<p>Variance of mean d from second-order individual-correction meta-analysis.</p>
</td></tr>
<tr><td><code id="record_fyis_+3A_neg_var_delta_ad_order2">neg_var_delta_ad_order2</code></td>
<td>
<p>Variance of mean d from second-order artifact-distribution meta-analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Table of FYI messages and message frequencies.
</p>

<hr>
<h2 id='record_warnings'>Summary of warning messages generated within a function</h2><span id='topic+record_warnings'></span>

<h3>Description</h3>

<p>Summary of warning messages generated within a function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>record_warnings()
</code></pre>


<h3>Value</h3>

<p>A data frame containing a summary of warning messages and their frequencies.
</p>

<hr>
<h2 id='reshape_mat2dat'>Extract a long-format correlation database from a correlation matrix and its supporting vectors/matrices of variable information</h2><span id='topic+reshape_mat2dat'></span>

<h3>Description</h3>

<p>This function is designed to extract data from a correlation matrix that is in the format commonly published in journals, with leading columns of construct names and descriptive statistics
being listed along with correlation data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape_mat2dat(
  var_names,
  cor_data,
  common_data = NULL,
  unique_data = NULL,
  diag_label = NULL,
  lower_tri = TRUE,
  data = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_mat2dat_+3A_var_names">var_names</code></td>
<td>
<p>Vector (or scalar column name to match with <code>data</code>) containing variable names.</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_cor_data">cor_data</code></td>
<td>
<p>Square matrix (or vector of column names to match with <code>data</code>) containing correlations among variables.</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_common_data">common_data</code></td>
<td>
<p>Vector or matrix (or vector of column names to match with <code>data</code>) of data common to both X and Y variables (e.g., sample size, study-wise moderators).</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_unique_data">unique_data</code></td>
<td>
<p>Vector or matrix (or vector of column names to match with <code>data</code>) of data unique to X and Y variables (e.g., mean, SD, reliability).</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_diag_label">diag_label</code></td>
<td>
<p>Optional name to attribute to values extracted from the diagonal of the matrix (if NULL, no values are extracted from the diagonal).</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_lower_tri">lower_tri</code></td>
<td>
<p>Logical scalar that identifies whether the correlations are in the lower triangle (<code>TRUE</code>) or in the upper triangle <code>FALSE</code> of the matrix.</p>
</td></tr>
<tr><td><code id="reshape_mat2dat_+3A_data">data</code></td>
<td>
<p>Matrix or data frame containing study data (when present, column names of <code>data</code> will be matched to column names provided as other arguments).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Long-format data frame of correlation data, variable names, and supporting information
</p>


<h3>Author(s)</h3>

<p>Jack W. Kostal
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create a hypothetical matrix of data from a small study:
mat &lt;- data.frame(var_names = c("X", "Y", "Z"),
                  n = c(100, 100, 100),
                  mean = c(4, 5, 3),
                  sd = c(2.4, 2.6, 2),
                  rel = c(.8, .7, .85),
                  reshape_vec2mat(cov = c(.3, .4, .5)))

## Arguments can be provided as quoted characters or as the unquoted names of `data`'s columns:
reshape_mat2dat(var_names = var_names,
               cor_data = c("Var1", "Var2", "Var3"),
               common_data = "n",
               unique_data = c("mean", "sd", "rel"),
               data = mat)

## Arguments can also provided as raw vectors, matrices, or data frames, without a data argument:
reshape_mat2dat(var_names = mat[,1],
               cor_data = mat[,6:8],
               common_data = mat[,2],
               unique_data = mat[,3:5])

## If data is not null, arguments can be a mix of matrix/data frame/vector and column-name arguments
reshape_mat2dat(var_names = mat[,1],
               cor_data = mat[,6:8],
               common_data = "n",
               unique_data = c("mean", "sd", "rel"),
               data = mat)
</code></pre>

<hr>
<h2 id='reshape_vec2mat'>Assemble a variance-covariance matrix</h2><span id='topic+reshape_vec2mat'></span>

<h3>Description</h3>

<p>The <code>reshape_vec2mat</code> function facilitates the creation of square correlation/covariance matrices from scalars or vectors of variances/covariances.
It allows the user to supply a vector of covariances that make up the lower triangle of a matrix, determines the order of the matrix necessary to hold those covariances, and constructs a matrix accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape_vec2mat(
  cov = NULL,
  var = NULL,
  order = NULL,
  var_names = NULL,
  by_row = FALSE,
  diag = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_vec2mat_+3A_cov">cov</code></td>
<td>
<p>Scalar or vector of covariance information to include the lower-triangle positions of the matrix (default value is zero).
If a vector, the elements must be provided in the order associated with concatenated column (<code>by_row = FALSE; default</code>) or row (<code>by_row = TRUE</code>) vectors of the lower triangle of the desired matrix.
If variances are included in these values, set the <code>diag</code> argument to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="reshape_vec2mat_+3A_var">var</code></td>
<td>
<p>Scalar or vector of variance information to include the diagonal positions of the matrix (default value is 1).</p>
</td></tr>
<tr><td><code id="reshape_vec2mat_+3A_order">order</code></td>
<td>
<p>If cov and var are scalars, this argument determines the number of variables to create in the output matrix.</p>
</td></tr>
<tr><td><code id="reshape_vec2mat_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names.</p>
</td></tr>
<tr><td><code id="reshape_vec2mat_+3A_by_row">by_row</code></td>
<td>
<p>Logical scalar indicating whether <code>cov</code> values should fill the lower triangle by row (<code>TRUE</code>) or by column (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="reshape_vec2mat_+3A_diag">diag</code></td>
<td>
<p>Logical scalar indicating whether <code>cov</code> values include variances (<code>FALSE</code> by default; if <code>TRUE</code>, the variance values supplied with the <code>cov</code> argument will supersede the <code>var</code> argument).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A variance-covariance matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Specify the lower triangle covariances
## Can provide names for the variables
reshape_vec2mat(cov = c(.3, .2, .4), var_names = c("x", "y", "z"))

## Specify scalar values to repeat for the covariances and variances
reshape_vec2mat(cov = .3, var = 2, order = 3)

## Give a vector of variances to create a diagonal matrix
reshape_vec2mat(var = 1:5)

## Specify order only to create identity matrix
reshape_vec2mat(order = 3)

## Specify order and scalar variance to create a scalar matrix
reshape_vec2mat(var = 2, order = 3)

## A quick way to make a 2x2 matrix for bivariate correlations
reshape_vec2mat(cov = .2)
</code></pre>

<hr>
<h2 id='reshape_wide2long'>Reshape database from wide format to long format</h2><span id='topic+reshape_wide2long'></span>

<h3>Description</h3>

<p>This function automates the process of converting a wide-format database (i.e., a database in which intercorrelations between construct pairs define the columns, such that there are multiple columns of correlations) to a long-format database (i.e., a database with just one column of correlations).
The meta-analysis functions in <span class="pkg">psychmeta</span> work best with long-format databases, so this function can be a helpful addition to one's workflow when data are organized in a wide format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape_wide2long(
  data,
  common_vars = NULL,
  es_design = NULL,
  n_design = NULL,
  other_design = NULL,
  es_name = "rxyi",
  missing_col_action = c("warn", "ignore", "stop")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_wide2long_+3A_data">data</code></td>
<td>
<p>Database of data for use in a meta-analysis in &quot;wide&quot; format.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_common_vars">common_vars</code></td>
<td>
<p>String vector of column names relevant to all variables in data.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_es_design">es_design</code></td>
<td>
<p>p x p matrix containing the names of columns of intercorrelations among variables in the lower triangle of the matrix.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_n_design">n_design</code></td>
<td>
<p>Scalar sample-size column name or a p x p matrix containing the names of columns of sample sizes the lower triangle of the matrix.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_other_design">other_design</code></td>
<td>
<p>A matrix with variable names on the rows and names of long-format variables to create on the columns. Elements of this
matrix must be column names of <code>data</code>.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_es_name">es_name</code></td>
<td>
<p>Name of the effect size represented in <code>data</code>.</p>
</td></tr>
<tr><td><code id="reshape_wide2long_+3A_missing_col_action">missing_col_action</code></td>
<td>
<p>Character scalar indicating how missing columns should be handled. Options are: &quot;warn&quot;, &quot;ignore&quot;, and &quot;stop&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A long-format database
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_params = c(mean = 150, sd = 20)
rho_params &lt;- list(c(.1, .3, .5),
                   c(mean = .3, sd = .05),
                   rbind(value = c(.1, .3, .5), weight = c(1, 2, 1)))
rel_params = list(c(.7, .8, .9),
                  c(mean = .8, sd = .05),
                  rbind(value = c(.7, .8, .9), weight = c(1, 2, 1)))
sr_params = c(list(1, 1, c(.5, .7)))
sr_composite_params = list(1, c(.5, .6, .7))
wt_params = list(list(c(1, 2, 3),
                      c(mean = 2, sd = .25),
                      rbind(value = c(1, 2, 3), weight = c(1, 2, 1))),
                 list(c(1, 2, 3),
                      c(mean = 2, sd = .25),
                      rbind(value = c(1, 2, 3), weight = c(1, 2, 1))))

## Simulate with wide format
## Not run: 
data &lt;- simulate_r_database(k = 10, n_params = n_params, rho_params = rho_params,
                          rel_params = rel_params, sr_params = sr_params,
                          sr_composite_params = sr_composite_params, wt_params = wt_params,
                          var_names = c("X", "Y", "Z"), format = "wide")$statistics

## End(Not run)

## Define values to abstract from the data object
common_vars &lt;- "sample_id"
es_design &lt;- matrix(NA, 3, 3)
var_names &lt;- c("X", "Y", "Z")
es_design[lower.tri(es_design)] &lt;- c("rxyi_X_Y", "rxyi_X_Z", "rxyi_Y_Z")
rownames(es_design) &lt;- colnames(es_design) &lt;- var_names
n_design &lt;- "ni"
other_design &lt;- cbind(rxxi = paste0("parallel_rxxi_", var_names),
                      ux_local = paste0("ux_local_", var_names),
                      ux_external = paste0("ux_external_", var_names))
rownames(other_design) &lt;- var_names

## Reshape the data to "long" format
reshape_wide2long(data = data, common_vars = common_vars, es_design = es_design,
                           n_design = n_design, other_design = other_design)
</code></pre>

<hr>
<h2 id='round2char'>Round numeric values to an exact number of digits and return as a character</h2><span id='topic+round2char'></span>

<h3>Description</h3>

<p>Round numeric values to an exact number of digits and return as a character
</p>


<h3>Usage</h3>

<pre><code class='language-R'>round2char(x, digits = 3, na_replace = "", omit_leading_zero = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="round2char_+3A_x">x</code></td>
<td>
<p>Numeric values</p>
</td></tr>
<tr><td><code id="round2char_+3A_digits">digits</code></td>
<td>
<p>Number of digits to which result should be rounded</p>
</td></tr>
<tr><td><code id="round2char_+3A_na_replace">na_replace</code></td>
<td>
<p>Scalar value: Character with which NA values should be replaced</p>
</td></tr>
<tr><td><code id="round2char_+3A_omit_leading_zero">omit_leading_zero</code></td>
<td>
<p>Logical scalar determining whether to omit leading zeros (<code>TRUE</code>) or retain them (<code>FALSE</code>; default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of rounded numbers converted to characters
</p>


<h3>Examples</h3>

<pre><code class='language-R'># round2char(x = .50000005)
# round2char(x = NA, na_replace = "---")
</code></pre>

<hr>
<h2 id='scalar_arg_warning'>Warning message for scalar arguments receiving multiple values</h2><span id='topic+scalar_arg_warning'></span>

<h3>Description</h3>

<p>Warning message for scalar arguments receiving multiple values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scalar_arg_warning(arg, arg_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalar_arg_warning_+3A_arg">arg</code></td>
<td>
<p>Argument value</p>
</td></tr>
<tr><td><code id="scalar_arg_warning_+3A_arg_name">arg_name</code></td>
<td>
<p>Argument name</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Warning if length of arg is greater than 1 and the first element of arg.
</p>

<hr>
<h2 id='screen_ad_int'>Screen to detect invalid interactive artifact distribution objects</h2><span id='topic+screen_ad_int'></span>

<h3>Description</h3>

<p>Screen to detect invalid interactive artifact distribution objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_ad_int(x, obj_name = "x")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_ad_int_+3A_x">x</code></td>
<td>
<p>Object to test for congruence with expected properties of interactive artifact distribution objects.</p>
</td></tr>
<tr><td><code id="screen_ad_int_+3A_obj_name">obj_name</code></td>
<td>
<p>Object name for x.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return a value; will trigger a warning if ad_obj_tsa is not a valid artifact distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ad_obj_int &lt;- create_ad_int(rxxa = c(.9, .8), wt_rxxa = c(50, 150),
                            rxxi = c(.8, .7), wt_rxxi = c(50, 150),
                            ux = c(.9, .8), wt_ux = c(50, 150),
                            ut = c(.8, .7), wt_ut = c(50, 150))

ad_obj_tsa &lt;- create_ad_tsa(rxxa = c(.9, .8), n_rxxa = c(50, 150),
                            rxxi = c(.8, .7), n_rxxi = c(50, 150),
                            ux = c(.9, .8), ni_ux = c(50, 150),
                            ut = c(.8, .7), ni_ut = c(50, 150))

screen_ad_int(x = ad_obj_int)
screen_ad_int(x = ad_obj_tsa)
screen_ad_int(x = data.frame(Value = 1, Weight = 1))

## End(Not run)
</code></pre>

<hr>
<h2 id='screen_ad_tsa'>Screen to detect invalid Taylor series artifact distribution objects</h2><span id='topic+screen_ad_tsa'></span>

<h3>Description</h3>

<p>Screen to detect invalid Taylor series artifact distribution objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_ad_tsa(x, obj_name = "x")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_ad_tsa_+3A_x">x</code></td>
<td>
<p>Object to test for congruence with expected properties of Taylor series artifact distribution objects.</p>
</td></tr>
<tr><td><code id="screen_ad_tsa_+3A_obj_name">obj_name</code></td>
<td>
<p>Object name for x.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return a value; will trigger a warning if ad_obj_tsa is not a valid artifact distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## ad_obj_int &lt;- create_ad_int(rxxa = c(.9, .8), wt_rxxa = c(50, 150),
##                                 rxxi = c(.8, .7), wt_rxxi = c(50, 150),
##                                 ux = c(.9, .8), wt_ux = c(50, 150),
##                                 ut = c(.8, .7), wt_ut = c(50, 150))
##
## ad_obj_tsa &lt;- create_ad_tsa(rxxa = c(.9, .8), n_rxxa = c(50, 150),
##                                 rxxi = c(.8, .7), n_rxxi = c(50, 150),
##                                 ux = c(.9, .8), ni_ux = c(50, 150),
##                                 ut = c(.8, .7), ni_ut = c(50, 150))
##
## screen_ad_tsa(x = ad_obj_tsa)
## screen_ad_tsa(x = ad_obj_int)
## screen_ad_tsa(x = data.frame(Value = 1, Weight = 1))
</code></pre>

<hr>
<h2 id='screen_r'>Screen to detect impossible values in vectors of correlations and sample sizes.</h2><span id='topic+screen_r'></span>

<h3>Description</h3>

<p>Screen to detect impossible values in vectors of correlations and sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_r(r_vec, n_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_r_+3A_r_vec">r_vec</code></td>
<td>
<p>Vector of correlations.</p>
</td></tr>
<tr><td><code id="screen_r_+3A_n_vec">n_vec</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of filtered correlations and sample sizes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## screen_r(r_vec = c(-.3, .5, 1.1), n_vec = c(100, 100, 100))
## screen_r(r_vec = c(-.3, .5, .8), n_vec = c(Inf, 100, 100))
## screen_r(r_vec = c(-.3, .5, .8), n_vec = c(2, 100, 100))
</code></pre>

<hr>
<h2 id='screen_rel'>Screen to detect impossible values in vectors of reliability estimates.</h2><span id='topic+screen_rel'></span>

<h3>Description</h3>

<p>Screen to detect impossible values in vectors of reliability estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_rel(rel_vec, art_name = "Reliability")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_rel_+3A_rel_vec">rel_vec</code></td>
<td>
<p>Vector of reliability estimates.</p>
</td></tr>
<tr><td><code id="screen_rel_+3A_art_name">art_name</code></td>
<td>
<p>Optional artifact name to use in warning messages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return values; stops processes if improper values are used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## screen_rel(rel_vec = c(.8, Inf), art_name = "rxxa")
## screen_rel(rel_vec = c(.8, -.2), art_name = "rxxa")
</code></pre>

<hr>
<h2 id='screen_u'>Screen to detect impossible values in vectors of u ratios.</h2><span id='topic+screen_u'></span>

<h3>Description</h3>

<p>Screen to detect impossible values in vectors of u ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_u(u_vec, art_name = "u ratio")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_u_+3A_u_vec">u_vec</code></td>
<td>
<p>Vector of u ratios.</p>
</td></tr>
<tr><td><code id="screen_u_+3A_art_name">art_name</code></td>
<td>
<p>Optional artifact name to use in warning messages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return values; stops processes if improper values are used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run
## screen_u(u_vec = c(0, .8), art_name = "ux")
## screen_u(u_vec = c(-1, .8), art_name = "ux")
## screen_u(u_vec = c(Inf, .8), art_name = "ux")
</code></pre>

<hr>
<h2 id='sensitivity'>Sensitivity analyses for meta-analyses</h2><span id='topic+sensitivity'></span><span id='topic+sensitivity_bootstrap'></span><span id='topic+sensitivity_cumulative'></span><span id='topic+sensitivity_leave1out'></span>

<h3>Description</h3>

<p>Wrapper function to compute bootstrap analyses, leave-one-out analyses, and cumulative meta-analyses.
This function helps researchers to examine the stability/fragility of their meta-analytic results with bootstrapping and leave-one-out analyses, as well as detect initial evidence of publication bias with cumulative meta-analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sensitivity(
  ma_obj,
  leave1out = TRUE,
  bootstrap = TRUE,
  cumulative = TRUE,
  sort_method = c("weight", "n", "inv_var"),
  boot_iter = 1000,
  boot_conf_level = 0.95,
  boot_ci_type = c("bca", "norm", "basic", "stud", "perc"),
  ...
)

sensitivity_bootstrap(
  ma_obj,
  boot_iter = 1000,
  boot_conf_level = 0.95,
  boot_ci_type = c("bca", "norm", "basic", "stud", "perc"),
  ...
)

sensitivity_cumulative(ma_obj, sort_method = c("weight", "n", "inv_var"), ...)

sensitivity_leave1out(ma_obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sensitivity_+3A_ma_obj">ma_obj</code></td>
<td>
<p>Meta-analysis object.</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_leave1out">leave1out</code></td>
<td>
<p>Logical scalar determining whether to compute leave-one-out analyses (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Logical scalar determining whether bootstrapping is to be performed (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_cumulative">cumulative</code></td>
<td>
<p>Logical scalar determining whether a cumulative meta-analysis is to be computed (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_sort_method">sort_method</code></td>
<td>
<p>Method to sort samples in the cumulative meta-analysis. Options are &quot;weight&quot; to sort by weight (default), &quot;n&quot; to sort by sample size, and &quot;inv_var&quot; to sort by inverse variance.</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_boot_iter">boot_iter</code></td>
<td>
<p>Number of bootstrap iterations to be computed.</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_boot_conf_level">boot_conf_level</code></td>
<td>
<p>Width of confidence intervals to be constructed for all bootstrapped statistics.</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_boot_ci_type">boot_ci_type</code></td>
<td>
<p>Type of bootstrapped confidence interval. Options are &quot;bca&quot;, &quot;norm&quot;, &quot;basic&quot;, &quot;stud&quot;, and &quot;perc&quot; (these are &quot;type&quot; options from the boot::boot.ci function). Default is &quot;bca&quot;.
Note: If you have too few iterations, the &quot;bca&quot; method will not work and you will need to either increase the iterations or choose a different method.</p>
</td></tr>
<tr><td><code id="sensitivity_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated meta-analysis object with sensitivity analyses added.
</p>

<ul>
<li><p> When bootstrapping is performed, the <code>bootstrap</code> section of the <code>follow_up_analyses</code> section of the updated <code>ma_obj</code> returned by this function will contain both a matrix summarizing the mean, variance, and confidence intervals of the bootstrapped samples and a table of meta-analytic results from all bootstrapped samples.
</p>
</li>
<li><p> When leave-one-out analyses are performed, the <code>ma_obj</code> will acquire a list of leave-one-out results in its <code>follow_up_analyses</code> section that contains a table of all leave-one-out meta-analyses along with plots of the mean and residual variance of the effect sizes in the meta-analyses.
</p>
</li>
<li><p> When cumulative meta-analysis is performed, the <code>ma_obj</code> will acquire a list of cumulative meta-analysis results in its <code>follow_up_analyses</code> section that contains a table of all meta-analyses computed along with plots of the mean and residual variance of the effect sizes in the meta-analyses, sorted by the order in which studies were added to the meta-analysis.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Run a meta-analysis using simulated correlation data:
ma_obj &lt;- ma_r_ic(rxyi = rxyi, n = n, rxx = rxxi, ryy = ryyi, ux = ux,
                  correct_rr_y = FALSE, data = data_r_uvirr)
ma_obj &lt;- ma_r_ad(ma_obj, correct_rr_y = FALSE)

## Pass the meta-analysis object to the sensitivity() function:
ma_obj &lt;- sensitivity(ma_obj = ma_obj, boot_iter = 10,
                      boot_ci_type = "norm", sort_method = "inv_var")

## Examine the tables and plots produced for the IC meta-analysis:
ma_obj$bootstrap[[1]]$barebones
ma_obj$bootstrap[[1]]$individual_correction$true_score
ma_obj$leave1out[[1]]$individual_correction$true_score
ma_obj$cumulative[[1]]$individual_correction$true_score

## Examine the tables and plots produced for the AD meta-analysis:
ma_obj$bootstrap[[1]]$artifact_distribution$true_score
ma_obj$leave1out[[1]]$artifact_distribution$true_score
ma_obj$cumulative[[1]]$artifact_distribution$true_score


## Run a meta-analysis using simulated d-value data:
ma_obj &lt;- ma_d_ic(d = d, n1 = n1, n2 = n2, ryy = ryyi,
                  data = filter(data_d_meas_multi, construct == "Y"))
ma_obj &lt;- ma_d_ad(ma_obj)
                  
## Pass the meta-analysis object to the sensitivity() function:
ma_obj &lt;- sensitivity(ma_obj = ma_obj, boot_iter = 10,
                      boot_ci_type = "norm", sort_method = "inv_var")

## Examine the tables and plots produced for the IC meta-analysis:
ma_obj$bootstrap[[1]]$barebones
ma_obj$bootstrap[[1]]$individual_correction$latentGroup_latentY
ma_obj$leave1out[[1]]$individual_correction$latentGroup_latentY
ma_obj$cumulative[[1]]$individual_correction$latentGroup_latentY

## Examine the tables and plots produced for the AD meta-analysis:
ma_obj$bootstrap[[1]]$artifact_distribution$latentGroup_latentY
ma_obj$leave1out[[1]]$artifact_distribution$latentGroup_latentY
ma_obj$cumulative[[1]]$artifact_distribution$latentGroup_latentY

## End(Not run)
</code></pre>

<hr>
<h2 id='simulate_alpha'>Generate a vector of simulated sample alpha coefficients</h2><span id='topic+simulate_alpha'></span>

<h3>Description</h3>

<p>This function generates inter-item covariance matrices from a population matrix and computes a coefficient alpha reliability estimate for each matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_alpha(
  item_mat = NULL,
  alpha = NULL,
  k_items = NULL,
  n_cases,
  k_samples,
  standarized = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_alpha_+3A_item_mat">item_mat</code></td>
<td>
<p>Item correlation/covariance matrix. If item_mat is not supplied, the user must supply both <code>alpha</code> and <code>k_items</code>.
If item_mat is <code>NULL</code>, the program will assume that all item intercorrelations are equal.</p>
</td></tr>
<tr><td><code id="simulate_alpha_+3A_alpha">alpha</code></td>
<td>
<p>Population alpha value. Must be supplied if <code>item_mat</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="simulate_alpha_+3A_k_items">k_items</code></td>
<td>
<p>Number of items on the test to be simulated. Must be supplied if <code>item_mat</code> is <code>NULL.</code></p>
</td></tr>
<tr><td><code id="simulate_alpha_+3A_n_cases">n_cases</code></td>
<td>
<p>Number of cases to simulate in sampling distribution of alpha.</p>
</td></tr>
<tr><td><code id="simulate_alpha_+3A_k_samples">k_samples</code></td>
<td>
<p>Number of samples to simulate.</p>
</td></tr>
<tr><td><code id="simulate_alpha_+3A_standarized">standarized</code></td>
<td>
<p>Should alpha be computed from correlation matrices (<code>TRUE</code>) or unstandardized covariance matrices (<code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of simulated sample alpha coefficients
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Define a hypothetical matrix:
item_mat &lt;- reshape_vec2mat(cov = .3, order = 12)

## Simulations of unstandardized alphas
set.seed(100)
simulate_alpha(item_mat = item_mat, n_cases = 50, k_samples = 10, standarized = FALSE)
set.seed(100)
simulate_alpha(alpha = mean(item_mat[lower.tri(item_mat)]) / mean(item_mat),
k_items = ncol(item_mat), n_cases = 50, k_samples = 10, standarized = FALSE)

## Simulations of standardized alphas
set.seed(100)
simulate_alpha(item_mat = item_mat, n_cases = 50, k_samples = 10, standarized = TRUE)
set.seed(100)
simulate_alpha(alpha = mean(item_mat[lower.tri(item_mat)]) / mean(item_mat),
k_items = ncol(item_mat), n_cases = 50, k_samples = 10, standarized = TRUE)
</code></pre>

<hr>
<h2 id='simulate_d_database'>Simulate d value databases of primary studies</h2><span id='topic+simulate_d_database'></span>

<h3>Description</h3>

<p>The <code>simulate_d_database</code> function generates databases of psychometric d value data from sample-size parameters, correlation parameters, mean parameters, standard deviation parameters, reliability parameters, and selection-ratio parameters.
The output database can be provided in a long format.
If composite variables are to be formed, parameters can also be defined for the weights used to form the composites as well as the selection ratios applied to the composites.
This function will return a database of statistics as well as a database of parameters - the parameter database contains the actual study parameters for each simulated sample (without sampleing error) to allow comparisons between meta-analytic results computed from the statistics and the actual means and variances of parameters.
The <code><a href="#topic+merge_simdat_d">merge_simdat_d</a></code> function can be used to merge multiple simulated databases and the <code><a href="#topic+sparsify_simdat_d">sparsify_simdat_d</a></code> function can be used to randomly delete artifact information (a procedure commonly done in simulations of artifact-distribution methods).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_d_database(
  k,
  n_params,
  rho_params,
  mu_params = NULL,
  sigma_params = 1,
  rel_params = 1,
  sr_params = 1,
  k_items_params = 1,
  wt_params = NULL,
  allow_neg_wt = FALSE,
  sr_composite_params = NULL,
  group_names = NULL,
  var_names = NULL,
  composite_names = NULL,
  diffs_as_obs = FALSE,
  show_applicant = FALSE,
  keep_vars = NULL,
  decimals = 2,
  max_iter = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_d_database_+3A_k">k</code></td>
<td>
<p>Number of studies to simulate.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_n_params">n_params</code></td>
<td>
<p>List of parameter distributions (or data-generation function; see details) for subgroup sample sizes.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_rho_params">rho_params</code></td>
<td>
<p>List containing a list of parameter distributions (or data-generation functions; see details) for correlations for each simulated group. If simulating data from a single fixed population matrix in each group, supply a list of those matrices for this argument (if the diagonals contains non-unity values and 'sigma_params' argument is not specified, those values will be used as variances).</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_mu_params">mu_params</code></td>
<td>
<p>List containing a list of parameter distributions (or data-generation functions; see details) for means for each simulated group. If <code>NULL</code>, all means will be set to zero.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_sigma_params">sigma_params</code></td>
<td>
<p>List containing a list of parameter distributions (or data-generation functions; see details) for standard deviations for each simulated group. If <code>NULL</code>, all standard deviations will be set to unity.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_rel_params">rel_params</code></td>
<td>
<p>List containing a list of parameter distributions (or data-generation functions; see details) for reliabilities for each simulated group. If <code>NULL</code>, all reliabilities will be set to unity.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_sr_params">sr_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for selection ratios. If <code>NULL</code>, all selection ratios will be set to unity.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_k_items_params">k_items_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for the number of test items comprising each of the variables to be simulated (all are single-item variables by default).</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_wt_params">wt_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) to create weights for use in forming composites.
If multiple composites are formed, the list should be a list of lists, with the general format: <code>list(comp1_params = list(...params...), comp2_params = list(...params...), etc.)</code>.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_allow_neg_wt">allow_neg_wt</code></td>
<td>
<p>Logical scalar that determines whether negative weights should be allowed (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_sr_composite_params">sr_composite_params</code></td>
<td>
<p>Parameter distributions (or data-generation functions; see details) for composite selection ratios.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_group_names">group_names</code></td>
<td>
<p>Optional vector of group names.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names for all non-composite variables.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_composite_names">composite_names</code></td>
<td>
<p>Optional vector of names for composite variables.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_diffs_as_obs">diffs_as_obs</code></td>
<td>
<p>Logical scalar that determines whether standard deviation parameters represent standard deviations of observed scores (<code>TRUE</code>) or of true scores (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_show_applicant">show_applicant</code></td>
<td>
<p>Should applicant data be shown for sample statistics (<code>TRUE</code>) or suppressed (<code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_keep_vars">keep_vars</code></td>
<td>
<p>Optional vector of variable names to be extracted from the simulation and returned in the output object. All variables are returned by default. Use this argument when
only some variables are of interest and others are generated solely to serve as selection variables.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimals to which statistical results (not parameters) should be rounded. Rounding to 2 decimal places best captures the precision of data available from published primary research.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations to allow in the parameter selection process before terminating with convergence failure. Must be finite.</p>
</td></tr>
<tr><td><code id="simulate_d_database_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Values supplied as any argument with the suffix &quot;params&quot; can take any of three forms (see Examples for a demonstration of usage):
</p>

<ul>
<li><p> A vector of values from which study parameters should be sampled.
</p>
</li>
<li><p> A vector containing a mean with a variance or standard deviation. These values must be named &quot;mean,&quot; &quot;var,&quot; and &quot;sd&quot;, respectively, for the program to recognize which value is which.
</p>
</li>
<li><p> A matrix containing a row of values (this row must be named &quot;values&quot;) from which study parameters should be sampled and a row of weights (this row must be labeled 'weights') associated
with the values to be sampled.
</p>
</li>
<li><p> A matrix containing a column of values (this column must be named &quot;values&quot;) from which study parameters should be sampled and a column of weights (this column must be labeled 'weights') associated
with the values to be sampled.
</p>
</li>
<li><p> A function that is configured to generate data using only one argument that defines the number of cases to generate, e.g., <code>fun(n = 10)</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A database of simulated primary studies' statistics and analytically determined parameter values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("nor1mix", quietly = TRUE)) {
  ## Define sample sizes, means, and other parameters for each of two groups:
  n_params &lt;- list(c(mean = 200, sd = 20),
                   c(mean = 100, sd = 20))
  rho_params &lt;- list(list(c(.3, .4, .5)),
                     list(c(.3, .4, .5)))
  mu_params &lt;- list(list(c(mean = .5, sd = .5), c(-.5, 0, .5)),
                    list(c(mean = 0, sd = .5), c(-.2, 0, .2)))
  sigma_params &lt;- list(list(1, 1),
                       list(1, 1))
  rel_params &lt;- list(list(.8, .8),
                     list(.8, .8))
  sr_params &lt;- list(1, .5)

  simulate_d_database(k = 5, n_params = n_params, rho_params = rho_params,
                      mu_params = mu_params, sigma_params = sigma_params,
                      rel_params = rel_params, sr_params = sr_params,
                      k_items = c(4, 4),
                      group_names = NULL, var_names = c("y1", "y2"),
                      show_applicant = TRUE, keep_vars = c("y1", "y2"), decimals = 2)
}
</code></pre>

<hr>
<h2 id='simulate_d_sample'>Simulate a sample of psychometric d value data with measurement error, direct range restriction, and/or indirect range restriction</h2><span id='topic+simulate_d_sample'></span>

<h3>Description</h3>

<p>This function generates a simulated psychometric sample consisting of any number of groups and computes the <em>d</em> values that result after introducing measurement error and/or range restriction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_d_sample(
  n_vec,
  rho_mat_list,
  mu_mat,
  sigma_mat = 1,
  rel_mat = 1,
  sr_vec = 1,
  k_items_vec = 1,
  wt_mat = NULL,
  sr_composites = NULL,
  group_names = NULL,
  var_names = NULL,
  composite_names = NULL,
  diffs_as_obs = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_d_sample_+3A_n_vec">n_vec</code></td>
<td>
<p>Vector of sample sizes (or a vector of proportions, if parameters are to be estimated).</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_rho_mat_list">rho_mat_list</code></td>
<td>
<p>List of true-score correlation matrices.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_mu_mat">mu_mat</code></td>
<td>
<p>Matrix of mean parameters, with groups on the rows and variables on the columns.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_sigma_mat">sigma_mat</code></td>
<td>
<p>Matrix of standard-deviation parameters, with groups on the rows and variables on the columns.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_rel_mat">rel_mat</code></td>
<td>
<p>Matrix of reliability parameters, with groups on the rows and variables on the columns.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_sr_vec">sr_vec</code></td>
<td>
<p>Vector of selection ratios.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_k_items_vec">k_items_vec</code></td>
<td>
<p>Number of test items comprising each of the variables to be simulated (all are single-item variables by default).</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_wt_mat">wt_mat</code></td>
<td>
<p>Optional matrix of weights to use in forming a composite of the variables in <code>rho_mat.</code> Matrix should have as many rows (or vector elements) as there are variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_sr_composites">sr_composites</code></td>
<td>
<p>Optional vector selection ratios for composite variables. If not <code>NULL</code>, <code>sr_composites</code> must have as many elements as there are columns in <code>wt_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_group_names">group_names</code></td>
<td>
<p>Optional vector of group names.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_composite_names">composite_names</code></td>
<td>
<p>Optional vector of names for composite variables.</p>
</td></tr>
<tr><td><code id="simulate_d_sample_+3A_diffs_as_obs">diffs_as_obs</code></td>
<td>
<p>Logical scalar that determines whether standard deviation parameters represent standard deviations of observed scores (<code>TRUE</code>) or of true scores (<code>FALSE</code>; default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sample of simulated mean differences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate statistics by providing integers as "n_vec":
simulate_d_sample(n_vec = c(200, 100), rho_mat_list = list(reshape_vec2mat(.5),
                                                           reshape_vec2mat(.4)),
                  mu_mat = rbind(c(1, .5), c(0, 0)), sigma_mat = rbind(c(1, 1), c(1, 1)),
                  rel_mat = rbind(c(.8, .7), c(.7, .7)), sr_vec = c(1, .5),
                  group_names = c("A", "B"))

## Simulate parameters by providing proportions as "n_vec":
simulate_d_sample(n_vec = c(2/3, 1/3), rho_mat_list = list(reshape_vec2mat(.5),
                                                           reshape_vec2mat(.4)),
                  mu_mat = rbind(c(1, .5), c(0, 0)), sigma_mat = rbind(c(1, 1), c(1, 1)),
                  rel_mat = rbind(c(.8, .7), c(.7, .7)), sr_vec = c(1, .5),
                  group_names = c("A", "B"))
</code></pre>

<hr>
<h2 id='simulate_matrix'>Generate a list of simulated sample matrices sampled from the Wishart distribution</h2><span id='topic+simulate_matrix'></span>

<h3>Description</h3>

<p>This function generates simulated sample matrices based on a population matrix and a sample size.
It uses the Wishart distribution (i.e., the multivariate <code class="reqn">\chi^{2}</code> distribution) to obtain data, rescales the data into the input metric, and can be standardized into a correlation matrix by setting <code>as_cor</code> to <code>TRUE</code>.
The function can produce a list of matrices for any number of samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_matrix(sigma, n, k = 1, as_cor = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_matrix_+3A_sigma">sigma</code></td>
<td>
<p>Population covariance matrix. May be standardized or unstandardized.</p>
</td></tr>
<tr><td><code id="simulate_matrix_+3A_n">n</code></td>
<td>
<p>Sample size for simulated sample matrices.</p>
</td></tr>
<tr><td><code id="simulate_matrix_+3A_k">k</code></td>
<td>
<p>Number of sample matrices to generate.</p>
</td></tr>
<tr><td><code id="simulate_matrix_+3A_as_cor">as_cor</code></td>
<td>
<p>Should the simulated matrices be standardized (<code>TRUE</code>) or unstandardized (<code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of simulated sample matrices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Define a hypothetical matrix:
sigma &lt;- reshape_vec2mat(cov = .4, order = 5)

## Simualte a list of unstandardized covariance matrices:
simulate_matrix(sigma = sigma, n = 50, k = 10, as_cor = FALSE)

## Simualte a list of correlation matrices:
simulate_matrix(sigma = sigma, n = 50, k = 10, as_cor = TRUE)
</code></pre>

<hr>
<h2 id='simulate_psych'>Simulate Monte Carlo psychometric data (observed, true, and error scores)</h2><span id='topic+simulate_psych'></span>

<h3>Description</h3>

<p>Simulate Monte Carlo psychometric data (observed, true, and error scores)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_psych(
  n,
  rho_mat,
  mu_vec = rep(0, ncol(rho_mat)),
  sigma_vec = rep(1, ncol(rho_mat)),
  rel_vec = rep(1, ncol(rho_mat)),
  sr_vec = rep(1, ncol(rho_mat)),
  k_items_vec = rep(1, ncol(rho_mat)),
  wt_mat = NULL,
  sr_composites = NULL,
  var_names = NULL,
  composite_names = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_psych_+3A_n">n</code></td>
<td>
<p>Number of cases to simulate before performing selection.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_rho_mat">rho_mat</code></td>
<td>
<p>Matrix of true-score correlations.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_mu_vec">mu_vec</code></td>
<td>
<p>Vector of means.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_sigma_vec">sigma_vec</code></td>
<td>
<p>Vector of observed-score standard deviations.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_rel_vec">rel_vec</code></td>
<td>
<p>Vector of reliabilities corresponding to the variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_sr_vec">sr_vec</code></td>
<td>
<p>Vector of selection ratios corresponding to the variables in <code>rho_mat</code>.
(set selection ratios to 1 for variables that should not be used in selection).</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_k_items_vec">k_items_vec</code></td>
<td>
<p>Number of test items comprising each of the variables to be simulated (all are single-item variables by default).</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_wt_mat">wt_mat</code></td>
<td>
<p>Optional matrix of weights to use in forming a composite of the variables in <code>rho_mat</code>. Matrix should have as many rows (or vector elements) as there are variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_sr_composites">sr_composites</code></td>
<td>
<p>Optional vector selection ratios for composite variables. If not <code>NULL</code>, <code>sr_composites</code> must have as many elements as there are columns in <code>wt_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_var_names">var_names</code></td>
<td>
<p>Vector of variable names corresponding to the variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_psych_+3A_composite_names">composite_names</code></td>
<td>
<p>Optional vector of names for composite variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of observed-score, true-score, and error-score data frames. If selection is requested, the data frames will include logical variables indicating whether each case would be selected on the basis of observed scores, true scores, or error scores.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data for a simple sample with two variables without selection:
simulate_psych(n = 1000, rho_mat = matrix(c(1, .5, .5, 1), 2, 2), sigma_vec = c(1, 1),
          rel_vec = c(.8, .8), var_names = c("Y", "X"))

## Generate data for a simple sample with two variables with selection:
simulate_psych(n = 1000, rho_mat = matrix(c(1, .5, .5, 1), 2, 2), sigma_vec = c(1, 1),
          rel_vec = c(.8, .8), sr_vec = c(1, .5), var_names = c("Y", "X"))

## Generate data for samples with five variables, of which subsets are used to form composites:
rho_mat &lt;- matrix(.5, 5, 5)
diag(rho_mat) &lt;- 1
simulate_psych(n = 1000, rho_mat = rho_mat,
                rel_vec = rep(.8, 5), sr_vec = c(1, 1, 1, 1, .5),
                wt_mat = cbind(c(0, 0, 0, .3, 1), c(1, .3, 0, 0, 0)), sr_composites = c(.7, .5))

## Generate data for similar scenario as above, but with scales consisting of 1-5 items:
rho_mat &lt;- matrix(.5, 5, 5)
diag(rho_mat) &lt;- 1
simulate_psych(n = 1000, rho_mat = rho_mat,
                rel_vec = rep(.8, 5), sr_vec = c(1, 1, 1, 1, .5),
                k_items_vec = 1:5,
                wt_mat = cbind(c(0, 0, 0, .3, 1), c(1, .3, 0, 0, 0)), sr_composites = c(.7, .5))
</code></pre>

<hr>
<h2 id='simulate_r_database'>Simulate correlation databases of primary studies</h2><span id='topic+simulate_r_database'></span>

<h3>Description</h3>

<p>The <code>simulate_r_database</code> function generates databases of psychometric correlation data from sample-size parameters, correlation parameters, reliability parameters, and selection-ratio parameters.
The output database can be provided in either a long format or a wide format.
If composite variables are to be formed, parameters can also be defined for the weights used to form the composites as well as the selection ratios applied to the composites.
This function will return a database of statistics as well as a database of parameters - the parameter database contains the actual study parameters for each simulated sample (without sampleing error) to allow comparisons between meta-analytic results computed from the statistics and the actual means and variances of parameters.
The <code><a href="#topic+merge_simdat_r">merge_simdat_r</a></code> function can be used to merge multiple simulated databases and the <code><a href="#topic+sparsify_simdat_r">sparsify_simdat_r</a></code> function can be used to randomly delete artifact information (a procedure commonly done in simulations of artifact-distribution methods).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_r_database(
  k,
  n_params,
  rho_params,
  mu_params = 0,
  sigma_params = 1,
  rel_params = 1,
  sr_params = 1,
  k_items_params = 1,
  wt_params = NULL,
  allow_neg_wt = FALSE,
  sr_composite_params = NULL,
  var_names = NULL,
  composite_names = NULL,
  n_as_ni = FALSE,
  show_applicant = FALSE,
  keep_vars = NULL,
  decimals = 2,
  format = "long",
  max_iter = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_r_database_+3A_k">k</code></td>
<td>
<p>Number of studies to simulate.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_n_params">n_params</code></td>
<td>
<p>Parameter distribution (or data-generation function; see details) for sample size.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_rho_params">rho_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for correlations. If simulating data from a single fixed population matrix, that matrix can be supplied for this argument (if the diagonal contains non-unity values and 'sigma_params' is not specified, those values will be used as variances).</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_mu_params">mu_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for means.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_sigma_params">sigma_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for standard deviations.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_rel_params">rel_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for reliabilities.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_sr_params">sr_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for selection ratios.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_k_items_params">k_items_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) for the number of test items comprising each of the variables to be simulated (all are single-item variables by default).</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_wt_params">wt_params</code></td>
<td>
<p>List of parameter distributions (or data-generation functions; see details) to create weights for use in forming composites.
If multiple composites are formed, the list should be a list of lists, with the general format: <code>list(comp1_params = list(...params...), comp2_params = list(...params...), etc.)</code>.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_allow_neg_wt">allow_neg_wt</code></td>
<td>
<p>Logical scalar that determines whether negative weights should be allowed (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_sr_composite_params">sr_composite_params</code></td>
<td>
<p>Parameter distributions (or data-generation functions; see details) for composite selection ratios.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names for all non-composite variables.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_composite_names">composite_names</code></td>
<td>
<p>Optional vector of names for composite variables.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_n_as_ni">n_as_ni</code></td>
<td>
<p>Logical argument determining whether n specifies the incumbent sample size (TRUE) or the applicant sample size (FALSE; default). This can only be TRUE when only one variable is involved in selection.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_show_applicant">show_applicant</code></td>
<td>
<p>Should applicant data be shown for sample statistics (<code>TRUE</code>) or suppressed (<code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_keep_vars">keep_vars</code></td>
<td>
<p>Optional vector of variable names to be extracted from the simulation and returned in the output object. All variables are returned by default. Use this argument when
only some variables are of interest and others are generated solely to serve as selection variables.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimals to which statistical results (not parameters) should be rounded. Rounding to 2 decimal places best captures the precision of data available from published primary research.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_format">format</code></td>
<td>
<p>Database format: &quot;long&quot; or &quot;wide.&quot;</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations to allow in the parameter selection process before terminating with convergence failure. Must be finite.</p>
</td></tr>
<tr><td><code id="simulate_r_database_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Values supplied as any argument with the suffix &quot;params&quot; can take any of three forms (see Examples for a demonstration of usage):
</p>

<ul>
<li><p> A vector of values from which study parameters should be sampled.
</p>
</li>
<li><p> A vector containing a mean with a variance or standard deviation. These values must be named &quot;mean,&quot; &quot;var,&quot; and &quot;sd&quot;, respectively, for the program to recognize which value is which.
</p>
</li>
<li><p> A matrix containing a row of values (this row must be named &quot;values&quot;) from which study parameters should be sampled and a row of weights (this row must be labeled 'weights') associated
with the values to be sampled.
</p>
</li>
<li><p> A matrix containing a column of values (this column must be named &quot;values&quot;) from which study parameters should be sampled and a column of weights (this column must be labeled 'weights') associated
with the values to be sampled.
</p>
</li>
<li><p> A function that is configured to generate data using only one argument that defines the number of cases to generate, e.g., <code>fun(n = 10)</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A database of simulated primary studies' statistics and analytically determined parameter values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Note the varying methods for defining parameters:
n_params = function(n) rgamma(n, shape = 100)
rho_params &lt;- list(c(.1, .3, .5),
                   c(mean = .3, sd = .05),
                   rbind(value = c(.1, .3, .5), weight = c(1, 2, 1)))
rel_params = list(c(.7, .8, .9),
                  c(mean = .8, sd = .05),
                  rbind(value = c(.7, .8, .9), weight = c(1, 2, 1)))
sr_params = c(list(1, 1, c(.5, .7)))
sr_composite_params = list(1, c(.5, .6, .7))
wt_params = list(list(c(1, 2, 3),
                      c(mean = 2, sd = .25),
                      rbind(value = c(1, 2, 3), weight = c(1, 2, 1))),
                 list(c(1, 2, 3),
                      c(mean = 2, sd = .25),
                      cbind(value = c(1, 2, 3), weight = c(1, 2, 1))))

## Simulate with long format
simulate_r_database(k = 10, n_params = n_params, rho_params = rho_params,
                  rel_params = rel_params, sr_params = sr_params,
                  sr_composite_params = sr_composite_params, wt_params = wt_params,
                  var_names = c("X", "Y", "Z"), format = "long")

## Simulate with wide format
simulate_r_database(k = 10, n_params = n_params, rho_params = rho_params,
                  rel_params = rel_params, sr_params = sr_params,
                  sr_composite_params = sr_composite_params, wt_params = wt_params,
                  var_names = c("X", "Y", "Z"), format = "wide")

## End(Not run)
</code></pre>

<hr>
<h2 id='simulate_r_sample'>Simulation of data with measurement error and range-restriction artifacts</h2><span id='topic+simulate_r_sample'></span>

<h3>Description</h3>

<p>This function simulates a psychometric sample and produces correlation matrices, artifact information, and other descriptive statistics that have been affected by measurement error and/or range restriction.
It allows the formation of composite variables within the simulation and allows selection to be performed on any or all variables, including composites.
By setting the sample size to <code>n = Inf</code>, users can explore the effects of measurement error and/or range restriction on parameters without the influence of sampling error.
To generate multiple samples and compile a database of simulated statistics, see the <code><a href="#topic+simulate_r_database">simulate_r_database</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_r_sample(
  n,
  rho_mat,
  rel_vec = rep(1, ncol(rho_mat)),
  mu_vec = rep(0, ncol(rho_mat)),
  sigma_vec = rep(1, ncol(rho_mat)),
  sr_vec = rep(1, ncol(rho_mat)),
  k_items_vec = rep(1, ncol(rho_mat)),
  wt_mat = NULL,
  sr_composites = NULL,
  var_names = NULL,
  composite_names = NULL,
  n_as_ni = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_r_sample_+3A_n">n</code></td>
<td>
<p>Number of cases to simulate before performing selection. If <code>Inf</code>, function will simulate parameter values.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_rho_mat">rho_mat</code></td>
<td>
<p>Matrix of true-score correlations.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_rel_vec">rel_vec</code></td>
<td>
<p>Vector of reliabilities corresponding to the variables in <code>rho_mat.</code></p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_mu_vec">mu_vec</code></td>
<td>
<p>Vector of means.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_sigma_vec">sigma_vec</code></td>
<td>
<p>Vector of observed-score standard deviations.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_sr_vec">sr_vec</code></td>
<td>
<p>Vector of selection ratios corresponding to the variables in <code>rho_mat</code>
(set selection ratios to 1 for variables that should not be used in selection).</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_k_items_vec">k_items_vec</code></td>
<td>
<p>Number of test items comprising each of the variables to be simulated (all are single-item variables by default).</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_wt_mat">wt_mat</code></td>
<td>
<p>Optional matrix of weights to use in forming a composite of the variables in <code>rho_mat.</code> Matrix should have as many rows (or vector elements) as there are variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_sr_composites">sr_composites</code></td>
<td>
<p>Optional vector selection ratios for composite variables. If not <code>NULL</code>, <code>sr_composites</code> must have as many elements as there are columns in <code>wt_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_var_names">var_names</code></td>
<td>
<p>Vector of variable names corresponding to the variables in <code>rho_mat</code>.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_composite_names">composite_names</code></td>
<td>
<p>Optional vector of names for composite variables.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_n_as_ni">n_as_ni</code></td>
<td>
<p>Logical argument determining whether n specifies the incumbent sample size (TRUE) or the applicant sample size (FALSE; default). This can only be TRUE when only one variable is involved in selection.</p>
</td></tr>
<tr><td><code id="simulate_r_sample_+3A_...">...</code></td>
<td>
<p>Further arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of study information, including correlations, reliabilities, standard deviations, means, and <em>u</em> ratios for true scores and for observed scores.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Generate data for a simple sample with two variables:
simulate_r_sample(n = 1000, rho_mat = matrix(c(1, .5, .5, 1), 2, 2),
          rel_vec = c(.8, .8), sr_vec = c(1, .5), var_names = c("Y", "X"))

## Generate data for samples with five variables, of which subsets are used to form composites:
rho_mat &lt;- matrix(.5, 5, 5)
diag(rho_mat) &lt;- 1

## Simulate parameters by supply n = Inf
simulate_r_sample(n = Inf, rho_mat = rho_mat,
                rel_vec = rep(.8, 5), sr_vec = c(1, 1, 1, 1, .5),
                wt_mat = cbind(c(0, 0, 0, .3, 1), c(1, .3, 0, 0, 0)), sr_composites = c(.7, .5))

## Finite sample sizes allow the generation of sample data
simulate_r_sample(n = 1000, rho_mat = rho_mat,
                rel_vec = rep(.8, 5), sr_vec = c(1, 1, 1, 1, .5),
                wt_mat = cbind(c(0, 0, 0, .3, 1), c(1, .3, 0, 0, 0)), sr_composites = c(.7, .5))

## End(Not run)                 
</code></pre>

<hr>
<h2 id='sparsify_simdat_d'>Create sparse artifact information in a &quot;simdat_d_database&quot; class object</h2><span id='topic+sparsify_simdat_d'></span>

<h3>Description</h3>

<p>This function can be used to randomly delete artifact from databases produced by the <code><a href="#topic+simulate_d_database">simulate_d_database</a></code> function.
Deletion of artifacts can be performed in either a study-wise fashion for complete missingness within randomly selected studies or element-wise missingness for completely random deletion of artifacts in the database.
Deletion can be applied to reliability estimates and/or u ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsify_simdat_d(
  data_obj,
  prop_missing,
  sparify_arts = c("rel", "u"),
  study_wise = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsify_simdat_d_+3A_data_obj">data_obj</code></td>
<td>
<p>Object created by the &quot;simdat_d_database&quot; function.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_d_+3A_prop_missing">prop_missing</code></td>
<td>
<p>Proportion of studies in from which artifact information should be deleted.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_d_+3A_sparify_arts">sparify_arts</code></td>
<td>
<p>Vector of codes for the artifacts to be sparsified: &quot;rel&quot; for reliabilities, &quot;u&quot; for u ratios, or c(&quot;rel&quot;, &quot;u&quot;) for both.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_d_+3A_study_wise">study_wise</code></td>
<td>
<p>Logical scalar argument determining whether artifact deletion should occur for all variables in a study (<code>TRUE</code>) or randomly across variables within studies (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparsified database
</p>

<hr>
<h2 id='sparsify_simdat_r'>Create sparse artifact information in a &quot;simdat_r_database&quot; class object</h2><span id='topic+sparsify_simdat_r'></span>

<h3>Description</h3>

<p>This function can be used to randomly delete artifact from databases produced by the <code><a href="#topic+simulate_r_database">simulate_r_database</a></code> function.
Deletion of artifacts can be performed in either a study-wise fashion for complete missingness within randomly selected studies or element-wise missingness for completely random deletion of artifacts in the database.
Deletion can be applied to reliability estimates and/or u ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsify_simdat_r(
  data_obj,
  prop_missing,
  sparify_arts = c("rel", "u"),
  study_wise = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsify_simdat_r_+3A_data_obj">data_obj</code></td>
<td>
<p>Object created by the &quot;simdat_r_database&quot; function.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_r_+3A_prop_missing">prop_missing</code></td>
<td>
<p>Proportion of studies in from which artifact information should be deleted.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_r_+3A_sparify_arts">sparify_arts</code></td>
<td>
<p>Vector of codes for the artifacts to be sparsified: &quot;rel&quot; for reliabilities, &quot;u&quot; for u ratios, or c(&quot;rel&quot;, &quot;u&quot;) for both.</p>
</td></tr>
<tr><td><code id="sparsify_simdat_r_+3A_study_wise">study_wise</code></td>
<td>
<p>Logical scalar argument determining whether artifact deletion should occur for all variables in a study (<code>TRUE</code>) or randomly across variables within studies (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparsified database
</p>

<hr>
<h2 id='summarize_ads'>Summarize artifact information from meta-analyses into table format</h2><span id='topic+summarize_ads'></span>

<h3>Description</h3>

<p>Summarize artifact information from meta-analyses into table format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarize_ads(ma_obj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summarize_ads_+3A_ma_obj">ma_obj</code></td>
<td>
<p>A meta-analysis object of correlations or d values with psychometric information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A table of artifact information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Artifact distributions from "ma_r" with individual corrections
ma_obj_ic_pairwise &lt;- ma_r(ma_method = "ic", rxyi = "rxyi", n = "n",
                           rxx = "rxxi", ryy = "ryyi",
                           pairwise_ads = TRUE,
                           correct_rr_x = FALSE, correct_rr_y = FALSE,
                           construct_x = "x_name", construct_y = "y_name",
                           sample_id = "sample_id", moderators = "moderator",
                           data = data_r_meas_multi)
summarize_ads(ma_obj = ma_obj_ic_pairwise)

## Artifact distributions from "ma_r" with artifact-distribution corrections (pairwise ADs)
ma_obj_ad_pairwise &lt;- ma_r(ma_method = "ad", rxyi = "rxyi", n = "n",
                           rxx = "rxxi", ryy = "ryyi",
                           pairwise_ads = TRUE,
                           correct_rr_x = FALSE, correct_rr_y = FALSE,
                           construct_x = "x_name", construct_y = "y_name",
                           sample_id = "sample_id", moderators = "moderator",
                           data = data_r_meas_multi)
summarize_ads(ma_obj = ma_obj_ad_pairwise)

## Artifact distributions from "ma_r" with artifact-distribution corrections (overall ADs)
ma_obj_ad_nonpairwise &lt;- ma_r(ma_method = "ad", rxyi = "rxyi", n = "n",
                              rxx = "rxxi", ryy = "ryyi",
                              pairwise_ads = FALSE,
                              correct_rr_x = FALSE, correct_rr_y = FALSE,
                              construct_x = "x_name", construct_y = "y_name",
                              sample_id = "sample_id", moderators = "moderator",
                              data = data_r_meas_multi)
summarize_ads(ma_obj = ma_obj_ad_nonpairwise)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary'>Summary methods for <span class="pkg">psychmeta</span></h2><span id='topic+summary'></span>

<h3>Description</h3>

<p>Summary methods for <span class="pkg">psychmeta</span> output objects with classes exported from <span class="pkg">psychmeta</span>.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_+3A_object">object</code></td>
<td>
<p>Object to be printed (object is used to select a method).</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary object.
</p>

<hr>
<h2 id='truncate_dist'>Truncation function for normal distributions (truncates both mean and variance)</h2><span id='topic+truncate_dist'></span>

<h3>Description</h3>

<p>This function computes the mean and variance of a normal distributions that has been truncated at one or both ends.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncate_dist(a = -Inf, b = Inf, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncate_dist_+3A_a">a</code></td>
<td>
<p>Quantile (i.e., cut score) below which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_dist_+3A_b">b</code></td>
<td>
<p>Quantile (i.e., cut score) above which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_dist_+3A_mean">mean</code></td>
<td>
<p>Scalar mean or vector of means.</p>
</td></tr>
<tr><td><code id="truncate_dist_+3A_sd">sd</code></td>
<td>
<p>Scalar standard deviation or vector of standard deviations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of truncated means (column 1) and truncated variances (column 2).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>truncate_dist(a = -1, b = 3, mean = 0, sd = 1)
truncate_dist(a = 1, b = Inf, mean = 0, sd = 1)
truncate_dist(a = c(-1, 1), b = c(3, Inf), mean = 0, sd = 1)
</code></pre>

<hr>
<h2 id='truncate_mean'>Truncation function for means</h2><span id='topic+truncate_mean'></span>

<h3>Description</h3>

<p>This function computes the mean of a normal distributions that has been truncated at one or both ends.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncate_mean(a = -Inf, b = Inf, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncate_mean_+3A_a">a</code></td>
<td>
<p>Quantile (i.e., cut score) below which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_mean_+3A_b">b</code></td>
<td>
<p>Quantile (i.e., cut score) above which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_mean_+3A_mean">mean</code></td>
<td>
<p>Scalar mean or vector of means.</p>
</td></tr>
<tr><td><code id="truncate_mean_+3A_sd">sd</code></td>
<td>
<p>Scalar standard deviation or vector of standard deviations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of truncated means.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>truncate_mean(a = -1, b = 3, mean = 0, sd = 1)
truncate_mean(a = 1, b = Inf, mean = 0, sd = 1)
truncate_mean(a = c(-1, 1), b = c(3, Inf), mean = 0, sd = 1)
</code></pre>

<hr>
<h2 id='truncate_var'>Truncation function for variances</h2><span id='topic+truncate_var'></span>

<h3>Description</h3>

<p>This function computes the variance of a normal distributions that has been truncated at one or both ends.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncate_var(a = -Inf, b = Inf, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncate_var_+3A_a">a</code></td>
<td>
<p>Quantile (i.e., cut score) below which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_var_+3A_b">b</code></td>
<td>
<p>Quantile (i.e., cut score) above which scores should be censored from the distribution.</p>
</td></tr>
<tr><td><code id="truncate_var_+3A_mean">mean</code></td>
<td>
<p>Scalar mean or vector of means.</p>
</td></tr>
<tr><td><code id="truncate_var_+3A_sd">sd</code></td>
<td>
<p>Scalar standard deviation or vector of standard deviations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of truncated variances
</p>


<h3>Examples</h3>

<pre><code class='language-R'>truncate_var(a = -1, b = 3, mean = 0, sd = 1)
truncate_var(a = 1, b = Inf, mean = 0, sd = 1)
truncate_var(a = c(-1, 1), b = c(3, Inf), mean = 0, sd = 1)
</code></pre>

<hr>
<h2 id='unmix_matrix'>Estimate average within-group covariance matrices from a mixture covariance matrix</h2><span id='topic+unmix_matrix'></span>

<h3>Description</h3>

<p>Estimate average within-group covariance matrices from a mixture covariance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unmix_matrix(
  sigma_mat,
  mu_mat,
  p_vec,
  N = Inf,
  group_names = NULL,
  var_names = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unmix_matrix_+3A_sigma_mat">sigma_mat</code></td>
<td>
<p>Mixture covariance matrix.</p>
</td></tr>
<tr><td><code id="unmix_matrix_+3A_mu_mat">mu_mat</code></td>
<td>
<p>Matrix of mean parameters, with groups on the rows and variables on the columns.</p>
</td></tr>
<tr><td><code id="unmix_matrix_+3A_p_vec">p_vec</code></td>
<td>
<p>Vector of proportion of cases in each group.</p>
</td></tr>
<tr><td><code id="unmix_matrix_+3A_n">N</code></td>
<td>
<p>Optional total sample size across all groups (used to compute unbiased covariance estimates).</p>
</td></tr>
<tr><td><code id="unmix_matrix_+3A_group_names">group_names</code></td>
<td>
<p>Optional vector of group names.</p>
</td></tr>
<tr><td><code id="unmix_matrix_+3A_var_names">var_names</code></td>
<td>
<p>Optional vector of variable names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of within-group covariances and means.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>out &lt;- unmix_matrix(sigma_mat = reshape_vec2mat(.5, order = 2),
                    mu_mat = rbind(c(0, 0), c(.5, 1)),
                    p_vec =  c(.3, .7), N = 100)

## Result of unmix_matrix:
out

## Simulated data reproduce the total parameter matrix:
dat &lt;- NULL
for(i in 1:2){
     dat &lt;- rbind(dat, cbind(group = i,
                             data.frame(MASS::mvrnorm(n = round(out$p_group[i] * out$N),
                                                      mu = out$means_raw[i,],
                                                      Sigma = out$cov_group_unbiased[[i]],
                                                      empirical = TRUE))))
}
cov(dat[,-1])
</code></pre>

<hr>
<h2 id='unmix_r_2group'>Estimate the average within-group correlation from a mixture correlation for two groups</h2><span id='topic+unmix_r_2group'></span>

<h3>Description</h3>

<p>Estimate the average within-group correlation from a mixture correlation for two groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unmix_r_2group(rxy, dx, dy, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unmix_r_2group_+3A_rxy">rxy</code></td>
<td>
<p>Overall mixture correlation.</p>
</td></tr>
<tr><td><code id="unmix_r_2group_+3A_dx">dx</code></td>
<td>
<p>Standardized mean difference between groups on X.</p>
</td></tr>
<tr><td><code id="unmix_r_2group_+3A_dy">dy</code></td>
<td>
<p>Standardized mean difference between groups on Y.</p>
</td></tr>
<tr><td><code id="unmix_r_2group_+3A_p">p</code></td>
<td>
<p>Proportion of cases in one of the two groups.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mixture correlation for two groups is estimated as:
</p>
<p style="text-align: center;"><code class="reqn">r_{xy_{Mix}}\frac{\rho_{xy_{WG}}+\sqrt{d_{x}^{2}d_{y}^{2}p^{2}(1-p)^{2}}}{\sqrt{\left(d_{x}^{2}p(1-p)+1\right)\left(d_{y}^{2}p(1-p)+1\right)}}</code>
</p>

<p>where <code class="reqn">\rho_{xy_{WG}}</code> is the average within-group correlation, <code class="reqn">\rho_{xy_{Mix}}</code> is the overall mixture correlation,
<code class="reqn">d_{x}</code> is the standardized mean difference between groups on X, <code class="reqn">d_{y}</code> is the standardized mean difference between groups on Y, and
<em>p</em> is the proportion of cases in one of the two groups.
</p>


<h3>Value</h3>

<p>A vector of average within-group correlations
</p>


<h3>References</h3>

<p>Oswald, F. L., Converse, P. D., &amp; Putka, D. J. (2014). Generating race, gender and other
subgroup data in personnel selection simulations: A pervasive issue with a simple
solution. <em>International Journal of Selection and Assessment, 22</em>(3), 310-320.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>unmix_r_2group(rxy = .5, dx = 1, dy = 1, p = .5)
</code></pre>

<hr>
<h2 id='var_error_A'>Estimate the error variance of the probability-based effect size (<code class="reqn">A</code>, AUC, the common language effect size [CLES])</h2><span id='topic+var_error_A'></span><span id='topic+var_error_auc'></span><span id='topic+var_error_cles'></span>

<h3>Description</h3>

<p>Estimates the error variance of the probability-based common language effect size (<code class="reqn">A</code>, AUC, CLES)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_A(A, n1, n2 = NA)

var_error_auc(A, n1, n2 = NA)

var_error_cles(A, n1, n2 = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_A_+3A_a">A</code></td>
<td>
<p>Vector of probability-based effect sizes (common language effect sizes)</p>
</td></tr>
<tr><td><code id="var_error_A_+3A_n1">n1</code></td>
<td>
<p>Vector of sample sizes from group 1 (or the total sample size with the assumption that groups are of equal size, if no group 2 sample size is supplied).</p>
</td></tr>
<tr><td><code id="var_error_A_+3A_n2">n2</code></td>
<td>
<p>Vector of sample sizes from group 2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a <code class="reqn">A</code> (also called <em>AUC</em> [area under curve] or <em>CLES</em> [common-language effect size]) value is:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\left[\left(\frac{1}{n_{1}}\right)+\left(\frac{1}{n_{2}}\right)+\left(\frac{1}{n_{1}n_{2}}\right)\right]}{12}</code>
</p>

<p>When groups 1 and 2 are of equal size, this reduces to
</p>
<p style="text-align: center;"><code class="reqn">\frac{\left[\left(\frac{1}{n}\right)+\left(\frac{1}{n^{2}}\right)\right]}{3}</code>
</p>



<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Ruscio, J. (2008).
A probability-based measure of effect size: Robustness to base rates and other factors.
*Psychological Methods, 13*(1), 19–30. doi: <a href="https://doi.org/10.1037/1082-989X.13.1.19">10.1037/1082-989X.13.1.19</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_A(A = 1, n1 = 30, n2 = 30)
var_error_auc(A = 1, n1 = 60, n2 = NA)
var_error_cles(A = 1, n1 = 30, n2 = 30)
</code></pre>

<hr>
<h2 id='var_error_alpha'>Analytic estimate of the sampling variance of coefficient <code class="reqn">\alpha</code></h2><span id='topic+var_error_alpha'></span>

<h3>Description</h3>

<p>Estimates the error variance of Cronbach's coefficient <code class="reqn">\alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_alpha(item_mat = NULL, alpha = NULL, k_items = NULL, n_cases)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_alpha_+3A_item_mat">item_mat</code></td>
<td>
<p>Item correlation/covariance matrix. If item_mat is not supplied, the user must supply both alpha and k_items.
If item_mat is NULL, the program will assume that all item intercorrelations are equal.</p>
</td></tr>
<tr><td><code id="var_error_alpha_+3A_alpha">alpha</code></td>
<td>
<p>Vector of population <code class="reqn">\alpha</code> values. Must be supplied if item_mat is NULL.</p>
</td></tr>
<tr><td><code id="var_error_alpha_+3A_k_items">k_items</code></td>
<td>
<p>Vector of numbers of items to be simulated. Must be supplied if item_mat is NULL.</p>
</td></tr>
<tr><td><code id="var_error_alpha_+3A_n_cases">n_cases</code></td>
<td>
<p>Vector of sample sizes to simulate in sampling distribution of alpha.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of sampling variances of the supplied <code class="reqn">\alpha</code> values.
</p>


<h3>References</h3>

<p>Duhachek, A., &amp; Iacobucci, D. (2004).
Alpha’s standard error (ASE): An accurate and precise confidence interval estimate.
*Journal of Applied Psychology, 89*(5), 792–808. doi: <a href="https://doi.org/10.1037/0021-9010.89.5.792">10.1037/0021-9010.89.5.792</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>item_mat &lt;- matrix(.3, 5, 5)
diag(item_mat) &lt;- 1
alpha &lt;- mean(item_mat[lower.tri(item_mat)]) / mean(item_mat)
k_items &lt;- nrow(item_mat)

var_error_alpha(item_mat = item_mat, n_cases = 50)
var_error_alpha(alpha = alpha, k_items = k_items, n_cases = 50)
var_error_alpha(alpha = c(alpha, alpha), k_items = c(k_items, k_items), n_cases = 50)
</code></pre>

<hr>
<h2 id='var_error_d'>Estimate the error variance Cohen's <code class="reqn">d</code> values</h2><span id='topic+var_error_d'></span>

<h3>Description</h3>

<p>Estimates the error variance of standardized mean differences (Cohen's <code class="reqn">d</code> values)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_d(d, n1, n2 = NA, correct_bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_d_+3A_d">d</code></td>
<td>
<p>Vector of Cohen's <code class="reqn">d</code> values.</p>
</td></tr>
<tr><td><code id="var_error_d_+3A_n1">n1</code></td>
<td>
<p>Vector of sample sizes from group 1 (or the total sample size with the assumption that groups are of equal size, if no group 2 sample size is supplied).</p>
</td></tr>
<tr><td><code id="var_error_d_+3A_n2">n2</code></td>
<td>
<p>Vector of sample sizes from group 2.</p>
</td></tr>
<tr><td><code id="var_error_d_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in d values (TRUE) or not (FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Allows for error variance to be estimated using total sample size of both groups being compared (in this case, supply sample sizes using only the n1 argument) or
using separate sample sizes for group 1 and group 2 (i.e., the groups being compared; in this case, supply sample sizes using both the n1 and n2 arguments).
</p>
<p>The sampling variance of a <code class="reqn">d</code> value is:
</p>
<p style="text-align: center;"><code class="reqn">\left(\frac{n-1}{n-3}\right)\left(\frac{n_{1}+n_{2}}{n_{1}n_{2}}+\frac{d^{2}}{2(n_{1}+n_{2})}\right)</code>
</p>

<p>When groups 1 and 2 are of equal size, this reduces to
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\left(\frac{n-1}{n-3}\right)\left(\frac{4}{n}\right)\left(1+\frac{d^{2}}{8}\right)</code>
</p>

<p>This can be corrected for bias by first correcting the <code class="reqn">d</code> value (see <code><a href="#topic+correct_d_bias">correct_d_bias()</a></code>) prior to estimating the error variance.
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. pp. 292–295.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_d(d = 1, n1 = 30, n2 = 30, correct_bias = TRUE)
var_error_d(d = 1, n1 = 60, n2 = NA, correct_bias = TRUE)
</code></pre>

<hr>
<h2 id='var_error_delta'>Estimate the error variance of Glass's <code class="reqn">\Delta</code> values</h2><span id='topic+var_error_delta'></span>

<h3>Description</h3>

<p>Estimates the error variance of standardized mean differences (Glass's <code class="reqn">\Delta</code> values)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_delta(delta, nc, ne = NA, use_pooled_sd = FALSE, correct_bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_delta_+3A_delta">delta</code></td>
<td>
<p>Vector of Glass' <code class="reqn">\Delta</code> values.</p>
</td></tr>
<tr><td><code id="var_error_delta_+3A_nc">nc</code></td>
<td>
<p>Vector of control-group sample sizes (or the total sample size with the assumption that groups are of equal size, if no experimental-group sample size is supplied).</p>
</td></tr>
<tr><td><code id="var_error_delta_+3A_ne">ne</code></td>
<td>
<p>Vector of experimental-group sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_delta_+3A_use_pooled_sd">use_pooled_sd</code></td>
<td>
<p>Logical vector determining whether the pooled standard deviation was used ('TRUE') or not ('FALSE', default).</p>
</td></tr>
<tr><td><code id="var_error_delta_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in d values ('TRUE') or not ('FALSE').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_delta(delta = .3, nc = 30, ne = 30)
var_error_delta(delta = .3, nc = 60, ne = NA)
</code></pre>

<hr>
<h2 id='var_error_g'>Estimate the error variance Hedges's <code class="reqn">g</code> values</h2><span id='topic+var_error_g'></span>

<h3>Description</h3>

<p>Allows for error variance to be estimated using total sample size of both groups being compared (in this case, supply sample sizes using only the n1 argument) or
using separate sample sizes for group 1 and group 2 (i.e., the groups being compared; in this case, supply sample sizes using both the n1 and n2 arguments).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_g(g, n1, n2 = NA, a_method = c("gamma", "approx"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_g_+3A_g">g</code></td>
<td>
<p>Vector of Hedges's <code class="reqn">g</code> values.</p>
</td></tr>
<tr><td><code id="var_error_g_+3A_n1">n1</code></td>
<td>
<p>Vector of sample sizes from group 1 (or the total sample size with the assumption that groups are of equal size, if no group 2 sample size is supplied).</p>
</td></tr>
<tr><td><code id="var_error_g_+3A_n2">n2</code></td>
<td>
<p>Vector of sample sizes from group 2.</p>
</td></tr>
<tr><td><code id="var_error_g_+3A_a_method">a_method</code></td>
<td>
<p>Method used to correct the bias in Cohen's d to convert to Hedges's g. Options are &quot;gamma&quot; (default) for the exact method based on the gamma function (Hedges &amp; Olkin, 1985) or &quot;approx&quot; for the computationally trivial approximation (Borenstein et al., 2006).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Hedges, L. V., &amp; Olkin, I. (1985).
<em>Statistical methods for meta-analysis</em>.
Academic Press. p. 104
</p>
<p>Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. R. (2009).
<em>Introduction to meta-analysis</em>.
Wiley. p. 27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_g(g = 1, n1 = 30, n2 = 30)
var_error_g(g = 1, n1 = 60, n2 = NA)
</code></pre>

<hr>
<h2 id='var_error_mult_R'>Estimate the error variance of linear regression multiple <em>R</em>(-squared)</h2><span id='topic+var_error_mult_R'></span><span id='topic+var_error_mult_Rsq'></span><span id='topic+var_error_R'></span><span id='topic+var_error_Rsq'></span>

<h3>Description</h3>

<p>This function estimates the error variance for linear regression model (squared) multiple correlations (<code class="reqn">R</code> and <code class="reqn">R^{2}</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_mult_R(R, n, p)

var_error_mult_Rsq(Rsq, n, p)

var_error_R(R, n, p)

var_error_Rsq(Rsq, n, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_mult_R_+3A_r">R</code></td>
<td>
<p>Vector of multiple correlation coefficients.</p>
</td></tr>
<tr><td><code id="var_error_mult_R_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_mult_R_+3A_p">p</code></td>
<td>
<p>Vector of numbers of predictors in the model.</p>
</td></tr>
<tr><td><code id="var_error_mult_R_+3A_rsq">Rsq</code></td>
<td>
<p>Vector of squared multiple correlation coefficients.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a multiple correlation is approximately:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{(1-R^{2})^{2}(n-p-1)^{2}}{(n^{2}-1)(n+3)}</code>
</p>

<p>The sampling variance of a squared multiple correlation is approximately:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{4R^{2}(1-R^{2})^{2}(n-p-1)^{2}}{(n^{2}-1)(n+3)}</code>
</p>



<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2003).
<em>Applied multiple regression/correlation analysis for the behavioral sciences</em> (3rd ed.).
Lawrence Erlbaum and Associates. doi: <a href="https://doi.org/10.4324/9780203774441">10.4324/9780203774441</a>. p. 88.
</p>
<p>Olkin, I., &amp; Finn, J. D. (1995). Correlations redux.
<em>Psychological Bulletin, 118</em>(1), 155–164. doi: <a href="https://doi.org/10.1037/0033-2909.118.1.155">10.1037/0033-2909.118.1.155</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_mult_R(R = .5, n = 30, p = 4)
var_error_mult_R(R = .5, n = 30, p = 4)
var_error_mult_Rsq(Rsq = .25, n = 30, p = 4)
var_error_mult_Rsq(Rsq = .25, n = 30, p = 4)
</code></pre>

<hr>
<h2 id='var_error_q'>Estimate the error variance of square roots of reliability estimates</h2><span id='topic+var_error_q'></span>

<h3>Description</h3>

<p>Estimate error variance for square-root reliability coefficients (measure quality indices; <code class="reqn">\sqrt{r_{xx}}</code> or <code class="reqn">q_{XX}</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_q(q, n, rel_type = "alpha", k_items = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_q_+3A_q">q</code></td>
<td>
<p>Vector of square roots of reliability estimates.</p>
</td></tr>
<tr><td><code id="var_error_q_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_q_+3A_rel_type">rel_type</code></td>
<td>
<p>Character vector indicating the type(s) of reliabilities being analyzed. See documentation for <code><a href="#topic+ma_r">ma_r()</a></code> for a full list of acceptable reliability types.
NOTE: Currently, only alpha has its own dedicated error-variance estimate; the error variance of other reliability types is estimated using the generic definition of reliability as the squared correlation between observed scores and true scores.</p>
</td></tr>
<tr><td><code id="var_error_q_+3A_k_items">k_items</code></td>
<td>
<p>Optional numeric vector indicating the number of items in each scale for which reliabilities are being analyzed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of the square root of a reliability coefficient is:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{(1-q_{X}^{2})^{2}}{n-1}</code>
</p>

<p>For the equation to estimate the variance of coefficient alpha, see Duhachek and Iacobucci (2004).
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Duhachek, A., &amp; Iacobucci, D. (2004).
Alpha’s standard error (ASE): An accurate and precise confidence interval estimate.
<em>Journal of Applied Psychology, 89</em>(5), 792–808. doi: <a href="https://doi.org/10.1037/0021-9010.89.5.792">10.1037/0021-9010.89.5.792</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_q(q = .8, n = 100)
var_error_q(q = .8, n = 100, rel_type = "alpha", k_items = 10)
</code></pre>

<hr>
<h2 id='var_error_r'>Estimate the error variance of correlations</h2><span id='topic+var_error_r'></span>

<h3>Description</h3>

<p>Estimates the error variance of Pearson correlations (<code class="reqn">r</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_r(r, n, correct_bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_r_+3A_r">r</code></td>
<td>
<p>Vector of correlations.</p>
</td></tr>
<tr><td><code id="var_error_r_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_r_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in correlations (TRUE) or not (FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a Pearson correlation is approximately:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{(1-r^{2})^{2}}{n-1}</code>
</p>

<p>This can be corrected for bias in the sample correlation by first correcting the correlation (see <code><a href="#topic+correct_r_bias">correct_r_bias()</a></code>) prior to estimating the error variance.
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Schmidt, F. L., &amp; Hunter, J. E. (2015).
<em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.).
Sage. doi: <a href="https://doi.org/10.4135/9781483398105">10.4135/9781483398105</a>. p. 99.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_r(r = .3, n = 30, correct_bias = TRUE)
var_error_r(r = .3, n = 30, correct_bias = FALSE)
</code></pre>

<hr>
<h2 id='var_error_r_bvirr'>Taylor series approximation of the sampling variance of correlations corrected using the bivariate indirect range restriction correction (Case V)</h2><span id='topic+var_error_r_bvirr'></span>

<h3>Description</h3>

<p>This function propagates error in the bivariate indirect range-restriction correction formula to allow for the computation of a pseudo compound attenuation factor in individual-correction meta-analysis.
Traditional methods for estimating compound attenuation factors (i.e., dividing the observed correlation by the corrected correlation) do not work with the BVIRR correction because BVIRR has an additive term that makes the corrected correlation inappropriate for use in estimating the effect of the correction on the variance of the sampling distribution of correlations.
The equation-implied adjustment for the BVIRR correction (i.e., the first derivative of the correction equation with respect to the observed correlation) underestimates the error of corrected correlations, so this function helps to account for that additional error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_r_bvirr(
  rxyi,
  var_e = NULL,
  ni,
  na = NA,
  ux = rep(1, length(rxyi)),
  ux_observed = rep(TRUE, length(rxyi)),
  uy = rep(1, length(rxyi)),
  uy_observed = rep(TRUE, length(rxyi)),
  qx = rep(1, length(rxyi)),
  qx_restricted = rep(TRUE, length(rxyi)),
  qx_type = rep("alpha", length(rxyi)),
  k_items_x = rep(NA, length(rxyi)),
  qy = rep(1, length(rxyi)),
  qy_restricted = rep(TRUE, length(rxyi)),
  qy_type = rep("alpha", length(rxyi)),
  k_items_y = rep(NA, length(rxyi)),
  mean_rxyi = NULL,
  mean_ux = NULL,
  mean_uy = NULL,
  mean_qxa = NULL,
  mean_qya = NULL,
  var_rxyi = NULL,
  var_ux = NULL,
  var_uy = NULL,
  var_qxa = NULL,
  var_qya = NULL,
  cor_rxyi_ux = 0,
  cor_rxyi_uy = 0,
  cor_rxyi_qxa = 0,
  cor_rxyi_qya = 0,
  cor_ux_uy = 0,
  cor_ux_qxa = 0,
  cor_ux_qya = 0,
  cor_uy_qxa = 0,
  cor_uy_qya = 0,
  cor_qxa_qya = 0,
  sign_rxz = 1,
  sign_ryz = 1,
  r_deriv_only = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_r_bvirr_+3A_rxyi">rxyi</code></td>
<td>
<p>Vector of observed correlations.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_e">var_e</code></td>
<td>
<p>Vector of estimated sampling variances for rxyi values.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_ni">ni</code></td>
<td>
<p>Vector of incumbent sample sizes (necessary when variances of correlations/artifacts are not supplied).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_na">na</code></td>
<td>
<p>Optional vector of applicant sample sizes (for estimating error variance of u ratios and applicant reliabilities).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_ux">ux</code></td>
<td>
<p>Vector of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_ux_observed">ux_observed</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding ux value is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_uy">uy</code></td>
<td>
<p>Vector of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_uy_observed">uy_observed</code></td>
<td>
<p>Logical vector in which each entry specifies whether the corresponding uy value is an observed-score u ratio (<code>TRUE</code>) or a true-score u ratio. All entries are <code>TRUE</code> by default.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_qx">qx</code></td>
<td>
<p>Vector of square roots of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_qx_restricted">qx_restricted</code></td>
<td>
<p>Logical vector determining whether each element of qx is derived from an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_qx_type">qx_type</code>, <code id="var_error_r_bvirr_+3A_qy_type">qy_type</code></td>
<td>
<p>String vector identifying the types of reliability estimates supplied (e.g., &quot;alpha&quot;, &quot;retest&quot;, &quot;interrater_r&quot;, &quot;splithalf&quot;). See the documentation for <code><a href="#topic+ma_r">ma_r()</a></code> for a full list of acceptable reliability types.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_k_items_x">k_items_x</code>, <code id="var_error_r_bvirr_+3A_k_items_y">k_items_y</code></td>
<td>
<p>Numeric vector identifying the number of items in each scale.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_qy">qy</code></td>
<td>
<p>Vector of square roots of reliability estimates for X.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_qy_restricted">qy_restricted</code></td>
<td>
<p>Logical vector determining whether each element of qy is derived from an incumbent reliability (<code>TRUE</code>) or an applicant reliability (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_mean_rxyi">mean_rxyi</code></td>
<td>
<p>Mean observed correlation.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_mean_ux">mean_ux</code></td>
<td>
<p>Mean observed-score u ratio for X (for use in estimating sampling errors in the context of a meta-analysis).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_mean_uy">mean_uy</code></td>
<td>
<p>Mean observed-score u ratio for Y (for use in estimating sampling errors in the context of a meta-analysis).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_mean_qxa">mean_qxa</code></td>
<td>
<p>Mean square-root applicant reliability estimate for X (for use in estimating sampling errors in the context of a meta-analysis).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_mean_qya">mean_qya</code></td>
<td>
<p>Mean square-root applicant reliability estimate for Y (for use in estimating sampling errors in the context of a meta-analysis).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_rxyi">var_rxyi</code></td>
<td>
<p>Optional pre-specified variance of correlations.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_ux">var_ux</code></td>
<td>
<p>Optional pre-specified variance of observed-score u ratios for X.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_uy">var_uy</code></td>
<td>
<p>Optional pre-specified variance of observed-score u ratios for Y.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_qxa">var_qxa</code></td>
<td>
<p>Optional pre-specified variance of square-root applicant reliability estimate for X.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_var_qya">var_qya</code></td>
<td>
<p>Optional pre-specified variance of square-root applicant reliability estimate for Y.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_rxyi_ux">cor_rxyi_ux</code></td>
<td>
<p>Correlation between rxyi and ux (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_rxyi_uy">cor_rxyi_uy</code></td>
<td>
<p>Correlation between rxyi and uy (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_rxyi_qxa">cor_rxyi_qxa</code></td>
<td>
<p>Correlation between rxyi and qxa (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_rxyi_qya">cor_rxyi_qya</code></td>
<td>
<p>Correlation between rxyi and qya (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_ux_uy">cor_ux_uy</code></td>
<td>
<p>Correlation between ux and uy (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_ux_qxa">cor_ux_qxa</code></td>
<td>
<p>Correlation between ux and qxa (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_ux_qya">cor_ux_qya</code></td>
<td>
<p>Correlation between ux and qya (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_uy_qxa">cor_uy_qxa</code></td>
<td>
<p>Correlation between uy and qxa (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_uy_qya">cor_uy_qya</code></td>
<td>
<p>Correlation between uy and qya (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_cor_qxa_qya">cor_qxa_qya</code></td>
<td>
<p>Correlation between qxa and qya (zero by default).</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_sign_rxz">sign_rxz</code></td>
<td>
<p>Sign of the relationship between X and the selection mechanism.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_sign_ryz">sign_ryz</code></td>
<td>
<p>Sign of the relationship between Y and the selection mechanism.</p>
</td></tr>
<tr><td><code id="var_error_r_bvirr_+3A_r_deriv_only">r_deriv_only</code></td>
<td>
<p>Logical scalar determining whether to use the partial derivative with respect to rxyi only (<code>TRUE</code>) or a full Taylor series approximation of the disattenuation formula (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Per the principles of propagation of uncertainty and assuming that <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{a}}</code>, <code class="reqn">u_{X}</code>, <code class="reqn">u_{Y}</code>, and <code class="reqn">\rho_{XY_{i}}</code>, are independent, we can derive a linear approximation of the sampling error of <code class="reqn">\rho_{TP_{a}}</code>. We begin with the bivariate indirect range restriction formula,
</p>
<p style="text-align: center;"><code class="reqn">\rho_{TP_{a}}=\frac{\rho_{XY_{i}}u_{X}u_{Y}+\lambda\sqrt{\left|1-u_{X}^{2}\right|\left|1-u_{Y}^{2}\right|}}{q_{X_{a}}q_{Y_{a}}}</code>
</p>

<p>which implies the following linear approximation of the sampling variance of <code class="reqn">\rho_{TP_{a}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">SE_{\rho_{TP_{a}}}^{2}=b_{1}^{2}SE_{q_{X_{a}}}^{2}+b_{2}^{2}SE_{q_{Y_{a}}}^{2}+b_{3}^{2}SE_{u_{X}}^{2}+b_{4}^{2}SE_{u_{Y}}^{2}+b_{5}^{2}SE_{\rho_{XY_{i}}}^{2}</code>
</p>

<p>where <code class="reqn">b_{1}</code>, <code class="reqn">b_{2}</code>, <code class="reqn">b_{3}</code>, <code class="reqn">b_{4}</code>, and <code class="reqn">b_{5}</code> are the first-order partial derivatives of the disattenuation formula with respect to <code class="reqn">q_{X_{a}}</code>, <code class="reqn">q_{Y_{a}}</code>, <code class="reqn">u_{X}</code>, <code class="reqn">u_{Y}</code>, and <code class="reqn">\rho_{XY_{i}}</code>, respectively. These partial derivatives are computed as follows:
</p>
<p style="text-align: center;"><code class="reqn">b_{1}=\frac{\partial\rho_{TP_{a}}}{\partial q_{X_{a}}}=-\frac{\rho_{TP_{a}}}{q_{X_{a}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{2}=\frac{\partial\rho_{TP_{a}}}{\partial q_{Y_{a}}}=-\frac{\rho_{TP_{a}}}{q_{Y_{a}}}</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{3}=\frac{\partial\rho_{TP_{a}}}{\partial u_{X}}=\left[\rho_{XY_{i}}u_{Y}-\frac{\lambda u_{X}\left(1-u_{X}^{2}\right)\sqrt{\left|1-u_{Y}^{2}\right|}}{\left|1-u_{X}^{2}\right|^{1.5}}\right]/\left(q_{X_{a}}q_{Y_{a}}\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{4}=\frac{\partial\rho_{TP_{a}}}{\partial u_{Y}}=\left[\rho_{XY_{i}}u_{X}-\frac{\lambda u_{Y}\left(1-u_{Y}^{2}\right)\sqrt{\left|1-u_{X}^{2}\right|}}{\left|1-u_{Y}^{2}\right|^{1.5}}\right]/\left(q_{X_{a}}q_{Y_{a}}\right)</code>
</p>

<p style="text-align: center;"><code class="reqn">b_{5}=\frac{\partial\rho_{TP_{a}}}{\partial\rho_{XY_{i}}}=\frac{u_{X}u_{Y}}{q_{X_{a}}q_{Y_{a}}}</code>
</p>



<h3>Value</h3>

<p>A vector of corrected correlations' sampling-error variances.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_r_bvirr(rxyi = .3, var_e = var_error_r(r = .3, n = 100), ni = 100,
                ux = .8, uy = .8,
                qx = .9, qx_restricted = TRUE,
                qy = .9, qy_restricted = TRUE,
                sign_rxz = 1, sign_ryz = 1)
</code></pre>

<hr>
<h2 id='var_error_rel'>Estimate the error variance of reliability estimates</h2><span id='topic+var_error_rel'></span>

<h3>Description</h3>

<p>Estimate error variance for reliability coefficients (<code class="reqn">r_{XX}</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_rel(rel, n, rel_type = "alpha", k_items = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_rel_+3A_rel">rel</code></td>
<td>
<p>Vector of reliability estimates.</p>
</td></tr>
<tr><td><code id="var_error_rel_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_rel_+3A_rel_type">rel_type</code></td>
<td>
<p>Character vector indicating the type(s) of reliabilities being analyzed. See documentation for <code><a href="#topic+ma_r">ma_r()</a></code> for a full list of acceptable reliability types.
NOTE: Currently, only <code class="reqn">\alpha</code> has its own dedicated error-variance estimate; the error variance of other reliability types is estimated using the generic definition of reliability as the squared correlation between observed scores and true scores.</p>
</td></tr>
<tr><td><code id="var_error_rel_+3A_k_items">k_items</code></td>
<td>
<p>Optional numeric vector indicating the number of items in each scale for which reliabilities are being analyzed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a reliability coefficient is:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{4r_{XX}(1-r_{XX})^{2}}{n-1}</code>
</p>

<p>For the equation to estimate the variance of coefficient <code class="reqn">\alpha</code>, see Duhachek and Iacobucci (2004).
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>
<p>Duhachek, A., &amp; Iacobucci, D. (2004).
Alpha’s standard error (ASE): An accurate and precise confidence interval estimate.
<em>Journal of Applied Psychology, 89</em>(5), 792–808. doi: <a href="https://doi.org/10.1037/0021-9010.89.5.792">10.1037/0021-9010.89.5.792</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_rel(rel = .8, n = 100)
var_error_rel(rel = .8, n = 100, rel_type = "alpha", k_items = 10)
</code></pre>

<hr>
<h2 id='var_error_spearman'>Estimate the error variance of Spearman rank correlations</h2><span id='topic+var_error_spearman'></span>

<h3>Description</h3>

<p>Estimates the variance of Spearman rank correlations (<code class="reqn">\rho</code>) using the Fieller correction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_spearman(r, n, correct_bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_spearman_+3A_r">r</code></td>
<td>
<p>Vector of rank correlations.</p>
</td></tr>
<tr><td><code id="var_error_spearman_+3A_n">n</code></td>
<td>
<p>Vector of sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_spearman_+3A_correct_bias">correct_bias</code></td>
<td>
<p>Logical argument that determines whether to correct error-variance estimates for small-sample bias in correlations (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a Spearman rank correlation is approximately:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{1.06 \times (1-r^{2})^{2}}{n-1}</code>
</p>

<p>This can be corrected for bias in the sample correlation by first correcting the correlation (see <code><a href="#topic+correct_r_bias">correct_r_bias()</a></code>) prior to estimating the error variance.
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Bishara, A. J., &amp; Hittner, J. B. (2017).
Confidence intervals for correlations when data are not normal.
<em>Behavior Research Methods, 49</em>(1), 294–309. doi: <a href="https://doi.org/10.3758/s13428-016-0702-8">10.3758/s13428-016-0702-8</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_spearman(r = .3, n = 30, correct_bias = TRUE)
var_error_spearman(r = .3, n = 30, correct_bias = FALSE)
</code></pre>

<hr>
<h2 id='var_error_u'>Estimate the error variance of <code class="reqn">u</code> ratios</h2><span id='topic+var_error_u'></span>

<h3>Description</h3>

<p>Estimates the error variance of standard deviation (<code class="reqn">u</code>) ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_error_u(u, ni, na = NA, dependent_sds = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_error_u_+3A_u">u</code></td>
<td>
<p>Vector of <code class="reqn">u</code> ratios.</p>
</td></tr>
<tr><td><code id="var_error_u_+3A_ni">ni</code></td>
<td>
<p>Vector of incumbent-group sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_u_+3A_na">na</code></td>
<td>
<p>Vector of applicant-group sample sizes.</p>
</td></tr>
<tr><td><code id="var_error_u_+3A_dependent_sds">dependent_sds</code></td>
<td>
<p>Logical vector identifying whether each <code class="reqn">u</code> ratio is based on standard deviations from independent samples (<code>FALSE</code>) or based on
standard deviations from an applicant sample and an incumbent sample that is a subset of that applicant sample (<code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling variance of a u ratio is computed differently for independent samples (i.e., settings where the referent unrestricted standard deviation comes from an different sample than the range-restricted standard deviation) than for dependent samples (i.e., unrestricted samples from which a subset of individuals are selected to be in the incumbent sample).
</p>
<p>The sampling variance for independent samples (the more common case) is:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{u^{2}}{2}\left(\frac{1}{n_{i}-1}+\frac{1}{n_{a}-1}\right)</code>
</p>

<p>and the sampling variance for dependent samples is:
</p>
<p style="text-align: center;"><code class="reqn">var_{e}=\frac{u^{2}}{2}\left(\frac{1}{n_{i}-1}-\frac{1}{n_{a}-1}\right)</code>
</p>

<p>where <code class="reqn">u</code> is the u ratio, <code class="reqn">n_{i}</code> is the incumbent sample size, and <code class="reqn">n_{a}</code> is the applicant sample size.
</p>


<h3>Value</h3>

<p>A vector of sampling-error variances.
</p>


<h3>References</h3>

<p>Dahlke, J. A., &amp; Wiernik, B. M. (2020). Not restricted to selection research:
Accounting for indirect range restriction in organizational research.
<em>Organizational Research Methods, 23</em>(4), 717–749. doi: <a href="https://doi.org/10.1177/1094428119859398">10.1177/1094428119859398</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_error_u(u = .8, ni = 100, na = 200)
var_error_u(u = .8, ni = 100, na = NA)
</code></pre>

<hr>
<h2 id='warning_variance'>Warning message for invalid variances</h2><span id='topic+warning_variance'></span>

<h3>Description</h3>

<p>Warning message for invalid variances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>warning_variance(var, var_name = NULL, sd_warning = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="warning_variance_+3A_var">var</code></td>
<td>
<p>Variance object</p>
</td></tr>
<tr><td><code id="warning_variance_+3A_var_name">var_name</code></td>
<td>
<p>Name of variance object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A warning, if the supplied variance does not produce a valid standard deviation
</p>

<hr>
<h2 id='wt_cov'>Compute weighted covariances</h2><span id='topic+wt_cov'></span><span id='topic+wt_cor'></span>

<h3>Description</h3>

<p>Compute the weighted covariance among variables in a matrix or between the variables in two separate matrices/vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wt_cov(
  x,
  y = NULL,
  wt = NULL,
  as_cor = FALSE,
  use = c("everything", "listwise", "pairwise"),
  unbiased = TRUE
)

wt_cor(x, y = NULL, wt = NULL, use = "everything")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wt_cov_+3A_x">x</code></td>
<td>
<p>Vector or matrix of x variables.</p>
</td></tr>
<tr><td><code id="wt_cov_+3A_y">y</code></td>
<td>
<p>Vector or matrix of y variables</p>
</td></tr>
<tr><td><code id="wt_cov_+3A_wt">wt</code></td>
<td>
<p>Vector of weights</p>
</td></tr>
<tr><td><code id="wt_cov_+3A_as_cor">as_cor</code></td>
<td>
<p>Logical scalar that determines whether the covariances should be standardized (TRUE) or unstandardized (FALSE).</p>
</td></tr>
<tr><td><code id="wt_cov_+3A_use">use</code></td>
<td>
<p>Method for handling missing values. &quot;everything&quot; uses all values and does not account for missingness, &quot;listwise&quot; uses only complete cases, and &quot;pairwise&quot; uses pairwise deletion.</p>
</td></tr>
<tr><td><code id="wt_cov_+3A_unbiased">unbiased</code></td>
<td>
<p>Logical scalar determining whether variance should be unbiased (TRUE) or maximum-likelihood (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar, vector, or matrix of covariances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wt_cov(x = c(1, 0, 2), y = c(1, 2, 3), wt = c(1, 2, 2), as_cor = FALSE, use = "everything")
wt_cov(x = c(1, 0, 2), y = c(1, 2, 3), wt = c(1, 2, 2), as_cor = TRUE, use = "everything")
wt_cov(x = cbind(c(1, 0, 2), c(1, 2, 3)), wt = c(1, 2, 2), as_cor = FALSE, use = "everything")
wt_cov(x = cbind(c(1, 0, 2), c(1, 2, 3)), wt = c(1, 2, 2), as_cor = TRUE, use = "everything")
wt_cov(x = cbind(c(1, 0, 2, NA), c(1, 2, 3, 3)),
       wt = c(1, 2, 2, 1), as_cor = FALSE, use = "listwise")
wt_cov(x = cbind(c(1, 0, 2, NA), c(1, 2, 3, 3)),
       wt = c(1, 2, 2, 1), as_cor = TRUE, use = "listwise")
</code></pre>

<hr>
<h2 id='wt_dist'>Weighted descriptive statistics for a vector of numbers</h2><span id='topic+wt_dist'></span><span id='topic+wt_mean'></span><span id='topic+wt_var'></span>

<h3>Description</h3>

<p>Compute the weighted mean and variance of a vector of numeric values.
If no weights are supplied, defaults to computing the unweighted mean and the unweighted maximum-likelihood variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wt_dist(
  x,
  wt = rep(1, length(x)),
  unbiased = TRUE,
  df_type = c("count", "sum_wts")
)

wt_mean(x, wt = rep(1, length(x)))

wt_var(
  x,
  wt = rep(1, length(x)),
  unbiased = TRUE,
  df_type = c("count", "sum_wts")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wt_dist_+3A_x">x</code></td>
<td>
<p>Vector of values to be analyzed.</p>
</td></tr>
<tr><td><code id="wt_dist_+3A_wt">wt</code></td>
<td>
<p>Weights associated with the values in x.</p>
</td></tr>
<tr><td><code id="wt_dist_+3A_unbiased">unbiased</code></td>
<td>
<p>Logical scalar determining whether variance should be unbiased (TRUE) or maximum-likelihood (FALSE).</p>
</td></tr>
<tr><td><code id="wt_dist_+3A_df_type">df_type</code></td>
<td>
<p>Character scalar determining whether the degrees of freedom for unbiased estimates should be based on numbers of cases (&quot;count&quot;; default) or sums of weights (&quot;sum_wts&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weighted mean is computed as
</p>
<p style="text-align: center;"><code class="reqn">\bar{x}_{w}=\frac{\Sigma_{i=1}^{k}x_{i}w_{i}}{\Sigma_{i=1}^{k}w_{i}}</code>
</p>

<p>where <em>x</em> is a numeric vector and <em>w</em> is a vector of weights.
</p>
<p>The weighted variance is computed as
</p>
<p style="text-align: center;"><code class="reqn">var_{w}(x)=\frac{\Sigma_{i=1}^{k}\left(x_{i}-\bar{x}_{w}\right)^{2}w_{i}}{\Sigma_{i=1}^{k}w_{i}}</code>
</p>

<p>and the unbiased weighted variance is estimated by multiplying <code class="reqn">var_{w}(x)</code> by <code class="reqn">\frac{k}{k-1}</code>.
</p>


<h3>Value</h3>

<p>A weighted mean and variance if weights are supplied or an unweighted mean and variance if weights are not supplied.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wt_dist(x = c(.1, .3, .5), wt = c(100, 200, 300))
wt_mean(x = c(.1, .3, .5), wt = c(100, 200, 300))
wt_var(x = c(.1, .3, .5), wt = c(100, 200, 300))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
