<!DOCTYPE html><html lang="en"><head><title>Help for package rtiktoken</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rtiktoken}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#decode_tokens'><p>Decodes tokens back to text</p></a></li>
<li><a href='#get_token_count'><p>Returns the number of tokens in a text</p></a></li>
<li><a href='#get_tokens'><p>Converts text to tokens</p></a></li>
<li><a href='#model_to_tokenizer'><p>Gets the name of the tokenizer used by a model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>A Byte-Pair-Encoding (BPE) Tokenizer for OpenAI's Large Language
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.6</td>
</tr>
<tr>
<td>Description:</td>
<td>A thin wrapper around the tiktoken-rs crate, allowing to encode text into Byte-Pair-Encoding (BPE) tokens and decode tokens back to text. This is useful to understand how Large Language Models (LLMs) perceive text. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://davzim.github.io/rtiktoken/">https://davzim.github.io/rtiktoken/</a>,
<a href="https://github.com/DavZim/rtiktoken/">https://github.com/DavZim/rtiktoken/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/DavZim/rtiktoken/issues">https://github.com/DavZim/rtiktoken/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Cargo (Rust's package manager), rustc &gt;= 1.65.0</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Config/rextendr/version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/rtiktoken/MSRV:</td>
<td>1.65.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-04 12:18:08 UTC; dazimme</td>
</tr>
<tr>
<td>Author:</td>
<td>David Zimmermann-Kollenda [aut, cre],
  Roger Zurawicki [aut] (tiktoken-rs Rust library),
  Authors of the dependent Rust crates [aut] (see AUTHORS file)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Zimmermann-Kollenda &lt;david_j_zimmermann@hotmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-06 15:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='decode_tokens'>Decodes tokens back to text</h2><span id='topic+decode_tokens'></span>

<h3>Description</h3>

<p>Decodes tokens back to text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decode_tokens(tokens, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decode_tokens_+3A_tokens">tokens</code></td>
<td>
<p>a vector of tokens to decode, or a list of tokens</p>
</td></tr>
<tr><td><code id="decode_tokens_+3A_model">model</code></td>
<td>
<p>a model to use for tokenization, either a model name, e.g., <code style="white-space: pre;">&#8288;gpt-4o&#8288;</code>
or a tokenizer, e.g., <code>o200k_base</code>.
See also <a href="https://github.com/zurawiki/tiktoken-rs/blob/main/tiktoken-rs/src/tokenizer.rs">available tokenizers</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character string of the decoded tokens or a vector or strings
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model_to_tokenizer">model_to_tokenizer()</a></code>, <code><a href="#topic+get_tokens">get_tokens()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokens &lt;- get_tokens("Hello World", "gpt-4o")
tokens
decode_tokens(tokens, "gpt-4o")

tokens &lt;- get_tokens(c("Hello World", "Alice Bob Charlie"), "gpt-4o")
tokens
decode_tokens(tokens, "gpt-4o")
</code></pre>

<hr>
<h2 id='get_token_count'>Returns the number of tokens in a text</h2><span id='topic+get_token_count'></span>

<h3>Description</h3>

<p>Returns the number of tokens in a text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_token_count(text, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_token_count_+3A_text">text</code></td>
<td>
<p>a character string to encode to tokens, can be a vector</p>
</td></tr>
<tr><td><code id="get_token_count_+3A_model">model</code></td>
<td>
<p>a model to use for tokenization, either a model name, e.g., <code style="white-space: pre;">&#8288;gpt-4o&#8288;</code>
or a tokenizer, e.g., <code>o200k_base</code>.
See also <a href="https://github.com/zurawiki/tiktoken-rs/blob/main/tiktoken-rs/src/tokenizer.rs">available tokenizers</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the number of tokens in the text, vector of integers
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model_to_tokenizer">model_to_tokenizer()</a></code>, <code><a href="#topic+get_tokens">get_tokens()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_token_count("Hello World", "gpt-4o")
</code></pre>

<hr>
<h2 id='get_tokens'>Converts text to tokens</h2><span id='topic+get_tokens'></span>

<h3>Description</h3>

<p>Converts text to tokens
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tokens(text, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_tokens_+3A_text">text</code></td>
<td>
<p>a character string to encode to tokens, can be a vector</p>
</td></tr>
<tr><td><code id="get_tokens_+3A_model">model</code></td>
<td>
<p>a model to use for tokenization, either a model name, e.g., <code style="white-space: pre;">&#8288;gpt-4o&#8288;</code>
or a tokenizer, e.g., <code>o200k_base</code>.
See also <a href="https://github.com/zurawiki/tiktoken-rs/blob/main/tiktoken-rs/src/tokenizer.rs">available tokenizers</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of tokens for the given text as integer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model_to_tokenizer">model_to_tokenizer()</a></code>, <code><a href="#topic+decode_tokens">decode_tokens()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_tokens("Hello World", "gpt-4o")
get_tokens("Hello World", "o200k_base")
</code></pre>

<hr>
<h2 id='model_to_tokenizer'>Gets the name of the tokenizer used by a model</h2><span id='topic+model_to_tokenizer'></span>

<h3>Description</h3>

<p>Gets the name of the tokenizer used by a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_to_tokenizer(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="model_to_tokenizer_+3A_model">model</code></td>
<td>
<p>the model to use, e.g., <code style="white-space: pre;">&#8288;gpt-4o&#8288;</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the tokenizer used by the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model_to_tokenizer("gpt-4o")
model_to_tokenizer("gpt-4-1106-preview")
model_to_tokenizer("text-davinci-002")
model_to_tokenizer("text-embedding-ada-002")
model_to_tokenizer("text-embedding-3-small")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
