<!DOCTYPE html><html><head><title>Help for package localModel</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {localModel}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gaussian_kernel'><p>LIME kernel from the original article with sigma = 1.</p></a></li>
<li><a href='#identity_kernel'><p>LIME kernel that treats all observations as equally similar to the observation of interest.</p></a></li>
<li><a href='#individual_surrogate_model'><p>LIME-like explanations based on Ceteris Paribus curves</p></a></li>
<li><a href='#localModel'><p>localModel: LIME-like explanations with interpretable features based on Ceteris Paribus profiles</p></a></li>
<li><a href='#plot_interpretable_feature'><p>Plot Ceteris Paribus Profile and discretization</p></a></li>
<li><a href='#plot.local_surrogate_explainer'><p>Generic plot function for local surrogate explainers</p></a></li>
<li><a href='#print.local_surrogate_explainer'><p>Generic print function for local surrogate explainers</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>LIME-Based Explanations with Interpretable Inputs Based on
Ceteris Paribus Profiles</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Przemyslaw Biecek &lt;przemyslaw.biecek@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Local explanations of machine learning models describe, how features contributed to a single prediction. 
    This package implements an explanation method based on LIME 
    (Local Interpretable Model-agnostic Explanations, 
    see Tulio Ribeiro, Singh, Guestrin (2016) &lt;<a href="https://doi.org/10.1145%2F2939672.2939778">doi:10.1145/2939672.2939778</a>&gt;) in which interpretable
    inputs are created based on local rather than global behaviour of each original feature.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ModelOriented/localModel">https://github.com/ModelOriented/localModel</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ModelOriented/localModel/issues">https://github.com/ModelOriented/localModel/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, DALEX, ggplot2, partykit, ingredients</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, randomForest, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-09-03 19:29:03 UTC; pbiecek</td>
</tr>
<tr>
<td>Author:</td>
<td>Przemyslaw Biecek [aut, cre],
  Mateusz Staniak [aut],
  Krystian Igras [ctb],
  Alicja Gosiewska [ctb],
  Harel Lustiger [ctb],
  Willy Tadema [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-09-14 16:50:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='gaussian_kernel'>LIME kernel from the original article with sigma = 1.</h2><span id='topic+gaussian_kernel'></span>

<h3>Description</h3>

<p>Since only binary features are used, the weight associated with an observation
is simply exp(-{number of features that were changed compared to the original observation}).
Kernels are meant to be used as an argument to individual_surrogate_model function.
Other custom functions can be used. Such functions take two vectors and
return a single number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_kernel(explained_instance, simulated_instance)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_kernel_+3A_explained_instance">explained_instance</code></td>
<td>
<p>explained instance</p>
</td></tr>
<tr><td><code id="gaussian_kernel_+3A_simulated_instance">simulated_instance</code></td>
<td>
<p>new observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(DALEX)
library(randomForest)
library(localModel)
data('apartments')
mrf &lt;- randomForest(m2.price ~., data = apartments, ntree = 50)
explainer &lt;- explain(model = mrf,
                     data = apartments[, -1])
model_lok &lt;- individual_surrogate_model(explainer, apartments[5, -1],
                                        size = 500, seed = 17,
                                        kernel = gaussian_kernel)
# In this case each simulated observation has weight
# that is small when the distance from original observation is large,
# so closer observation have more weight.
model_lok
plot(model_lok)

</code></pre>

<hr>
<h2 id='identity_kernel'>LIME kernel that treats all observations as equally similar to the observation of interest.</h2><span id='topic+identity_kernel'></span>

<h3>Description</h3>

<p>Kernels are meant to be used as an argument to individual_surrogate_model function.
Other custom functions can be used. Such functions take two vectors and
return a single number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identity_kernel(explained_instance, simulated_instance)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="identity_kernel_+3A_explained_instance">explained_instance</code></td>
<td>
<p>explained instance</p>
</td></tr>
<tr><td><code id="identity_kernel_+3A_simulated_instance">simulated_instance</code></td>
<td>
<p>new observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(DALEX)
library(randomForest)
library(localModel)
data('apartments')
mrf &lt;- randomForest(m2.price ~., data = apartments, ntree = 50)
explainer &lt;- explain(model = mrf,
                     data = apartments[, -1])
model_lok &lt;- individual_surrogate_model(explainer, apartments[5, -1],
                                        size = 500, seed = 17,
                                        kernel = identity_kernel)
# In this case each simulated observation has equal weight
# when explanation model (LASSO) is fitted.
model_lok
plot(model_lok)

</code></pre>

<hr>
<h2 id='individual_surrogate_model'>LIME-like explanations based on Ceteris Paribus curves</h2><span id='topic+individual_surrogate_model'></span>

<h3>Description</h3>

<p>This function fits a LIME-type explanation of a single prediction.
Interpretable binary features that describe the local impact of features on
the prediction are created based on Ceteris Paribus Profiles.
Thend, a new dataset of similar observations is created and black box model
predictions (scores in case of classification) are calculated for this dataset
and LASSO regression model is fitted to them.
This way, explanations are simplified and include only the most important features.
More details about the methodology can be found in the vignettes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>individual_surrogate_model(
  x,
  new_observation,
  size,
  seed = NULL,
  kernel = identity_kernel,
  sampling = "uniform",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="individual_surrogate_model_+3A_x">x</code></td>
<td>
<p>an explainer created with the function DALEX::explain().</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_new_observation">new_observation</code></td>
<td>
<p>an observation to be explained. Columns in should correspond to columns in the data argument to x.</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_size">size</code></td>
<td>
<p>number of similar observation to be sampled.</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_seed">seed</code></td>
<td>
<p>If not NULL, seed will be set to this value for reproducibility.</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_kernel">kernel</code></td>
<td>
<p>Kernel function which will be used to weight simulated observations.</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_sampling">sampling</code></td>
<td>
<p>Parameter that controls sampling while creating new observations.</p>
</td></tr>
<tr><td><code id="individual_surrogate_model_+3A_...">...</code></td>
<td>
<p>Additional arguments that will be passed to ingredients::ceteris_paribus.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of class local_surrogate_explainer
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example based on apartments data from DALEX package.
library(DALEX)
library(randomForest)
library(localModel)
data('apartments')
mrf &lt;- randomForest(m2.price ~., data = apartments, ntree = 50)
explainer &lt;- explain(model = mrf,
                     data = apartments[, -1])
model_lok &lt;- individual_surrogate_model(explainer, apartments[5, -1],
                                        size = 500, seed = 17)
model_lok
plot(model_lok)

</code></pre>

<hr>
<h2 id='localModel'>localModel: LIME-like explanations with interpretable features based on Ceteris Paribus profiles</h2><span id='topic+localModel'></span>

<h3>Description</h3>

<p>This package implements LIME-like explanation method
(see Tulio Ribeiro, Singh, Guestrin (2016) &lt;doi:10.1145/2939672.2939778&gt;) in which interpretable
inputs are created based on local rather than global behaviour of each original feature.#'
</p>


<h3>Important functions</h3>

<p><code><a href="#topic+individual_surrogate_model">individual_surrogate_model</a></code> generates an explanation for a single prediction with
interpretable features based on Ceteris Paribus profiles.
<code><a href="#topic+plot.local_surrogate_explainer">plot.local_surrogate_explainer</a></code> plots the explanation.
</p>

<hr>
<h2 id='plot_interpretable_feature'>Plot Ceteris Paribus Profile and discretization</h2><span id='topic+plot_interpretable_feature'></span>

<h3>Description</h3>

<p>Plot Ceteris Paribus Profile and discretization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_interpretable_feature(x, variable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_interpretable_feature_+3A_x">x</code></td>
<td>
<p>local_surrogate_explainer object</p>
</td></tr>
<tr><td><code id="plot_interpretable_feature_+3A_variable">variable</code></td>
<td>
<p>chr, name of the variable to be plotted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 object
</p>

<hr>
<h2 id='plot.local_surrogate_explainer'>Generic plot function for local surrogate explainers</h2><span id='topic+plot.local_surrogate_explainer'></span>

<h3>Description</h3>

<p>Generic plot function for local surrogate explainers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'local_surrogate_explainer'
plot(x, ..., geom = "bar")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.local_surrogate_explainer_+3A_x">x</code></td>
<td>
<p>object of class local_surrogate_explainer</p>
</td></tr>
<tr><td><code id="plot.local_surrogate_explainer_+3A_...">...</code></td>
<td>
<p>other objects of class local_surrogate_explainer.
If provided, models will be plotted in rows, response levels in columns.</p>
</td></tr>
<tr><td><code id="plot.local_surrogate_explainer_+3A_geom">geom</code></td>
<td>
<p>If &quot;point&quot;, lines with points at the end will be plotted,
if &quot;bar&quot;, bars will be plotted and if &quot;arrow&quot;, arrows.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Example based on apartments data from DALEX package.
library(DALEX)
library(randomForest)
library(localModel)
data('apartments')
mrf &lt;- randomForest(m2.price ~., data = apartments, ntree = 50)
explainer &lt;- explain(model = mrf,
                     data = apartments[, -1])
model_lok &lt;- individual_surrogate_model(explainer, apartments[5, -1],
                                        size = 500, seed = 17)
model_lok
plot(model_lok)

</code></pre>

<hr>
<h2 id='print.local_surrogate_explainer'>Generic print function for local surrogate explainers</h2><span id='topic+print.local_surrogate_explainer'></span>

<h3>Description</h3>

<p>Generic print function for local surrogate explainers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'local_surrogate_explainer'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.local_surrogate_explainer_+3A_x">x</code></td>
<td>
<p>object of class local_surrogate_explainer</p>
</td></tr>
<tr><td><code id="print.local_surrogate_explainer_+3A_...">...</code></td>
<td>
<p>currently ignored</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Example based on apartments data from DALEX package.
library(DALEX)
library(randomForest)
library(localModel)
data('apartments')
mrf &lt;- randomForest(m2.price ~., data = apartments, ntree = 50)
explainer &lt;- explain(model = mrf,
                     data = apartments[, -1])
model_lok &lt;- individual_surrogate_model(explainer, apartments[5, -1],
                                        size = 500, seed = 17)
plot(model_lok)
model_lok

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
