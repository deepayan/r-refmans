<!DOCTYPE html><html lang="en-US"><head><title>Help for package hmclearn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hmclearn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#coef.hmclearn'><p>Extract Model Coefficients</p></a></li>
<li><a href='#diagplots'><p>Diagnostic plots for <code>hmclearn</code></p></a></li>
<li><a href='#diagplots.hmclearn'><p>Diagnostic plots for <code>hmclearn</code></p></a></li>
<li><a href='#Drugs'><p>Student Drug Usage Dataset</p></a></li>
<li><a href='#Endometrial'><p>Endometrial Cancer Dataset</p></a></li>
<li><a href='#Gdat'><p>Count of Fresh Gopher Tortoise Shells</p></a></li>
<li><a href='#hmc'><p>Fit a generic model using Hamiltonian Monte Carlo (HMC)</p></a></li>
<li><a href='#hmc.fit'><p>Fitter function for Hamiltonian Monte Carlo (HMC)</p></a></li>
<li><a href='#hmclearn-glm-posterior'><p>Sample log posterior and gradient functions for select generalized linear models</p>
and mixed effect models</a></li>
<li><a href='#hmclearn-plots'><p>Plotting for MCMC visualization and diagnostics provided by <code>bayesplot</code> package</p></a></li>
<li><a href='#leapfrog'><p>Leapfrog Algorithm for Hamiltonian Monte Carlo</p></a></li>
<li><a href='#mh'><p>Fit a generic model using Metropolis-Hastings (MH)</p></a></li>
<li><a href='#mh.fit'><p>Fitter function for Metropolis-Hastings (MH)</p></a></li>
<li><a href='#neff'><p>Effective sample size calculation</p></a></li>
<li><a href='#neff.hmclearn'><p>Effective sample size calculation</p></a></li>
<li><a href='#plot.hmclearn'><p>Plot Histograms of the Posterior Distribution</p></a></li>
<li><a href='#predict.hmclearn'><p>Model Predictions for HMC or MH</p></a></li>
<li><a href='#psrf'><p>Calculates Potential Scale Reduction Factor (psrf), also called the Rhat statistic,</p>
from models fit via <code>mh</code> or <code>hmc</code></a></li>
<li><a href='#psrf.hmclearn'><p>Calculates Potential Scale Reduction Factor (psrf), also called the Rhat statistic,</p>
from models fit via <code>mh</code> or <code>hmc</code></a></li>
<li><a href='#qfun'><p>Multivariate Normal Density of Theta1 | Theta2</p></a></li>
<li><a href='#qprop'><p>Simulate from Multivariate Normal Density for Metropolis Algorithm</p></a></li>
<li><a href='#summary.hmclearn'><p>Summarizing HMC Model Fits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fit Statistical Models Using Hamiltonian Monte Carlo</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.5</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Samuel Thomas &lt;samthoma@iu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provide users with a framework to learn the intricacies of the Hamiltonian Monte Carlo algorithm with hands-on experience by tuning and fitting their own models.  All of the code is written in R.  Theoretical references are listed below:.
    Neal, Radford (2011) "Handbook of Markov Chain Monte Carlo" ISBN: 978-1420079418, 
    Betancourt, Michael (2017) "A Conceptual Introduction to Hamiltonian Monte Carlo" &lt;<a href="https://doi.org/10.48550/arXiv.1701.02434">doi:10.48550/arXiv.1701.02434</a>&gt;, 
    Thomas, S., Tu, W. (2020) "Learning Hamiltonian Monte Carlo in R" &lt;<a href="https://doi.org/10.48550/arXiv.2006.16194">doi:10.48550/arXiv.2006.16194</a>&gt;,
    Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013) "Bayesian Data Analysis" ISBN: 978-1439840955, 
    Agresti, Alan (2015) "Foundations of Linear and Generalized Linear Models ISBN: 978-1118730034, 
    Pinheiro, J., Bates, D. (2006) "Mixed-effects Models in S and S-Plus" ISBN: 978-1441903174.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, Matrix, lme4, carData, mlbench, ggplot2,
mlmRev, testthat, MCMCpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>bayesplot, parallel, MASS, mvtnorm</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-10-04 22:13:27 UTC; samuelthomas</td>
</tr>
<tr>
<td>Author:</td>
<td>Samuel Thomas [cre, aut],
  Wanzhu Tu [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-10-05 06:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='coef.hmclearn'>Extract Model Coefficients</h2><span id='topic+coef.hmclearn'></span>

<h3>Description</h3>

<p>Method for <code>hmclearn</code> objects created by <code>mh</code> and <code>hmc</code> functions.  Extracts the specified quantile of the posterior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
coef(object, burnin = NULL, prob = 0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coef.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="coef.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="coef.hmclearn_+3A_prob">prob</code></td>
<td>
<p>quantile to extract coefficients</p>
</td></tr>
<tr><td><code id="coef.hmclearn_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code>quantile</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of parameter point estimates based on the given <code>prob</code>, with a default of the median estimate.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f1 &lt;- hmc(N = 500,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(f1)
coef(f1)
</code></pre>

<hr>
<h2 id='diagplots'>Diagnostic plots for <code>hmclearn</code></h2><span id='topic+diagplots'></span>

<h3>Description</h3>

<p>Plots histograms of the posterior estimates.  Optionally, displays the 'actual'
values given a simulated dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagplots(
  object,
  burnin = NULL,
  plotfun = 2,
  comparison.theta = NULL,
  cols = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diagplots_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="diagplots_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="diagplots_+3A_plotfun">plotfun</code></td>
<td>
<p>integer 1 or 2 indicating which plots to display.  1 shows trace plots.  2 shows a histogram</p>
</td></tr>
<tr><td><code id="diagplots_+3A_comparison.theta">comparison.theta</code></td>
<td>
<p>optional numeric vector of true parameter values</p>
</td></tr>
<tr><td><code id="diagplots_+3A_cols">cols</code></td>
<td>
<p>optional integer index indicating which parameters to display</p>
</td></tr>
<tr><td><code id="diagplots_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a customized <code>ggplot</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(522)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f &lt;- hmc(N = 1000,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

diagplots(f, burnin=300, comparison.theta=c(betavals, 2*log(.2)))

</code></pre>

<hr>
<h2 id='diagplots.hmclearn'>Diagnostic plots for <code>hmclearn</code></h2><span id='topic+diagplots.hmclearn'></span>

<h3>Description</h3>

<p>Plots histograms of the posterior estimates.  Optionally, displays the 'actual'
values given a simulated dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
diagplots(
  object,
  burnin = NULL,
  plotfun = 2,
  comparison.theta = NULL,
  cols = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diagplots.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="diagplots.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="diagplots.hmclearn_+3A_plotfun">plotfun</code></td>
<td>
<p>integer 1 or 2 indicating which plots to display.  1 shows trace plots.  2 shows a histogram</p>
</td></tr>
<tr><td><code id="diagplots.hmclearn_+3A_comparison.theta">comparison.theta</code></td>
<td>
<p>optional numeric vector of parameter values to compare to the Bayesian estimates</p>
</td></tr>
<tr><td><code id="diagplots.hmclearn_+3A_cols">cols</code></td>
<td>
<p>optional integer index indicating which parameters to display</p>
</td></tr>
<tr><td><code id="diagplots.hmclearn_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a customized <code>ggplot</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(522)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f &lt;- hmc(N = 1000,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

diagplots(f, burnin=300, comparison.theta=c(betavals, 2*log(.2)))

</code></pre>

<hr>
<h2 id='Drugs'>Student Drug Usage Dataset</h2><span id='topic+Drugs'></span>

<h3>Description</h3>

<p>Data from a survey of 2276 high school students about drug usage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Drugs
</code></pre>


<h3>Format</h3>

<p>A data frame with 8 rows and 4 variables:
</p>

<dl>
<dt>A</dt><dd><p>Alcohol usage (Yes/No)</p>
</dd>
<dt>C</dt><dd><p>Cigarette usage (Yes/No)</p>
</dd>
<dt>M</dt><dd><p>Marijuana usage (Yes/No)</p>
</dd>
<dt>count</dt><dd><p>number of responses</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data originally provided by Harry Khamis, Wright State University
</p>


<h3>References</h3>

<p>Agresti, A. (2015). <em>Foundations of linear and generalized linear models</em>. John Wiley &amp; Sons.  <a href="http://users.stat.ufl.edu/~aa/glm/data/Drugs.dat">http://users.stat.ufl.edu/~aa/glm/data/Drugs.dat</a>
</p>

<hr>
<h2 id='Endometrial'>Endometrial Cancer Dataset</h2><span id='topic+Endometrial'></span>

<h3>Description</h3>

<p>Data from a study about Endometrial Cancer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Endometrial
</code></pre>


<h3>Format</h3>

<p>A data frame with 79 rows and 4 variables:
</p>

<dl>
<dt>NV</dt><dd><p>Neovasculation risk factor indicator (0=Absent, 1=Present)</p>
</dd>
<dt>PI</dt><dd><p>Pulsatility index of arteria uterina</p>
</dd>
<dt>EH</dt><dd><p>Endometrium height</p>
</dd>
<dt>HG</dt><dd><p>histology of patient (0=Low, 1=High)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Heinze, G., &amp; Schemper, M. (2002). <em>A solution to the problem of separation in logistic regression</em>. Statistics in medicine, 21(16), 2409-2419.
</p>


<h3>References</h3>

<p>Agresti, A. (2015). <em>Foundations of linear and generalized linear models</em>. John Wiley &amp; Sons.  <a href="http://users.stat.ufl.edu/~aa/glm/data/Endometrial.dat">http://users.stat.ufl.edu/~aa/glm/data/Endometrial.dat</a>
</p>

<hr>
<h2 id='Gdat'>Count of Fresh Gopher Tortoise Shells</h2><span id='topic+Gdat'></span>

<h3>Description</h3>

<p>A dataset containing the count of fresh gopher shells by area.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gdat
</code></pre>


<h3>Format</h3>

<p>A data frame with 30 rows and 7 variables:
</p>

<dl>
<dt>Site</dt><dd><p>name of site</p>
</dd>
<dt>year</dt><dd><p>years 2004, 2005, 2006</p>
</dd>
<dt>shells</dt><dd><p>count of shells</p>
</dd>
<dt>type</dt><dd><p>fresh water</p>
</dd>
<dt>Area</dt><dd><p>area of the site</p>
</dd>
<dt>density</dt><dd><p>estimated tortoise density</p>
</dd>
<dt>prev</dt><dd><p>Seroprevalence to Mycoplasma agassizii</p>
</dd>
</dl>



<h3>Source</h3>

<p>Ozgul, A., Oli, M. K., Bolker, B. M., &amp; Perez-Heydrich, C. (2009). <em>Upper respiratory tract disease, force of infection, and effects on survival of gopher tortoises</em>. Ecological Applications, 19(3), 786–798
</p>


<h3>References</h3>

<p>Fox, G. A., Negrete-Yankelevich, S., &amp; Sosa, V. J. (Eds.). (2015). <em>Ecological statistics: contemporary theory and application</em>. Oxford University Press, USA.
</p>
<p>Bolker, Ben (2018) GLMM Worked Examples  <a href="https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html">https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html</a>
</p>

<hr>
<h2 id='hmc'>Fit a generic model using Hamiltonian Monte Carlo (HMC)</h2><span id='topic+hmc'></span>

<h3>Description</h3>

<p>This function runs the HMC algorithm on a generic model provided
the <code>logPOSTERIOR</code> and gradient <code>glogPOSTERIOR</code> functions.
All parameters specified within the list <code>param</code>are passed to these two functions.
The tuning parameters <code>epsilon</code> and <code>L</code> are passed to the
Leapfrog algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmc(
  N = 10000,
  theta.init,
  epsilon = 0.01,
  L = 10,
  logPOSTERIOR,
  glogPOSTERIOR,
  randlength = FALSE,
  Mdiag = NULL,
  constrain = NULL,
  verbose = FALSE,
  varnames = NULL,
  param = list(),
  chains = 1,
  parallel = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hmc_+3A_n">N</code></td>
<td>
<p>Number of MCMC samples</p>
</td></tr>
<tr><td><code id="hmc_+3A_theta.init">theta.init</code></td>
<td>
<p>Vector of initial values for the parameters</p>
</td></tr>
<tr><td><code id="hmc_+3A_epsilon">epsilon</code></td>
<td>
<p>Step-size parameter for <code>leapfrog</code></p>
</td></tr>
<tr><td><code id="hmc_+3A_l">L</code></td>
<td>
<p>Number of <code>leapfrog</code> steps parameter</p>
</td></tr>
<tr><td><code id="hmc_+3A_logposterior">logPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the log posterior given a vector of values of <code>theta</code></p>
</td></tr>
<tr><td><code id="hmc_+3A_glogposterior">glogPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the gradient of the log posterior given a vector of values of  <code>theta</code></p>
</td></tr>
<tr><td><code id="hmc_+3A_randlength">randlength</code></td>
<td>
<p>Logical to determine whether to apply some randomness to the number of leapfrog steps tuning parameter <code>L</code></p>
</td></tr>
<tr><td><code id="hmc_+3A_mdiag">Mdiag</code></td>
<td>
<p>Optional vector of the diagonal of the mass matrix <code>M</code>.  Defaults to unit diagonal.</p>
</td></tr>
<tr><td><code id="hmc_+3A_constrain">constrain</code></td>
<td>
<p>Optional vector of which parameters in <code>theta</code> accept positive values only.  Default is that all parameters accept all real numbers</p>
</td></tr>
<tr><td><code id="hmc_+3A_verbose">verbose</code></td>
<td>
<p>Logical to determine whether to display the progress of the HMC algorithm</p>
</td></tr>
<tr><td><code id="hmc_+3A_varnames">varnames</code></td>
<td>
<p>Optional vector of theta parameter names</p>
</td></tr>
<tr><td><code id="hmc_+3A_param">param</code></td>
<td>
<p>List of additional parameters for <code>logPOSTERIOR</code> and <code>glogPOSTERIOR</code></p>
</td></tr>
<tr><td><code id="hmc_+3A_chains">chains</code></td>
<td>
<p>Number of MCMC chains to run</p>
</td></tr>
<tr><td><code id="hmc_+3A_parallel">parallel</code></td>
<td>
<p>Logical to set whether multiple MCMC chains should be run in parallel</p>
</td></tr>
<tr><td><code id="hmc_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code>logPOSTERIOR</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>hmclearn</code>
</p>


<h3>Elements for <code>hmclearn</code> objects</h3>


<dl>
<dt><code>N</code></dt><dd>
<p>Number of MCMC samples
</p>
</dd>
<dt><code>theta</code></dt><dd>
<p>Nested list of length <code>N</code> of the sampled values of <code>theta</code> for each chain
</p>
</dd>
<dt><code>thetaCombined</code></dt><dd>
<p>List of dataframes containing sampled values, one for each chain
</p>
</dd>
<dt><code>r</code></dt><dd>
<p>List of length <code>N</code> of the sampled momenta
</p>
</dd>
<dt><code>theta.all</code></dt><dd>
<p>Nested list of all parameter values of <code>theta</code> sampled prior to accept/reject step for each
</p>
</dd>
<dt><code>r.all</code></dt><dd>
<p>List of all values of the momenta <code>r</code> sampled prior to accept/reject
</p>
</dd>
<dt><code>accept</code></dt><dd>
<p>Number of accepted proposals.  The ratio <code>accept</code> / <code>N</code> is the acceptance rate
</p>
</dd>
<dt><code>accept_v</code></dt><dd>
<p>Vector of length <code>N</code> indicating which samples were accepted
</p>
</dd>
<dt><code>M</code></dt><dd>
<p>Mass matrix used in the HMC algorithm
</p>
</dd>
<dt><code>algorithm</code></dt><dd>
<p><code>HMC</code> for Hamiltonian Monte Carlo
</p>
</dd>
<dt><code>varnames</code></dt><dd>
<p>Optional vector of parameter names
</p>
</dd>
<dt><code>chains</code></dt><dd>
<p>Number of MCMC chains
</p>
</dd>
</dl>



<h3>Available <code>logPOSTERIOR</code> and <code>glogPOSTERIOR</code> functions</h3>


<dl>
<dt><code>linear_posterior</code></dt><dd>
<p>Linear regression:  log posterior
</p>
</dd>
<dt><code>g_linear_posterior</code></dt><dd>
<p>Linear regression:  gradient of the log posterior
</p>
</dd>
<dt><code>logistic_posterior</code></dt><dd>
<p>Logistic regression:  log posterior
</p>
</dd>
<dt><code>g_logistic_posterior</code></dt><dd>
<p>Logistic regression:  gradient of the log posterior
</p>
</dd>
<dt><code>poisson_posterior</code></dt><dd>
<p>Poisson (count) regression:  log posterior
</p>
</dd>
<dt><code>g_poisson_posterior</code></dt><dd>
<p>Poisson (count) regression: gradient of the log posterior
</p>
</dd>
<dt><code>lmm_posterior</code></dt><dd>
<p>Linear mixed effects model:  log posterior
</p>
</dd>
<dt><code>g_lmm_posterior</code></dt><dd>
<p>Linear mixed effects model:  gradient of the log posterior
</p>
</dd>
<dt><code>glmm_bin_posterior</code></dt><dd>
<p>Logistic mixed effects model:  log posterior
</p>
</dd>
<dt><code>g_glmm_bin_posterior</code></dt><dd>
<p>Logistic mixed effects model:  gradient of the log posterior
</p>
</dd>
<dt><code>glmm_poisson_posterior</code></dt><dd>
<p>Poisson mixed effects model:  log posterior
</p>
</dd>
<dt><code>g_glmm_poisson_posterior</code></dt><dd>
<p>Poisson mixed effects model:  gradient of the log posterior
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Samuel Thomas <a href="mailto:samthoma@iu.edu">samthoma@iu.edu</a>, Wanzhu Tu <a href="mailto:wtu@iu.edu">wtu@iu.edu</a>
</p>


<h3>References</h3>

<p>Neal, Radford. 2011. <em>MCMC Using Hamiltonian Dynamics.</em> In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 116–62. Chapman; Hall/CRC.
</p>
<p>Betancourt, Michael. 2017.  <em>A Conceptual Introduction to Hamiltonian Monte Carlo</em>.
</p>
<p>Thomas, S., Tu, W. 2020. <em>Learning Hamiltonian Monte Carlo in R</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

fm1_hmc &lt;- hmc(N = 500,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(fm1_hmc, burnin=100)


# poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

fm2_hmc &lt;- hmc(N = 500,
          theta.init = rep(0, 3),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=1)

summary(fm2_hmc, burnin=100)


</code></pre>

<hr>
<h2 id='hmc.fit'>Fitter function for Hamiltonian Monte Carlo (HMC)</h2><span id='topic+hmc.fit'></span>

<h3>Description</h3>

<p>This is the basic computing function for HMC and should not be called directly except by experienced users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmc.fit(
  N,
  theta.init,
  epsilon,
  L,
  logPOSTERIOR,
  glogPOSTERIOR,
  varnames = NULL,
  randlength = FALSE,
  Mdiag = NULL,
  constrain = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hmc.fit_+3A_n">N</code></td>
<td>
<p>Number of MCMC samples</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_theta.init">theta.init</code></td>
<td>
<p>Vector of initial values for the parameters</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_epsilon">epsilon</code></td>
<td>
<p>Step-size parameter for <code>leapfrog</code></p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_l">L</code></td>
<td>
<p>Number of <code>leapfrog</code> steps parameter</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_logposterior">logPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the log posterior given a vector of values of <code>theta</code></p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_glogposterior">glogPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the gradient of the log posterior given a vector of values of  <code>theta</code></p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_varnames">varnames</code></td>
<td>
<p>Optional vector of theta parameter names</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_randlength">randlength</code></td>
<td>
<p>Logical to determine whether to apply some randomness to the number of leapfrog steps tuning parameter <code>L</code></p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_mdiag">Mdiag</code></td>
<td>
<p>Optional vector of the diagonal of the mass matrix <code>M</code>.  Defaults to unit diagonal.</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_constrain">constrain</code></td>
<td>
<p>Optional vector of which parameters in <code>theta</code> accept positive values only.  Default is that all parameters accept all real numbers</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_verbose">verbose</code></td>
<td>
<p>Logical to determine whether to display the progress of the HMC algorithm</p>
</td></tr>
<tr><td><code id="hmc.fit_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code>logPOSTERIOR</code> and <code>glogPOSTERIOR</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List for <code>hmc</code>
</p>


<h3>Elements for <code>hmclearn</code> objects</h3>


<dl>
<dt><code>N</code></dt><dd>
<p>Number of MCMC samples
</p>
</dd>
<dt><code>theta</code></dt><dd>
<p>Nested list of length <code>N</code> of the sampled values of <code>theta</code> for each chain
</p>
</dd>
<dt><code>thetaCombined</code></dt><dd>
<p>List of dataframes containing sampled values, one for each chain
</p>
</dd>
<dt><code>r</code></dt><dd>
<p>List of length <code>N</code> of the sampled momenta
</p>
</dd>
<dt><code>theta.all</code></dt><dd>
<p>Nested list of all parameter values of <code>theta</code> sampled prior to accept/reject step for each
</p>
</dd>
<dt><code>r.all</code></dt><dd>
<p>List of all values of the momenta <code>r</code> sampled prior to accept/reject
</p>
</dd>
<dt><code>accept</code></dt><dd>
<p>Number of accepted proposals.  The ratio <code>accept</code> / <code>N</code> is the acceptance rate
</p>
</dd>
<dt><code>accept_v</code></dt><dd>
<p>Vector of length <code>N</code> indicating which samples were accepted
</p>
</dd>
<dt><code>M</code></dt><dd>
<p>Mass matrix used in the HMC algorithm
</p>
</dd>
<dt><code>algorithm</code></dt><dd>
<p><code>HMC</code> for Hamiltonian Monte Carlo
</p>
</dd>
</dl>



<h3>References</h3>

<p>Neal, Radford. 2011. <em>MCMC Using Hamiltonian Dynamics.</em> In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 116–62. Chapman; Hall/CRC.
</p>
<p>Betancourt, Michael. 2017.  <em>A Conceptual Introduction to Hamiltonian Monte Carlo</em>.
</p>
<p>Thomas, S., Tu, W. 2020. <em>Learning Hamiltonian Monte Carlo in R</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Logistic regression example
X &lt;- cbind(1, seq(-100, 100, by=0.25))
betavals &lt;- c(-0.9, 0.2)
lodds &lt;- X %*% betavals
prob1 &lt;- as.numeric(1 / (1 + exp(-lodds)))

set.seed(9874)
y &lt;- sapply(prob1, function(xx) {
  sample(c(0, 1), 1, prob=c(1-xx, xx))
})

f1 &lt;- hmc.fit(N = 500,
          theta.init = rep(0, 2),
          epsilon = c(0.1, 0.002),
          L = 10,
          logPOSTERIOR = logistic_posterior,
          glogPOSTERIOR = g_logistic_posterior,
          y=y, X=X)

f1$accept / f1$N
</code></pre>

<hr>
<h2 id='hmclearn-glm-posterior'>Sample log posterior and gradient functions for select generalized linear models
and mixed effect models</h2><span id='topic+hmclearn-glm-posterior'></span><span id='topic+linear_posterior'></span><span id='topic+g_linear_posterior'></span><span id='topic+logistic_posterior'></span><span id='topic+g_logistic_posterior'></span><span id='topic+poisson_posterior'></span><span id='topic+g_poisson_posterior'></span><span id='topic+lmm_posterior'></span><span id='topic+g_lmm_posterior'></span><span id='topic+glmm_bin_posterior'></span><span id='topic+g_glmm_bin_posterior'></span><span id='topic+glmm_poisson_posterior'></span><span id='topic+g_glmm_poisson_posterior'></span>

<h3>Description</h3>

<p>These functions can be used to fit common generalized linear models and mixed effect models.
See the accompanying vignettes for details on the derivations of the log posterior and gradient.
In addition, these functions can be used as templates to build custom models to fit using HMC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, sig2beta = 1000)

g_linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, sig2beta = 1000)

logistic_posterior(theta, y, X, sig2beta = 1000)

g_logistic_posterior(theta, y, X, sig2beta = 1000)

poisson_posterior(theta, y, X, sig2beta = 1000)

g_poisson_posterior(theta, y, X, sig2beta = 1000)

lmm_posterior(
  theta,
  y,
  X,
  Z,
  n,
  d,
  nrandom = 1,
  nugamma = 1,
  nuxi = 1,
  Agamma = 25,
  Axi = 25,
  sig2beta = 1000
)

g_lmm_posterior(
  theta,
  y,
  X,
  Z,
  n,
  d,
  nrandom = 1,
  nugamma = 1,
  nuxi = 1,
  Agamma = 25,
  Axi = 25,
  sig2beta = 1000
)

glmm_bin_posterior(
  theta,
  y,
  X,
  Z,
  n,
  nrandom = 1,
  nuxi = 1,
  Axi = 25,
  sig2beta = 1000
)

g_glmm_bin_posterior(
  theta,
  y,
  X,
  Z,
  n,
  nrandom = 1,
  nuxi = 1,
  Axi = 25,
  sig2beta = 1000
)

glmm_poisson_posterior(
  theta,
  y,
  X,
  Z,
  n,
  nrandom = 1,
  nuxi = 1,
  Axi = 25,
  sig2beta = 1000
)

g_glmm_poisson_posterior(
  theta,
  y,
  X,
  Z,
  n,
  nrandom = 1,
  nuxi = 1,
  Axi = 25,
  sig2beta = 1000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hmclearn-glm-posterior_+3A_theta">theta</code></td>
<td>
<p>vector of parameters.  See details below for the order of parameters for each model</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_y">y</code></td>
<td>
<p>numeric vector for the dependent variable for all models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_x">X</code></td>
<td>
<p>numeric design matrix of fixed effect parameters for all models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_a">a</code></td>
<td>
<p>hyperparameter for the Inverse Gamma shape parameter for <code class="reqn">\sigma_\epsilon</code> in linear regression models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_b">b</code></td>
<td>
<p>hyperparameter for the Inverse Gamma scale parameter for <code class="reqn">\sigma_\epsilon</code> in linear regression models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_sig2beta">sig2beta</code></td>
<td>
<p>diagonal covariance of prior for linear predictors is multivariate normal with mean 0 for linear regression and linear mixed effect models.</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_z">Z</code></td>
<td>
<p>numeric design matrix of random effect parameters for all mixed effects models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_n">n</code></td>
<td>
<p>number of observations for standard glm models, or number of subjects for all mixed effect models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_d">d</code></td>
<td>
<p>number of observations per subject for mixed effects models, but an input for linear mixed effect models only.</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_nrandom">nrandom</code></td>
<td>
<p>number of random effects covariance parameters for all mixed effects models</p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_nugamma">nugamma</code></td>
<td>
<p>hyperparameter <code class="reqn">\nu</code> for the half-t prior of the log transformed error for linear mixed effects model <code class="reqn">\gamma</code></p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_nuxi">nuxi</code></td>
<td>
<p>hyperparameter <code class="reqn">\nu</code> for the half-t prior of the random effects diagonal for all mixed effects models <code class="reqn">\xi</code></p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_agamma">Agamma</code></td>
<td>
<p>hyperparameter <code class="reqn">A</code> for the half-t prior of the log transformed error for linear mixed effects model <code class="reqn">\gamma</code></p>
</td></tr>
<tr><td><code id="hmclearn-glm-posterior_+3A_axi">Axi</code></td>
<td>
<p>hyperparameter <code class="reqn">A</code> for the half-t prior of the random effects diagonal for all mixed effects models<code class="reqn">\xi</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector for the log posterior or gradient of the log posterior
</p>


<h3>Generalized Linear Models with available posterior and gradient functions</h3>


<dl>
<dt>'linear_posterior(theta, y, X, a=1e-4, b=1e-4, sig2beta = 1e3)'</dt><dd>
<p>The log posterior function for linear regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, \beta, \sigma) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}</code>
</p>

<p>with priors <code class="reqn">p(\sigma^2) \sim IG(a, b)</code> and <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.  The variance term is log transformed <code class="reqn">\gamma = \log\sigma</code>
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The first <code class="reqn">k-1</code> parameters are for <code class="reqn">\beta</code>, and the last parameter is <code class="reqn">\gamma</code>
Note that the Inverse Gamma prior can be problematic for certain applications with low variance, such as hierarchical models.  See Gelman (2006)
</p>
</dd>
<dt>'g_linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, sig2beta=1e3)'</dt><dd>
<p>Gradient of the log posterior for a linear regression model with Normal prior for the linear parameters and Inverse Gamma for the error term.
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, \beta, \sigma) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}</code>
</p>

<p>with priors <code class="reqn">p(\sigma^2) \sim IG(a, b)</code> and <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.  The variance term is log transformed <code class="reqn">\gamma = \log\sigma</code>
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The first <code class="reqn">k-1</code> parameters are for <code class="reqn">\beta</code>, and the last parameter is <code class="reqn">\gamma</code>
Note that the Inverse Gamma prior can be problematic for certain applications with low variance, such as hierarchical models.  See Gelman (2006)
</p>
</dd>
<dt>'logistic_posterior(theta, y, X, sig2beta=1e3) '</dt><dd>
<p>Log posterior for a logistic regression model with Normal prior for the linear parameters.
The likelihood function for logistic regression
</p>
<p style="text-align: center;"><code class="reqn">f(\beta| X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>, containing parameter values for <code class="reqn">\beta</code>
</p>
</dd>
<dt>'g_logistic_posterior(theta, y, X, sig2beta=1e3) '</dt><dd>
<p>Gradient of the log posterior for a logistic regression model with Normal prior for the linear parameters.
The likelihood function for logistic regression
</p>
<p style="text-align: center;"><code class="reqn">f(\beta| X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>, containing parameter values for <code class="reqn">\beta</code>
</p>
</dd>
<dt>'poisson_posterior(theta, y, X, sig2beta=1e3) '</dt><dd>
<p>Log posterior for a Poisson regression model with Normal prior for the linear parameters.
The likelihood function for poisson regression
</p>
<p style="text-align: center;"><code class="reqn">f(\beta| y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>, containing parameter values for <code class="reqn">\beta</code>
</p>
</dd>
<dt>'g_poisson_posterior(theta, y, X, sig2beta=1e3) '</dt><dd>
<p>Gradient of the log posterior for a Poisson regression model with Normal prior for the linear parameters.
The likelihood function for poisson regression
</p>
<p style="text-align: center;"><code class="reqn">f(\beta| y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>.
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>, containing parameter values for <code class="reqn">\beta</code>
</p>
</dd>
</dl>



<h3>Generalized Linear Mixed Effect with available posterior and gradient functions</h3>


<dl>
<dt>'lmm_posterior(theta, y, X, Z, n, d, nrandom = 1, nueps = 1, nuxi = 1, Aeps = 25, Axi = 25, sig2beta = 1e3) '</dt><dd>
<p>The log posterior function for linear mixed effects regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | \beta, u, \sigma_\epsilon) \propto (\sigma_\epsilon^2)^{-nd/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t</code>.
The vector <code class="reqn">\xi</code> is the diagonal of the covariance <code class="reqn">G</code> log transformed hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The standard deviation of the error is log transformed, where <code class="reqn">\gamma = \log \sigma_\epsilon</code> and <code class="reqn">\sigma_\epsilon \sim half-t</code>. The parameters for <code class="reqn">\gamma</code> are <code class="reqn">A_\gamma, \nu_\gamma</code>
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \gamma, \xi</code>.
</p>
</dd>
<dt>'g_lmm_posterior(theta, y, X, Z, n, d, nrandom = 1, nueps = 1, nuxi = 1, Aeps = 25, Axi = 25, sig2beta = 1e3)'</dt><dd>
<p>Gradient of the log posterior for a linear mixed effects regression model
</p>
<p style="text-align: center;"><code class="reqn">f(y | \beta, u, \sigma_\epsilon) \propto (\sigma_\epsilon^2)^{-n/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)}</code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t</code>.
The vector <code class="reqn">\xi</code> is the diagonal of the covariance <code class="reqn">G</code> log transformed hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The standard deviation of the error is log transformed, where <code class="reqn">\gamma = \log \sigma_\epsilon</code> and <code class="reqn">\sigma_\epsilon \sim half-t</code>. The parameters for <code class="reqn">\gamma</code> are <code class="reqn">A_\gamma, \nu_\gamma</code>
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \gamma, \xi</code>
</p>
</dd>
<dt>'glmm_bin_posterior(theta, y, X, Z, n, nrandom = 1, nuxi = 1, Axi = 25, sig2beta=1e3)'</dt><dd>
<p>The log posterior function for logistic mixed effects regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, Z, \beta, u) = \prod_{i=1}^n\prod_{j=1}^d \left(\frac{1}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{y_{ij}} \left(\frac{e^{-X_i\beta - Z_{ij}u_i}}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{1-y_{ij}} </code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t(A_\lambda, nu_\lambda )</code>.
The vector <code class="reqn">\lambda</code> is the diagonal of the covariance <code class="reqn">G</code> hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \xi</code>
</p>
</dd>
<dt>'g_glmm_bin_posterior(theta, y, X, Z, n, nrandom = 1, nuxi = 1, Axi = 25, sig2beta = 1e3) '</dt><dd>
<p>Gradient of the log posterior function for logistic mixed effects regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, Z, \beta, u) = \prod_{i=1}^n\prod_{j=1}^m \left(\frac{1}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{y_{ij}} \left(\frac{e^{-X_i\beta - Z_{ij}u_i}}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{1-y_{ij}} </code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t(A_\lambda, nu_\lambda )</code>.
The vector <code class="reqn">\lambda</code> is the diagonal of the covariance <code class="reqn">G</code> hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \xi</code>
</p>
</dd>
<dt>'glmm_poisson_posterior(theta, y, X, Z, n, nrandom = 1, nuxi = 1, Axi = 25, sig2beta = 1e3) '</dt><dd>
<p>Log posterior for a Poisson mixed effect regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, Z, \beta, u) = \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} </code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t(A_\lambda, nu_\lambda )</code>.
The vector <code class="reqn">\lambda</code> is the diagonal of the covariance <code class="reqn">G</code> hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \xi</code>
</p>
</dd>
<dt>'g_glmm_poisson_posterior(theta, y, X, Z, n, nrandom = 1, nuxi = 1, Axi = 25, sig2beta = 1e3) '</dt><dd>
<p>Gradient of the log posterior for a Poisson mixed effect regression
</p>
<p style="text-align: center;"><code class="reqn">f(y | X, Z, \beta, u) = \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} </code>
</p>

<p>with priors <code class="reqn">\beta \sim N(0, \sigma_\beta^2 I)</code>, <code class="reqn">\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)</code>, <code class="reqn">\lambda \sim half-t(A_\lambda, nu_\lambda )</code>.
The vector <code class="reqn">\lambda</code> is the diagonal of the covariance <code class="reqn">G</code> hyperprior where <code class="reqn">u \sim N(0, G</code>, <code class="reqn">\xi = \log\lambda</code> and <code class="reqn">A_\xi, \nu_\xi</code> are parameters for the transformed distribution
The input parameter vector <code class="reqn">theta</code> is of length <code class="reqn">k</code>.  The order of parameters for the vector is <code class="reqn">\beta, \tau, \xi</code>
</p>
</dd>
</dl>



<h3>References</h3>

<p>Gelman, A. (2006). <em>Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)</em>. Bayesian analysis, 1(3), 515-534.
</p>
<p>Chan, J. C. C., &amp; Jeliazkov, I. (2009). <em>MCMC estimation of restricted covariance matrices</em>. Journal of Computational and Graphical Statistics, 18(2), 457-480.
</p>
<p>Betancourt, M., &amp; Girolami, M. (2015). <em>Hamiltonian Monte Carlo for hierarchical models</em>. Current trends in Bayesian methodology with applications, 79, 30.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f1_hmc &lt;- hmc(N = 500,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(f1_hmc, burnin=100)


# poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f2_hmc &lt;- hmc(N = 500,
          theta.init = rep(0, 3),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=1)

</code></pre>

<hr>
<h2 id='hmclearn-plots'>Plotting for MCMC visualization and diagnostics provided by <code>bayesplot</code> package</h2><span id='topic+hmclearn-plots'></span><span id='topic+mcmc_intervals'></span><span id='topic+mcmc_intervals.hmclearn'></span><span id='topic+mcmc_areas'></span><span id='topic+mcmc_areas.hmclearn'></span><span id='topic+mcmc_hist'></span><span id='topic+mcmc_hist.hmclearn'></span><span id='topic+mcmc_hist_by_chain'></span><span id='topic+mcmc_hist_by_chain.hmclearn'></span><span id='topic+mcmc_dens'></span><span id='topic+mcmc_dens.hmclearn'></span><span id='topic+mcmc_scatter'></span><span id='topic+mcmc_scatter.hmclearn'></span><span id='topic+mcmc_hex'></span><span id='topic+mcmc_hex.hmclearn'></span><span id='topic+mcmc_pairs'></span><span id='topic+mcmc_pairs.hmclearn'></span><span id='topic+mcmc_acf'></span><span id='topic+mcmc_acf.hmclearn'></span><span id='topic+mcmc_acf_bar'></span><span id='topic+mcmc_acf_bar.hmclearn'></span><span id='topic+mcmc_trace'></span><span id='topic+mcmc_trace.hmclearn'></span><span id='topic+mcmc_rhat'></span><span id='topic+mcmc_rhat.hmclearn'></span><span id='topic+mcmc_rhat_hist'></span><span id='topic+mcmc_rhat_hist.hmclearn'></span><span id='topic+mcmc_neff'></span><span id='topic+mcmc_neff.hmclearn'></span><span id='topic+mcmc_neff_hist'></span><span id='topic+mcmc_neff_hist.hmclearn'></span><span id='topic+mcmc_neff_data'></span><span id='topic+mcmc_neff_data.hmclearn'></span><span id='topic+mcmc_violin'></span><span id='topic+mcmc_violin.hmclearn'></span>

<h3>Description</h3>

<p>Plots of Rhat statistics, ratios of effective sample size to total sample
size, and autocorrelation of MCMC draws.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_intervals(object, ...)

## S3 method for class 'hmclearn'
mcmc_intervals(object, burnin = NULL, ...)

mcmc_areas(object, ...)

## S3 method for class 'hmclearn'
mcmc_areas(object, burnin = NULL, ...)

mcmc_hist(object, ...)

## S3 method for class 'hmclearn'
mcmc_hist(object, burnin = NULL, ...)

mcmc_hist_by_chain(object, ...)

## S3 method for class 'hmclearn'
mcmc_hist_by_chain(object, burnin = NULL, ...)

mcmc_dens(object, ...)

## S3 method for class 'hmclearn'
mcmc_dens(object, burnin = NULL, ...)

mcmc_scatter(object, ...)

## S3 method for class 'hmclearn'
mcmc_scatter(object, burnin = NULL, ...)

mcmc_hex(object, ...)

## S3 method for class 'hmclearn'
mcmc_hex(object, burnin = NULL, ...)

mcmc_pairs(object, ...)

## S3 method for class 'hmclearn'
mcmc_pairs(object, burnin = NULL, ...)

mcmc_acf(object, ...)

## S3 method for class 'hmclearn'
mcmc_acf(object, burnin = NULL, ...)

mcmc_acf_bar(object, ...)

## S3 method for class 'hmclearn'
mcmc_acf_bar(object, burnin = NULL, ...)

mcmc_trace(object, ...)

## S3 method for class 'hmclearn'
mcmc_trace(object, burnin = NULL, ...)

mcmc_rhat(object, ...)

## S3 method for class 'hmclearn'
mcmc_rhat(object, burnin = NULL, ...)

mcmc_rhat_hist(object, ...)

## S3 method for class 'hmclearn'
mcmc_rhat_hist(object, burnin = NULL, ...)

mcmc_neff(object, ...)

## S3 method for class 'hmclearn'
mcmc_neff(object, burnin = NULL, lagmax = NULL, ...)

mcmc_neff_hist(object, ...)

## S3 method for class 'hmclearn'
mcmc_neff_hist(object, burnin = NULL, lagmax = NULL, ...)

mcmc_neff_data(object, ...)

## S3 method for class 'hmclearn'
mcmc_neff_data(object, burnin = NULL, lagmax = NULL, ...)

mcmc_violin(object, ...)

## S3 method for class 'hmclearn'
mcmc_violin(object, burnin = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hmclearn-plots_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="hmclearn-plots_+3A_...">...</code></td>
<td>
<p>optional additional arguments to pass to the <code>bayesplot</code> functions</p>
</td></tr>
<tr><td><code id="hmclearn-plots_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="hmclearn-plots_+3A_lagmax">lagmax</code></td>
<td>
<p>maximum lag to extract for determining effective sample sizes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>These functions call various plotting functions from the <code>bayesplot</code> package, which returns a list including <code>ggplot2</code> objects.
</p>


<h3>Plot Descriptions from the <code>bayesplot</code> package documentation</h3>


<dl>
<dt>'mcmc_hist(object, burnin=NULL, ...)'</dt><dd>
<p>Default plot called by 'plot' function.  Histograms of posterior draws with all chains merged.
</p>
</dd>
<dt>'mcmc_dens(object, burnin=NULL, ...)'</dt><dd>
<p>Kernel density plots of posterior draws with all chains merged.
</p>
</dd>
<dt>'mcmc_hist_by_chain(object, burnin=NULL, ...)'</dt><dd>
<p>Histograms of posterior draws with chains separated via faceting.
</p>
</dd>
<dt>'mcmc_dens_overlay(object, burnin=NULL, ...)'</dt><dd>
<p>Kernel density plots of posterior draws with chains separated but
overlaid on a single plot.
</p>
</dd>
<dt>'mcmc_violin(object, burnin=NULL, ...)'</dt><dd>
<p>The density estimate of each chain is plotted as a violin with
horizontal lines at notable quantiles.
</p>
</dd>
<dt>'mcmc_dens_chains(object, burnin=NULL, ...)'</dt><dd>
<p>Ridgeline kernel density plots of posterior draws with chains separated
but overlaid on a single plot. In 'mcmc_dens_overlay()' parameters
appear in separate facets; in 'mcmc_dens_chains()' they appear in the
same panel and can overlap vertically.
</p>
</dd>
<dt>'mcmc_intervals(object, burnin=NULL, ...)'</dt><dd>
<p>Plots of uncertainty intervals computed from posterior draws with all
chains merged.
</p>
</dd>
<dt>'mcmc_areas(object, burnin=NULL, ...)'</dt><dd>
<p>Density plots computed from posterior draws with all chains merged,
with uncertainty intervals shown as shaded areas under the curves.
</p>
</dd>
<dt>'mcmc_scatter(object, burnin=NULL, ...)'</dt><dd>
<p>Bivariate scatterplot of posterior draws. If using a very large number of
posterior draws then 'mcmc_hex()' may be preferable to avoid
overplotting.
</p>
</dd>
<dt>'mcmc_hex(object, burnin=NULL, ...)'</dt><dd>
<p>Hexagonal heatmap of 2-D bin counts. This plot is useful in cases where
the posterior sample size is large enough that 'mcmc_scatter()' suffers
from overplotting.
</p>
</dd>
<dt>'mcmc_pairs(object, burnin=NULL, ...)'</dt><dd>
<p>A square plot matrix with univariate marginal distributions along the
diagonal (as histograms or kernel density plots) and bivariate
distributions off the diagonal (as scatterplots or hex heatmaps).
</p>
<p>For the off-diagonal plots, the default is to split the chains so that
(roughly) half are displayed above the diagonal and half are below (all
chains are always merged together for the plots along the diagonal). Other
possibilities are available by setting the 'condition' argument.
</p>
</dd>
<dt>'mcmc_rhat(object, burnin=NULL, ...)', 'mcmc_rhat_hist(object, burnin=NULL, ...)'</dt><dd>
<p>Rhat values as either points or a histogram. Values are colored using
different shades (lighter is better). The chosen thresholds are somewhat
arbitrary, but can be useful guidelines in practice.
* _light_: below 1.05 (good)
* _mid_: between 1.05 and 1.1 (ok)
* _dark_: above 1.1 (too high)
</p>
</dd>
<dt>'mcmc_neff(object, burnin=NULL, ...)', 'mcmc_neff_hist(object, burnin=NULL, ...)'</dt><dd>
<p>Ratios of effective sample size to total sample size as either points or a
histogram. Values are colored using different shades (lighter is better).
The chosen thresholds are somewhat arbitrary, but can be useful guidelines
in practice.
* _light_: between 0.5 and 1 (high)
* _mid_: between 0.1 and 0.5 (good)
* _dark_: below 0.1 (low)
</p>
</dd>
<dt>'mcmc_acf(object, burnin=NULL, ...)', 'mcmc_acf_bar(object, burnin=NULL, ...)'</dt><dd>
<p>Grid of autocorrelation plots by chain and parameter. The 'lags' argument
gives the maximum number of lags at which to calculate the autocorrelation
function. 'mcmc_acf()' is a line plot whereas 'mcmc_acf_bar()' is a
barplot.
</p>
</dd>
</dl>



<h3>References</h3>

<p>Gabry, Jonah and Mahr, Tristan (2019).  <em>bayesplot:  Plotting for Bayesian Models</em>.  <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>
</p>
<p>Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., and Gelman, A (2019).  <em>Visualization in Bayesian Workflow</em>.  Journal of the Royal Statistical Society: Series A. Vol 182.  Issue 2.  p.389-402.
</p>
<p>Gelman, A. and Rubin, D. (1992) <em>Inference from Iterative Simulation Using Multiple Sequences</em>.  Statistical Science 7(4) 457-472.
</p>
<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = c(0.03, 0.02, 0.015),
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

mcmc_trace(f, burnin=100)
mcmc_hist(f, burnin=100)
mcmc_intervals(f, burnin=100)
mcmc_rhat(f, burnin=100)
mcmc_violin(f, burnin=100)
</code></pre>

<hr>
<h2 id='leapfrog'>Leapfrog Algorithm for Hamiltonian Monte Carlo</h2><span id='topic+leapfrog'></span>

<h3>Description</h3>

<p>Runs a single iteration of the leapfrog algorithm.  Typically called directly from <code>hmc</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leapfrog(
  theta_lf,
  r,
  epsilon,
  glogPOSTERIOR,
  Minv,
  constrain,
  lastSTEP = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leapfrog_+3A_theta_lf">theta_lf</code></td>
<td>
<p>starting parameter vector</p>
</td></tr>
<tr><td><code id="leapfrog_+3A_r">r</code></td>
<td>
<p>starting momentum vector</p>
</td></tr>
<tr><td><code id="leapfrog_+3A_epsilon">epsilon</code></td>
<td>
<p>Step-size parameter for <code>leapfrog</code></p>
</td></tr>
<tr><td><code id="leapfrog_+3A_glogposterior">glogPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the gradient of the log posterior given a vector of values of <code>theta</code></p>
</td></tr>
<tr><td><code id="leapfrog_+3A_minv">Minv</code></td>
<td>
<p>Inverse Mass matrix</p>
</td></tr>
<tr><td><code id="leapfrog_+3A_constrain">constrain</code></td>
<td>
<p>Optional vector of which parameters in <code>theta</code> accept positive values only.  Default is that all parameters accept all real numbers</p>
</td></tr>
<tr><td><code id="leapfrog_+3A_laststep">lastSTEP</code></td>
<td>
<p>Boolean indicating whether to calculate the last half-step of the momentum update</p>
</td></tr>
<tr><td><code id="leapfrog_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to glogPOSTERIOR</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing two elements:  <code>theta.new</code> the ending value of theta and <code>r.new</code> the ending value of the momentum
</p>


<h3>References</h3>

<p>Neal, Radford. 2011. <em>MCMC Using Hamiltonian Dynamics.</em> In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 116–62. Chapman; Hall/CRC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(321)
X &lt;- cbind(1, rnorm(10))
y &lt;- rnorm(10)
p &lt;- runif(3) - 0.5
leapfrog(rep(0,3), p, 0.01, g_linear_posterior,
         diag(3), FALSE, X=X, y=y)
</code></pre>

<hr>
<h2 id='mh'>Fit a generic model using Metropolis-Hastings (MH)</h2><span id='topic+mh'></span>

<h3>Description</h3>

<p>This function runs the MH algorithm on a generic model provided
the <code>logPOSTERIOR</code> function.
All parameters specified within the list <code>param</code> are passed to these the posterior function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mh(
  N,
  theta.init,
  qPROP,
  qFUN,
  logPOSTERIOR,
  nu = 0.001,
  varnames = NULL,
  param = list(),
  chains = 1,
  parallel = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mh_+3A_n">N</code></td>
<td>
<p>Number of MCMC samples</p>
</td></tr>
<tr><td><code id="mh_+3A_theta.init">theta.init</code></td>
<td>
<p>Vector of initial values for the parameters</p>
</td></tr>
<tr><td><code id="mh_+3A_qprop">qPROP</code></td>
<td>
<p>Function to generate proposal</p>
</td></tr>
<tr><td><code id="mh_+3A_qfun">qFUN</code></td>
<td>
<p>Probability for proposal function.  First argument is where to evaluate, and second argument is the conditional parameter</p>
</td></tr>
<tr><td><code id="mh_+3A_logposterior">logPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the log posterior given a vector of values of <code>theta</code></p>
</td></tr>
<tr><td><code id="mh_+3A_nu">nu</code></td>
<td>
<p>Single value or vector parameter passed to <code>qPROP</code> or <code>qFUN</code> for the proposal density</p>
</td></tr>
<tr><td><code id="mh_+3A_varnames">varnames</code></td>
<td>
<p>Optional vector of theta parameter names</p>
</td></tr>
<tr><td><code id="mh_+3A_param">param</code></td>
<td>
<p>List of additional parameters for <code>logPOSTERIOR</code> and <code>glogPOSTERIOR</code></p>
</td></tr>
<tr><td><code id="mh_+3A_chains">chains</code></td>
<td>
<p>Number of MCMC chains to run</p>
</td></tr>
<tr><td><code id="mh_+3A_parallel">parallel</code></td>
<td>
<p>Logical to set whether multiple MCMC chains should be run in parallel</p>
</td></tr>
<tr><td><code id="mh_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code>logPOSTERIOR</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>hmclearn</code>
</p>


<h3>Elements for <code>hmclearn</code> objects</h3>


<dl>
<dt><code>N</code></dt><dd>
<p>Number of MCMC samples
</p>
</dd>
<dt><code>theta</code></dt><dd>
<p>Nested list of length <code>N</code> of the sampled values of <code>theta</code> for each chain
</p>
</dd>
<dt><code>thetaCombined</code></dt><dd>
<p>List of dataframes containing sampled values, one for each chain
</p>
</dd>
<dt><code>r</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>theta.all</code></dt><dd>
<p>Nested list of all parameter values of <code>theta</code> sampled prior to accept/reject step for each
</p>
</dd>
<dt><code>r.all</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>accept</code></dt><dd>
<p>Number of accepted proposals.  The ratio <code>accept</code> / <code>N</code> is the acceptance rate
</p>
</dd>
<dt><code>accept_v</code></dt><dd>
<p>Vector of length <code>N</code> indicating which samples were accepted
</p>
</dd>
<dt><code>M</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>algorithm</code></dt><dd>
<p><code>MH</code> for Metropolis-Hastings
</p>
</dd>
<dt><code>varnames</code></dt><dd>
<p>Optional vector of parameter names
</p>
</dd>
<dt><code>chains</code></dt><dd>
<p>Number of MCMC chains
</p>
</dd>
</dl>



<h3>Available <code>logPOSTERIOR</code> functions</h3>


<dl>
<dt><code>linear_posterior</code></dt><dd>
<p>Linear regression:  log posterior
</p>
</dd>
<dt><code>logistic_posterior</code></dt><dd>
<p>Logistic regression:  log posterior
</p>
</dd>
<dt><code>poisson_posterior</code></dt><dd>
<p>Poisson (count) regression:  log posterior
</p>
</dd>
<dt><code>lmm_posterior</code></dt><dd>
<p>Linear mixed effects model:  log posterior
</p>
</dd>
<dt><code>glmm_bin_posterior</code></dt><dd>
<p>Logistic mixed effects model:  log posterior
</p>
</dd>
<dt><code>glmm_poisson_posterior</code></dt><dd>
<p>Poisson mixed effects model:  log posterior
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Samuel Thomas <a href="mailto:samthoma@iu.edu">samthoma@iu.edu</a>, Wanzhu Tu <a href="mailto:wtu@iu.edu">wtu@iu.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f1_mh &lt;- mh(N = 3e3,
         theta.init = c(rep(0, 4), 1),
         nu &lt;- c(rep(0.001, 4), 0.1),
         qPROP = qprop,
         qFUN = qfun,
         logPOSTERIOR = linear_posterior,
         varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
         param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(f1_mh, burnin=1000)


</code></pre>

<hr>
<h2 id='mh.fit'>Fitter function for Metropolis-Hastings (MH)</h2><span id='topic+mh.fit'></span>

<h3>Description</h3>

<p>This is the basic computing function for MH and should not be called directly except by experienced users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mh.fit(
  N,
  theta.init,
  qPROP,
  qFUN,
  logPOSTERIOR,
  nu = 0.001,
  varnames = NULL,
  param = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mh.fit_+3A_n">N</code></td>
<td>
<p>Number of MCMC samples</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_theta.init">theta.init</code></td>
<td>
<p>Vector of initial values for the parameters</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_qprop">qPROP</code></td>
<td>
<p>Function to generate proposal</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_qfun">qFUN</code></td>
<td>
<p>Probability for proposal function.  First argument is where to evaluate, and second argument is the conditional parameter</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_logposterior">logPOSTERIOR</code></td>
<td>
<p>Function to calculate and return the log posterior given a vector of values of <code>theta</code></p>
</td></tr>
<tr><td><code id="mh.fit_+3A_nu">nu</code></td>
<td>
<p>Single value or vector parameter passed to <code>qPROP</code> or <code>qFUN</code> for the proposal density</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_varnames">varnames</code></td>
<td>
<p>Optional vector of theta parameter names</p>
</td></tr>
<tr><td><code id="mh.fit_+3A_param">param</code></td>
<td>
<p>List of additional parameters for <code>logPOSTERIOR</code></p>
</td></tr>
<tr><td><code id="mh.fit_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code>logPOSTERIOR</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List for <code>mh</code>
</p>


<h3>Elements in <code>mh</code> list</h3>


<dl>
<dt><code>N</code></dt><dd>
<p>Number of MCMC samples
</p>
</dd>
<dt><code>theta</code></dt><dd>
<p>Nested list of length <code>N</code> of the sampled values of <code>theta</code> for each chain
</p>
</dd>
<dt><code>thetaCombined</code></dt><dd>
<p>List of dataframes containing sampled values, one for each chain
</p>
</dd>
<dt><code>r</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>theta.all</code></dt><dd>
<p>Nested list of all parameter values of <code>theta</code> sampled prior to accept/reject step for each
</p>
</dd>
<dt><code>r.all</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>accept</code></dt><dd>
<p>Number of accepted proposals.  The ratio <code>accept</code> / <code>N</code> is the acceptance rate
</p>
</dd>
<dt><code>accept_v</code></dt><dd>
<p>Vector of length <code>N</code> indicating which samples were accepted
</p>
</dd>
<dt><code>M</code></dt><dd>
<p>NULL for Metropolis-Hastings
</p>
</dd>
<dt><code>algorithm</code></dt><dd>
<p><code>MH</code> for Metropolis-Hastings
</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># Logistic regression example
X &lt;- cbind(1, seq(-100, 100, by=0.25))
betavals &lt;- c(-0.9, 0.2)
lodds &lt;- X %*% betavals
prob1 &lt;- as.numeric(1 / (1 + exp(-lodds)))

set.seed(9874)
y &lt;- sapply(prob1, function(xx) {
  sample(c(0, 1), 1, prob=c(1-xx, xx))
})

f1 &lt;- mh.fit(N = 2000,
         theta.init = rep(0, 2),
         nu = c(0.03, 0.001),
         qPROP = qprop,
         qFUN = qfun,
         logPOSTERIOR = logistic_posterior,
         varnames = paste0("beta", 0:1),
         y=y, X=X)

f1$accept / f1$N
</code></pre>

<hr>
<h2 id='neff'>Effective sample size calculation</h2><span id='topic+neff'></span>

<h3>Description</h3>

<p>Calculates an estimate of the adjusted MCMC sample size per parameter
adjusted for autocorrelation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neff(object, burnin = NULL, lagmax = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neff_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="neff_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="neff_+3A_lagmax">lagmax</code></td>
<td>
<p>maximum lag to extract for determining effective sample sizes</p>
</td></tr>
<tr><td><code id="neff_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector with effective sample sizes for each parameter in the model
</p>


<h3>References</h3>

<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.  Section 11.5
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = c(0.03, 0.02, 0.015),
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

neff(f, burnin=100)
</code></pre>

<hr>
<h2 id='neff.hmclearn'>Effective sample size calculation</h2><span id='topic+neff.hmclearn'></span>

<h3>Description</h3>

<p>Calculates an estimate of the adjusted MCMC sample size per parameter
adjusted for autocorrelation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
neff(object, burnin = NULL, lagmax = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neff.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="neff.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="neff.hmclearn_+3A_lagmax">lagmax</code></td>
<td>
<p>maximum lag to extract for determining effective sample sizes</p>
</td></tr>
<tr><td><code id="neff.hmclearn_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector with effective sample sizes for each parameter in the model
</p>


<h3>References</h3>

<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.  Section 11.5
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = c(0.03, 0.02, 0.015),
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

neff(f, burnin=100)
</code></pre>

<hr>
<h2 id='plot.hmclearn'>Plot Histograms of the Posterior Distribution</h2><span id='topic+plot.hmclearn'></span>

<h3>Description</h3>

<p>Calls <code>mcmc_hist</code> from the <code>bayesplot</code> package to display histograms of the posterior
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
plot(x, burnin = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.hmclearn_+3A_x">x</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="plot.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="plot.hmclearn_+3A_...">...</code></td>
<td>
<p>optional additional arguments to pass to the <code>bayesplot</code> functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Calls <code>mcmc_hist</code> from the <code>bayesplot</code> package, which returns a list including a <code>ggplot2</code> object.
</p>


<h3>References</h3>

<p>Gabry, Jonah and Mahr, Tristan (2019).  <em>bayesplot:  Plotting for Bayesian Models</em>.  <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = c(0.03, 0.02, 0.015),
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

plot(f, burnin=100)
</code></pre>

<hr>
<h2 id='predict.hmclearn'>Model Predictions for HMC or MH</h2><span id='topic+predict.hmclearn'></span>

<h3>Description</h3>

<p><code>predict</code> generates simulated data from the posterior predictive distribution.
This simulated data can be used for posterior predictive check diagnostics from the <code>bayesplot</code> package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
predict(object, X, fam = "linear", burnin = NULL, draws = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="predict.hmclearn_+3A_x">X</code></td>
<td>
<p>design matrix, either from fitting the model or new data</p>
</td></tr>
<tr><td><code id="predict.hmclearn_+3A_fam">fam</code></td>
<td>
<p>generalized linear model family.  Currently &quot;linear&quot;, &quot;binomial&quot;, and &quot;poisson&quot; are supported</p>
</td></tr>
<tr><td><code id="predict.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="predict.hmclearn_+3A_draws">draws</code></td>
<td>
<p>Number of simulated values from the posterior conditioned on <code>X</code></p>
</td></tr>
<tr><td><code id="predict.hmclearn_+3A_...">...</code></td>
<td>
<p>additional parameters, currently unsupported</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>hmclearnpred</code>.
</p>


<h3>Elements of <code>hmclearnpred</code> objects</h3>


<dl>
<dt><code>y</code></dt><dd>
<p>Median simulated values for each observation in <code>X</code>
</p>
</dd>
<dt><code>yrep</code></dt><dd>
<p>Matrix of simulated values where each row is a draw from the posterior predictive distribution
</p>
</dd>
<dt><code>X</code></dt><dd>
<p>Numeric design matrix
</p>
</dd>
</dl>



<h3>References</h3>

<p>Gabry, Jonah and Mahr, Tristan (2019).  <em>bayesplot:  Plotting for Bayesian Models</em>.  <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>
</p>
<p>Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., and Gelman, A (2019).  <em>Visualization in Bayesian Workflow</em>.  Journal of the Royal Statistical Society: Series A. Vol 182.  Issue 2.  p.389-402.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f1 &lt;- hmc(N = 500,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(f1)

p &lt;- predict(f1, X)
predvals &lt;- p$y
plot(predvals, y, xlab="predicted", ylab="actual")

X2 &lt;- cbind(1, matrix(rnorm(30), ncol=3))
p2 &lt;- predict(f1, X2)
p2$y

</code></pre>

<hr>
<h2 id='psrf'>Calculates Potential Scale Reduction Factor (psrf), also called the Rhat statistic,
from models fit via <code>mh</code> or <code>hmc</code></h2><span id='topic+psrf'></span>

<h3>Description</h3>

<p>Gelman and Rubin's diagnostic assesses the mix of multiple MCMC chain with different initial parameter values
Values close to 1 indicate that the posterior simulation has sufficiently converged, while
values above 1 indicate that additional samples may be necessary to ensure convergence.  A general
guideline suggests that values less than 1.05 are good, between 1.05 and 1.10 are ok, and above 1.10
have not converged well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psrf(object, burnin, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="psrf_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="psrf_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="psrf_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of Rhat statistics for each parameter
</p>


<h3>References</h3>

<p>Gelman, A. and Rubin, D. (1992) <em>Inference from Iterative Simulation Using Multiple Sequences</em>.  Statistical Science 7(4) 457-472.
</p>
<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.
</p>
<p>Gabry, Jonah and Mahr, Tristan (2019).  <em>bayesplot:  Plotting for Bayesian Models</em>.  <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

psrf(f, burnin=100)
</code></pre>

<hr>
<h2 id='psrf.hmclearn'>Calculates Potential Scale Reduction Factor (psrf), also called the Rhat statistic,
from models fit via <code>mh</code> or <code>hmc</code></h2><span id='topic+psrf.hmclearn'></span>

<h3>Description</h3>

<p>Gelman and Rubin's diagnostic assesses the mix of multiple MCMC chain with different initial parameter values
Values close to 1 indicate that the posterior simulation has sufficiently converged, while
values above 1 indicate that additional samples may be necessary to ensure convergence.  A general
guideline suggests that values less than 1.05 are good, between 1.05 and 1.10 are ok, and above 1.10
have not converged well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
psrf(object, burnin = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="psrf.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="psrf.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="psrf.hmclearn_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of Rhat statistics for each parameter
</p>


<h3>References</h3>

<p>Gelman, A. and Rubin, D. (1992) <em>Inference from Iterative Simulation Using Multiple Sequences</em>.  Statistical Science 7(4) 457-472.
</p>
<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.
</p>
<p>Gabry, Jonah and Mahr, Tristan (2019).  <em>bayesplot:  Plotting for Bayesian Models</em>.  <a href="https://mc-stan.org/bayesplot/">https://mc-stan.org/bayesplot/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># poisson regression example
set.seed(7363)
X &lt;- cbind(1, matrix(rnorm(40), ncol=2))
betavals &lt;- c(0.8, -0.5, 1.1)
lmu &lt;- X %*% betavals
y &lt;- sapply(exp(lmu), FUN = rpois, n=1)

f &lt;- hmc(N = 1000,
          theta.init = rep(0, 3),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = poisson_posterior,
          glogPOSTERIOR = g_poisson_posterior,
          varnames = paste0("beta", 0:2),
          param = list(y=y, X=X),
          parallel=FALSE, chains=2)

psrf(f, burnin=100)

</code></pre>

<hr>
<h2 id='qfun'>Multivariate Normal Density of Theta1 | Theta2</h2><span id='topic+qfun'></span>

<h3>Description</h3>

<p>Provided for Random Walk Metropolis algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qfun(theta1, theta2, nu)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qfun_+3A_theta1">theta1</code></td>
<td>
<p>Vector of current quantiles</p>
</td></tr>
<tr><td><code id="qfun_+3A_theta2">theta2</code></td>
<td>
<p>Vector for mean parameter</p>
</td></tr>
<tr><td><code id="qfun_+3A_nu">nu</code></td>
<td>
<p>Either a single numeric value for the covariance matrix, or a vector for the diagonal</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Multivariate normal density vector log-transformed
</p>


<h3>References</h3>

<p>Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei
Mi, Friedrich Leisch, Fabian Scheipl and Torsten Hothorn (2019).  <em>mvtnorm: Multivariate Normal and t Distributions</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>qfun(0, 0, 1)
log(1/sqrt(2*pi))

</code></pre>

<hr>
<h2 id='qprop'>Simulate from Multivariate Normal Density for Metropolis Algorithm</h2><span id='topic+qprop'></span>

<h3>Description</h3>

<p>Provided for Random Walk Metropolis algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qprop(theta1, nu)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qprop_+3A_theta1">theta1</code></td>
<td>
<p>Vector of current quantiles</p>
</td></tr>
<tr><td><code id="qprop_+3A_nu">nu</code></td>
<td>
<p>Either a single numeric value for the covariance matrix, or a vector for the diagonal</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a single numeric simulated value from a Normal distribution or vector of length <code>theta1</code>.
<code>length(mu)</code> matrix with one sample in each row.
</p>


<h3>References</h3>

<p>B. D. Ripley (1987) <em>Stochastic Simulation</em>. Wiley.  Page 98
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied Statistics with S.</em> Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s &lt;- replicate(1000, qprop(0, 1))
summary(s)
hist(s, col='light blue')
</code></pre>

<hr>
<h2 id='summary.hmclearn'>Summarizing HMC Model Fits</h2><span id='topic+summary.hmclearn'></span>

<h3>Description</h3>

<p>summary method for class <code>hmclearn</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hmclearn'
summary(
  object,
  burnin = NULL,
  probs = c(0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.hmclearn_+3A_object">object</code></td>
<td>
<p>an object of class <code>hmclearn</code>, usually a result of a call to <code>mh</code> or <code>hmc</code></p>
</td></tr>
<tr><td><code id="summary.hmclearn_+3A_burnin">burnin</code></td>
<td>
<p>optional numeric parameter for the number of initial MCMC samples to omit from the summary</p>
</td></tr>
<tr><td><code id="summary.hmclearn_+3A_probs">probs</code></td>
<td>
<p>quantiles to summarize the posterior distribution</p>
</td></tr>
<tr><td><code id="summary.hmclearn_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code>quantile</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a matrix with posterior quantiles and the posterior scale reduction factor statistic for each parameter.
</p>


<h3>References</h3>

<p>Gelman, A., et. al. (2013) <em>Bayesian Data Analysis</em>.  Chapman and Hall/CRC.
</p>
<p>Gelman, A. and Rubin, D. (1992) <em>Inference from Iterative Simulation Using Multiple Sequences</em>.  Statistical Science 7(4) 457-472.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Linear regression example
set.seed(521)
X &lt;- cbind(1, matrix(rnorm(300), ncol=3))
betavals &lt;- c(0.5, -1, 2, -3)
y &lt;- X%*%betavals + rnorm(100, sd=.2)

f1 &lt;- hmc(N = 500,
          theta.init = c(rep(0, 4), 1),
          epsilon = 0.01,
          L = 10,
          logPOSTERIOR = linear_posterior,
          glogPOSTERIOR = g_linear_posterior,
          varnames = c(paste0("beta", 0:3), "log_sigma_sq"),
          param=list(y=y, X=X), parallel=FALSE, chains=1)

summary(f1)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
