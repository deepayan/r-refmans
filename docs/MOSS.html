<!DOCTYPE html><html><head><title>Help for package MOSS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MOSS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aest.f'><p>Assign point color and shape aesthetics.</p></a></li>
<li><a href='#cov_adj'><p>Adjust omic blocks for covariates effects.</p></a></li>
<li><a href='#metdat'><p>Extracts (and merges) chunks of characters.</p></a></li>
<li><a href='#moss'><p>Multi-Omic integration via Sparse Singular value decomposition.</p></a></li>
<li><a href='#moss_heatmap'><p>Creates a heatmap from the output of MOSS.</p></a></li>
<li><a href='#moss_select'><p>Returns features and subject selected by latent dimension.</p></a></li>
<li><a href='#moss_signatures'><p>Returns signatures of features by groups of subjects</p></a></li>
<li><a href='#moss_venn'><p>Useful Venn diagrams to study the overlap between samples row names.</p></a></li>
<li><a href='#pca2tsne'><p>Mapping principal components onto a 2D map via tSNE.</p></a></li>
<li><a href='#prepro_na'><p>Missing values imputation by the mean of each column.</p></a></li>
<li><a href='#prepro_sub'><p>Scale and normalize columns of a matrix.</p></a></li>
<li><a href='#simulate_data'><p>Simple simulation of regulatory modules.</p></a></li>
<li><a href='#ssvdEN'><p>Sparse Singular Value Decomposition via Elastic Net.</p></a></li>
<li><a href='#ssvdEN_sol_path'><p>'Solution path' for sparse Singular Value Decomposition via Elastic Net.</p></a></li>
<li><a href='#ssvdEN_sol_path_par'><p>'Solution path' for sparse Singular Value Decomposition via Elastic Net</p>
using parallel computing.</a></li>
<li><a href='#tsne2clus'><p>t-Stochastic Neighbor Embedding to Clusters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Multi-Omic Integration via Sparse Singular Value Decomposition</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Description:</td>
<td>High dimensionality, noise and heterogeneity among
    samples and features challenge the omic integration task. Here we
    present an omic integration method based on sparse singular value
    decomposition (SVD) to deal with these limitations, by: a. obtaining
    the main axes of variation of the combined omics, b. imposing sparsity
    constraints at both subjects (rows) and features (columns) levels
    using Elastic Net type of shrinkage, and c. allowing both linear and
    non-linear projections (via t-Stochastic Neighbor Embedding) of the
    omic data to detect clusters in very convoluted data
    (Gonzalez-Reymundez et. al, 2022) &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtac179">doi:10.1093/bioinformatics/btac179</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/agugonrey/MOSS">https://github.com/agugonrey/MOSS</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/agugonrey/MOSS/issues">https://github.com/agugonrey/MOSS/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>cluster, dbscan, Rtsne, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>annotate, bigparallelr, bigstatsr, future.apply, scatterpie,
clValid, ComplexHeatmap, fpc, ggplot2, ggpmisc, ggthemes,
gridExtra, irlba, knitr, MASS, rmarkdown, testthat, viridis,
spelling, VennDiagram, grid</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-25 14:36:36 UTC; Agustin</td>
</tr>
<tr>
<td>Author:</td>
<td>Agustin Gonzalez-Reymundez [aut, cre, cph],
  Alexander Grueneberg [aut],
  Ana Vazquez [ctb, ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Agustin Gonzalez-Reymundez &lt;agugonrey@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-25 15:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='aest.f'>Assign point color and shape aesthetics.</h2><span id='topic+aest.f'></span>

<h3>Description</h3>

<p>This function is called by moss whenever a plot is produced.
It simply assigns colors and shape to points based on input labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aest.f(x, n.cat = 2, option = "D")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aest.f_+3A_x">x</code></td>
<td>
<p>Character vector with labels, or a numerical vector to be 
discretized in 'n.cat' categories.</p>
</td></tr>
<tr><td><code id="aest.f_+3A_n.cat">n.cat</code></td>
<td>
<p>Number of categories to split vector 'x'. Numeric.
Ignored if 'x' is a character vector.</p>
</td></tr>
<tr><td><code id="aest.f_+3A_option">option</code></td>
<td>
<p>Controls color palette.
One of the possible 'option' arguments for the 'viridis' function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with labels as rownames and two
columns representing point colors and shape, respectively.
</p>

<hr>
<h2 id='cov_adj'>Adjust omic blocks for covariates effects.</h2><span id='topic+cov_adj'></span>

<h3>Description</h3>

<p>This function is called by moss to adjust a series of omic 
blocks for covariates effects. However, if the covariates object
is too big, the user is recommended to call cov_adj ahead of moss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_adj(data.blocks, covs, n, dim.names = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_adj_+3A_data.blocks">data.blocks</code></td>
<td>
<p>List containing omic blocks of class 'matrix' or
'FBM'. In each block, rows represent subjects and columns features.</p>
</td></tr>
<tr><td><code id="cov_adj_+3A_covs">covs</code></td>
<td>
<p>Covariates which effect we wish to adjust for. 
Accepts objects of class matrix, data.frame, numeric, or 
character vectors.</p>
</td></tr>
<tr><td><code id="cov_adj_+3A_n">n</code></td>
<td>
<p>Number of subjects. Numeric.</p>
</td></tr>
<tr><td><code id="cov_adj_+3A_dim.names">dim.names</code></td>
<td>
<p>list of vectors with samples names, and features names
by omic block. If NULL, a list of artificial names is created.
Defaults to NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the covariates-adjusted elements in data.blocks.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("MOSS")
sim_data &lt;- simulate_data()
set.seed(43)

# Extracting simulated omic blocks.
sim_blocks &lt;- sim_data$sim_blocks[-4]

# Using fourth block as covariates.
covs &lt;- sim_data$sim_blocks[[4]]

# Adjust omic blocks for covariates effects.
sim_blocks_adj &lt;- cov_adj(sim_blocks,covs,nrow(covs))
</code></pre>

<hr>
<h2 id='metdat'>Extracts (and merges) chunks of characters.</h2><span id='topic+metdat'></span>

<h3>Description</h3>

<p>Extracts (and merges) chunks of characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metdat(x, i, sep = "-", collapse = sep)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metdat_+3A_x">x</code></td>
<td>
<p>A character vector.</p>
</td></tr>
<tr><td><code id="metdat_+3A_i">i</code></td>
<td>
<p>Index specifying which chunks of characters 
will be extracted (and merged).</p>
</td></tr>
<tr><td><code id="metdat_+3A_sep">sep</code></td>
<td>
<p>Chunks separator character. Defaults to &quot;-&quot;.</p>
</td></tr>
<tr><td><code id="metdat_+3A_collapse">collapse</code></td>
<td>
<p>New chunks separator character.
Default to 'sep'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with the extracted (and merged) chunks of
characters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- "this is one chunk of characters &amp; this is another one"
metdat(x, 1, " &amp; ")
metdat(x, 2, " &amp; ")
metdat(x, c(1, 2), " &amp; ")
metdat(x, c(1, 2), " &amp; ", " and ")
</code></pre>

<hr>
<h2 id='moss'>Multi-Omic integration via Sparse Singular value decomposition.</h2><span id='topic+moss'></span>

<h3>Description</h3>

<p>This function integrates omic blocks to perform sparse singular
value decomposition (SVD), non-linear embedding, and/or cluster
analysis. Both supervised and unsupervised methods are supported.
In both cases, if multiple omic blocks are used as predictors, they
are concatenated and normalized to form an 'extended' omic matrix 'X' 
(Gonzalez-Reymundez and Vazquez, 2020). Supervised analysis can be
obtained by indicating which omic block defines a multivariate 
response 'Y'. Each method within MOSS returns a matrix 'B', 
which form depends on the technique used
(e.g. B = X in pca; B = X'Y, for pls; B = (X'X)^-X'Y, for lrr).
A sparse SVD of matrix B is then obtained to summarize the variability
among samples and features in terms of latent factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moss(
  data.blocks,
  scale.arg = TRUE,
  norm.arg = TRUE,
  method = "pca",
  resp.block = NULL,
  covs = NULL,
  K.X = 5,
  K.Y = K.X,
  verbose = TRUE,
  nu.parallel = FALSE,
  nu.u = NULL,
  nu.v = NULL,
  alpha.u = 1,
  alpha.v = 1,
  plot = FALSE,
  cluster = FALSE,
  clus.lab = NULL,
  tSNE = FALSE,
  axes.pos = seq_len(K.Y),
  approx.arg = FALSE,
  exact.dg = FALSE,
  use.fbm = FALSE,
  lib.thresh = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moss_+3A_data.blocks">data.blocks</code></td>
<td>
<p>List containing omic blocks of class 'matrix' or
'FBM'. In
each block, rows represent subjects and columns features.</p>
</td></tr>
<tr><td><code id="moss_+3A_scale.arg">scale.arg</code></td>
<td>
<p>Should the omic blocks be centered and
scaled? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="moss_+3A_norm.arg">norm.arg</code></td>
<td>
<p>Should omic blocks be
normalized? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="moss_+3A_method">method</code></td>
<td>
<p>Multivariate method. Character.
Defaults to 'pca'. Possible options are pca, mbpca, pca-lda,
mbpca-lda, pls,
mbpls, pls-lda, mbpls-lda, lrr, mblrr, lrr-lda, mblrr-lda.</p>
</td></tr>
<tr><td><code id="moss_+3A_resp.block">resp.block</code></td>
<td>
<p>What block should be used as response? Integer.
Only used when the specified
method is supervised.</p>
</td></tr>
<tr><td><code id="moss_+3A_covs">covs</code></td>
<td>
<p>Covariates which effect we wish to adjust for. Accepts matrix,
data.frame, numeric, or character vectors.</p>
</td></tr>
<tr><td><code id="moss_+3A_k.x">K.X</code></td>
<td>
<p>Number of principal components for predictors.
Integer. Defaults to 5.</p>
</td></tr>
<tr><td><code id="moss_+3A_k.y">K.Y</code></td>
<td>
<p>Number of responses PC index when method is
supervised. Defaults to K.X.</p>
</td></tr>
<tr><td><code id="moss_+3A_verbose">verbose</code></td>
<td>
<p>Should we print messages? Logical.
Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="moss_+3A_nu.parallel">nu.parallel</code></td>
<td>
<p>Tuning degrees of sparsity in parallel. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="moss_+3A_nu.u">nu.u</code></td>
<td>
<p>A grid with
increasing integers representing degrees of sparsity for
left Eigenvectors.
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_+3A_nu.v">nu.v</code></td>
<td>
<p>Same but for right Eigenvectors. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_+3A_alpha.u">alpha.u</code></td>
<td>
<p>Elastic Net parameter for left Eigenvectors.
Numeric between 0
and 1. Defaults to 1.</p>
</td></tr>
<tr><td><code id="moss_+3A_alpha.v">alpha.v</code></td>
<td>
<p>Elastic Net parameter for right
Eigenvectors. Numeric between 0 and 1. Defaults to 1.</p>
</td></tr>
<tr><td><code id="moss_+3A_plot">plot</code></td>
<td>
<p>Should results be plotted?
Logical. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="moss_+3A_cluster">cluster</code></td>
<td>
<p>Arguments
passed to the function tsne2clus as a list. Defaults to FALSE.
If cluster=TRUE,
default parameters are used (eps_range=c(0,4), eps_res=100).</p>
</td></tr>
<tr><td><code id="moss_+3A_clus.lab">clus.lab</code></td>
<td>
<p>A
vector of same length than number of subjects with labels used to
visualize clusters. Factor.
Defaults to NULL.
When sparsity is imposed on the left
Eigenvectors, the association between non-zero loadings and
labels' groups is shown by a Chi-2 statistics for each pc.
When sparsity is not imposed, the association between
labels and PC
is addressed by a Kruskal-Wallis statistics.</p>
</td></tr>
<tr><td><code id="moss_+3A_tsne">tSNE</code></td>
<td>
<p>Arguments passed to the function pca2tsne as a list.
Defaults to
FALSE. If tSNE=T, default parameters are used
(perp=50,n.samples=1,n.iter=1e3).</p>
</td></tr>
<tr><td><code id="moss_+3A_axes.pos">axes.pos</code></td>
<td>
<p>PC index used for tSNE.
Defaults to 1 : K.Y. Used only when tSNE
is different than NULL.</p>
</td></tr>
<tr><td><code id="moss_+3A_approx.arg">approx.arg</code></td>
<td>
<p>Should we use standard SVD or
random approximations? Defaults to FALSE.
If TRUE and at least one block is of
class 'matrix', irlba is called. If TRUE &amp; is(O,'FBM')==TRUE,
big_randomSVD is called.</p>
</td></tr>
<tr><td><code id="moss_+3A_exact.dg">exact.dg</code></td>
<td>
<p>Should we compute exact degrees of sparsity? Logical.
Defaults to FALSE. Only relevant When alpha.s or alpha.f are in
the (0,1)
interval and exact.dg = TRUE.</p>
</td></tr>
<tr><td><code id="moss_+3A_use.fbm">use.fbm</code></td>
<td>
<p>Should we treat omic blocks as
Filed Backed Matrix (FBM)? Logical. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="moss_+3A_lib.thresh">lib.thresh</code></td>
<td>
<p>Should we use a liberal or conservative
threshold to tune degrees of sparsity? Logical. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Once 'dense' solutions are found (the result of SVD on a matrix B),
the function ssvdEN_sol_path is called to perform sparse SVD (sSVD)
on a grid of possible degrees of sparsity (nu),
for a possible value of the elastic net parameter (alpha).
The sSVD is performed using the algorithm of Shen and Huang (2008),
extended to include Elastic Net type of regularization.
For one latent factor (rank 1 case), the algorithm finds vectors u
and v' and scalar d that minimize:
</p>

<table>
<tr>
 <td style="text-align: right;">
||B-d* uv'||^2 +
lambda(nu_v)(alpha_v||v'||_1 +
(1-alpha_v)||v'||^2) + lambda(nu_u)(alpha_u||u||_1 +
(1-alpha_u)||u||^2)
</td>
</tr>

</table>

<p>such that ||u|| = 1.
The right Eigenvector is obtained from v / ||v|| and the
corresponding d from u'Bv.
The element lambda(nu_.) is a monotonically decreasing function of
nu_. (the number of desired element different from zero)
onto positive real numbers, and alpha_. is any number between
zero and one
balancing shrinking and variable selection.
Selecting degree of sparsity: The function allows to tune the
degree of sparsity using an ad-hoc method based on
the one presented in Shen &amp; Huang (2008, see reference) and
generalized for tuning both nu_v and nu_u.
This is done by exploring the proportion of
explained variance (PEV) on a grid of possible values.
Drastic and/or steep
changes in the PEV trajectory across degrees of sparsity are
used for automatic selection
(see help for the function ssvdEN_sol_path).
By imposing the additional assumption of omic blocks
being conditionally independent, each multivariate technique can
be extended using a 'multi-block' approach, where the
contribution of each omic block to the total (co)variance is
addressed.
When response Y is a character column matrix,
with classes or categories by subject,
each multivariate technique can be extended to perform
linear discriminant analysis.
</p>


<h3>Value</h3>

<p>Returns a list with the results of the sparse SVD.
If <em>plot</em>=TRUE, a series of plots is generated as well.
</p>

<ul>
<li> <p><em><strong>B:</strong></em>  The object of the (sparse) SVD.
Depending of the method used,
B can be a extended matrix of normalized omic blocks,
a variance-covariance matrix,
or a matrix of regression coefficients.
If at least one of the blocks in 'data.blocks' is of class FBM,
is(B,'FBM') is TRUE.
Otherwise, is(B,'matrix') is TRUE.
</p>
</li>
<li> <p><em><strong>Q:</strong></em> Matrix with the SVD projections at the 
level of subjects.
</p>
</li>
<li> <p><em><strong>selected_items:</strong></em> List containing the position, name, and loadings
of selected features and subjects by latent dimension.
if 'plot=TRUE', a scatterplot is displayed, where the x-axis
represents the latent dimensions, the y-axis the total number 
of features selected in log scale, and each point is a pie chart
showing the relative contribution of each omic to the number of
features selected. The radio of the pie-chart represents the 
coefficient of variation among squared loadings 
(mean squared loadings divided by their standard deviation)
</p>
</li>
<li> <p><em><strong>dense:</strong></em> A list containing the results of the
dense SVD.</p>

<ul>
<li> <p><strong>u:</strong> Matrix with left Eigenvectors.
</p>
</li>
<li> <p><strong>v:</strong> Matrix with right Eigenvectors.
</p>
</li>
<li> <p><strong>d:</strong> Matrix with singular values.
</p>
</li></ul>

</li>
<li> <p><em><strong>sparse:</strong></em> A list containing the results of the
sparse SVD.
</p>

<ul>
<li> <p><strong>u:</strong> Matrix with left Eigenvectors.
</p>
</li>
<li> <p><strong>v:</strong> Matrix with right Eigenvectors.
</p>
</li>
<li> <p><strong>d:</strong> Matrix with singular values.
</p>
</li>
<li> <p><strong>opt_dg_v</strong> Selected degrees of sparsity for
right Eigenvectors.
</p>
</li>
<li> <p><strong>opt_dg_u:</strong> Selected degrees of sparsity for
left Eigenvectors.
</p>
</li></ul>

</li>
<li><p> Graphical displays: Depending on the values in 'plot',
'tSNE','cluster',
and 'clus.lab' arguments, the following ggplot objects can be
obtained.
They contain:</p>

<ul>
<li> <p><strong>scree_plot:</strong> Plots of Eigenvalues and their
first and
second order empirical derivatives along PC indexes.
</p>
</li>
<li> <p><strong>tun_dgSpar_plot:</strong> Plots with the PEV trajectory,
as well as
its first and second empirical derivatives along the degrees of
sparsity path.
</p>
</li>
<li> <p><strong>PC_plot:</strong> Plot of the first principal
components according to axes.pos. By default the first two
are plotted.
</p>
</li>
<li> <p><strong>tSNE_plot:</strong> Plot with the tSNE mapping onto
two dimensions.
</p>
</li>
<li> <p><strong>clus_plot:</strong> The output of function tsne2clus.
</p>
</li>
<li> <p><strong>subLabels_vs_PCs:</strong> Plot of the Kruskal-Wallis
(or Chi-square)
statistics of the association test between PC
(or selected subjects) and
pre-established subjects groups.
</p>
</li>
<li> <p><strong>clusters_vs_PCs:</strong> Plot of the Kruskal-Wallis
(or Chi-square)
statistics of the association test between PC
(or selected subjects) and
detected clusters.
</p>
</li></ul>

</li></ul>



<h3>Note</h3>


<ol>
<li><p> The function does not return PEV for EN parameter
(alpha_v and/or alpha_u), the user needs to provide
a single value for each.
</p>
</li>
<li><p> When number of PC index &gt; 1,
columns of T might not be orthogonal.
</p>
</li>
<li><p> Although the user is encouraged to
perform data projection and
cluster separately, MOSS allows to do this automatically.
However, both tasks might require further tuning than the
provided by default, and computations could become cumbersome.
</p>
</li>
<li><p> Tuning of degrees of sparsity is done heuristically
on training set. In our experience, this results in
high specificity, but rather low sensitivity.
(i.e. too liberal cutoffs, as compared with
extensive cross-validation on testing set).
</p>
</li>
<li><p> When 'method' is an unsupervised technique,
'K.X' is the number of
latent factors returned and used in further analysis.
When 'method' is a supervised technique,
'K.X' is used to perform a SVD
to facilitate the product of large matrices and inverses.
</p>
</li>
<li><p> If 'K.X' (or 'K.Y') equal 1, no plots are returned.
</p>
</li>
<li><p> Although the degree of sparsity maps onto number of
features/subjects for Lasso, the user needs to be aware that this
conceptual correspondence
is lost for full EN (alpha belonging to (0, 1);
e.g. the number of features selected with alpha &lt; 1 will
be eventually larger than the optimal degree of sparsity).
This allows to rapidly increase the number of
non-zero elements
when tuning the degrees of sparsity.
In order to get exact values for the degrees of sparsity
at subjects
or features levels, the user needs to
set the value of 'exact.dg' parameter from 'FALSE'
(the default) to 'TRUE'.
</p>
</li></ol>



<h3>References</h3>


<ul>
<li><p> Gonzalez-Reymundez, and Vazquez. 2020. Multi-omic Signatures
identify pan-cancer classes of tumors beyond tissue of origin.
Scientific Reports 10 (1):8341  
</p>
</li>
<li><p> Shen, Haipeng, and Jianhua Z. Huang. 2008.
Sparse Principal Component
Analysis via Regularized Low Rank Matrix approximation.
Journal of Multivariate Analysis 99 (6). Academic Press: 1015_34.
</p>
</li>
<li><p> Baglama, Jim, Lothar Reichel, and B W Lewis. 2018.
Irlba: Fast Truncated Singular Value Decomposition and
Principal Components Analysis for Large Dense and
Sparse Matrices.
</p>
</li>
<li><p> Taskesen, Erdogan, Sjoerd M. H. Huisman, Ahmed Mahfouz,
Jesse H. Krijthe, Jeroen de Ridder, Anja van de Stolpe,
Erik van den Akker, Wim Verheagh, and Marcel J. T. Reinders. 2016.
Pan-Cancer Subtyping in a 2D-Map Shows Substructures
That Are Driven by Specific Combinations of Molecular
Characteristics. Scientific Reports 6 (1):24949.
</p>
</li>
<li><p> van der Maaten L, Hinton G. Visualizing Data using t-SNE.
J Mach Learn Res. 2008;9: 2579–2605
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example1: sparse PCA of a list of omic blocks.
library("MOSS")
sim_data &lt;- simulate_data()
set.seed(43)

# Extracting simulated omic blocks.
sim_blocks &lt;- sim_data$sim_blocks

# Extracting subjects and features labels.
lab.sub &lt;- sim_data$labels$lab.sub
lab.feat &lt;- sim_data$labels$lab.feat
out &lt;- moss(sim_blocks[-4],
  method = "pca",
  nu.v = seq(1, 200, by = 100),
  nu.u = seq(1, 100, by = 50),
  alpha.v = 0.5,
  alpha.u = 1
)

library(ggplot2)
library(ggthemes)
library(viridis)
library(cluster)
library(fpc)

set.seed(43)

# Example2: sparse PCA with t-SNE, clustering, and association with
# predefined groups of subjects.
out &lt;- moss(sim_blocks[-4],axes.pos=c(1:5),
  method = "pca",
  nu.v = seq(1, 200, by = 10),
  nu.u = seq(1, 100, by = 2),
  alpha.v = 0.5,
  alpha.u = 1,
  tSNE = TRUE,
  cluster = TRUE,
  clus.lab = lab.sub,
  plot = TRUE
)

# This shows clusters obtained with labels from pre-defined groups
# of subjects.
out$clus_plot

# This shows the statistical overlap between PCs and the pre-defined
# groups of subjects.
out$subLabels_vs_PCs

# This shows the contribution of each omic to the features 
# selected by PC index.
out$selected_items

# This shows features forming signatures across clusters.
out$feat_signatures

# Example3: Multi-block PCA with sparsity.
out &lt;- moss(sim_blocks[-4],axes.pos=1:5,
  method = "mbpca",
  nu.v = seq(1, 200, by = 10),
  nu.u = seq(1, 100, by = 2),
  alpha.v = 0.5,
  alpha.u = 1,
  tSNE = TRUE,
  cluster = TRUE,
  clus.lab = lab.sub,
  plot = TRUE
)
out$clus_plot

# This shows the 'weight' each omic block has on the variability
# explained by each PC. Weights in each PC add up to one.
out$block_weights

# Example4: Partial least squares with sparsity (PLS).
out &lt;- moss(sim_blocks[-4],axes.pos=1:5,
  K.X = 500,
  K.Y = 5,
  method = "pls",
  nu.v = seq(1, 100, by = 2),
  nu.u = seq(1, 100, by = 2),
  alpha.v = 1,
  alpha.u = 1,
  tSNE = TRUE,
  cluster = TRUE,
  clus.lab = lab.sub,
  resp.block = 3,
  plot = TRUE
)
out$clus_plot

# Get some measurement of accuracy at detecting features with signal
# versus background noise.
table(out$sparse$u[, 1] != 0, lab.feat[1:2000])
table(out$sparse$v[, 1] != 0, lab.feat[2001:3000])

# Example5: PCA-LDA
out &lt;- moss(sim_blocks,
  method = "pca-lda",
  cluster = TRUE,
  resp.block = 4,
  clus.lab = lab.sub,
  plot = TRUE
)
out$clus_plot


</code></pre>

<hr>
<h2 id='moss_heatmap'>Creates a heatmap from the output of MOSS.</h2><span id='topic+moss_heatmap'></span>

<h3>Description</h3>

<p>Creates a heatmap from the output of MOSS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moss_heatmap(B, SVD, right.lab, left.lab, axes.pos = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moss_heatmap_+3A_b">B</code></td>
<td>
<p>An bject of class 'matrix' or 'FBM'.</p>
</td></tr>
<tr><td><code id="moss_heatmap_+3A_svd">SVD</code></td>
<td>
<p>List with the results of a sparse or dense SVD.</p>
</td></tr>
<tr><td><code id="moss_heatmap_+3A_right.lab">right.lab</code></td>
<td>
<p>Columns title. Character.</p>
</td></tr>
<tr><td><code id="moss_heatmap_+3A_left.lab">left.lab</code></td>
<td>
<p>Rows title. Character.</p>
</td></tr>
<tr><td><code id="moss_heatmap_+3A_axes.pos">axes.pos</code></td>
<td>
<p>What SVD dimensions should be used to plot the heatmap? 
If NULL, all the SVD dimensions are use. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_heatmap_+3A_verbose">verbose</code></td>
<td>
<p>Should we print messages? Logical.
Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a 'ComplexHeatmap' plot representing the cross-product
between left and right Eigenvectors.
</p>

<hr>
<h2 id='moss_select'>Returns features and subject selected by latent dimension.</h2><span id='topic+moss_select'></span>

<h3>Description</h3>

<p>This function is meant to used after moss.
Its main purpose is to extract the features and subjects by 
latent dimension.
The selection depends on loadings at each dimension being 
different from zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moss_select(data.blocks, SVD, resp.block = NULL, K = NULL, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moss_select_+3A_data.blocks">data.blocks</code></td>
<td>
<p>a list of omic blocks as provided to moss.</p>
</td></tr>
<tr><td><code id="moss_select_+3A_svd">SVD</code></td>
<td>
<p>a list with SVD results. The function is meant to work
with the results from sparse SVD. 
However, 'dense' solutions are also accepted.</p>
</td></tr>
<tr><td><code id="moss_select_+3A_resp.block">resp.block</code></td>
<td>
<p>Which omic block was used as response in moss? 
Integer. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_select_+3A_k">K</code></td>
<td>
<p>How many dimensions should be displayed? Vector.
Defaults to the 1 : ncol(SVD$v).</p>
</td></tr>
<tr><td><code id="moss_select_+3A_plot">plot</code></td>
<td>
<p>Should the results be plotted? Logical. 
Defaults to FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing the position, name, and loadings
of selected features and subjects by latent dimension.
if 'plot=TRUE', a scatterplot is displayed, where the x-axis
represents the latent dimensions, the y-axis the total number 
of features selected in log scale, and each point is a pie chart
showing the relative contribution of each omic to the number of
features selected. The radio of the pie chart represents the 
coefficient of variation among squared loadings 
(mean squared loadings divided by their standard deviation).
</p>

<hr>
<h2 id='moss_signatures'>Returns signatures of features by groups of subjects</h2><span id='topic+moss_signatures'></span>

<h3>Description</h3>

<p>This function is meant to used after moss_select.
Its main purpose is to visualize how each selected feature (
non-zero loading feature) contributes to each group of subjects by 
latent dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moss_signatures(
  data.blocks,
  moss_select.out,
  clus_lab = NULL,
  plot = FALSE,
  feature.labels = NULL,
  th = 1,
  only.candidates = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moss_signatures_+3A_data.blocks">data.blocks</code></td>
<td>
<p>A list of omic blocks as provided to moss.</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_moss_select.out">moss_select.out</code></td>
<td>
<p>The output of moss_select.</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_clus_lab">clus_lab</code></td>
<td>
<p>A vector of same length than number of subjects 
with labels used to visualize clusters. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_plot">plot</code></td>
<td>
<p>Should the results be plotted? Logical. 
Defaults to FALSE</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_feature.labels">feature.labels</code></td>
<td>
<p>List with with features names for each omic.
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_th">th</code></td>
<td>
<p>Show the th
Default to th=1 (all the features). Numeric.</p>
</td></tr>
<tr><td><code id="moss_signatures_+3A_only.candidates">only.candidates</code></td>
<td>
<p>Should we plot only candidate features? Logical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with 'signatures', and if plot=TRUE,
a ggplot object named 'sig_plot'. The element 'signatures'
is a data frame with columns corresponding to 'Cluster' (groups
of subjects), 'Omic', 'Dim' (PC index or latent dimension),
'Feature_name', 'Feature_pos' (column index of the selected
feature within the corresponding omic), 'Loadings' (non-zero 
loadings from moss), 'Means', 'L1' and 'L2' (mean +/- standard
error of the selected feature values within an omic).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("MOSS")
# Extracting simulated omic blocks.
sim_data &lt;- simulate_data()
sim_blocks &lt;- sim_data$sim_blocks

# Extracting subjects and features labels.
lab.sub &lt;- sim_data$labels$lab.sub

out &lt;- moss(sim_blocks[-4],
  method = "pca",
  nu.v = 10,
  exact.dg = TRUE,
  plot = TRUE,
  alpha.v = 0.5
)
out2 &lt;- moss_select(data.blocks = sim_blocks[-4],
                    SVD = out$sparse,
                    plot = TRUE)

# Display signature plots.
out3 &lt;- moss_signatures(data.blocks = sim_blocks[-4],
                        clus_lab=lab.sub,
                        moss_select.out = out2,
                        plot = TRUE)
out3$sig_plot 
                               
</code></pre>

<hr>
<h2 id='moss_venn'>Useful Venn diagrams to study the overlap between samples row names.</h2><span id='topic+moss_venn'></span>

<h3>Description</h3>

<p>Useful Venn diagrams to study the overlap between samples row names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moss_venn(L, a, lty = "blank", fill = NULL, element_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moss_venn_+3A_l">L</code></td>
<td>
<p>List of elements which overlap we wish to check (e.g., row names 
by omic blocks).</p>
</td></tr>
<tr><td><code id="moss_venn_+3A_a">a</code></td>
<td>
<p>Elements of the list we want to focus on (e.g., a subset of omic 
blocks). Numerical.</p>
</td></tr>
<tr><td><code id="moss_venn_+3A_lty">lty</code></td>
<td>
<p>Line width of circles circumferences.</p>
</td></tr>
<tr><td><code id="moss_venn_+3A_fill">fill</code></td>
<td>
<p>Color for each circle. Character vector. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="moss_venn_+3A_element_names">element_names</code></td>
<td>
<p>Names of each category. Character vector.
Defaults to NULL</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with labels as rownames and two
</p>

<hr>
<h2 id='pca2tsne'>Mapping principal components onto a 2D map via tSNE.</h2><span id='topic+pca2tsne'></span>

<h3>Description</h3>

<p>This function is called by moss whenever 'moss(tSNE=TRUE)' to project
latent factors onto two dimensions via t-stochastic neighbor embedding
(tSNE) However, it can be used on any generic data matrix.
The function uses the Barnes-Hut tSNE algorithm from Rtsne package,
and uses an iterative procedure to select a tSNE map minimizing the
projection cost across several random initial conditions.
The function is inspired by the iterative procedure discussed in
Taskesen et al. 2016 and code originally provided with the publication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca2tsne(Z, perp = 50, n.samples = 1, n.iter = 1000, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pca2tsne_+3A_z">Z</code></td>
<td>
<p>A matrix with axes of variation (typically PCs) as columns
and subjects as rows.</p>
</td></tr>
<tr><td><code id="pca2tsne_+3A_perp">perp</code></td>
<td>
<p>Perplexity value. Defaults to 50.</p>
</td></tr>
<tr><td><code id="pca2tsne_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of times the algorithm starts from 
different random initial conditions. Defaults to 1.</p>
</td></tr>
<tr><td><code id="pca2tsne_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of iterations for each run of the algorithm.</p>
</td></tr>
<tr><td><code id="pca2tsne_+3A_parallel">parallel</code></td>
<td>
<p>Should random starts be done in parallel? Logical. 
Default to FALSE.
Defaults to 1000.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns output of function 'Rtsne::Rtsne' from the random initial
condition with the smallest 'reconstruction error'.
</p>


<h3>References</h3>


<ul>
<li><p> van der Maaten L, Hinton G. Visualizing Data using t-SNE.
J Mach Learn Res. 2008;9: 2579–2605
</p>
</li>
<li><p> Krijthe JH. Rtsne: T-Distributed Stochastic Neighbor
Embedding using a Barnes-Hut Implementation. 2015
</p>
</li>
<li><p> Taskesen, Erdogan, Sjoerd M. H. Huisman, Ahmed Mahfouz,
Jesse H. Krijthe, Jeroen de Ridder, Anja van de Stolpe,
Erik van den Akker, Wim Verheagh, and Marcel J. T. Reinders. 2016.
Pan-Cancer Subtyping in a 2D-Map Shows Substructures
That Are Driven by Specific Combinations of Molecular
Characteristics. Scientific Reports 6 (1):24949.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library("MOSS")
sim_blocks &lt;- simulate_data()$sim_blocks

# Example of pca2tsne usage.
Z &lt;- pca2tsne(sim_blocks$`Block 3`, 
              perp = 50, 
              n.samples = 1,
              n.iter = 1e3)$Y
plot(Z, xlab = "x_tSNE(X)", ylab = "y_tSNE(X)")

# Example of usage within moss.
set.seed(34)
moss(sim_blocks[-4],
  tSNE = list(
    perp = 50,
    n.samples = 1,
    n.iter = 1e3
  ),
  plot = TRUE
)$tSNE_plot

</code></pre>

<hr>
<h2 id='prepro_na'>Missing values imputation by the mean of each column.</h2><span id='topic+prepro_na'></span>

<h3>Description</h3>

<p>This function is called by moss to count the impute missing values
by the mean of each column within omic blocks.
If any column has more than 20
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepro_na(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepro_na_+3A_x">X</code></td>
<td>
<p>An object of class 'matrix', 'FBM', or 'array'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Meant for objects of class 'matrix', 'FBM', or 'array'.
</p>


<h3>Value</h3>

<p>Returns input with imputed missing values.
</p>

<hr>
<h2 id='prepro_sub'>Scale and normalize columns of a matrix.</h2><span id='topic+prepro_sub'></span>

<h3>Description</h3>

<p>This function is called by moss to scale and normalize (extended) matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepro_sub(X, scale.arg, norm.arg)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepro_sub_+3A_x">X</code></td>
<td>
<p>An object of class 'matrix', 'FBM', or 'array'.</p>
</td></tr>
<tr><td><code id="prepro_sub_+3A_scale.arg">scale.arg</code></td>
<td>
<p>Should we scale columns? Logical.</p>
</td></tr>
<tr><td><code id="prepro_sub_+3A_norm.arg">norm.arg</code></td>
<td>
<p>Should we normalize columns? Logical.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ment for objects of class 'matrix', 'FBM', or 'array'.
</p>


<h3>Value</h3>

<p>A matrix with scaled and/or normalized columns.
</p>

<hr>
<h2 id='simulate_data'>Simple simulation of regulatory modules.</h2><span id='topic+simulate_data'></span>

<h3>Description</h3>

<p>This a simple simulation to use in MOSS' examples.
The specifics of the simulation are shown in the &quot;Examples&quot; section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_data(moss_seed = 42)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_data_+3A_moss_seed">moss_seed</code></td>
<td>
<p>The seed for random number generator.
Numeric. Defaults to 42.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of two elements 'sim_blocks' and 'labels'.
First element 'sim_blocks' is a list of three numeric matrices,
and one character matrix.
Second element 'labels' has two character vectors.
The first element 'lab.sub' identifies the groups of 'signal' subjects.
The second element 'lab.feat' identifies the groups 'signal' features
from background 'noise'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sim_data &lt;- simulate_data()

# Extracting simulated omic blocks.
sim_blocks &lt;- sim_data$sim_blocks

# Extracting subjects and features labels.
lab.sub &lt;- sim_data$labels$lab.sub
lab.feat &lt;- sim_data$labels$lab.feat

# Check dimensions and objects class.
lapply(sim_blocks, dim)
lapply(sim_blocks, function(x) class(x[, 1]))

# Showing how the data was generated.
set.seed(42)
O1 &lt;- matrix(data = 0, nrow = 5e2, ncol = 1e3)
O2 &lt;- O1
O1[1:20, 1:150] &lt;- 1
O1 &lt;- O1 + rnorm(5e5, mean = 0, sd = 0.5)
O2[71:90, 71:200] &lt;- 1
O2 &lt;- O2 + rnorm(5e5, mean = 0, sd = 0.5)
# Simulating a continous response blocks.
O3 &lt;- 3 * O1 - 5 * O2 + rnorm(5e5, mean = 0, sd = 0.5)

# Creating a vector labeling clusters of subjects.
aux &lt;- rep("Background", 500)
aux[1:20] &lt;- "Group 1"
aux[71:90] &lt;- "Group 2"
all.equal(aux, lab.sub)

# Generating a classification response.
O4 &lt;- as.matrix(aux)

# Storing all blocks within a list.
all.equal(sim_blocks, list(
  "Block 1" = O1,
  "Block 2" = O2,
  "Block 3" = O3,
  "Block 4" = O4
))

# Creating a vector labeling signal and background features.
aux &lt;- rep("Background features", 3000)
aux[c(1:150, 1072:1200, 2001:2200)] &lt;- "Signal features"
all.equal(aux, lab.feat)
</code></pre>

<hr>
<h2 id='ssvdEN'>Sparse Singular Value Decomposition via Elastic Net.</h2><span id='topic+ssvdEN'></span>

<h3>Description</h3>

<p>This function performs sparse singular value decomposition (SVD)
on a matrix 'x' via Elastic Net types of penalties.
For one PC (rank 1 case), the algorithm finds left and right Eigenvectors
(u and w, respectively), that minimize:
||x - u w'||_F^2 +
lambda_w (alpha_w||w||_1 +
(1 - alpha_w)||w||_F^2) +
lambda_u (alpha||u||_1 + (1 - alpha_u)||u||_F^2)
such that ||u|| = 1.
The right Eigen vector is obtained from v = w / ||w|| and the
corresponding Eigen value = u^T x v.
The penalties lambda_u and lambda_w are mapped from
specified desired degree of sparsity
(dg.spar.features &amp; dg.spar.subjects).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssvdEN(
  O,
  n.PC = 1,
  dg.spar.features = NULL,
  dg.spar.subjects = NULL,
  maxit = 500,
  tol = 0.001,
  scale.arg = TRUE,
  center.arg = TRUE,
  approx.arg = FALSE,
  alpha.f = 1,
  alpha.s = 1,
  svd.0 = NULL,
  s.values = TRUE,
  ncores = 1,
  exact.dg = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssvdEN_+3A_o">O</code></td>
<td>
<p>Numeric matrix of n subjects (rows) and p features (columns).
It can be a File-backed Big Matrix.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_n.pc">n.PC</code></td>
<td>
<p>Number of desired principal axes. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_dg.spar.features">dg.spar.features</code></td>
<td>
<p>Degree of sparsity at the features level.
Numeric. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_dg.spar.subjects">dg.spar.subjects</code></td>
<td>
<p>Degree of sparsity at the subjects level.
Numeric. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations for the sparse SVD algorithm.
Numeric. Defaults to 500.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the sparse SVD algorithm.
Numeric. Defaults to 0.001.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_scale.arg">scale.arg</code></td>
<td>
<p>Should O be scaled? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_center.arg">center.arg</code></td>
<td>
<p>Should O be centered? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_approx.arg">approx.arg</code></td>
<td>
<p>Should we use standard SVD or random approximations?
Defaults to FALSE. If TRUE &amp; is(O,'matrix') == TRUE, irlba is called.
If TRUE &amp; is(O, &quot;FBM&quot;) == TRUE, big_randomSVD is called.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_alpha.f">alpha.f</code></td>
<td>
<p>Elastic net mixture parameter at the features level.
Measures the compromise between lasso (alpha = 1) and
ridge (alpha = 0) types of sparsity. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_alpha.s">alpha.s</code></td>
<td>
<p>Elastic net mixture parameter at the subjects level.
Defaults to alpha.s = 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_svd.0">svd.0</code></td>
<td>
<p>List containing an initial SVD. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_s.values">s.values</code></td>
<td>
<p>Should the singular values be calculated? Logical.
Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used by big_randomSVD.
Default does not use parallelism. Ignored when class(O)!=FBM.</p>
</td></tr>
<tr><td><code id="ssvdEN_+3A_exact.dg">exact.dg</code></td>
<td>
<p>Should we compute exact degrees of sparsity? Logical.
Defaults to FALSE. Only relevant When alpha.s or alpha.f are
in the (0,1) interval and exact.dg = TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function allows the use of the base svd function
for relatively small problems.
For larger problems, functions for fast-partial SVD
(irlba and big_randomSVD, from irlba and bigstatsr packages) are used.
</p>


<h3>Value</h3>

<p>A list with the results of the (sparse) SVD, containing:
</p>

<ul>
<li><p> u: Matrix with left eigenvectors.
</p>
</li>
<li><p> v: Matrix with right eigenvectors.
</p>
</li>
<li><p> d: Matrix with singular values.
</p>
</li></ul>



<h3>Note</h3>

<p>When elastic net is used ('alpha.s' or 'alpha.f' in the (0,1)
interval), the resulting number of non-zero subjects or features
is larger than the 'dg.spar.subjects' or 'dg.spar.features' values.
This allows to rapidly increase the number of non-zero elements when
tuning the degrees of sparsity with function ssvdEN_sol_path. 
In order to get exact values for the degrees
of sparsity at subjects or features levels, the user needs to
set the value of 'exact.dg' parameter from 'FALSE' (the default)
to 'TRUE'.
</p>


<h3>References</h3>


<ul>
<li><p> Shen, Haipeng, and Jianhua Z. Huang. 2008.
Sparse Principal Component Analysis via Regularized Low
Rank Matrix Approximation.
Journal of Multivariate Analysis 99 (6).
Academic Press:1015_34.
</p>
</li>
<li><p> Baglama, Jim, Lothar Reichel, and B W Lewis. 2018.
Irlba: Fast Truncated Singular Value Decomposition and
Principal Components Analysis for Large Dense and Sparse Matrices.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("MOSS")

# Extracting simulated omic blocks.
sim_blocks &lt;- simulate_data()$sim_blocks

X &lt;- sim_blocks$`Block 3`

# Equal to svd solution: exact singular vectors and values.
out &lt;- ssvdEN(X, approx.arg = FALSE)

# Uses irlba to get approximated singular vectors and values.
library(irlba)
out &lt;- ssvdEN(X, approx.arg = TRUE)
# Uses bigstatsr to get approximated singular vectors and values
# of a Filebacked Big Matrix.
library(bigstatsr)
out &lt;- ssvdEN(as_FBM(X), approx.arg = TRUE)

# Sampling a number of subjects and features for a fix sparsity degree.
s.u &lt;- sample(1:nrow(X), 1)
s.v &lt;- sample(1:ncol(X), 1)

# Lasso penalties.
all.equal(sum(ssvdEN(X, dg.spar.features = s.v)$v != 0), s.v)
all.equal(
  unique(colSums(ssvdEN(X, dg.spar.features = s.v, n.PC = 5)$v
  != 0)),
  s.v
)

all.equal(sum(ssvdEN(X, dg.spar.subjects = s.u)$u != 0), s.u)
all.equal(
  unique(colSums(ssvdEN(X, dg.spar.subjects = s.u, n.PC = 5)$u
  != 0)),
  s.u
)

out &lt;- ssvdEN(X, dg.spar.features = s.v, dg.spar.subjects = s.u)
all.equal(sum(out$u != 0), s.u)
all.equal(sum(out$v != 0), s.v)

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  n.PC = 10
)
all.equal(unique(colSums(out$u != 0)), s.u)
all.equal(unique(colSums(out$v != 0)), s.v)

# Ridge penalties.
all.equal(
  sum(ssvdEN(X, dg.spar.features = s.v, alpha.f = 0)$v != 0),
  ncol(X)
)
all.equal(
  unique(colSums(ssvdEN(X,
    dg.spar.features = s.v, n.PC = 5,
    alpha.f = 0
  )$v != 0)),
  ncol(X)
)

all.equal(
  sum(ssvdEN(X, dg.spar.subjects = s.u, alpha.s = 0)$u != 0),
  nrow(X)
)
all.equal(
  unique(colSums(ssvdEN(X,
    dg.spar.subjects = s.u, n.PC = 5,
    alpha.s = 0
  )$u != 0)),
  nrow(X)
)

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  alpha.f = 0, alpha.s = 0
)
all.equal(sum(out$u != 0), nrow(X))
all.equal(sum(out$v != 0), ncol(X))

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  n.PC = 10, alpha.f = 0,
  alpha.s = 0
)
all.equal(unique(colSums(out$u != 0)), nrow(X))
all.equal(unique(colSums(out$v != 0)), ncol(X))

# Elastic Net penalties.
sum(ssvdEN(X, dg.spar.features = s.v, alpha.f = 0.5)$v != 0) &gt;= s.v
all(unique(colSums(ssvdEN(X,
  dg.spar.features = s.v, n.PC = 5,
  alpha.f = 0.5
)$v != 0)) &gt;= s.v)

sum(ssvdEN(X, dg.spar.subjects = s.u, alpha.s = 0.5)$u != 0) &gt;= s.u
all(unique(colSums(ssvdEN(X,
  dg.spar.subjects = s.u, n.PC = 5,
  alpha.s = 0.5
)$u != 0)) &gt;= s.u)

# Elastic Net penalties with exact degrees of sparsity.
sum(ssvdEN(X,
  dg.spar.features = s.v, alpha.f = 0.5,
  exact.dg = TRUE
)$v != 0) == s.v
all(unique(colSums(ssvdEN(X,
  dg.spar.features = s.v, n.PC = 5,
  alpha.f = 0.5, exact.dg = TRUE
)$v != 0)) == s.v)

sum(ssvdEN(X,
  dg.spar.subjects = s.u, alpha.s = 0.5,
  exact.dg = TRUE
)$u != 0) == s.u
all(unique(colSums(ssvdEN(X,
  dg.spar.subjects = s.u, n.PC = 5,
  alpha.s = 0.5, exact.dg = TRUE
)$u != 0)) == s.u)

</code></pre>

<hr>
<h2 id='ssvdEN_sol_path'>'Solution path' for sparse Singular Value Decomposition via Elastic Net.</h2><span id='topic+ssvdEN_sol_path'></span>

<h3>Description</h3>

<p>This function allows to explore values on the solution path of the
sparse singular value decomposition (SVD) problem.
The goal of this is to tune the degree of sparsity of subjects,
features, or both subjects/features.
The function performs a penalized SVD that imposes sparsity/smoothing
in both left and right singular vectors.
The penalties at both levels are Elastic Net-like,
and the trade-off between ridge and Lasso like penalties is controlled
by two 'alpha' parameters. The proportion of variance explained is
the criteria used to choose the optimal degrees of sparsity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssvdEN_sol_path(
  O,
  center = TRUE,
  scale = TRUE,
  dg.grid.right = seq_len(ncol(O)) - 1,
  dg.grid.left = NULL,
  n.PC = 1,
  svd.0 = NULL,
  alpha.f = 1,
  alpha.s = 1,
  maxit = 500,
  tol = 0.001,
  approx = FALSE,
  plot = FALSE,
  ncores = 1,
  verbose = TRUE,
  lib.thresh = TRUE,
  left.lab = "Subjects",
  right.lab = "Features",
  exact.dg = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssvdEN_sol_path_+3A_o">O</code></td>
<td>
<p>Numeric matrix of n subjects (rows) and p features (columns).
Only objects supported are 'matrix' and 'FBM'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_center">center</code></td>
<td>
<p>Should we center? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_scale">scale</code></td>
<td>
<p>Should we scale? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_dg.grid.right">dg.grid.right</code></td>
<td>
<p>Grid with degrees of sparsity at the features level.
Numeric. Default is the entire solution path for features
(i.e. 1 : (ncol(O) - 1)).</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_dg.grid.left">dg.grid.left</code></td>
<td>
<p>Grid with degrees of sparsity at the subjects level.
Numeric. Defaults to dg.grid.left = nrow(O).</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_n.pc">n.PC</code></td>
<td>
<p>Number of desired principal axes. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_svd.0">svd.0</code></td>
<td>
<p>Initial SVD (i.e. least squares solution).
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_alpha.f">alpha.f</code></td>
<td>
<p>Elastic net mixture parameter at the features level.
Measures the compromise between lasso (alpha = 1) and
ridge (alpha = 0) types of sparsity. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_alpha.s">alpha.s</code></td>
<td>
<p>Elastic net mixture parameter at the subjects level.
Defaults to alpha.s = 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations. Defaults to 500.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_tol">tol</code></td>
<td>
<p>Convergence is determined when ||U_j - U_j-1||_F &lt; tol,
where U_j is the matrix of estimated left regularized singular
vectors at iteration j.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_approx">approx</code></td>
<td>
<p>Should we use standard SVD or random approximations?
Defaults to FALSE. If TRUE &amp; is(O,'matrix') == TRUE, irlba is called.
If TRUE &amp; is(O, &quot;FBM&quot;) == TRUE, big_randomSVD is called.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_plot">plot</code></td>
<td>
<p>Should we plot the solution path? Logical. Defaults to FALSE</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used by big_randomSVD.
Default does not use parallelism. Ignored when is(O, &quot;FBM&quot;) == TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_verbose">verbose</code></td>
<td>
<p>Should we print messages?. Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_lib.thresh">lib.thresh</code></td>
<td>
<p>Should we use a liberal or conservative
threshold to tune degrees of sparsity? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_left.lab">left.lab</code></td>
<td>
<p>Label for the subjects level. Character.
Defaults to 'subjects'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_right.lab">right.lab</code></td>
<td>
<p>Label for the features level. Character.
Defaults to 'features'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_+3A_exact.dg">exact.dg</code></td>
<td>
<p>Should we compute exact degrees of sparsity? Logical.
Defaults to FALSE. Only relevant When alpha.s or alpha.f are in the (0,1)
interval and exact.dg = TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the degree of sparsity for which the change in PEV
is the steepest ('liberal' option), or for which the change in PEV 
stabilizes ('conservative' option).
This heuristics relax the need of tuning parameters on a testing set.
</p>
<p>For one PC (rank 1 case), the algorithm finds vectors u, w that minimize:
||x - u w'||_F^2 + lambda_w (alpha_w||w||_1 + (1 - alpha_w)||w||_F^2)
+
lambda_u (alpha||u||_1 + (1 - alpha_u)||u||_F^2)
such that ||u|| = 1. The right Eigen vector is obtained
from v = w / ||w|| and the corresponding Eigen value = u^T x v.
The penalties lambda_u and lambda_w are mapped from specified
desired degrees of sparsity (dg.spar.features &amp; dg.spar.subjects).
</p>


<h3>Value</h3>


<p>A list with the results of the (sparse) SVD and (if argument 'plot'=TRUE)
the corresponding graphical displays.
</p>
<ul>
<li><p> SVD: a list with the results of the (sparse) SVD, containing:
</p>

<ul>
<li><p> u: Matrix with left eigenvectors.
</p>
</li>
<li><p> v: Matrix with right eigenvectors.
</p>
</li>
<li><p> d: Matrix with singular values.
</p>
</li>
<li><p> opt.dg.right: Selected degrees of sparsity for right eigenvectors.
</p>
</li>
<li><p> opt.dg.left: Selected degrees of sparsity for left eigenvectors.
</p>
</li></ul>

</li>
<li><p> plot: A ggplot object.
</p>
</li></ul>



<h3>Note</h3>

<p>Although the degree of sparsity maps onto number of
features/subjects for Lasso, the user needs to be aware that
this conceptual correspondence
is lost for full EN (alpha belonging to (0, 1);
e.g. the number of features selected with alpha &lt; 1
will be eventually larger than the optimal degree of sparsity).
This allows to rapidly increase the number of non-zero elements
when tuning the degrees of sparsity.
In order to get exact values for the degrees of sparsity at subjects or
features levels, the user needs to
set the value of 'exact.dg' parameter from 'FALSE' (the default) to
'TRUE'.
</p>


<h3>References</h3>


<ul>
<li><p> Shen, Haipeng, and Jianhua Z. Huang. 2008. Sparse Principal
Component Analysis via Regularized Low Rank Matrix Approximation.
Journal of Multivariate Analysis 99 (6).
</p>
</li>
<li><p> Baglama, Jim, Lothar Reichel, and B W Lewis. 2018.
Irlba: Fast Truncated Singular Value Decomposition and Principal
Components Analysis for Large Dense and Sparse Matrices.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("MOSS")

# Extracting simulated omic blocks.
sim_blocks &lt;- simulate_data()$sim_blocks
X &lt;- sim_blocks$`Block 3`

# Tuning sparsity degree for features (increments of 20 units).
out &lt;- ssvdEN_sol_path(X, dg.grid.right = seq(1, 1000, by = 20))
</code></pre>

<hr>
<h2 id='ssvdEN_sol_path_par'>'Solution path' for sparse Singular Value Decomposition via Elastic Net
using parallel computing.</h2><span id='topic+ssvdEN_sol_path_par'></span>

<h3>Description</h3>

<p>This function is a copy of 'ssvdEN_sol_path' meant to be used
in combination with the future.apply package to allow for
parallel computing of the optimal degrees of sparsity by
subjects and/or features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssvdEN_sol_path_par(
  O,
  center = TRUE,
  scale = TRUE,
  dg.grid.right = seq_len(ncol(O)) - 1,
  dg.grid.left = NULL,
  n.PC = 1,
  svd.0 = NULL,
  alpha.f = 1,
  alpha.s = 1,
  maxit = 500,
  tol = 0.001,
  approx = FALSE,
  plot = FALSE,
  ncores = 1,
  verbose = TRUE,
  lib.thresh = TRUE,
  left.lab = "Subjects",
  right.lab = "Features",
  exact.dg = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssvdEN_sol_path_par_+3A_o">O</code></td>
<td>
<p>Numeric matrix of n subjects (rows) and p features (columns).
Only objects supported are 'matrix' and 'FBM'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_center">center</code></td>
<td>
<p>Should we center? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_scale">scale</code></td>
<td>
<p>Should we scale? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_dg.grid.right">dg.grid.right</code></td>
<td>
<p>Grid with degrees of sparsity at the features level.
Numeric. Default is the entire solution path for features
(i.e. 1 : (ncol(O) - 1)).</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_dg.grid.left">dg.grid.left</code></td>
<td>
<p>Grid with degrees of sparsity at the subjects level.
Numeric. Defaults to dg.grid.left = nrow(O).</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_n.pc">n.PC</code></td>
<td>
<p>Number of desired principal axes. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_svd.0">svd.0</code></td>
<td>
<p>Initial SVD (i.e. least squares solution).
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_alpha.f">alpha.f</code></td>
<td>
<p>Elastic net mixture parameter at the features level.
Measures the compromise between lasso (alpha = 1) and
ridge (alpha = 0) types of sparsity. Numeric. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_alpha.s">alpha.s</code></td>
<td>
<p>Elastic net mixture parameter at the subjects level.
Defaults to alpha.s = 1.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations. Defaults to 500.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_tol">tol</code></td>
<td>
<p>Convergence is determined when ||U_j - U_j-1||_F &lt; tol,
where U_j is the matrix of estimated left regularized singular
vectors at iteration j.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_approx">approx</code></td>
<td>
<p>Should we use standard SVD or random approximations?
Defaults to FALSE. If TRUE &amp; is(O,'matrix') == TRUE, irlba is called.
If TRUE &amp; is(O, &quot;FBM&quot;) == TRUE, big_randomSVD is called.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_plot">plot</code></td>
<td>
<p>Should we plot the solution path? Logical. Defaults to FALSE</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_ncores">ncores</code></td>
<td>
<p>Number of cores used by big_randomSVD.
Default does not use parallelism. Ignored when is(O, &quot;FBM&quot;) == TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_verbose">verbose</code></td>
<td>
<p>Should we print messages?. Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_lib.thresh">lib.thresh</code></td>
<td>
<p>Should we use a liberal or conservative
threshold to tune degrees of sparsity? Logical. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_left.lab">left.lab</code></td>
<td>
<p>Label for the subjects level. Character.
Defaults to 'subjects'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_right.lab">right.lab</code></td>
<td>
<p>Label for the features level. Character.
Defaults to 'features'.</p>
</td></tr>
<tr><td><code id="ssvdEN_sol_path_par_+3A_exact.dg">exact.dg</code></td>
<td>
<p>Should we compute exact degrees of sparsity? Logical.
Defaults to FALSE. Only relevant When alpha.s or alpha.f are in the (0,1)
interval and exact.dg = TRUE.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Although the degree of sparsity maps onto number of
features/subjects for Lasso, the user needs to be aware that
this conceptual correspondence
is lost for full EN (alpha belonging to (0, 1);
e.g. the number of features selected with alpha &lt; 1
will be eventually larger than the optimal degree of sparsity).
This allows to rapidly increase the number of non-zero elements
when tuning the degrees of sparsity.
In order to get exact values for the degrees of sparsity at subjects or
features levels, the user needs to
set the value of 'exact.dg' parameter from 'FALSE' (the default) to
'TRUE'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("MOSS")

# Extracting simulated omic blocks.
sim_blocks &lt;- simulate_data()$sim_blocks
X &lt;- sim_blocks$`Block 3`

# Comparing ssvdEN_sol_path_par and ssvdEN_sol_path.
t1 &lt;- proc.time()
out1 &lt;- ssvdEN_sol_path(X, dg.grid.right = 1:1000, dg.grid.left = 1:500)
t1 &lt;- proc.time() - t1

t2 &lt;- proc.time()
out2 &lt;- ssvdEN_sol_path_par(X, dg.grid.right = 1:1000, dg.grid.left = 1:500)
t2 &lt;- proc.time() - t2

</code></pre>

<hr>
<h2 id='tsne2clus'>t-Stochastic Neighbor Embedding to Clusters</h2><span id='topic+tsne2clus'></span>

<h3>Description</h3>

<p>Finds clusters on a 2 dimensional map using
Density-based spatial clustering of applications with noise
(DBSCAN; Esther et al. 1996).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsne2clus(
  S.tsne,
  ann = NULL,
  labels,
  aest = NULL,
  eps_res = 100,
  eps_range = c(0, 4),
  min.clus.size = 10,
  group.names = "Groups",
  xlab = "x: tSNE(X)",
  ylab = "y: tSNE(X)",
  clus = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsne2clus_+3A_s.tsne">S.tsne</code></td>
<td>
<p>Outcome of function &quot;pca2tsne&quot;</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_ann">ann</code></td>
<td>
<p>Subjects' annotation data.
An incidence matrix assigning subjects to classes of biological
relevance.
Meant to tune cluster assignation via Biological Homogeneity Index (BHI).
If ann=NULL, the number of clusters is tuned with the
Silhouette index instead of BHI. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_labels">labels</code></td>
<td>
<p>Character vector with labels describing subjects.
Meant to assign aesthetics to the visual display of clusters.</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_aest">aest</code></td>
<td>
<p>Data frame containing points shape and color.
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_eps_res">eps_res</code></td>
<td>
<p>How many eps values should be explored between the 
specified range?</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_eps_range">eps_range</code></td>
<td>
<p>Vector containing the minimum and
maximum eps values to be explored. Defaults to c(0, 4).</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_min.clus.size">min.clus.size</code></td>
<td>
<p>Minimum size for a cluster to appear in the visual
display. Defaults to 10</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_group.names">group.names</code></td>
<td>
<p>The title for the legend's key if 'aest' is specified.</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_xlab">xlab</code></td>
<td>
<p>Name of the 'xlab'. Defaults to &quot;x: tSNE(X)&quot;</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_ylab">ylab</code></td>
<td>
<p>Name of the 'ylab'. Defaults to &quot;y: tSNE(X)&quot;</p>
</td></tr>
<tr><td><code id="tsne2clus_+3A_clus">clus</code></td>
<td>
<p>Should we do clustering? Defaults to TRUE. If false, only
point aesthetics are applied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes the outcome of pca2tsne (or a list containing any
two-columns matrix) and finds clusters via DBSCAN.
It extends code from the MEREDITH (Taskesen et al. 2016) and
clValid (Datta &amp; Datta, 2018) R packages to tune DBSCAN parameters
with Silhouette or
Biological Homogeneity indexes.
</p>


<h3>Value</h3>

<p>A list with the results of the DBSCAN
clustering and (if argument 'plot'=TRUE) the corresponding
graphical displays.
</p>
<ul>
<li><p> dbscan.res: a list with the results of the (sparse) SVD,
containing:
</p>

<ul>
<li><p> cluster: Cluster partition.
</p>
</li>
<li><p> eps: Optimal eps according to the Silhouette or Biological
Homogeneity indexes criteria.
</p>
</li>
<li><p> SIL: Maximum peak in the trajectory of the Silhouette index.
</p>
</li>
<li><p> BHI: Maximum peak in the trajectory of the Biological
Homogeneity index.
</p>
</li></ul>

</li>
<li><p> clusters.plot: A ggplot object with the clusters' graphical display.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Ester, Martin, Martin Ester, Hans-Peter Kriegel,
Jorg Sander, and Xiaowei Xu. 1996.
&quot;A Density-Based Algorithm for Discovering Clusters in
Large Spatial Databases with Noise,&quot; 226_231.
</p>
</li>
<li><p> Hahsler, Michael, and Matthew Piekenbrock. 2017.
&quot;Dbscan: Density Based Clustering of Applications with Noise
(DBSCAN) and Related Algorithms.&quot;
https://cran.r-project.org/package=dbscan.
</p>
</li>
<li><p> Datta, Susmita, and Somnath Datta. 2006. Methods for
Evaluating Clustering Algorithms for Gene Expression Data
Using a Reference Set of Functional Classes.
BMC Bioinformatics 7 (1). BioMed Central:397.
</p>
</li>
<li><p> Taskesen, Erdogan, Sjoerd M. H. Huisman, Ahmed Mahfouz,
Jesse H. Krijthe, Jeroen de Ridder, Anja van de Stolpe,
Erik van den Akker, Wim Verheagh, and Marcel J. T. Reinders. 2016.
Pan-Cancer Subtyping in a 2D-Map Shows Substructures
That Are Driven by Specific Combinations of Molecular
Characteristics. Scientific Reports 6 (1):24949.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(MOSS)
library(viridis)
library(cluster)
library(annotate)

# Using the 'iris' data tow show cluster definition via BHI criterion.
set.seed(42)
data(iris)
# Scaling columns.
X &lt;- scale(iris[, -5])
# Calling pca2tsne to map the three variables onto a 2-D map.
Z &lt;- pca2tsne(X, perp = 30, n.samples = 1, n.iter = 1000)
# Using 'species' as previous knoledge to identify clusters.
ann &lt;- model.matrix(~ -1 + iris[, 5])
# Getting clusters.
tsne2clus(Z,
  ann = ann,
  labels = iris[, 5],
  aest = aest.f(iris[, 5]),
  group.names = "Species",
  eps_range = c(0, 3)
)

# Example of usage within moss.
set.seed(43)
sim_blocks &lt;- simulate_data()$sim_blocks
out &lt;- moss(sim_blocks[-4],
  tSNE = TRUE,
  cluster = list(eps_range = c(0, 4), eps_res = 100, min_clus_size = 1),
  plot = TRUE
)
out$clus_plot
out$clusters_vs_PCs

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
