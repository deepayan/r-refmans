<!DOCTYPE html><html><head><title>Help for package ROCR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ROCR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#performance'><p>Function to create performance objects</p></a></li>
<li><a href='#performance-class'><p>Class <code>performance</code></p></a></li>
<li><a href='#plot-methods'><p>Plot method for performance objects</p></a></li>
<li><a href='#prediction'><p>Function to create prediction objects</p></a></li>
<li><a href='#prediction-class'><p>Class <code>prediction</code></p></a></li>
<li><a href='#ROCR.hiv'><p>Data set: Support vector machines and neural networks applied to the</p>
prediction of HIV-1 coreceptor usage</a></li>
<li><a href='#ROCR.simple'><p>Data set: Simple artificial prediction data for use with ROCR</p></a></li>
<li><a href='#ROCR.xval'><p>Data set: Artificial cross-validation data for use with ROCR</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0-11</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-05-01</td>
</tr>
<tr>
<td>Title:</td>
<td>Visualizing the Performance of Scoring Classifiers</td>
</tr>
<tr>
<td>Description:</td>
<td>ROC graphs, sensitivity/specificity curves, lift charts,
  and precision/recall plots are popular examples of trade-off
  visualizations for specific pairs of performance measures. ROCR is a
  flexible tool for creating cutoff-parameterized 2D performance curves
  by freely combining two from over 25 performance measures (new
  performance measures can be added using a standard interface).
  Curves from different cross-validation or bootstrapping runs can be
  averaged by different methods, and standard deviations, standard
  errors or box plots can be used to visualize the variability across
  the runs. The parameterization can be visualized by printing cutoff
  values at the corresponding curve positions, or by coloring the
  curve according to cutoff. All components of a performance plot can
  be quickly adjusted using a flexible parameter dispatching
  mechanism. Despite its flexibility, ROCR is easy to use, with only
  three commands and reasonable default values for all optional
  parameters.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, graphics, grDevices, gplots, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://ipa-tys.github.io/ROCR/">http://ipa-tys.github.io/ROCR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ipa-tys/ROCR/issues">https://github.com/ipa-tys/ROCR/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-05-01 11:43:23 UTC; flixr</td>
</tr>
<tr>
<td>Author:</td>
<td>Tobias Sing [aut],
  Oliver Sander [aut],
  Niko Beerenwinkel [aut],
  Thomas Lengauer [aut],
  Thomas Unterthiner [ctb],
  Felix G.M. Ernst <a href="https://orcid.org/0000-0001-5064-0928"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Felix G.M. Ernst &lt;felix.gm.ernst@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-05-02 14:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='performance'>Function to create performance objects</h2><span id='topic+performance'></span>

<h3>Description</h3>

<p>All kinds of predictor evaluations are performed using this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance(prediction.obj, measure, x.measure = "cutoff", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_+3A_prediction.obj">prediction.obj</code></td>
<td>
<p>An object of class <code>prediction</code>.</p>
</td></tr>
<tr><td><code id="performance_+3A_measure">measure</code></td>
<td>
<p>Performance measure to use for the evaluation. A complete list
of the performance measures that are available for <code>measure</code> and
<code>x.measure</code> is given in the 'Details' section.</p>
</td></tr>
<tr><td><code id="performance_+3A_x.measure">x.measure</code></td>
<td>
<p>A second performance measure. If different from the default,
a two-dimensional curve, with <code>x.measure</code> taken to be the unit in
direction of the x axis, and <code>measure</code> to be the unit in direction of
the y axis, is created. This curve is parametrized with the cutoff.</p>
</td></tr>
<tr><td><code id="performance_+3A_...">...</code></td>
<td>
<p>Optional arguments (specific to individual performance measures).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here is the list of available performance measures. Let Y and
<code class="reqn">\hat{Y}</code> be random variables representing the class and the prediction for
a randomly drawn sample, respectively. We denote by
<code class="reqn">\oplus</code> and <code class="reqn">\ominus</code> the positive and
negative class, respectively. Further, we use the following
abbreviations for empirical quantities: P (\# positive
samples), N (\# negative samples), TP (\# true positives), TN (\# true
negatives), FP (\# false positives), FN (\# false negatives).
</p>

<dl>
<dt><code>acc</code>:</dt><dd><p>Accuracy. <code class="reqn">P(\hat{Y}=Y)</code>. Estimated
as: <code class="reqn">\frac{TP+TN}{P+N}</code>.</p>
</dd>
<dt><code>err</code>:</dt><dd><p>Error rate. <code class="reqn">P(\hat{Y}\ne Y)</code>. Estimated as: <code class="reqn">\frac{FP+FN}{P+N}</code>.</p>
</dd>
<dt><code>fpr</code>:</dt><dd><p>False positive rate. <code class="reqn">P(\hat{Y}=\oplus | Y =
                                                   \ominus)</code>. Estimated as:
<code class="reqn">\frac{FP}{N}</code>.</p>
</dd>
<dt><code>fall</code>:</dt><dd><p>Fallout. Same as <code>fpr</code>.</p>
</dd>
<dt><code>tpr</code>:</dt><dd><p>True positive
rate. <code class="reqn">P(\hat{Y}=\oplus|Y=\oplus)</code>. Estimated
as: <code class="reqn">\frac{TP}{P}</code>.</p>
</dd>
<dt><code>rec</code>:</dt><dd><p>Recall. Same as <code>tpr</code>.</p>
</dd>
<dt><code>sens</code>:</dt><dd><p>Sensitivity. Same as <code>tpr</code>.</p>
</dd>
<dt><code>fnr</code>:</dt><dd><p>False negative
rate. <code class="reqn">P(\hat{Y}=\ominus|Y=\oplus)</code>. Estimated as: <code class="reqn">\frac{FN}{P}</code>.</p>
</dd>
<dt><code>miss</code>:</dt><dd><p>Miss. Same as <code>fnr</code>.</p>
</dd>
<dt><code>tnr</code>:</dt><dd><p>True negative rate. <code class="reqn">P(\hat{Y} =
                                                  \ominus|Y=\ominus)</code>.</p>
</dd>
<dt><code>spec</code>:</dt><dd><p>Specificity. Same as <code>tnr</code>.</p>
</dd>
<dt><code>ppv</code>:</dt><dd><p>Positive predictive
value. <code class="reqn">P(Y=\oplus|\hat{Y}=\oplus)</code>. Estimated as: <code class="reqn">\frac{TP}{TP+FP}</code>.</p>
</dd>
<dt><code>prec</code>:</dt><dd><p>Precision. Same as <code>ppv</code>.</p>
</dd>
<dt><code>npv</code>:</dt><dd><p>Negative predictive
value. <code class="reqn">P(Y=\ominus|\hat{Y}=\ominus)</code>. Estimated as: <code class="reqn">\frac{TN}{TN+FN}</code>.</p>
</dd>
<dt><code>pcfall</code>:</dt><dd><p>Prediction-conditioned
fallout. <code class="reqn">P(Y=\ominus|\hat{Y}=\oplus)</code>. Estimated as: <code class="reqn">\frac{FP}{TP+FP}</code>.</p>
</dd>
<dt><code>pcmiss</code>:</dt><dd><p>Prediction-conditioned
miss. <code class="reqn">P(Y=\oplus|\hat{Y}=\ominus)</code>. Estimated as: <code class="reqn">\frac{FN}{TN+FN}</code>.</p>
</dd>
<dt><code>rpp</code>:</dt><dd><p>Rate of positive predictions. <code class="reqn">P( \hat{Y} =
                                                           \oplus)</code>. Estimated as: (TP+FP)/(TP+FP+TN+FN).</p>
</dd>
<dt><code>rnp</code>:</dt><dd><p>Rate of negative predictions. <code class="reqn">P( \hat{Y} =
                                                           \ominus)</code>. Estimated as: (TN+FN)/(TP+FP+TN+FN).</p>
</dd>
<dt><code>phi</code>:</dt><dd><p>Phi correlation coefficient. <code class="reqn">\frac{TP \cdot
   TN - FP \cdot FN}{\sqrt{ (TP+FN) \cdot (TN+FP) \cdot (TP+FP)
     \cdot (TN+FN)}}</code>. Yields a
number between -1 and 1, with 1 indicating a perfect
prediction, 0 indicating a random prediction. Values below 0
indicate a worse than random prediction.</p>
</dd>
<dt><code>mat</code>:</dt><dd><p>Matthews correlation coefficient. Same as <code>phi</code>.</p>
</dd>
<dt><code>mi</code>:</dt><dd><p>Mutual information. <code class="reqn">I(\hat{Y},Y) := H(Y) -
     H(Y|\hat{Y})</code>, where H is the
(conditional) entropy. Entropies are estimated naively (no bias
correction).</p>
</dd>
<dt><code>chisq</code>:</dt><dd><p>Chi square test statistic. <code>?chisq.test</code>
for details. Note that R might raise a warning if the sample size
is too small.</p>
</dd>
<dt><code>odds</code>:</dt><dd><p>Odds ratio. <code class="reqn">\frac{TP \cdot TN}{FN \cdot
   FP}</code>. Note that odds ratio produces
Inf or NA values for all cutoffs corresponding to FN=0 or
FP=0. This can substantially decrease the plotted cutoff region.</p>
</dd>
<dt><code>lift</code>:</dt><dd><p>Lift
value. <code class="reqn">\frac{P(\hat{Y}=\oplus|Y=\oplus)}{P(\hat{Y}=\oplus)}</code>.</p>
</dd>
<dt><code>f</code>:</dt><dd><p>Precision-recall F measure (van Rijsbergen, 1979). Weighted
harmonic mean of precision (P) and recall (R). <code class="reqn">F =
     \frac{1}{\alpha \frac{1}{P} + (1-\alpha)\frac{1}{R}}</code>. If
<code class="reqn">\alpha=\frac{1}{2}</code>, the mean is balanced. A
frequent equivalent formulation is
<code class="reqn">F = \frac{(\beta^2+1) \cdot P \cdot R}{R + \beta^2 \cdot
     P}</code>. In this formulation, the
mean is balanced if <code class="reqn">\beta=1</code>. Currently, ROCR only accepts
the alpha version as input (e.g. <code class="reqn">\alpha=0.5</code>). If no 
value for alpha is given, the mean will be balanced by default.</p>
</dd>
<dt><code>rch</code>:</dt><dd><p>ROC convex hull. A ROC (=<code>tpr</code> vs <code>fpr</code>) curve 
with concavities (which represent suboptimal choices of cutoff) removed 
(Fawcett 2001). Since the result is already a parametric performance 
curve, it cannot be used in combination with other measures.</p>
</dd>
<dt><code>auc</code>:</dt><dd><p>Area under the ROC curve. This is equal to the value of the
Wilcoxon-Mann-Whitney test statistic and also the probability that the
classifier will score are randomly drawn positive sample higher than a
randomly drawn negative sample. Since the output of
<code>auc</code> is cutoff-independent, this
measure cannot be combined with other measures into a parametric
curve. The partial area under the ROC curve up to a given false
positive rate can be calculated by passing the optional parameter
<code>fpr.stop=0.5</code> (or any other value between 0 and 1) to 
<code>performance</code>.</p>
</dd>
<dt><code>aucpr</code>:</dt><dd><p>Area under the Precision/Recall curve. Since the output
of <code>aucpr</code> is cutoff-independent, this measure cannot be combined 
with other measures into a parametric curve.</p>
</dd>
<dt><code>prbe</code>:</dt><dd><p>Precision-recall break-even point. The cutoff(s) where
precision and recall are equal. At this point, positive and negative
predictions are made at the same rate as their prevalence in the
data. Since the output of
<code>prbe</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt><code>cal</code>:</dt><dd><p>Calibration error. The calibration error is the
absolute difference between predicted confidence and actual reliability. This
error is estimated at all cutoffs by sliding a window across the
range of possible cutoffs. The default window size of 100 can be
adjusted by passing the optional parameter <code>window.size=200</code>
to <code>performance</code>. E.g., if for several
positive samples the output of the classifier is around 0.75, you might
expect from a well-calibrated classifier that the fraction of them
which is correctly predicted as positive is also around 0.75. In a
well-calibrated classifier, the probabilistic confidence estimates
are realistic. Only for use with
probabilistic output (i.e. scores between 0 and 1).</p>
</dd>
<dt><code>mxe</code>:</dt><dd><p>Mean cross-entropy. Only for use with
probabilistic output. <code class="reqn">MXE :=-\frac{1}{P+N}( \sum_{y_i=\oplus}
                                                   ln(\hat{y}_i) + \sum_{y_i=\ominus} ln(1-\hat{y}_i))</code>. Since the output of
<code>mxe</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt><code>rmse</code>:</dt><dd><p>Root-mean-squared error. Only for use with
numerical class labels. <code class="reqn">RMSE:=\sqrt{\frac{1}{P+N}\sum_i (y_i
                                                                 - \hat{y}_i)^2}</code>. Since the output of
<code>rmse</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt><code>sar</code>:</dt><dd><p>Score combinining performance measures of different
characteristics, in the attempt of creating a more &quot;robust&quot;
measure (cf. Caruana R., ROCAI2004):
SAR = 1/3 * ( Accuracy + Area under the ROC curve + Root
mean-squared error ).</p>
</dd>
<dt><code>ecost</code>:</dt><dd><p>Expected cost. For details on cost curves,
cf. Drummond&amp;Holte 2000,2004. <code>ecost</code> has an obligatory x
axis, the so-called 'probability-cost function'; thus it cannot be
combined with other measures. While using <code>ecost</code> one is
interested in the lower envelope of a set of lines, it might be
instructive to plot the whole set of lines in addition to the lower
envelope. An example is given in <code>demo(ROCR)</code>.</p>
</dd>
<dt><code>cost</code>:</dt><dd><p>Cost of a classifier when
class-conditional misclassification costs are explicitly given.
Accepts the optional parameters <code>cost.fp</code> and
<code>cost.fn</code>, by which the costs for false positives and
negatives can be adjusted, respectively. By default, both are set
to 1.</p>
</dd>
</dl>



<h3>Value</h3>

<p>An S4 object of class <code>performance</code>.
</p>


<h3>Note</h3>

<p>Here is how to call <code>performance()</code> to create some standard
evaluation plots:
</p>

<dl>
<dt>ROC curves:</dt><dd><p>measure=&quot;tpr&quot;, x.measure=&quot;fpr&quot;.</p>
</dd>
<dt>Precision/recall graphs:</dt><dd><p>measure=&quot;prec&quot;, x.measure=&quot;rec&quot;.</p>
</dd>
<dt>Sensitivity/specificity plots:</dt><dd><p>measure=&quot;sens&quot;, x.measure=&quot;spec&quot;.</p>
</dd>
<dt>Lift charts:</dt><dd><p>measure=&quot;lift&quot;, x.measure=&quot;rpp&quot;.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction">prediction</a></code>,
<code><a href="#topic+prediction-class">prediction-class</a></code>,
<code><a href="#topic+performance-class">performance-class</a></code>,
<code><a href="#topic+plot.performance">plot.performance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
data(ROCR.simple)
pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels)
pred
perf &lt;- performance(pred,"tpr","fpr")
perf
plot(perf)

# precision/recall curve (x-axis: recall, y-axis: precision)
perf &lt;- performance(pred, "prec", "rec")
perf
plot(perf)

# sensitivity/specificity curve (x-axis: specificity,
# y-axis: sensitivity)
perf &lt;- performance(pred, "sens", "spec")
perf
plot(perf)
</code></pre>

<hr>
<h2 id='performance-class'>Class <code>performance</code></h2><span id='topic+performance-class'></span>

<h3>Description</h3>

<p>Object to capture the result of a performance evaluation, optionally
collecting evaluations from several cross-validation or bootstrapping runs.
</p>


<h3>Details</h3>

<p>A <code>performance</code> object can capture information from four
different evaluation scenarios:
</p>

<ul>
<li><p> The behaviour of a cutoff-dependent performance measure across
the range of all cutoffs (e.g. <code>performance( predObj, 'acc' )</code> ). Here,
<code>x.values</code> contains the cutoffs, <code>y.values</code> the
corresponding values of the performance measure, and
<code>alpha.values</code> is empty.<br />
</p>
</li>
<li><p> The trade-off between two performance measures across the
range of all cutoffs (e.g. <code>performance( predObj,
                                                  'tpr', 'fpr' )</code> ). In this case, the cutoffs are stored in
<code>alpha.values</code>, while <code>x.values</code> and <code>y.values</code>
contain the corresponding values of the two performance measures.<br />
</p>
</li>
<li><p> A performance measure that comes along with an obligatory
second axis (e.g. <code>performance( predObj, 'ecost' )</code> ). Here, the measure values are
stored in <code>y.values</code>, while the corresponding values of the
obligatory axis are stored in <code>x.values</code>, and <code>alpha.values</code>
is empty.<br />
</p>
</li>
<li><p> A performance measure whose value is just a scalar
(e.g. <code>performance( predObj, 'auc' )</code> ). The value is then stored in
<code>y.values</code>, while <code>x.values</code> and <code>alpha.values</code> are
empty.
</p>
</li></ul>



<h3>Slots</h3>


<dl>
<dt><code>x.name</code></dt><dd><p>Performance measure used for the x axis.</p>
</dd>
<dt><code>y.name</code></dt><dd><p>Performance measure used for the y axis.</p>
</dd>
<dt><code>alpha.name</code></dt><dd><p>Name of the unit that is used to create the parametrized
curve. Currently, curves can only be parametrized by cutoff, so
<code>alpha.name</code> is either <code>none</code> or <code>cutoff</code>.</p>
</dd>
<dt><code>x.values</code></dt><dd><p>A list in which each entry contains the x values of the curve
of this particular cross-validation run. <code>x.values[[i]]</code>,
<code>y.values[[i]]</code>, and <code>alpha.values[[i]]</code> correspond to each
other.</p>
</dd>
<dt><code>y.values</code></dt><dd><p>A list in which each entry contains the y values of the curve
of this particular cross-validation run.</p>
</dd>
<dt><code>alpha.values</code></dt><dd><p>A list in which each entry contains the cutoff values of
the curve of this particular cross-validation run.</p>
</dd>
</dl>


<h3>Objects from the Class</h3>

<p>Objects can be created by using the <code>performance</code> function.
</p>


<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction">prediction</a></code>
<code><a href="#topic+performance">performance</a></code>,
<code><a href="#topic+prediction-class">prediction-class</a></code>,
<code><a href="#topic+plot.performance">plot.performance</a></code>
</p>

<hr>
<h2 id='plot-methods'>Plot method for performance objects</h2><span id='topic+plot-methods'></span><span id='topic+plot+2Cperformance+2Cmissing-method'></span><span id='topic+plot.performance'></span>

<h3>Description</h3>

<p>This is the method to plot all objects of class performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'performance,missing'
plot(
  x,
  y,
  ...,
  avg = "none",
  spread.estimate = "none",
  spread.scale = 1,
  show.spread.at = c(),
  colorize = FALSE,
  colorize.palette = rev(rainbow(256, start = 0, end = 4/6)),
  colorkey = colorize,
  colorkey.relwidth = 0.25,
  colorkey.pos = "right",
  print.cutoffs.at = c(),
  cutoff.label.function = function(x) {     round(x, 2) },
  downsampling = 0,
  add = FALSE
)

## S3 method for class 'performance'
plot(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot-methods_+3A_x">x</code></td>
<td>
<p>an object of class <code>performance</code></p>
</td></tr>
<tr><td><code id="plot-methods_+3A_y">y</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_...">...</code></td>
<td>
<p>Optional graphical parameters to adjust different components of
the performance plot. Parameters are directed to their target component by
prefixing them with the name of the component (<code>component.parameter</code>,
e.g. <code>text.cex</code>). The following components are available:
<code>xaxis</code>, <code>yaxis</code>, <code>coloraxis</code>, <code>box</code> (around the
plotting region), <code>points</code>, <code>text</code>, <code>plotCI</code> (error bars),
<code>boxplot</code>. The names of these components are influenced by the R
functions that are used to create them. Thus, <code>par(component)</code> can be
used to see which parameters are available for a given component (with the
expection of the three axes; use <code>par(axis)</code> here). To adjust the
canvas or the performance curve(s), the standard <code>plot</code> parameters can
be used without any prefix.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_avg">avg</code></td>
<td>
<p>If the performance object describes several curves (from
cross-validation runs or bootstrap evaluations of one particular method),
the curves from each of the runs can be averaged. Allowed values are
<code>none</code> (plot all curves separately), <code>horizontal</code> (horizontal
averaging), <code>vertical</code> (vertical averaging), and <code>threshold</code>
(threshold (=cutoff) averaging). Note that while threshold averaging is
always feasible, vertical and horizontal averaging are not well-defined if
the graph cannot be represented as a function x-&gt;y and y-&gt;x, respectively.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_spread.estimate">spread.estimate</code></td>
<td>
<p>When curve averaging is enabled, the variation around
the average curve can be visualized as standard error bars
(<code>stderror</code>), standard deviation bars (<code>stddev</code>), or by using box
plots (<code>boxplot</code>). Note that the function <code>plotCI</code>, which is used
internally by ROCR to draw error bars, might raise a warning if the spread
of the curves at certain positions is 0.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_spread.scale">spread.scale</code></td>
<td>
<p>For <code>stderror</code> or <code>stddev</code>, this is a scalar
factor to be multiplied with the length of the standard error/deviation
bar. For example, under normal assumptions, <code>spread.scale=2</code> can be
used to get approximate 95% confidence intervals.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_show.spread.at">show.spread.at</code></td>
<td>
<p>For vertical averaging, this vector determines the x
positions for which the spread estimates should be visualized. In contrast,
for horizontal and threshold averaging, the y positions and cutoffs are
determined, respectively. By default, spread estimates are shown at 11
equally spaced positions.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_colorize">colorize</code></td>
<td>
<p>This logical determines whether the curve(s) should be
colorized according to cutoff.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_colorize.palette">colorize.palette</code></td>
<td>
<p>If curve colorizing is enabled, this determines the
color palette onto which the cutoff range is mapped.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_colorkey">colorkey</code></td>
<td>
<p>If true, a color key is drawn into the 4% border
region (default of <code>par(xaxs)</code> and <code>par(yaxs)</code>) of the
plot. The color key visualizes the mapping from cutoffs to colors.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_colorkey.relwidth">colorkey.relwidth</code></td>
<td>
<p>Scalar between 0 and 1 that determines the
fraction of the 4% border region that is occupied by the colorkey.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_colorkey.pos">colorkey.pos</code></td>
<td>
<p>Determines if the colorkey is drawn vertically at
the <code>right</code> side, or horizontally at the <code>top</code> of the
plot.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_print.cutoffs.at">print.cutoffs.at</code></td>
<td>
<p>This vector specifies the cutoffs which should
be printed as text along the curve at the corresponding curve positions.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_cutoff.label.function">cutoff.label.function</code></td>
<td>
<p>By default, cutoff annotations along the curve
or at the color key are rounded to two decimal places before printing.
Using a custom <code>cutoff.label.function</code>, any other transformation can
be performed on the cutoffs instead (e.g. rounding with different precision
or taking the logarithm).</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_downsampling">downsampling</code></td>
<td>
<p>ROCR can efficiently compute most performance measures
even for data sets with millions of elements. However, plotting of large
data sets can be slow and lead to PS/PDF documents of considerable size. In
that case, performance curves that are indistinguishable from the original
can be obtained by using only a fraction of the computed performance
values. Values for downsampling between 0 and 1 indicate the fraction of
the original data set size to which the performance object should be
downsampled, integers above 1 are interpreted as the actual number of
performance values to which the curve(s) should be downsampled.</p>
</td></tr>
<tr><td><code id="plot-methods_+3A_add">add</code></td>
<td>
<p>If <code>TRUE</code>, the curve(s) is/are added to an already existing
plot; otherwise a new plot is drawn.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction">prediction</a></code>,
<code><a href="#topic+performance">performance</a></code>,
<code><a href="#topic+prediction-class">prediction-class</a></code>,
<code><a href="#topic+performance-class">performance-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plotting a ROC curve:
library(ROCR)
data(ROCR.simple)
pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )
pred
perf &lt;- performance( pred, "tpr", "fpr" )
perf
plot( perf )

# To entertain your children, make your plots nicer
# using ROCR's flexible parameter passing mechanisms
# (much cheaper than a finger painting set)
par(bg="lightblue", mai=c(1.2,1.5,1,1))
plot(perf, main="ROCR fingerpainting toolkit", colorize=TRUE,
     xlab="Mary's axis", ylab="", box.lty=7, box.lwd=5,
     box.col="gold", lwd=17, colorkey.relwidth=0.5, xaxis.cex.axis=2,
     xaxis.col='blue', xaxis.col.axis="blue", yaxis.col='green', yaxis.cex.axis=2,
     yaxis.at=c(0,0.5,0.8,0.85,0.9,1), yaxis.las=1, xaxis.lwd=2, yaxis.lwd=3,
     yaxis.col.axis="orange", cex.lab=2, cex.main=2)
</code></pre>

<hr>
<h2 id='prediction'>Function to create prediction objects</h2><span id='topic+prediction'></span>

<h3>Description</h3>

<p>Every classifier evaluation using ROCR starts with creating a
<code>prediction</code> object. This function is used to transform the input data
(which can be in vector, matrix, data frame, or list form) into a
standardized format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction(predictions, labels, label.ordering = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_+3A_predictions">predictions</code></td>
<td>
<p>A vector, matrix, list, or data frame containing the
predictions.</p>
</td></tr>
<tr><td><code id="prediction_+3A_labels">labels</code></td>
<td>
<p>A vector, matrix, list, or data frame containing the true class
labels. Must have the same dimensions as <code>predictions</code>.</p>
</td></tr>
<tr><td><code id="prediction_+3A_label.ordering">label.ordering</code></td>
<td>
<p>The default ordering (cf.details)  of the classes can
be changed by supplying a vector containing the negative and the positive
class label.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predictions</code> and <code>labels</code> can simply be vectors of the same
length. However, in the case of cross-validation data, different
cross-validation runs can be provided as the *columns* of a matrix or
data frame, or as the entries of a list. In the case of a matrix or
data frame, all cross-validation runs must have the same length, whereas
in the case of a list, the lengths can vary across the cross-validation
runs. Internally, as described in section 'Value', all of these input
formats are converted to list representation.
</p>
<p>Since scoring classifiers give relative tendencies towards a negative
(low scores) or positive (high scores) class, it has to be declared
which class label denotes the negative, and which the positive class.
Ideally, labels should be supplied as ordered factor(s), the lower
level corresponding to the negative class, the upper level to the
positive class. If the labels are factors (unordered), numeric,
logical or characters, ordering of the labels is inferred from
R's built-in <code>&lt;</code> relation (e.g. 0 &lt; 1, -1 &lt; 1, 'a' &lt; 'b',
FALSE &lt; TRUE). Use <code>label.ordering</code> to override this default
ordering. Please note that the ordering can be locale-dependent
e.g. for character labels '-1' and '1'.
</p>
<p>Currently, ROCR supports only binary classification (extensions toward
multiclass classification are scheduled for the next release,
however). If there are more than two distinct label symbols, execution
stops with an error message. If all predictions use the same two
symbols that are used for the labels, categorical predictions are
assumed. If there are more than two predicted values, but all numeric,
continuous predictions are assumed (i.e. a scoring
classifier). Otherwise, if more than two symbols occur in the
predictions, and not all of them are numeric, execution stops with an
error message.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>prediction</code>.
</p>


<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction-class">prediction-class</a></code>,
<code><a href="#topic+performance">performance</a></code>,
<code><a href="#topic+performance-class">performance-class</a></code>,
<code><a href="#topic+plot.performance">plot.performance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a simple prediction object
library(ROCR)
data(ROCR.simple)
pred &lt;- prediction(ROCR.simple$predictions,ROCR.simple$labels)
pred
</code></pre>

<hr>
<h2 id='prediction-class'>Class <code>prediction</code></h2><span id='topic+prediction-class'></span>

<h3>Description</h3>

<p>Object to encapsulate numerical predictions together with the
corresponding true class labels, optionally collecting predictions and
labels for several cross-validation or bootstrapping runs.
</p>


<h3>Slots</h3>


<dl>
<dt><code>predictions</code></dt><dd><p>A list, in which each element is a vector of predictions
(the list has length &gt; 1 for x-validation data.</p>
</dd>
<dt><code>labels</code></dt><dd><p>Analogously, a list in which each element is a vector of true
class labels.</p>
</dd>
<dt><code>cutoffs</code></dt><dd><p>A list in which each element is a vector of all necessary
cutoffs. Each cutoff vector consists of the predicted scores (duplicates
removed), in descending order.</p>
</dd>
<dt><code>fp</code></dt><dd><p>A list in which each element is a vector of the number (not the
rate!) of false positives induced by the cutoffs given in the corresponding
'cutoffs' list entry.</p>
</dd>
<dt><code>tp</code></dt><dd><p>As fp, but for true positives.</p>
</dd>
<dt><code>tn</code></dt><dd><p>As fp, but for true negatives.</p>
</dd>
<dt><code>fn</code></dt><dd><p>As fp, but for false negatives.</p>
</dd>
<dt><code>n.pos</code></dt><dd><p>A list in which each element contains the number of positive
samples in the given x-validation run.</p>
</dd>
<dt><code>n.neg</code></dt><dd><p>As n.pos, but for negative samples.</p>
</dd>
<dt><code>n.pos.pred</code></dt><dd><p>A list in which each element is a vector of the number of
samples predicted as positive at the cutoffs given in the corresponding
'cutoffs' entry.</p>
</dd>
<dt><code>n.neg.pred</code></dt><dd><p>As n.pos.pred, but for negatively predicted samples.</p>
</dd>
</dl>


<h3>Objects from the Class</h3>

<p>Objects can be created by using the <code>prediction</code> function.
</p>


<h3>Note</h3>

<p>Every <code>prediction</code> object contains information about the 2x2
contingency table consisting of tp,tn,fp, and fn, along with the
marginal sums n.pos,n.neg,n.pos.pred,n.neg.pred, because these form
the basis for many derived performance measures.
</p>


<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction">prediction</a></code>,
<code><a href="#topic+performance">performance</a></code>,
<code><a href="#topic+performance-class">performance-class</a></code>,
<code><a href="#topic+plot.performance">plot.performance</a></code>
</p>

<hr>
<h2 id='ROCR.hiv'>Data set: Support vector machines and neural networks applied to the
prediction of HIV-1 coreceptor usage</h2><span id='topic+ROCR.hiv'></span>

<h3>Description</h3>

<p>Linear support vector machines (libsvm) and neural networks (R package
nnet) were applied to predict usage of the coreceptors CCR5 and CXCR4
based on sequence data of the third variable loop of the HIV envelope
protein.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ROCR.hiv)
</code></pre>


<h3>Format</h3>

<p>A list consisting of the SVM (<code>ROCR.hiv$hiv.svm</code>) and NN
(<code>ROCR.hiv$hiv.nn</code>) classification data. Each of those is in turn a list
consisting of the two elements <code>$predictions</code> and <code>$labels</code> (10
element list representing cross-validation data).
</p>


<h3>References</h3>

<p>Sing, T. &amp; Beerenwinkel, N. &amp; Lengauer, T.  &quot;Learning mixtures
of localized rules by maximizing the area under the ROC curve&quot;.  1st
International Workshop on ROC Analysis in AI, 89-96, 2004.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ROCR)
data(ROCR.hiv)
attach(ROCR.hiv)
pred.svm &lt;- prediction(hiv.svm$predictions, hiv.svm$labels)
pred.svm
perf.svm &lt;- performance(pred.svm, 'tpr', 'fpr')
perf.svm
pred.nn &lt;- prediction(hiv.nn$predictions, hiv.svm$labels)
pred.nn
perf.nn &lt;- performance(pred.nn, 'tpr', 'fpr')
perf.nn
plot(perf.svm, lty=3, col="red",main="SVMs and NNs for prediction of
HIV-1 coreceptor usage")
plot(perf.nn, lty=3, col="blue",add=TRUE)
plot(perf.svm, avg="vertical", lwd=3, col="red",
     spread.estimate="stderror",plotCI.lwd=2,add=TRUE)
plot(perf.nn, avg="vertical", lwd=3, col="blue",
     spread.estimate="stderror",plotCI.lwd=2,add=TRUE)
legend(0.6,0.6,c('SVM','NN'),col=c('red','blue'),lwd=3)
</code></pre>

<hr>
<h2 id='ROCR.simple'>Data set: Simple artificial prediction data for use with ROCR</h2><span id='topic+ROCR.simple'></span>

<h3>Description</h3>

<p>A mock data set containing a simple set of predictions and corresponding
class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ROCR.simple)
</code></pre>


<h3>Format</h3>

<p>A two element list. The first element, <code>ROCR.simple$predictions</code>, is a
vector of numerical predictions. The second element,
<code>ROCR.simple$labels</code>, is a vector of corresponding class labels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plot a ROC curve for a single prediction run
# and color the curve according to cutoff.
library(ROCR)
data(ROCR.simple)
pred &lt;- prediction(ROCR.simple$predictions, ROCR.simple$labels)
pred
perf &lt;- performance(pred,"tpr","fpr")
perf
plot(perf,colorize=TRUE)
</code></pre>

<hr>
<h2 id='ROCR.xval'>Data set: Artificial cross-validation data for use with ROCR</h2><span id='topic+ROCR.xval'></span>

<h3>Description</h3>

<p>A mock data set containing 10 sets of predictions and corresponding labels as
would be obtained from 10-fold cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ROCR.xval)
</code></pre>


<h3>Format</h3>

<p>A two element list. The first element, <code>ROCR.xval$predictions</code>, is
itself a 10 element list. Each of these 10 elements is a vector of numerical
predictions for each cross-validation run. Likewise, the second list entry,
<code>ROCR.xval$labels</code> is a 10 element list in which each element is a
vector of true class labels corresponding to the predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plot ROC curves for several cross-validation runs (dotted
# in grey), overlaid by the vertical average curve and boxplots
# showing the vertical spread around the average.
library(ROCR)
data(ROCR.xval)
pred &lt;- prediction(ROCR.xval$predictions, ROCR.xval$labels)
pred
perf &lt;- performance(pred,"tpr","fpr")
perf
plot(perf,col="grey82",lty=3)
plot(perf,lwd=3,avg="vertical",spread.estimate="boxplot",add=TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
