<!DOCTYPE html><html><head><title>Help for package daltoolbox</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {daltoolbox}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#[.ts_data'><p>Extract a subset of a time series stored in an object</p></a></li>
<li><a href='#action'><p>Action</p></a></li>
<li><a href='#action.dal_transform'><p>Action implementation for transform</p></a></li>
<li><a href='#adjust_class_label'><p>adjust categorical mapping</p></a></li>
<li><a href='#adjust_data.frame'><p>Adjust to data frame</p></a></li>
<li><a href='#adjust_factor'><p>adjust factors</p></a></li>
<li><a href='#adjust_matrix'><p>adjust to matrix</p></a></li>
<li><a href='#adjust_ts_data'><p>adjust <code>ts_data</code></p></a></li>
<li><a href='#autoenc_encode'><p>Autoencoder - Encode</p></a></li>
<li><a href='#autoenc_encode_decode'><p>Autoencoder - Encode</p></a></li>
<li><a href='#Boston'><p>Boston Housing Data (Regression)</p></a></li>
<li><a href='#categ_mapping'><p>Categorical mapping</p></a></li>
<li><a href='#cla_dtree'><p>Decision Tree for classification</p></a></li>
<li><a href='#cla_knn'><p>K Nearest Neighbor Classification</p></a></li>
<li><a href='#cla_majority'><p>Majority Classification</p></a></li>
<li><a href='#cla_mlp'><p>MLP for classification</p></a></li>
<li><a href='#cla_nb'><p>Naive Bayes Classifier</p></a></li>
<li><a href='#cla_rf'><p>Random Forest for classification</p></a></li>
<li><a href='#cla_svm'><p>SVM for classification</p></a></li>
<li><a href='#cla_tune'><p>Classification Tune</p></a></li>
<li><a href='#classification'><p>classification</p></a></li>
<li><a href='#clu_tune'><p>Clustering Tune</p></a></li>
<li><a href='#cluster'><p>Cluster</p></a></li>
<li><a href='#cluster_dbscan'><p>DBSCAN</p></a></li>
<li><a href='#cluster_kmeans'><p>k-means</p></a></li>
<li><a href='#cluster_pam'><p>PAM</p></a></li>
<li><a href='#clusterer'><p>Clusterer</p></a></li>
<li><a href='#dal_base'><p>Class dal_base</p></a></li>
<li><a href='#dal_learner'><p>DAL Learner</p></a></li>
<li><a href='#dal_transform'><p>DAL Transform</p></a></li>
<li><a href='#dal_tune'><p>DAL Tune</p></a></li>
<li><a href='#data_sample'><p>Data Sample</p></a></li>
<li><a href='#do_fit'><p>do fit for time series</p></a></li>
<li><a href='#do_predict'><p>do predict for time series</p></a></li>
<li><a href='#dt_pca'><p>PCA</p></a></li>
<li><a href='#evaluate'><p>evaluate</p></a></li>
<li><a href='#fit'><p>Fit</p></a></li>
<li><a href='#fit_curvature_max'><p>maximum curvature analysis</p></a></li>
<li><a href='#fit_curvature_min'><p>minimum curvature analysis</p></a></li>
<li><a href='#fit.cla_tune'><p>tune hyperparameters of ml model</p></a></li>
<li><a href='#fit.cluster_dbscan'><p>fit dbscan model</p></a></li>
<li><a href='#inverse_transform'><p>Inverse Transform</p></a></li>
<li><a href='#k_fold'><p>k-fold sampling</p></a></li>
<li><a href='#minmax'><p>min-max normalization</p></a></li>
<li><a href='#MSE.ts'><p>MSE</p></a></li>
<li><a href='#outliers'><p>Outliers</p></a></li>
<li><a href='#plot_bar'><p>plot bar graph</p></a></li>
<li><a href='#plot_boxplot'><p>plot boxplot</p></a></li>
<li><a href='#plot_boxplot_class'><p>plot boxplot per class</p></a></li>
<li><a href='#plot_density'><p>plot density</p></a></li>
<li><a href='#plot_density_class'><p>plot density per class</p></a></li>
<li><a href='#plot_groupedbar'><p>plot grouped bar</p></a></li>
<li><a href='#plot_hist'><p>plot histogram</p></a></li>
<li><a href='#plot_lollipop'><p>plot lollipop</p></a></li>
<li><a href='#plot_pieplot'><p>plot pie</p></a></li>
<li><a href='#plot_points'><p>plot points</p></a></li>
<li><a href='#plot_radar'><p>plot radar</p></a></li>
<li><a href='#plot_scatter'><p>scatter graph</p></a></li>
<li><a href='#plot_series'><p>plot series</p></a></li>
<li><a href='#plot_stackedbar'><p>plot stacked bar</p></a></li>
<li><a href='#plot_ts'><p>Plot a time series chart</p></a></li>
<li><a href='#plot_ts_pred'><p>Plot a time series chart</p></a></li>
<li><a href='#predictor'><p>DAL Predict</p></a></li>
<li><a href='#R2.ts'><p>R2</p></a></li>
<li><a href='#reg_dtree'><p>Decision Tree for regression</p></a></li>
<li><a href='#reg_knn'><p>knn regression</p></a></li>
<li><a href='#reg_mlp'><p>MLP for regression</p></a></li>
<li><a href='#reg_rf'><p>Random Forest for regression</p></a></li>
<li><a href='#reg_svm'><p>SVM for regression</p></a></li>
<li><a href='#reg_tune'><p>Regression Tune</p></a></li>
<li><a href='#regression'><p>Regression</p></a></li>
<li><a href='#sample_random'><p>Sample Random</p></a></li>
<li><a href='#sample_stratified'><p>sample_stratified</p></a></li>
<li><a href='#select_hyper'><p>Selection hyper parameters</p></a></li>
<li><a href='#select_hyper.cla_tune'><p>selection of hyperparameters</p></a></li>
<li><a href='#select_hyper.ts_tune'><p>selection of hyperparameters (time series)</p></a></li>
<li><a href='#set_params'><p>Assign parameters</p></a></li>
<li><a href='#set_params.default'><p>Assign parameters</p></a></li>
<li><a href='#sin_data'><p>Time series example dataset</p></a></li>
<li><a href='#sMAPE.ts'><p>sMAPE</p></a></li>
<li><a href='#smoothing'><p>Smoothing</p></a></li>
<li><a href='#smoothing_cluster'><p>Smoothing by cluster</p></a></li>
<li><a href='#smoothing_freq'><p>Smoothing by Freq</p></a></li>
<li><a href='#smoothing_inter'><p>Smoothing by interval</p></a></li>
<li><a href='#train_test'><p>training and test</p></a></li>
<li><a href='#train_test_from_folds'><p>k-fold training and test partition object</p></a></li>
<li><a href='#transform'><p>Transform</p></a></li>
<li><a href='#ts_arima'><p>ARIMA</p></a></li>
<li><a href='#ts_conv1d'><p>Conv1D</p></a></li>
<li><a href='#ts_data'><p>ts_data</p></a></li>
<li><a href='#ts_elm'><p>ELM</p></a></li>
<li><a href='#ts_head'><p>ts_head</p></a></li>
<li><a href='#ts_knn'><p>knn time series prediction</p></a></li>
<li><a href='#ts_lstm'><p>LSTM</p></a></li>
<li><a href='#ts_mlp'><p>MLP</p></a></li>
<li><a href='#ts_norm_an'><p>Time Series Adaptive Normalization</p></a></li>
<li><a href='#ts_norm_diff'><p>Time Series Diff</p></a></li>
<li><a href='#ts_norm_ean'><p>Time Series Adaptive Normalization (Exponential Moving Average - EMA)</p></a></li>
<li><a href='#ts_norm_gminmax'><p>Time Series Global Min-Max</p></a></li>
<li><a href='#ts_norm_swminmax'><p>Time Series Sliding Window Min-Max</p></a></li>
<li><a href='#ts_projection'><p>Time Series Projection</p></a></li>
<li><a href='#ts_reg'><p>TSReg</p></a></li>
<li><a href='#ts_regsw'><p>TSRegSW</p></a></li>
<li><a href='#ts_rf'><p>Random Forest</p></a></li>
<li><a href='#ts_sample'><p>Time Series Sample</p></a></li>
<li><a href='#ts_svm'><p>SVM</p></a></li>
<li><a href='#ts_tune'><p>Time Series Tune</p></a></li>
<li><a href='#zscore'><p>z-score normalization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Leveraging Experiment Lines to Data Analytics</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.767</td>
</tr>
<tr>
<td>Description:</td>
<td>The natural increase in the complexity of current research experiments and data demands better tools to enhance productivity in Data Analytics. The package is a framework designed to address the modern challenges in data analytics workflows. The package is inspired by Experiment Line concepts. It aims to provide seamless support for users in developing their data mining workflows by offering a uniform data model and method API. It enables the integration of various data mining activities, including data preprocessing, classification, regression, clustering, and time series prediction. It also offers options for hyper-parameter tuning and supports integration with existing libraries and languages. Overall, the package provides researchers with a comprehensive set of functionalities for data science, promoting ease of use, extensibility, and integration with various tools and libraries. Information on Experiment Line is based on Ogasawara et al. (2009) &lt;<a href="https://doi.org/10.1007%2F978-3-642-02279-1_20">doi:10.1007/978-3-642-02279-1_20</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/cefet-rj-dal/daltoolbox">https://github.com/cefet-rj-dal/daltoolbox</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>FNN, MLmetrics, caret, class, cluster, dbscan, dplyr, e1071,
elmNNRcpp, forecast, ggplot2, nnet, randomForest, reshape,
tree, reticulate</td>
</tr>
<tr>
<td>Config/reticulate:</td>
<td>list( packages = list( list(package = "scipy"),
list(package = "torch"), list(package = "pandas"), list(package
= "numpy"), list(package = "matplotlib"), list(package =
"scikit-learn"), list(package = "functools"), list(package =
"operator"), list(package = "sys") ) )</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-31 00:54:37 UTC; gpca</td>
</tr>
<tr>
<td>Author:</td>
<td>Eduardo Ogasawara <a href="https://orcid.org/0000-0002-0466-0626"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths, cre],
  Antonio Castro [aut, ctb],
  Heraldo Borges [aut, ths],
  Diego Carvalho [aut, ths],
  Joel Santos [aut, ths],
  Eduardo Bezerra [aut, ths],
  Rafaelli Coutinho [aut, ths],
  Federal Center for Technological Education of Rio de Janeiro (CEFET/RJ)
    [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eduardo Ogasawara &lt;eogasawara@ieee.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-31 22:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+5B.ts_data'>Extract a subset of a time series stored in an object</h2><span id='topic++5B.ts_data'></span>

<h3>Description</h3>

<p>Receives as parameters the variables x, i, j ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ts_data'
x[i, j, ...]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B.ts_data_+3A_x">x</code></td>
<td>
<p>input variable</p>
</td></tr>
<tr><td><code id="+2B5B.ts_data_+3A_i">i</code></td>
<td>
<p>row i</p>
</td></tr>
<tr><td><code id="+2B5B.ts_data_+3A_j">j</code></td>
<td>
<p>column j</p>
</td></tr>
<tr><td><code id="+2B5B.ts_data_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new ts_data object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
data10 &lt;- ts_data(sin_data$y, 10)
ts_head(data10)
#single line
data10[12,]

#range of lines
data10[12:13,]

#single column
data10[,1]

#range of columns
data10[,1:2]

#range of rows and columns
data10[12:13,1:2]

#single line and a range of columns
#'data10[12,1:2]

#range of lines and a single column
data10[12:13,1]

#single observation
data10[12,1]
</code></pre>

<hr>
<h2 id='action'>Action</h2><span id='topic+action'></span>

<h3>Description</h3>

<p>Executes the action of model applied in provided data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>action(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="action_+3A_obj">obj</code></td>
<td>
<p>object: a dal_base object to apply the transformation on the input dataset.</p>
</td></tr>
<tr><td><code id="action_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of an action of the model applied in provided data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
# an example is minmax normalization
trans &lt;- minmax()
trans &lt;- fit(trans, iris)
tiris &lt;- action(trans, iris)
</code></pre>

<hr>
<h2 id='action.dal_transform'>Action implementation for transform</h2><span id='topic+action.dal_transform'></span>

<h3>Description</h3>

<p>A default function that defines the action to proxy transform method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dal_transform'
action(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="action.dal_transform_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="action.dal_transform_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Transformed data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?minmax for an example of transformation
</code></pre>

<hr>
<h2 id='adjust_class_label'>adjust categorical mapping</h2><span id='topic+adjust_class_label'></span>

<h3>Description</h3>

<p>vector <code>value</code> is adjusted to a categorical mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_class_label(x, valTrue = 1, valFalse = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_class_label_+3A_x">x</code></td>
<td>
<p>vector to be categorized</p>
</td></tr>
<tr><td><code id="adjust_class_label_+3A_valtrue">valTrue</code></td>
<td>
<p>value to represent true</p>
</td></tr>
<tr><td><code id="adjust_class_label_+3A_valfalse">valFalse</code></td>
<td>
<p>value to represent false</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an adjusted categorical mapping
</p>

<hr>
<h2 id='adjust_data.frame'>Adjust to data frame</h2><span id='topic+adjust_data.frame'></span>

<h3>Description</h3>

<p>dataset data is adjusted to a <code>data.frame</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_data.frame(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_data.frame_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The date argument
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
df &lt;- adjust_data.frame(iris)
</code></pre>

<hr>
<h2 id='adjust_factor'>adjust factors</h2><span id='topic+adjust_factor'></span>

<h3>Description</h3>

<p>vector <code>value</code> is adjusted to a factor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_factor(value, ilevels, slevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_factor_+3A_value">value</code></td>
<td>
<p>vector to be converted into factor</p>
</td></tr>
<tr><td><code id="adjust_factor_+3A_ilevels">ilevels</code></td>
<td>
<p>order for categorical values</p>
</td></tr>
<tr><td><code id="adjust_factor_+3A_slevels">slevels</code></td>
<td>
<p>labels for categorical values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an adjusted factor
</p>

<hr>
<h2 id='adjust_matrix'>adjust to matrix</h2><span id='topic+adjust_matrix'></span>

<h3>Description</h3>

<p>dataset data is adjusted to a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_matrix(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_matrix_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an adjusted matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
mat &lt;- adjust_matrix(iris)
</code></pre>

<hr>
<h2 id='adjust_ts_data'>adjust <code>ts_data</code></h2><span id='topic+adjust_ts_data'></span>

<h3>Description</h3>

<p>dataset data is adjusted to a <code>ts_data</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_ts_data(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust_ts_data_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an adjusted <code>ts_data</code>
</p>

<hr>
<h2 id='autoenc_encode'>Autoencoder - Encode</h2><span id='topic+autoenc_encode'></span>

<h3>Description</h3>

<p>Creates an deep learning autoencoder to encode a sequence of observations.
It wraps the pytorch library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoenc_encode(
  input_size,
  encoding_size,
  batch_size = 32,
  num_epochs = 1000,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoenc_encode_+3A_input_size">input_size</code></td>
<td>
<p>input size</p>
</td></tr>
<tr><td><code id="autoenc_encode_+3A_encoding_size">encoding_size</code></td>
<td>
<p>encoding size</p>
</td></tr>
<tr><td><code id="autoenc_encode_+3A_batch_size">batch_size</code></td>
<td>
<p>size for batch learning</p>
</td></tr>
<tr><td><code id="autoenc_encode_+3A_num_epochs">num_epochs</code></td>
<td>
<p>number of epochs for training</p>
</td></tr>
<tr><td><code id="autoenc_encode_+3A_learning_rate">learning_rate</code></td>
<td>
<p>learning rate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>autoenc_encode</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See example at https://nbviewer.org/github/cefet-rj-dal/daltoolbox-examples
</code></pre>

<hr>
<h2 id='autoenc_encode_decode'>Autoencoder - Encode</h2><span id='topic+autoenc_encode_decode'></span>

<h3>Description</h3>

<p>Creates an deep learning autoencoder to encode a sequence of observations.
It wraps the pytorch library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoenc_encode_decode(
  input_size,
  encoding_size,
  batch_size = 32,
  num_epochs = 1000,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoenc_encode_decode_+3A_input_size">input_size</code></td>
<td>
<p>input size</p>
</td></tr>
<tr><td><code id="autoenc_encode_decode_+3A_encoding_size">encoding_size</code></td>
<td>
<p>encoding size</p>
</td></tr>
<tr><td><code id="autoenc_encode_decode_+3A_batch_size">batch_size</code></td>
<td>
<p>size for batch learning</p>
</td></tr>
<tr><td><code id="autoenc_encode_decode_+3A_num_epochs">num_epochs</code></td>
<td>
<p>number of epochs for training</p>
</td></tr>
<tr><td><code id="autoenc_encode_decode_+3A_learning_rate">learning_rate</code></td>
<td>
<p>learning rate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>autoenc_encode_decode</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See example at https://nbviewer.org/github/cefet-rj-dal/daltoolbox-examples
</code></pre>

<hr>
<h2 id='Boston'>Boston Housing Data (Regression)</h2><span id='topic+Boston'></span>

<h3>Description</h3>

<p>housing values in suburbs of Boston.
</p>

<ul>
<li><p> crim: per capita crime rate by town.
</p>
</li>
<li><p> zn: proportion of residential land zoned for lots over 25,000 sq.ft.
</p>
</li>
<li><p> indus: proportion of non-retail business acres per town
</p>
</li>
<li><p> chas: Charles River dummy variable (= 1 if tract bounds)
</p>
</li>
<li><p> nox: nitric oxides concentration (parts per 10 million)
</p>
</li>
<li><p> rm: average number of rooms per dwelling
</p>
</li>
<li><p> age: proportion of owner-occupied units built prior to 1940
</p>
</li>
<li><p> dis: weighted distances to five Boston employment centres
</p>
</li>
<li><p> rad: index of accessibility to radial highways
</p>
</li>
<li><p> tax: full-value property-tax rate per $10,000
</p>
</li>
<li><p> ptratio: pupil-teacher ratio by town
</p>
</li>
<li><p> black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
</p>
</li>
<li><p> lstat: percentage of lower status of the population
</p>
</li>
<li><p> medv: Median value of owner-occupied homes in $1000's
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(Boston)
</code></pre>


<h3>Format</h3>

<p>Regression Dataset.
</p>


<h3>Source</h3>

<p>This dataset was obtained from the MASS library.
</p>


<h3>References</h3>

<p>Creator: Harrison, D. and Rubinfeld, D.L.
Hedonic prices and the demand for clean air, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
head(Boston)
</code></pre>

<hr>
<h2 id='categ_mapping'>Categorical mapping</h2><span id='topic+categ_mapping'></span>

<h3>Description</h3>

<p>Categorical mapping provides a way to map the levels of a categorical variable to new values.
Each possible value is converted to a binary attribute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categ_mapping(attribute)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="categ_mapping_+3A_attribute">attribute</code></td>
<td>
<p>attribute to be categorized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with binary attributes, one for each possible category.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cm &lt;- categ_mapping("Species")
iris_cm &lt;- transform(cm, iris)

# can be made in a single column
species &lt;- iris[,"Species", drop=FALSE]
iris_cm &lt;- transform(cm, species)
</code></pre>

<hr>
<h2 id='cla_dtree'>Decision Tree for classification</h2><span id='topic+cla_dtree'></span>

<h3>Description</h3>

<p>Creates a classification object that
uses the Decision Tree algorithm for classification.
It wraps the tree library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_dtree(attribute, slevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_dtree_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building.</p>
</td></tr>
<tr><td><code id="cla_dtree_+3A_slevels">slevels</code></td>
<td>
<p>The possible values for the target classification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A classification object that uses the
Decision Tree algorithm for classification.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_dtree("Species", slevels)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_knn'>K Nearest Neighbor Classification</h2><span id='topic+cla_knn'></span>

<h3>Description</h3>

<p>Classifies using the K-Nearest Neighbor algorithm.
It wraps the class library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_knn(attribute, slevels, k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_knn_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building.</p>
</td></tr>
<tr><td><code id="cla_knn_+3A_slevels">slevels</code></td>
<td>
<p>Possible values for the target classification.</p>
</td></tr>
<tr><td><code id="cla_knn_+3A_k">k</code></td>
<td>
<p>A vector of integers indicating the number of neighbors to be considered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A knn object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_knn("Species", slevels, k=3)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_majority'>Majority Classification</h2><span id='topic+cla_majority'></span>

<h3>Description</h3>

<p>This function creates a classification object that uses the majority vote strategy to predict the target attribute. Given a target attribute, the function counts the number of occurrences of each value in the dataset and selects the one that appears most often.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_majority(attribute, slevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_majority_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building.</p>
</td></tr>
<tr><td><code id="cla_majority_+3A_slevels">slevels</code></td>
<td>
<p>Possible values for the target classification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a classification object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_majority("Species", slevels)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_mlp'>MLP for classification</h2><span id='topic+cla_mlp'></span>

<h3>Description</h3>

<p>Creates a classification object that
uses the Multi-Layer Perceptron (MLP) method.
It wraps the nnet library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_mlp(attribute, slevels, size = NULL, decay = 0.1, maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_mlp_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="cla_mlp_+3A_slevels">slevels</code></td>
<td>
<p>possible values for the target classification</p>
</td></tr>
<tr><td><code id="cla_mlp_+3A_size">size</code></td>
<td>
<p>number of nodes that will be used in the hidden layer</p>
</td></tr>
<tr><td><code id="cla_mlp_+3A_decay">decay</code></td>
<td>
<p>how quickly it decreases in gradient descent</p>
</td></tr>
<tr><td><code id="cla_mlp_+3A_maxit">maxit</code></td>
<td>
<p>maximum iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a classification object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_mlp("Species", slevels, size=3, decay=0.03)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_nb'>Naive Bayes Classifier</h2><span id='topic+cla_nb'></span>

<h3>Description</h3>

<p>Classification using the Naive Bayes algorithm
It wraps the e1071 library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_nb(attribute, slevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_nb_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building.</p>
</td></tr>
<tr><td><code id="cla_nb_+3A_slevels">slevels</code></td>
<td>
<p>Possible values for the target classification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A classification object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_nb("Species", slevels)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_rf'>Random Forest for classification</h2><span id='topic+cla_rf'></span>

<h3>Description</h3>

<p>Creates a classification object that
uses the Random Forest method
It wraps the randomForest library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_rf(attribute, slevels, nodesize = 5, ntree = 10, mtry = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_rf_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="cla_rf_+3A_slevels">slevels</code></td>
<td>
<p>possible values for the target classification</p>
</td></tr>
<tr><td><code id="cla_rf_+3A_nodesize">nodesize</code></td>
<td>
<p>node size</p>
</td></tr>
<tr><td><code id="cla_rf_+3A_ntree">ntree</code></td>
<td>
<p>number of trees</p>
</td></tr>
<tr><td><code id="cla_rf_+3A_mtry">mtry</code></td>
<td>
<p>number of attributes to build tree</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_rf("Species", slevels, ntree=5)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_svm'>SVM for classification</h2><span id='topic+cla_svm'></span>

<h3>Description</h3>

<p>Creates a classification object that
uses the Support Vector Machine (SVM) method for classification
It wraps the e1071 library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_svm(attribute, slevels, epsilon = 0.1, cost = 10, kernel = "radial")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_svm_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="cla_svm_+3A_slevels">slevels</code></td>
<td>
<p>possible values for the target classification</p>
</td></tr>
<tr><td><code id="cla_svm_+3A_epsilon">epsilon</code></td>
<td>
<p>parameter that controls the width of the margin around the separating hyperplane</p>
</td></tr>
<tr><td><code id="cla_svm_+3A_cost">cost</code></td>
<td>
<p>parameter that controls the trade-off between having a wide margin and correctly classifying training data points</p>
</td></tr>
<tr><td><code id="cla_svm_+3A_kernel">kernel</code></td>
<td>
<p>the type of kernel function to be used in the SVM algorithm (linear, radial, polynomial, sigmoid)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A SVM classification object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_svm("Species", slevels, epsilon=0.0,cost=20.000)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

prediction &lt;- predict(model, test)
predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='cla_tune'>Classification Tune</h2><span id='topic+cla_tune'></span>

<h3>Description</h3>

<p>Classification Tune
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_tune(base_model, folds = 10, metric = "accuracy")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cla_tune_+3A_base_model">base_model</code></td>
<td>
<p>base model for tuning</p>
</td></tr>
<tr><td><code id="cla_tune_+3A_folds">folds</code></td>
<td>
<p>number of folds for cross-validation</p>
</td></tr>
<tr><td><code id="cla_tune_+3A_metric">metric</code></td>
<td>
<p>metric used to optimize</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>cla_tune</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, iris)
train &lt;- sr$train
test &lt;- sr$test

# hyper parameter setup
tune &lt;- cla_tune(cla_mlp("Species", levels(iris$Species)))
ranges &lt;- list(size=c(3:5), decay=c(0.1))

# hyper parameter optimization
model &lt;- fit(tune, train, ranges)

# testing optimization
test_prediction &lt;- predict(model, test)
test_predictand &lt;- adjust_class_label(test[,"Species"])
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='classification'>classification</h2><span id='topic+classification'></span>

<h3>Description</h3>

<p>Ancestor class for classification problems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classification(attribute, slevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classification_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="classification_+3A_slevels">slevels</code></td>
<td>

<ul>
<li><p> possible values for the target classification
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>classification object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cla_dtree for a classification example using a decision tree
</code></pre>

<hr>
<h2 id='clu_tune'>Clustering Tune</h2><span id='topic+clu_tune'></span>

<h3>Description</h3>

<p>Clustering Tune
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clu_tune(base_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clu_tune_+3A_base_model">base_model</code></td>
<td>
<p>base model for tuning</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>clu_tune</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)

# fit model
model &lt;- clu_tune(cluster_kmeans(k = 0))
ranges &lt;- list(k = 1:10)
model &lt;- fit(model, iris[,1:4], ranges)
model$k
</code></pre>

<hr>
<h2 id='cluster'>Cluster</h2><span id='topic+cluster'></span>

<h3>Description</h3>

<p>Defines a cluster method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_+3A_obj">obj</code></td>
<td>
<p>a <code>clusterer</code> object.</p>
</td></tr>
<tr><td><code id="cluster_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>clustered data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cluster_kmeans for an example of transformation
</code></pre>

<hr>
<h2 id='cluster_dbscan'>DBSCAN</h2><span id='topic+cluster_dbscan'></span>

<h3>Description</h3>

<p>Creates a clusterer object that
uses the DBSCAN method
It wraps the dbscan library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_dbscan(minPts = 3, eps = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_dbscan_+3A_minpts">minPts</code></td>
<td>
<p>minimum number of points</p>
</td></tr>
<tr><td><code id="cluster_dbscan_+3A_eps">eps</code></td>
<td>
<p>distance value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dbscan object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># setup clustering
model &lt;- cluster_dbscan(minPts = 3)

#load dataset
data(iris)

# build model
model &lt;- fit(model, iris[,1:4])
clu &lt;- cluster(model, iris[,1:4])
table(clu)

# evaluate model using external metric
eval &lt;- evaluate(model, clu, iris$Species)
eval
</code></pre>

<hr>
<h2 id='cluster_kmeans'>k-means</h2><span id='topic+cluster_kmeans'></span>

<h3>Description</h3>

<p>Creates a clusterer object that
uses the k-means method
It wraps the stats library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_kmeans(k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_kmeans_+3A_k">k</code></td>
<td>
<p>The number of clusters to form.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A k-means object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># setup clustering
model &lt;- cluster_kmeans(k=3)

#load dataset
data(iris)

# build model
model &lt;- fit(model, iris[,1:4])
clu &lt;- cluster(model, iris[,1:4])
table(clu)

# evaluate model using external metric
eval &lt;- evaluate(model, clu, iris$Species)
eval
</code></pre>

<hr>
<h2 id='cluster_pam'>PAM</h2><span id='topic+cluster_pam'></span>

<h3>Description</h3>

<p>Creates a clusterer object that
uses the Partition Around Medoids (PAM) method
It wraps the cluster library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_pam(k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_pam_+3A_k">k</code></td>
<td>
<p>The number of clusters to generate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A PAM object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># setup clustering
model &lt;- cluster_pam(k = 3)

#load dataset
data(iris)

# build model
model &lt;- fit(model, iris[,1:4])
clu &lt;- cluster(model, iris[,1:4])
table(clu)

# evaluate model using external metric
eval &lt;- evaluate(model, clu, iris$Species)
eval
</code></pre>

<hr>
<h2 id='clusterer'>Clusterer</h2><span id='topic+clusterer'></span>

<h3>Description</h3>

<p>Ancestor class for clustering problems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterer()
</code></pre>


<h3>Value</h3>

<p>a <code>clusterer</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cluster_kmeans for an example of transformation
</code></pre>

<hr>
<h2 id='dal_base'>Class dal_base</h2><span id='topic+dal_base'></span>

<h3>Description</h3>

<p>The dal_base class is an abstract class for all dal descendants classes. It provides both fit() and action() functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dal_base()
</code></pre>


<h3>Value</h3>

<p>A dal_base object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>trans &lt;- dal_base()
</code></pre>

<hr>
<h2 id='dal_learner'>DAL Learner</h2><span id='topic+dal_learner'></span>

<h3>Description</h3>

<p>A ancestor class for clustering, classification, regression, and time series regression.
It also provides the basis for specialized evaluation of learning performance.
</p>
<p>An example of a learner is a decision tree (cla_dtree)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dal_learner()
</code></pre>


<h3>Value</h3>

<p>a learner
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cla_dtree for a classification example using a decision tree
</code></pre>

<hr>
<h2 id='dal_transform'>DAL Transform</h2><span id='topic+dal_transform'></span>

<h3>Description</h3>

<p>A transformation method applied to a dataset.
If needed, the fit can be called to adjust the transform.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dal_transform()
</code></pre>


<h3>Value</h3>

<p>a <code>dal_transform</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?minmax for an example of transformation
</code></pre>

<hr>
<h2 id='dal_tune'>DAL Tune</h2><span id='topic+dal_tune'></span>

<h3>Description</h3>

<p>Ancestor class for hyper parameter optimization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dal_tune(base_model, folds = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dal_tune_+3A_base_model">base_model</code></td>
<td>
<p>base model for tuning</p>
</td></tr>
<tr><td><code id="dal_tune_+3A_folds">folds</code></td>
<td>
<p>number of folds for cross-validation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>dal_tune</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cla_tune for classification tuning
#See ?reg_tune for regression tuning
#See ?ts_tune for time series tuning
</code></pre>

<hr>
<h2 id='data_sample'>Data Sample</h2><span id='topic+data_sample'></span>

<h3>Description</h3>

<p>The data_sample function in R is used to randomly
sample data from a given data frame. It can be used to obtain
a subset of data for further analysis or modeling.
</p>
<p>Two basic specializations of data_sample are sample_random and sample_stratified.
They provide random sampling and stratified sampling, respectively.
</p>
<p>Data sample provides both training and testing partitioning (train_test) and
k-fold partitioning (k_fold) of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_sample()
</code></pre>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using random sampling
sample &lt;- sample_random()
tt &lt;- train_test(sample, iris)

# distribution of train
table(tt$train$Species)

# preparing dataset into four folds
folds &lt;- k_fold(sample, iris, 4)

# distribution of folds
tbl &lt;- NULL
for (f in folds) {
 tbl &lt;- rbind(tbl, table(f$Species))
}
head(tbl)
</code></pre>

<hr>
<h2 id='do_fit'>do fit for time series</h2><span id='topic+do_fit'></span>

<h3>Description</h3>

<p>The actual time series model fitting.
This method should be override by descendants.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do_fit(obj, x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do_fit_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="do_fit_+3A_x">x</code></td>
<td>
<p>input variable</p>
</td></tr>
<tr><td><code id="do_fit_+3A_y">y</code></td>
<td>
<p>output variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fitted object
</p>

<hr>
<h2 id='do_predict'>do predict for time series</h2><span id='topic+do_predict'></span>

<h3>Description</h3>

<p>The actual time series model prediction.
This method should be override by descendants.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do_predict(obj, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do_predict_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="do_predict_+3A_x">x</code></td>
<td>
<p>input variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>predicted values
</p>

<hr>
<h2 id='dt_pca'>PCA</h2><span id='topic+dt_pca'></span>

<h3>Description</h3>

<p>PCA (Principal Component Analysis) is an unsupervised
dimensionality reduction technique used in data analysis and
machine learning. It transforms a dataset of possibly
correlated variables into a new set of uncorrelated
variables called principal components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dt_pca(attribute = NULL, components = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dt_pca_+3A_attribute">attribute</code></td>
<td>
<p>target attribute to model building</p>
</td></tr>
<tr><td><code id="dt_pca_+3A_components">components</code></td>
<td>
<p>number of components for PCA</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mypca &lt;- dt_pca("Species")
# Automatically fitting number of components
mypca &lt;- fit(mypca, iris)
iris.pca &lt;- transform(mypca, iris)
head(iris.pca)
head(mypca$pca.transf)
# Manual establishment of number of components
mypca &lt;- dt_pca("Species", 3)
mypca &lt;- fit(mypca, datasets::iris)
iris.pca &lt;- transform(mypca, iris)
head(iris.pca)
head(mypca$pca.transf)
</code></pre>

<hr>
<h2 id='evaluate'>evaluate</h2><span id='topic+evaluate'></span>

<h3>Description</h3>

<p>evaluate learner performance.
The actual evaluate varies according to the type of learner (clustering, classification, regression, time series regression)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evaluate_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="evaluate_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>evaluation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
slevels &lt;- levels(iris$Species)
model &lt;- cla_dtree("Species", slevels)
model &lt;- fit(model, iris)
prediction &lt;- predict(model, iris)
predictand &lt;- adjust_class_label(iris[,"Species"])
test_eval &lt;- evaluate(model, predictand, prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='fit'>Fit</h2><span id='topic+fit'></span>

<h3>Description</h3>

<p>Fits a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="fit_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
# an example is minmax normalization
trans &lt;- minmax()
trans &lt;- fit(trans, iris)
tiris &lt;- action(trans, iris)
</code></pre>

<hr>
<h2 id='fit_curvature_max'>maximum curvature analysis</h2><span id='topic+fit_curvature_max'></span>

<h3>Description</h3>

<p>Fitting a curvature model in a sequence of observations. It extracts the the maximum curvature computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_curvature_max()
</code></pre>


<h3>Value</h3>

<p>Returns an object of class fit_curvature_max, which inherits from the fit_curvature and dal_transform classes.
The object contains a list with the following elements:
</p>

<ul>
<li><p> x: The position in which the maximum curvature is reached.
</p>
</li>
<li><p> y: The value where the the maximum curvature occurs.
</p>
</li>
<li><p> yfit: The value of the maximum curvature.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(from=1,to=10,by=0.5)
dat &lt;- data.frame(x = x, value = -log(x), variable = "log")
myfit &lt;- fit_curvature_max()
res &lt;- transform(myfit, dat$value)
head(res)
</code></pre>

<hr>
<h2 id='fit_curvature_min'>minimum curvature analysis</h2><span id='topic+fit_curvature_min'></span>

<h3>Description</h3>

<p>Fitting a curvature model in a sequence of observations. It extracts the the minimum curvature computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_curvature_min()
</code></pre>


<h3>Value</h3>

<p>Returns an object of class fit_curvature_max, which inherits from the fit_curvature and dal_transform classes.
The object contains a list with the following elements:
</p>

<ul>
<li><p> x: The position in which the minimum curvature is reached.
</p>
</li>
<li><p> y: The value where the the minimum curvature occurs.
</p>
</li>
<li><p> yfit: The value of the minimum curvature.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(from=1,to=10,by=0.5)
dat &lt;- data.frame(x = x, value = log(x), variable = "log")
myfit &lt;- fit_curvature_min()
res &lt;- transform(myfit, dat$value)
head(res)
</code></pre>

<hr>
<h2 id='fit.cla_tune'>tune hyperparameters of ml model</h2><span id='topic+fit.cla_tune'></span>

<h3>Description</h3>

<p>tune hyperparameters of ml model for classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cla_tune'
fit(obj, data, ranges, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.cla_tune_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="fit.cla_tune_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
<tr><td><code id="fit.cla_tune_+3A_ranges">ranges</code></td>
<td>
<p>hyperparameters ranges</p>
</td></tr>
<tr><td><code id="fit.cla_tune_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fitted obj
</p>

<hr>
<h2 id='fit.cluster_dbscan'>fit dbscan model</h2><span id='topic+fit.cluster_dbscan'></span>

<h3>Description</h3>

<p>fit dbscan model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cluster_dbscan'
fit(obj, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.cluster_dbscan_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="fit.cluster_dbscan_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
<tr><td><code id="fit.cluster_dbscan_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fitted obj
</p>

<hr>
<h2 id='inverse_transform'>Inverse Transform</h2><span id='topic+inverse_transform'></span>

<h3>Description</h3>

<p>Reverses the transformation applied to data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inverse_transform(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inverse_transform_+3A_obj">obj</code></td>
<td>
<p>a dal_transform object.</p>
</td></tr>
<tr><td><code id="inverse_transform_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataset inverse transformed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?minmax for an example of transformation
</code></pre>

<hr>
<h2 id='k_fold'>k-fold sampling</h2><span id='topic+k_fold'></span>

<h3>Description</h3>

<p>k-fold partition of a dataset using a sampling method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_fold(obj, data, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_fold_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="k_fold_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
<tr><td><code id="k_fold_+3A_k">k</code></td>
<td>
<p>number of folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>k folds
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using random sampling
sample &lt;- sample_random()

# preparing dataset into four folds
folds &lt;- k_fold(sample, iris, 4)

# distribution of folds
tbl &lt;- NULL
for (f in folds) {
 tbl &lt;- rbind(tbl, table(f$Species))
}
head(tbl)
</code></pre>

<hr>
<h2 id='minmax'>min-max normalization</h2><span id='topic+minmax'></span>

<h3>Description</h3>

<p>The minmax performs scales data between [0,1].
minmax = (x-min(x))/(max(x)-min(x)).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minmax()
</code></pre>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
head(iris)

trans &lt;- minmax()
trans &lt;- fit(trans, iris)
tiris &lt;- transform(trans, iris)
head(tiris)

itiris &lt;- inverse_transform(trans, tiris)
head(itiris)
</code></pre>

<hr>
<h2 id='MSE.ts'>MSE</h2><span id='topic+MSE.ts'></span>

<h3>Description</h3>

<p>Compute the mean squared error (MSE) between actual values and forecasts of a time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSE.ts(actual, prediction)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSE.ts_+3A_actual">actual</code></td>
<td>
<p>real observations</p>
</td></tr>
<tr><td><code id="MSE.ts_+3A_prediction">prediction</code></td>
<td>
<p>predicted observations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A number, which is the calculated MSE
</p>

<hr>
<h2 id='outliers'>Outliers</h2><span id='topic+outliers'></span>

<h3>Description</h3>

<p>The outliers class uses box-plot definition for outliers.
An outlier is a value that is below than <code class="reqn">Q_1 - 1.5 \cdot IQR</code> or higher than <code class="reqn">Q_3 + 1.5 \cdot IQR</code>.
The class remove outliers for numeric attributes.
Users can set alpha to 3 to remove extreme values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outliers(alpha = 1.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outliers_+3A_alpha">alpha</code></td>
<td>
<p>boxplot outlier threshold (default 1.5, but can be 3.0 to remove extreme values)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An outlier object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code for outlier removal
out_obj &lt;- outliers() # class for outlier analysis
out_obj &lt;- fit(out_obj, iris) # computing boundaries
iris.clean &lt;- transform(out_obj, iris) # returning cleaned dataset

#inspection of cleaned dataset
nrow(iris.clean)

idx &lt;- attr(iris.clean, "idx")
table(idx)
iris.outliers &lt;- iris[idx,]
iris.outliers
</code></pre>

<hr>
<h2 id='plot_bar'>plot bar graph</h2><span id='topic+plot_bar'></span>

<h3>Description</h3>

<p>plot bar graph
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_bar(data, label_x = "", label_y = "", colors = NULL, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_bar_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_bar_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_bar_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_bar_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_bar_+3A_alpha">alpha</code></td>
<td>
<p>level of transparency</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#summarizing iris dataset
data &lt;- iris |&gt; dplyr::group_by(Species) |&gt;
 dplyr::summarize(Sepal.Length=mean(Sepal.Length))
head(data)

#ploting data
grf &lt;- plot_bar(data, colors="blue")
plot(grf)
</code></pre>

<hr>
<h2 id='plot_boxplot'>plot boxplot</h2><span id='topic+plot_boxplot'></span>

<h3>Description</h3>

<p>plot boxplot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_boxplot(data, label_x = "", label_y = "", colors = NULL, barwith = 0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_boxplot_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_boxplot_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_boxplot_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_boxplot_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_boxplot_+3A_barwith">barwith</code></td>
<td>
<p>width of bar</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_boxplot(iris, colors="white")
plot(grf)
</code></pre>

<hr>
<h2 id='plot_boxplot_class'>plot boxplot per class</h2><span id='topic+plot_boxplot_class'></span>

<h3>Description</h3>

<p>plot boxplot per class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_boxplot_class(
  data,
  class_label,
  label_x = "",
  label_y = "",
  colors = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_boxplot_class_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_boxplot_class_+3A_class_label">class_label</code></td>
<td>
<p>name of attribute for class label</p>
</td></tr>
<tr><td><code id="plot_boxplot_class_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_boxplot_class_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_boxplot_class_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_boxplot_class(iris |&gt; dplyr::select(Sepal.Width, Species),
 class = "Species", colors=c("red", "green", "blue"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_density'>plot density</h2><span id='topic+plot_density'></span>

<h3>Description</h3>

<p>plot density
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_density(
  data,
  label_x = "",
  label_y = "",
  colors = NULL,
  bin = NULL,
  alpha = 0.25
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_density_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_density_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_density_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_density_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_density_+3A_bin">bin</code></td>
<td>
<p>bin width</p>
</td></tr>
<tr><td><code id="plot_density_+3A_alpha">alpha</code></td>
<td>
<p>level of transparency</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_density(iris |&gt; dplyr::select(Sepal.Width), colors="blue")
plot(grf)
</code></pre>

<hr>
<h2 id='plot_density_class'>plot density per class</h2><span id='topic+plot_density_class'></span>

<h3>Description</h3>

<p>plot density per class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_density_class(
  data,
  class_label,
  label_x = "",
  label_y = "",
  colors = NULL,
  bin = NULL,
  alpha = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_density_class_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_class_label">class_label</code></td>
<td>
<p>name of attribute for class label</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_bin">bin</code></td>
<td>
<p>bin width</p>
</td></tr>
<tr><td><code id="plot_density_class_+3A_alpha">alpha</code></td>
<td>
<p>level of transparency</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_density_class(iris |&gt; dplyr::select(Sepal.Width, Species),
 class = "Species", colors=c("red", "green", "blue"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_groupedbar'>plot grouped bar</h2><span id='topic+plot_groupedbar'></span>

<h3>Description</h3>

<p>plot grouped bar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_groupedbar(data, label_x = "", label_y = "", colors = NULL, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_groupedbar_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_groupedbar_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_groupedbar_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_groupedbar_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_groupedbar_+3A_alpha">alpha</code></td>
<td>
<p>level of transparency</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- iris |&gt; dplyr::group_by(Species) |&gt;
 dplyr::summarize(Sepal.Length=mean(Sepal.Length), Sepal.Width=mean(Sepal.Width))
grf &lt;- plot_groupedbar(data, colors=c("blue", "red"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_hist'>plot histogram</h2><span id='topic+plot_hist'></span>

<h3>Description</h3>

<p>plot histogram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_hist(data, label_x = "", label_y = "", color = "white", alpha = 0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_hist_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_hist_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_hist_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_hist_+3A_color">color</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_hist_+3A_alpha">alpha</code></td>
<td>
<p>transparency level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_hist(iris |&gt; dplyr::select(Sepal.Width), color=c("blue"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_lollipop'>plot lollipop</h2><span id='topic+plot_lollipop'></span>

<h3>Description</h3>

<p>plot lollipop
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_lollipop(
  data,
  label_x = "",
  label_y = "",
  colors = NULL,
  color_text = "black",
  size_text = 3,
  size_ball = 8,
  alpha_ball = 0.2,
  min_value = 0,
  max_value_gap = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_lollipop_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_color_text">color_text</code></td>
<td>
<p>color of text inside ball</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_size_text">size_text</code></td>
<td>
<p>size of text inside ball</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_size_ball">size_ball</code></td>
<td>
<p>size of ball</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_alpha_ball">alpha_ball</code></td>
<td>
<p>transparency of ball</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_min_value">min_value</code></td>
<td>
<p>minimum value</p>
</td></tr>
<tr><td><code id="plot_lollipop_+3A_max_value_gap">max_value_gap</code></td>
<td>
<p>maximum value gap</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#summarizing iris dataset
data &lt;- iris |&gt; dplyr::group_by(Species) |&gt;
 dplyr::summarize(Sepal.Length=mean(Sepal.Length))
head(data)

#ploting data
grf &lt;- plot_lollipop(data, colors="blue", max_value_gap=0.2)
plot(grf)
</code></pre>

<hr>
<h2 id='plot_pieplot'>plot pie</h2><span id='topic+plot_pieplot'></span>

<h3>Description</h3>

<p>plot pie
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pieplot(
  data,
  label_x = "",
  label_y = "",
  colors = NULL,
  textcolor = "white",
  bordercolor = "black"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pieplot_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_pieplot_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_pieplot_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_pieplot_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_pieplot_+3A_textcolor">textcolor</code></td>
<td>
<p>text color</p>
</td></tr>
<tr><td><code id="plot_pieplot_+3A_bordercolor">bordercolor</code></td>
<td>
<p>border color</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#summarizing iris dataset
data &lt;- iris |&gt; dplyr::group_by(Species) |&gt;
 dplyr::summarize(Sepal.Length=mean(Sepal.Length))
head(data)

#ploting data
grf &lt;- plot_pieplot(data, colors=c("red", "green", "blue"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_points'>plot points</h2><span id='topic+plot_points'></span>

<h3>Description</h3>

<p>plot points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_points(data, label_x = "", label_y = "", colors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_points_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_points_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_points_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_points_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0, 10, 0.25)
data &lt;- data.frame(x, sin=sin(x), cosine=cos(x)+5)
head(data)

grf &lt;- plot_points(data, colors=c("red", "green"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_radar'>plot radar</h2><span id='topic+plot_radar'></span>

<h3>Description</h3>

<p>plot radar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_radar(data, label_x = "", label_y = "", colors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_radar_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_radar_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_radar_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_radar_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- data.frame(name = "Petal.Length", value = mean(iris$Petal.Length))
data &lt;- rbind(data, data.frame(name = "Petal.Width", value = mean(iris$Petal.Width)))
data &lt;- rbind(data, data.frame(name = "Sepal.Length", value = mean(iris$Sepal.Length)))
data &lt;- rbind(data, data.frame(name = "Sepal.Width", value = mean(iris$Sepal.Width)))

grf &lt;- plot_radar(data, colors="red") + ggplot2::ylim(0, NA)
plot(grf)
</code></pre>

<hr>
<h2 id='plot_scatter'>scatter graph</h2><span id='topic+plot_scatter'></span>

<h3>Description</h3>

<p>scatter graph
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_scatter(data, label_x = "", label_y = "", colors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_scatter_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_scatter_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_scatter_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_scatter_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grf &lt;- plot_scatter(iris |&gt; dplyr::select(x = Sepal.Length,
 value = Sepal.Width, variable = Species),
 label_x = "Sepal.Length", label_y = "Sepal.Width",
 colors=c("red", "green", "blue"))
 plot(grf)
</code></pre>

<hr>
<h2 id='plot_series'>plot series</h2><span id='topic+plot_series'></span>

<h3>Description</h3>

<p>plot series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_series(data, label_x = "", label_y = "", colors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_series_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_series_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_series_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_series_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plot
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0, 10, 0.25)
data &lt;- data.frame(x, sin=sin(x))
head(data)

grf &lt;- plot_series(data, colors=c("red"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_stackedbar'>plot stacked bar</h2><span id='topic+plot_stackedbar'></span>

<h3>Description</h3>

<p>plot stacked bar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_stackedbar(data, label_x = "", label_y = "", colors = NULL, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_stackedbar_+3A_data">data</code></td>
<td>
<p>data.frame contain x, value, and variable</p>
</td></tr>
<tr><td><code id="plot_stackedbar_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_stackedbar_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_stackedbar_+3A_colors">colors</code></td>
<td>
<p>color vector</p>
</td></tr>
<tr><td><code id="plot_stackedbar_+3A_alpha">alpha</code></td>
<td>
<p>level of transparency</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- iris |&gt; dplyr::group_by(Species) |&gt;
 dplyr::summarize(Sepal.Length=mean(Sepal.Length), Sepal.Width=mean(Sepal.Width))
grf &lt;- plot_stackedbar(data, colors=c("blue", "red"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_ts'>Plot a time series chart</h2><span id='topic+plot_ts'></span>

<h3>Description</h3>

<p>The function receives six variables as a parameter, which are obj and y, yadj, main and xlabels. The graph is plotted with 3 lines: the original series (in black), the adjusted series (in blue) and the predicted series (in green)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ts(x = NULL, y, label_x = "", label_y = "", color = "black")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ts_+3A_x">x</code></td>
<td>
<p>input variable</p>
</td></tr>
<tr><td><code id="plot_ts_+3A_y">y</code></td>
<td>
<p>output variable</p>
</td></tr>
<tr><td><code id="plot_ts_+3A_label_x">label_x</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="plot_ts_+3A_label_y">label_y</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="plot_ts_+3A_color">color</code></td>
<td>
<p>color for time series</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0, 10, 0.25)
data &lt;- data.frame(x, sin=sin(x))
head(data)

grf &lt;- plot_ts(x = data$x, y = data$sin, color=c("red"))
plot(grf)
</code></pre>

<hr>
<h2 id='plot_ts_pred'>Plot a time series chart</h2><span id='topic+plot_ts_pred'></span>

<h3>Description</h3>

<p>The function receives six variables as a parameter, which are obj and y, yadj, main and xlabels. The graph is plotted with 3 lines: the original series (in black), the adjusted series (in blue) and the predicted series (in green)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ts_pred(
  x = NULL,
  y,
  yadj,
  ypred = NULL,
  label_x = "",
  label_y = "",
  color = "black",
  color_adjust = "blue",
  color_prediction = "green"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ts_pred_+3A_x">x</code></td>
<td>
<p>time index</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_y">y</code></td>
<td>
<p>time series</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_yadj">yadj</code></td>
<td>
<p>adjustment of time series</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_ypred">ypred</code></td>
<td>
<p>prediction of the time series</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_label_x">label_x</code></td>
<td>
<p>x-axis title</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_label_y">label_y</code></td>
<td>
<p>y-axis title</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_color">color</code></td>
<td>
<p>color for the time series</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_color_adjust">color_adjust</code></td>
<td>
<p>color for the adjusted values</p>
</td></tr>
<tr><td><code id="plot_ts_pred_+3A_color_prediction">color_prediction</code></td>
<td>
<p>color for the predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot graphic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 0)
ts_head(ts, 3)


samp &lt;- ts_sample(ts, test_size= 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_arima()
model &lt;- fit(model, x=io_train$input, y=io_train$output)
adjust &lt;- predict(model, io_train$input)

prediction &lt;- predict(model, x=io_test$input, steps_ahead=5)
prediction &lt;- as.vector(prediction)

yvalues &lt;- c(io_train$output, io_test$output)
grf &lt;- plot_ts_pred(y=yvalues, yadj=adjust, ypre=prediction)
plot(grf)
</code></pre>

<hr>
<h2 id='predictor'>DAL Predict</h2><span id='topic+predictor'></span>

<h3>Description</h3>

<p>Ancestor class for regression and classification
It provides basis for fit and predict methods.
Besides, action method proxies to predict.
</p>
<p>An example of learner is a decision tree (cla_dtree)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictor()
</code></pre>


<h3>Value</h3>

<p>a predictor object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?cla_dtree for a classification example using a decision tree
</code></pre>

<hr>
<h2 id='R2.ts'>R2</h2><span id='topic+R2.ts'></span>

<h3>Description</h3>

<p>Compute the R-squared (R2) between actual values and forecasts of a time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2.ts(actual, prediction)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2.ts_+3A_actual">actual</code></td>
<td>
<p>real observations</p>
</td></tr>
<tr><td><code id="R2.ts_+3A_prediction">prediction</code></td>
<td>
<p>predicted observations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A number, which is the calculated R2
</p>

<hr>
<h2 id='reg_dtree'>Decision Tree for regression</h2><span id='topic+reg_dtree'></span>

<h3>Description</h3>

<p>Creates a regression object that
uses the Decision Tree method for regression
It wraps the tree library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_dtree(attribute)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_dtree_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A decision tree regression object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
model &lt;- reg_dtree("medv")

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='reg_knn'>knn regression</h2><span id='topic+reg_knn'></span>

<h3>Description</h3>

<p>Creates a regression object that
uses the K-Nearest Neighbors (knn) method for regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_knn(attribute, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_knn_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="reg_knn_+3A_k">k</code></td>
<td>
<p>number of k neighbors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A knn regression object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
model &lt;- reg_knn("medv", k=3)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='reg_mlp'>MLP for regression</h2><span id='topic+reg_mlp'></span>

<h3>Description</h3>

<p>Creates a regression object that
uses the Multi-Layer Perceptron (MLP) method.
It wraps the nnet library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_mlp(attribute, size = NULL, decay = 0.05, maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_mlp_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="reg_mlp_+3A_size">size</code></td>
<td>
<p>number of neurons in hidden layers</p>
</td></tr>
<tr><td><code id="reg_mlp_+3A_decay">decay</code></td>
<td>
<p>decay learning rate</p>
</td></tr>
<tr><td><code id="reg_mlp_+3A_maxit">maxit</code></td>
<td>
<p>number of maximum iterations for training</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
model &lt;- reg_mlp("medv", size=5, decay=0.54)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='reg_rf'>Random Forest for regression</h2><span id='topic+reg_rf'></span>

<h3>Description</h3>

<p>Creates a regression object that
uses the Random Forest method.
It wraps the randomForest library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_rf(attribute, nodesize = 1, ntree = 10, mtry = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_rf_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="reg_rf_+3A_nodesize">nodesize</code></td>
<td>
<p>node size</p>
</td></tr>
<tr><td><code id="reg_rf_+3A_ntree">ntree</code></td>
<td>
<p>number of trees</p>
</td></tr>
<tr><td><code id="reg_rf_+3A_mtry">mtry</code></td>
<td>
<p>number of attributes to build tree</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
model &lt;- reg_rf("medv", ntree=10)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='reg_svm'>SVM for regression</h2><span id='topic+reg_svm'></span>

<h3>Description</h3>

<p>Creates a regression object that
uses the Support Vector Machine (SVM) method for regression
It wraps the e1071 library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_svm(attribute, epsilon = 0.1, cost = 10, kernel = "radial")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_svm_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
<tr><td><code id="reg_svm_+3A_epsilon">epsilon</code></td>
<td>
<p>parameter that controls the width of the margin around the separating hyperplane</p>
</td></tr>
<tr><td><code id="reg_svm_+3A_cost">cost</code></td>
<td>
<p>parameter that controls the trade-off between having a wide margin and correctly classifying training data points</p>
</td></tr>
<tr><td><code id="reg_svm_+3A_kernel">kernel</code></td>
<td>
<p>the type of kernel function to be used in the SVM algorithm (linear, radial, polynomial, sigmoid)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A SVM regression object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
model &lt;- reg_svm("medv", epsilon=0.2,cost=40.000)

# preparing dataset for random sampling
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

model &lt;- fit(model, train)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='reg_tune'>Regression Tune</h2><span id='topic+reg_tune'></span>

<h3>Description</h3>

<p>Regression Tune
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_tune(base_model, folds = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reg_tune_+3A_base_model">base_model</code></td>
<td>
<p>base model for tuning</p>
</td></tr>
<tr><td><code id="reg_tune_+3A_folds">folds</code></td>
<td>
<p>number of folds for cross-validation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>reg_tune</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># preparing dataset for random sampling
data(Boston)
sr &lt;- sample_random()
sr &lt;- train_test(sr, Boston)
train &lt;- sr$train
test &lt;- sr$test

# hyper parameter setup
tune &lt;- reg_tune(reg_mlp("medv"))
ranges &lt;- list(size=c(3), decay=c(0.1,0.5))

# hyper parameter optimization
model &lt;- fit(tune, train, ranges)

test_prediction &lt;- predict(model, test)
test_predictand &lt;- test[,"medv"]
test_eval &lt;- evaluate(model, test_predictand, test_prediction)
test_eval$metrics
</code></pre>

<hr>
<h2 id='regression'>Regression</h2><span id='topic+regression'></span>

<h3>Description</h3>

<p>Ancestor class for regression problems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression(attribute)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regression_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regression object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?reg_dtree for a regression example using a decision tree
</code></pre>

<hr>
<h2 id='sample_random'>Sample Random</h2><span id='topic+sample_random'></span>

<h3>Description</h3>

<p>The sample_random function in R is used to
generate a random sample of specified size from a given data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_random()
</code></pre>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using random sampling
sample &lt;- sample_random()
tt &lt;- train_test(sample, iris)

# distribution of train
table(tt$train$Species)

# preparing dataset into four folds
folds &lt;- k_fold(sample, iris, 4)

# distribution of folds
tbl &lt;- NULL
for (f in folds) {
 tbl &lt;- rbind(tbl, table(f$Species))
}
head(tbl)
</code></pre>

<hr>
<h2 id='sample_stratified'>sample_stratified</h2><span id='topic+sample_stratified'></span>

<h3>Description</h3>

<p>The sample_stratified function in R is used to generate a stratified random sample from a given dataset. Stratified sampling is a statistical method that is used when the population is divided into non-overlapping subgroups or strata, and a sample is selected from each stratum to represent the entire population. In stratified sampling, the sample is selected in such a way that it is representative of the entire population and the variability within each stratum is minimized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_stratified(attribute)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_stratified_+3A_attribute">attribute</code></td>
<td>
<p>attribute target to model building</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using stratified sampling
sample &lt;- sample_stratified("Species")
tt &lt;- train_test(sample, iris)

# distribution of train
table(tt$train$Species)

# preparing dataset into four folds
folds &lt;- k_fold(sample, iris, 4)

# distribution of folds
tbl &lt;- NULL
for (f in folds) {
 tbl &lt;- rbind(tbl, table(f$Species))
}
head(tbl)
</code></pre>

<hr>
<h2 id='select_hyper'>Selection hyper parameters</h2><span id='topic+select_hyper'></span>

<h3>Description</h3>

<p>Selection hyper parameters from a k-fold cross-validation execution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_hyper(obj, hyperparameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_hyper_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="select_hyper_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>data set with hyper parameters and quality measure from execution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>index of selected hyper parameter
</p>

<hr>
<h2 id='select_hyper.cla_tune'>selection of hyperparameters</h2><span id='topic+select_hyper.cla_tune'></span>

<h3>Description</h3>

<p>selection of hyperparameters (maximizing classification metric)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cla_tune'
select_hyper(obj, hyperparameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_hyper.cla_tune_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="select_hyper.cla_tune_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>hyperparameters dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>optimized key number of hyperparameters
</p>

<hr>
<h2 id='select_hyper.ts_tune'>selection of hyperparameters (time series)</h2><span id='topic+select_hyper.ts_tune'></span>

<h3>Description</h3>

<p>selection of hyperparameters (minimizing error)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ts_tune'
select_hyper(obj, hyperparameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_hyper.ts_tune_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="select_hyper.ts_tune_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>hyperparameters dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>optimized key number of hyperparameters
</p>

<hr>
<h2 id='set_params'>Assign parameters</h2><span id='topic+set_params'></span>

<h3>Description</h3>

<p>set_params function assigns all parameters to the attributes presented in the object.
It returns the object with the parameters set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_params(obj, params)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_params_+3A_obj">obj</code></td>
<td>
<p>object of class dal_base</p>
</td></tr>
<tr><td><code id="set_params_+3A_params">params</code></td>
<td>
<p>parameters to set obj</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj with parameters set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obj &lt;- set_params(dal_base(), list(x = 0))
</code></pre>

<hr>
<h2 id='set_params.default'>Assign parameters</h2><span id='topic+set_params.default'></span>

<h3>Description</h3>

<p>This function receives the obj and params variables as parameters.
It returns the obj as it is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
set_params(obj, params)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_params.default_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="set_params.default_+3A_params">params</code></td>
<td>
<p>parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>

<hr>
<h2 id='sin_data'>Time series example dataset</h2><span id='topic+sin_data'></span>

<h3>Description</h3>

<p>Synthetic dataset of sine function.
</p>

<ul>
<li><p> x: correspond time from 0 to 10.
</p>
</li>
<li><p> y: dependent variable for time series modeling.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(sin_data)
</code></pre>


<h3>Format</h3>

<p><code>data.frame</code>.
</p>


<h3>Source</h3>

<p>This dataset was generated for examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
head(sin_data)
</code></pre>

<hr>
<h2 id='sMAPE.ts'>sMAPE</h2><span id='topic+sMAPE.ts'></span>

<h3>Description</h3>

<p>Compute the symmetric mean absolute percent error (sMAPE)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sMAPE.ts(actual, prediction)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sMAPE.ts_+3A_actual">actual</code></td>
<td>
<p>real observations</p>
</td></tr>
<tr><td><code id="sMAPE.ts_+3A_prediction">prediction</code></td>
<td>
<p>predicted observations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The sMAPE between the actual and prediction vectors
</p>

<hr>
<h2 id='smoothing'>Smoothing</h2><span id='topic+smoothing'></span>

<h3>Description</h3>

<p>Smoothing is a statistical technique used to reduce
the noise in a signal or a dataset by removing the high-frequency components.
The smoothing level is associated with the number of bins used.
There are alternative methods to establish the smoothing:
equal interval, equal frequency, and clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothing(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothing_+3A_n">n</code></td>
<td>
<p>number of bins</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
obj &lt;- smoothing_inter(n = 2)
obj &lt;- fit(obj, iris$Sepal.Length)
sl.bi &lt;- transform(obj, iris$Sepal.Length)
table(sl.bi)
obj$interval

entro &lt;- evaluate(obj, as.factor(names(sl.bi)), iris$Species)
entro$entropy
</code></pre>

<hr>
<h2 id='smoothing_cluster'>Smoothing by cluster</h2><span id='topic+smoothing_cluster'></span>

<h3>Description</h3>

<p>Uses clustering method to perform data smoothing.
The input vector is divided into clusters using the k-means algorithm.
The mean of each cluster is then calculated and used as the
smoothed value for all observations within that cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothing_cluster(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothing_cluster_+3A_n">n</code></td>
<td>
<p>number of bins</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
obj &lt;- smoothing_cluster(n = 2)
obj &lt;- fit(obj, iris$Sepal.Length)
sl.bi &lt;- transform(obj, iris$Sepal.Length)
table(sl.bi)
obj$interval

entro &lt;- evaluate(obj, as.factor(names(sl.bi)), iris$Species)
entro$entropy
</code></pre>

<hr>
<h2 id='smoothing_freq'>Smoothing by Freq</h2><span id='topic+smoothing_freq'></span>

<h3>Description</h3>

<p>The 'smoothing_freq' function is used to smooth a given time series data by aggregating observations within a fixed frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothing_freq(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothing_freq_+3A_n">n</code></td>
<td>
<p>number of bins</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
obj &lt;- smoothing_freq(n = 2)
obj &lt;- fit(obj, iris$Sepal.Length)
sl.bi &lt;- transform(obj, iris$Sepal.Length)
table(sl.bi)
obj$interval

entro &lt;- evaluate(obj, as.factor(names(sl.bi)), iris$Species)
entro$entropy
</code></pre>

<hr>
<h2 id='smoothing_inter'>Smoothing by interval</h2><span id='topic+smoothing_inter'></span>

<h3>Description</h3>

<p>The &quot;smoothing by interval&quot; function is used to apply a smoothing technique to a vector or time series data using a moving window approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothing_inter(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothing_inter_+3A_n">n</code></td>
<td>
<p>number of bins</p>
</td></tr>
</table>


<h3>Value</h3>

<p>obj
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
obj &lt;- smoothing_inter(n = 2)
obj &lt;- fit(obj, iris$Sepal.Length)
sl.bi &lt;- transform(obj, iris$Sepal.Length)
table(sl.bi)
obj$interval

entro &lt;- evaluate(obj, as.factor(names(sl.bi)), iris$Species)
entro$entropy
</code></pre>

<hr>
<h2 id='train_test'>training and test</h2><span id='topic+train_test'></span>

<h3>Description</h3>

<p>training and test partition of a dataset using a sampling method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_test(obj, data, perc = 0.8, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_test_+3A_obj">obj</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="train_test_+3A_data">data</code></td>
<td>
<p>dataset</p>
</td></tr>
<tr><td><code id="train_test_+3A_perc">perc</code></td>
<td>
<p>percentage for training</p>
</td></tr>
<tr><td><code id="train_test_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>train and test sets
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#using random sampling
sample &lt;- sample_random()
tt &lt;- train_test(sample, iris)

# distribution of train
table(tt$train$Species)
</code></pre>

<hr>
<h2 id='train_test_from_folds'>k-fold training and test partition object</h2><span id='topic+train_test_from_folds'></span>

<h3>Description</h3>

<p>k-fold training and test partition object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_test_from_folds(folds, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_test_from_folds_+3A_folds">folds</code></td>
<td>
<p>data partitioned into folds</p>
</td></tr>
<tr><td><code id="train_test_from_folds_+3A_k">k</code></td>
<td>
<p>k-fold for test set, all reminder for training set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>train and test folds
</p>

<hr>
<h2 id='transform'>Transform</h2><span id='topic+transform'></span>

<h3>Description</h3>

<p>Defines a transformation method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transform_+3A_obj">obj</code></td>
<td>
<p>a <code>dal_transform</code> object.</p>
</td></tr>
<tr><td><code id="transform_+3A_...">...</code></td>
<td>
<p>optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>transformed data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?minmax for an example of transformation
</code></pre>

<hr>
<h2 id='ts_arima'>ARIMA</h2><span id='topic+ts_arima'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that
uses the AutoRegressive Integrated Moving Average (ARIMA).
It wraps the forecast library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_arima()
</code></pre>


<h3>Value</h3>

<p>a <code>ts_arima</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 0)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_arima()
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_conv1d'>Conv1D</h2><span id='topic+ts_conv1d'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that uses the Conv1D.
It wraps the pytorch library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_conv1d(preprocess = NA, input_size = NA, epochs = 10000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_conv1d_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_conv1d_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_conv1d_+3A_epochs">epochs</code></td>
<td>
<p>maximum number of epochs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_conv1d</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Use the same example of ts_mlp changing the constructor to:
model &lt;- ts_conv1d(ts_norm_gminmax(), input_size=4, epochs = 10000L)
</code></pre>

<hr>
<h2 id='ts_data'>ts_data</h2><span id='topic+ts_data'></span>

<h3>Description</h3>

<p>Time series data structure used in DAL Toolbox.
It receives a vector (representing a time series) or
a matrix <code>y</code> (representing a sliding windows).
Internal ts_data is matrix of sliding windows with size <code>sw</code>.
If sw equals to zero, it store a time series as a single matrix column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_data(y, sw = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_data_+3A_y">y</code></td>
<td>
<p>output variable</p>
</td></tr>
<tr><td><code id="ts_data_+3A_sw">sw</code></td>
<td>
<p>integer: sliding window size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_data</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
head(sin_data)

data &lt;- ts_data(sin_data$y)
ts_head(data)

data10 &lt;- ts_data(sin_data$y, 10)
ts_head(data10)
</code></pre>

<hr>
<h2 id='ts_elm'>ELM</h2><span id='topic+ts_elm'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that
uses the Extreme Learning Machine (ELM).
It wraps the elmNNRcpp library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_elm(preprocess = NA, input_size = NA, nhid = NA, actfun = "purelin")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_elm_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_elm_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_elm_+3A_nhid">nhid</code></td>
<td>
<p>ensemble size</p>
</td></tr>
<tr><td><code id="ts_elm_+3A_actfun">actfun</code></td>
<td>
<p>defines the type to use, possible values: 'sig',
'radbas', 'tribas', 'relu', 'purelin' (default).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_elm</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_elm(ts_norm_gminmax(), input_size=4, nhid=3, actfun="purelin")
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_head'>ts_head</h2><span id='topic+ts_head'></span>

<h3>Description</h3>

<p>Returns the first n observations from a <code>ts_data</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_head(x, n = 6L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_head_+3A_x">x</code></td>
<td>
<p><code>ts_data</code></p>
</td></tr>
<tr><td><code id="ts_head_+3A_n">n</code></td>
<td>
<p>number of rows to return</p>
</td></tr>
<tr><td><code id="ts_head_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The first n observations of a <code>ts_data</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
data10 &lt;- ts_data(sin_data$y, 10)
ts_head(data10)
</code></pre>

<hr>
<h2 id='ts_knn'>knn time series prediction</h2><span id='topic+ts_knn'></span>

<h3>Description</h3>

<p>Creates a prediction object that
uses the K-Nearest Neighbors (knn) method for time series regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_knn(preprocess = NA, input_size = NA, k = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_knn_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_knn_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_knn_+3A_k">k</code></td>
<td>
<p>number of k neighbors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_knn</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_knn(ts_norm_gminmax(), input_size=4, k=3)
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_lstm'>LSTM</h2><span id='topic+ts_lstm'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that uses the LSTM.
It wraps the pytorch library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_lstm(preprocess = NA, input_size = NA, epochs = 10000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_lstm_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_lstm_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_lstm_+3A_epochs">epochs</code></td>
<td>
<p>maximum number of epochs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_lstm</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Use the same example of ts_mlp changing the constructor to:
model &lt;- ts_lstm(ts_norm_gminmax(), input_size=4, epochs = 10000L)
</code></pre>

<hr>
<h2 id='ts_mlp'>MLP</h2><span id='topic+ts_mlp'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that
uses the Multilayer Perceptron (MLP).
It wraps the nnet library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_mlp(preprocess = NA, input_size = NA, size = NA, decay = 0.01, maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_mlp_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_mlp_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_mlp_+3A_size">size</code></td>
<td>
<p>number of neurons inside hidden layer</p>
</td></tr>
<tr><td><code id="ts_mlp_+3A_decay">decay</code></td>
<td>
<p>decay parameter for MLP</p>
</td></tr>
<tr><td><code id="ts_mlp_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_mlp</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_mlp(ts_norm_gminmax(), input_size=4, size=4, decay=0)
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_norm_an'>Time Series Adaptive Normalization</h2><span id='topic+ts_norm_an'></span>

<h3>Description</h3>

<p>Transform data to a common scale while taking into account the
changes in the statistical properties of the data over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_norm_an(remove_outliers = TRUE, nw = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_norm_an_+3A_remove_outliers">remove_outliers</code></td>
<td>
<p>logical: if TRUE (default) outliers will be removed.</p>
</td></tr>
<tr><td><code id="ts_norm_an_+3A_nw">nw</code></td>
<td>
<p>integer: window size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_norm_an</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time series to normalize
data(sin_data)

# convert to sliding windows
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)
summary(ts[,10])

# normalization
preproc &lt;- ts_norm_an()
preproc &lt;- fit(preproc, ts)
tst &lt;- transform(preproc, ts)
ts_head(tst, 3)
summary(tst[,10])
</code></pre>

<hr>
<h2 id='ts_norm_diff'>Time Series Diff</h2><span id='topic+ts_norm_diff'></span>

<h3>Description</h3>

<p>It receives as parameter the variable remove_outliters. This function calculates the difference between the values of a time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_norm_diff(remove_outliers = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_norm_diff_+3A_remove_outliers">remove_outliers</code></td>
<td>
<p>logical: if TRUE (default) outliers will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_norm_diff</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time series to normalize
data(sin_data)

# convert to sliding windows
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)
summary(ts[,10])

# normalization
preproc &lt;- ts_norm_diff()
preproc &lt;- fit(preproc, ts)
tst &lt;- transform(preproc, ts)
ts_head(tst, 3)
summary(tst[,9])
</code></pre>

<hr>
<h2 id='ts_norm_ean'>Time Series Adaptive Normalization (Exponential Moving Average - EMA)</h2><span id='topic+ts_norm_ean'></span>

<h3>Description</h3>

<p>It takes 2 parameters: remove_outliers and nw
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_norm_ean(remove_outliers = TRUE, nw = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_norm_ean_+3A_remove_outliers">remove_outliers</code></td>
<td>
<p>logical: if TRUE (default) outliers will be removed.</p>
</td></tr>
<tr><td><code id="ts_norm_ean_+3A_nw">nw</code></td>
<td>
<p>windows size</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_norm_ean</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time series to normalize
data(sin_data)

# convert to sliding windows
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)
summary(ts[,10])

# normalization
preproc &lt;- ts_norm_ean()
preproc &lt;- fit(preproc, ts)
tst &lt;- transform(preproc, ts)
ts_head(tst, 3)
summary(tst[,10])
</code></pre>

<hr>
<h2 id='ts_norm_gminmax'>Time Series Global Min-Max</h2><span id='topic+ts_norm_gminmax'></span>

<h3>Description</h3>

<p>Rescales data, so the minimum value is mapped to 0 and the maximum value is mapped to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_norm_gminmax(remove_outliers = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_norm_gminmax_+3A_remove_outliers">remove_outliers</code></td>
<td>
<p>logical: if TRUE (default) outliers will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_norm_gminmax</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time series to normalize
data(sin_data)

# convert to sliding windows
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)
summary(ts[,10])

# normalization
preproc &lt;- ts_norm_gminmax()
preproc &lt;- fit(preproc, ts)
tst &lt;- transform(preproc, ts)
ts_head(tst, 3)
summary(tst[,10])
</code></pre>

<hr>
<h2 id='ts_norm_swminmax'>Time Series Sliding Window Min-Max</h2><span id='topic+ts_norm_swminmax'></span>

<h3>Description</h3>

<p>It takes as parameter the variable remove_outliers. The ts_norm_swminmax function creates an object for normalizing a time series based on the &quot;sliding window min-max scaling&quot; method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_norm_swminmax(remove_outliers = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_norm_swminmax_+3A_remove_outliers">remove_outliers</code></td>
<td>
<p>logical: if TRUE (default) outliers will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_norm_swminmax</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time series to normalize
data(sin_data)

# convert to sliding windows
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)
summary(ts[,10])

# normalization
preproc &lt;- ts_norm_swminmax()
preproc &lt;- fit(preproc, ts)
tst &lt;- transform(preproc, ts)
ts_head(tst, 3)
summary(tst[,10])
</code></pre>

<hr>
<h2 id='ts_projection'>Time Series Projection</h2><span id='topic+ts_projection'></span>

<h3>Description</h3>

<p>Separates the <code>ts_data</code> into input and output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_projection(ts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_projection_+3A_ts">ts</code></td>
<td>
<p>matrix or data.frame containing the time series.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_projection</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#setting up a ts_data
data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)

io &lt;- ts_projection(ts)

#input data
ts_head(io$input)

#output data
ts_head(io$output)
</code></pre>

<hr>
<h2 id='ts_reg'>TSReg</h2><span id='topic+ts_reg'></span>

<h3>Description</h3>

<p>Time Series Regression directly from time series
Ancestral class for non-sliding windows implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_reg()
</code></pre>


<h3>Value</h3>

<p>A <code>ts_reg</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?ts_arima for an example using Auto-regressive Integrated Moving Average
</code></pre>

<hr>
<h2 id='ts_regsw'>TSRegSW</h2><span id='topic+ts_regsw'></span>

<h3>Description</h3>

<p>Time Series Regression from Sliding Windows.
Ancestral class for Machine Learning Implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_regsw(preprocess = NA, input_size = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_regsw_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_regsw_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ts_regsw</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#See ?ts_elm for an example using Extreme Learning Machine
</code></pre>

<hr>
<h2 id='ts_rf'>Random Forest</h2><span id='topic+ts_rf'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that
uses the Random Forest.
It wraps the randomForest library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_rf(preprocess = NA, input_size = NA, nodesize = 1, ntree = 10, mtry = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_rf_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_rf_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_rf_+3A_nodesize">nodesize</code></td>
<td>
<p>node size</p>
</td></tr>
<tr><td><code id="ts_rf_+3A_ntree">ntree</code></td>
<td>
<p>number of trees</p>
</td></tr>
<tr><td><code id="ts_rf_+3A_mtry">mtry</code></td>
<td>
<p>number of attributes to build tree</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_rf</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_rf(ts_norm_gminmax(), input_size=4, nodesize=3, ntree=50)
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_sample'>Time Series Sample</h2><span id='topic+ts_sample'></span>

<h3>Description</h3>

<p>Separates the <code>ts_data</code> into training and test.
It separates the test size from the last observations minus an offset.
The offset is important to allow replication under different recent origins.
The data for train uses the number of rows of a <code>ts_data</code> minus the test size and offset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_sample(ts, test_size = 1, offset = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_sample_+3A_ts">ts</code></td>
<td>
<p>time series.</p>
</td></tr>
<tr><td><code id="ts_sample_+3A_test_size">test_size</code></td>
<td>
<p>integer: size of test data (default = 1).</p>
</td></tr>
<tr><td><code id="ts_sample_+3A_offset">offset</code></td>
<td>
<p>integer: starting point (default = 0).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the two samples
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#setting up a ts_data
data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)

#separating into train and test
test_size &lt;- 3
samp &lt;- ts_sample(ts, test_size)

#first five rows from training data
ts_head(samp$train, 5)

#last five rows from training data
ts_head(samp$train[-c(1:(nrow(samp$train)-5)),])

#testing data
ts_head(samp$test)
</code></pre>

<hr>
<h2 id='ts_svm'>SVM</h2><span id='topic+ts_svm'></span>

<h3>Description</h3>

<p>Creates a time series prediction object that
uses the Support Vector Machine (SVM).
It wraps the e1071 library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_svm(
  preprocess = NA,
  input_size = NA,
  kernel = "radial",
  epsilon = 0,
  cost = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_svm_+3A_preprocess">preprocess</code></td>
<td>
<p>normalization</p>
</td></tr>
<tr><td><code id="ts_svm_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_svm_+3A_kernel">kernel</code></td>
<td>
<p>SVM kernel (linear, radial, polynomial, sigmoid)</p>
</td></tr>
<tr><td><code id="ts_svm_+3A_epsilon">epsilon</code></td>
<td>
<p>error threshold</p>
</td></tr>
<tr><td><code id="ts_svm_+3A_cost">cost</code></td>
<td>
<p>cost</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_svm</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

model &lt;- ts_svm(ts_norm_gminmax(), input_size=4)
model &lt;- fit(model, x=io_train$input, y=io_train$output)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='ts_tune'>Time Series Tune</h2><span id='topic+ts_tune'></span>

<h3>Description</h3>

<p>Time Series Tune
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_tune(input_size, base_model, folds = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_tune_+3A_input_size">input_size</code></td>
<td>
<p>input size for machine learning model</p>
</td></tr>
<tr><td><code id="ts_tune_+3A_base_model">base_model</code></td>
<td>
<p>base model for tuning</p>
</td></tr>
<tr><td><code id="ts_tune_+3A_folds">folds</code></td>
<td>
<p>number of folds for cross-validation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ts_tune</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sin_data)
ts &lt;- ts_data(sin_data$y, 10)
ts_head(ts, 3)

samp &lt;- ts_sample(ts, test_size = 5)
io_train &lt;- ts_projection(samp$train)
io_test &lt;- ts_projection(samp$test)

tune &lt;- ts_tune(input_size=c(3:5), base_model = ts_elm(ts_norm_gminmax()))
ranges &lt;- list(nhid = 1:5, actfun=c('purelin'))

# Generic model tunning
model &lt;- fit(tune, x=io_train$input, y=io_train$output, ranges)

prediction &lt;- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction &lt;- as.vector(prediction)
output &lt;- as.vector(io_test$output)

ev_test &lt;- evaluate(model, output, prediction)
ev_test
</code></pre>

<hr>
<h2 id='zscore'>z-score normalization</h2><span id='topic+zscore'></span>

<h3>Description</h3>

<p>Scale data using z-score normalization.
zscore = (x - mean(x))/sd(x).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zscore(nmean = 0, nsd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zscore_+3A_nmean">nmean</code></td>
<td>
<p>new mean for normalized data</p>
</td></tr>
<tr><td><code id="zscore_+3A_nsd">nsd</code></td>
<td>
<p>new standard deviation for normalized data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>z-score transformation object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
head(iris)

trans &lt;- zscore()
trans &lt;- fit(trans, iris)
tiris &lt;- transform(trans, iris)
head(tiris)

itiris &lt;- inverse_transform(trans, tiris)
head(itiris)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
