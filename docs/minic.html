<!DOCTYPE html><html lang="en"><head><title>Help for package minic</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {minic}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rnewton'><p>Regularized quasi-Newton optimization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Minimization Methods for Ill-Conditioned Problems</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-15</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bert van der Veen &lt;bert_van_der_veen@hotmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of methods for minimizing ill-conditioned problems. Currently only includes regularized (quasi-)newton optimization (Kanzow and Steck et al. (2023), &lt;<a href="https://doi.org/10.1007%2Fs12532-023-00238-4">doi:10.1007/s12532-023-00238-4</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.12)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/BertvanderVeen/minic">https://github.com/BertvanderVeen/minic</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/BertvanderVeen/minic/issues">https://github.com/BertvanderVeen/minic/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-15 18:54:39 UTC; bertv</td>
</tr>
<tr>
<td>Author:</td>
<td>Bert van der Veen [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-15 19:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rnewton'>Regularized quasi-Newton optimization</h2><span id='topic+rnewton'></span><span id='topic+rnewt'></span>

<h3>Description</h3>

<p>Performs regularized (quasi-)Newton optimisation with limited-memory BFGS, SR1, or PSB updates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnewton(x0, fn, gr,
  he = NULL,
  quasi = TRUE,
  method = "LBFGS",
  verbose = FALSE,
  return.hess = FALSE,
  control = list(maxit = 1000, m = 5, sigma1 = 0.5, sigma2 = 4, c1 = 0.001,
    c2 = 0.9, pmin = 0.001, tol.g = 1e-08, tol.gamma = 1e-05, tol.obj = 1e-08, 
    tol.step = 1e-14, tol.mu = 1e-04, tol.mu2 = 1e+15, tol.c = 1e-08, report.iter = 10, 
    grad.reject = FALSE, max.reject = 50, mu0 = 5),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rnewton_+3A_x0">x0</code></td>
<td>
<p>Initial values for the parameters.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_he">he</code></td>
<td>
<p>A function that returns the hessian (only used when quasi = FALSE).</p>
</td></tr>
<tr><td><code id="rnewton_+3A_quasi">quasi</code></td>
<td>
<p>logical. Defaults to TRUE. If FALSE implements regularised Newton optimization.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_method">method</code></td>
<td>
<p>The method to be used when <code>quasi = TRUE</code>. Defaults to &quot;LBFGS&quot;, alternatives are &quot;LPSB&quot;, &quot;LSR1&quot; and ther full-memory alternatives &quot;BFGS&quot;, &quot;SR1&quot;, &quot;PSB&quot;. The latter three options should probably not be used in practice (see details).</p>
</td></tr>
<tr><td><code id="rnewton_+3A_verbose">verbose</code></td>
<td>
<p>logical. Defaults to FALSE. If TRUE prints reports on each iteration.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_return.hess">return.hess</code></td>
<td>
<p>logical. Defaults to FALSE. If TRUE returns (approximation of) the hessian at the final iteration.</p>
</td></tr>
<tr><td><code id="rnewton_+3A_control">control</code></td>
<td>
<p>a <code>"<a href="base.html#topic+list">list</a>"</code> of control options.
</p>

<ul>
<li> <p><em>maxit</em>: The maximum number of iterations. Defaults to 1000.
</p>
</li>
<li> <p><em>m</em>: The number of gradients to remember from previous optimisation steps. Defaults to 5.
</p>
</li>
<li> <p><em>sigma1</em>: Step decrement factor. Defaults to 0.5. Must be smaller than 1 but larger than 0.
</p>
</li>
<li> <p><em>sigma2</em>: Step increment factor. Defaults to 4. Must be larger than 1.
</p>
</li>
<li> <p><em>c1</em>: First constant for determining step success. Defaults to 1e-3. See details.
</p>
</li>
<li> <p><em>c2</em>: Second constant for determining step success. Defaults to 0.9. See details.
</p>
</li>
<li> <p><em>pmin</em>: Third constant for determining (lack of) step success. Defaults to 1e-3.
</p>
</li>
<li> <p><em>tol.g</em>: Convergence tolerance for gradient. Defaults to 1e-8.
</p>
</li>
<li> <p><em>tol.gamma</em>: Threshold for gamma paramter. Defaults to 1e-5.
</p>
</li>
<li> <p><em>tol.obj</em>: Convergence tolerance for reduction in the objective. Defaults to 1e-8.
</p>
</li>
<li> <p><em>tol.step</em>: Convergence tolerance for squared norm of the step. Defaults to 1e-14.
</p>
</li>
<li> <p><em>tol.mu</em>: Minimum threshold for the regularisation parameter. Defaults to 1e-4.
</p>
</li>
<li> <p><em>tol.mu2</em>: Maximum threshold for the regularisation parameter. Defaults to 1e15.
</p>
</li>
<li> <p><em>tol.c</em>: Tolerance for cautious updating. Defaults to 1e-8.
</p>
</li>
<li> <p><em>report.iter</em>: If 'verbose = TRUE', how often should a report be printed? Defaults to every 10 iterations.
</p>
</li>
<li> <p><em>max.reject</em>: Maximum number of consecutive rejections before algorithm terminates.
</p>
</li>
<li> <p><em>grad.reject</em>: Logical. If TRUE the gradient is evaluated at every iteration and information of rejected steps is incorporated in limited-memory methods. Defaults to FALSE.
</p>
</li>
<li> <p><em>mu0.reject</em>: Initial value of the regularisation parameter. Defaults to 5.
</p>
</li></ul>
</td></tr>
<tr><td><code id="rnewton_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements some of the regularised (quasi-)Newton optimisation methods presented in Kanzow and Steck (2023) with one modification; gradient information of rejected steps is incorporated by default. The full-memory options that are implemented rely on explicitly inverting the approximated Hessian and regularisation penalty, are thus slow, and should probably not be used in practice. 
</p>
<p>The function start with a single More-Thuente line search along the normalized negative gradient direction. 
The code for this was originally written in matlab by Burdakov et al. (2017), translated to python by Kanzow and Steck (2023), and separately translated to R code for this package. 
</p>
<p>A step is considered somewhat successful for <code class="reqn">c_1&lt;\rho\leq c_2</code>, where <code class="reqn">\rho</code> is the proportion of achieved and predicted reduction in the objective function. Note the requirement <code class="reqn">c_1 \in (0,1)</code> and <code class="reqn">c_2 \in (c_1,1)</code>.
A step is considered highly successful for <code class="reqn">c_2 &lt; \rho</code>, where rho is the proportion of achieved and predicted reduction in the objective function.
</p>
<p>The <code class="reqn">\sigma_1</code> constant controls the decrement of the regularisation parameter <code class="reqn">\mu</code> on a highly succesful step.
The <code class="reqn">\sigma_2</code> constant controls the increment of the regularisation parameter <code class="reqn">\mu</code> on a unsuccesful step. A step is defned as unsuccesful if 1) the predicted reduction less than pmin times the product of the l2 norm for step direction and gradient, or 2) if <code class="reqn">\rho \leq c_1</code>.
</p>


<h3>Value</h3>

<p>An object of class &quot;rnewton&quot; including the following components:
</p>
<table role = "presentation">
<tr><td><code>objective:</code></td>
<td>
<p> The value of fn corresponding to par.</p>
</td></tr>
<tr><td><code>iterations:</code></td>
<td>
<p> Number of completed iterations.</p>
</td></tr>
<tr><td><code>evalg:</code></td>
<td>
<p> Number of calls to gr.</p>
</td></tr>
<tr><td><code>par:</code></td>
<td>
<p> The best set of parameters found.</p>
</td></tr>
<tr><td><code>info:</code></td>
<td>
<p> Convergence code.</p>
</td></tr>
<tr><td><code>maxgr:</code></td>
<td>
<p> Maximum absolute gradient component.</p>
</td></tr>
<tr><td><code>convergence:</code></td>
<td>
<p> logical, TRUE indicating succesful convergence (reached tol.obj, tol.g, or tol.step).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bert van der Veen
</p>


<h3>References</h3>

<p>Burdakov, O., Gong, L., Zikrin, S., &amp; Yuan, Y. X. (2017). On efficiently combining limited-memory and trust-region techniques. Mathematical Programming Computation, 9, 101-134.
</p>
<p>Kanzow, C., &amp; Steck, D. (2023). Regularization of limited memory quasi-Newton methods for large-scale nonconvex minimization. Mathematical Programming Computation, 15(3), 417-444.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Powell's quartic function
fn &lt;- function(x) {
  (x[1] + 10*x[2])^2 + 5 * (x[3] - x[4])^2 + 
  (x[2] - 2*x[3])^4 + 10 * (x[1] - x[4])^4
}

# Gradient
gr &lt;- function(x) {
  c(2 * (x[1] + 10*x[2]) + 40 * (x[1] - x[4])^3,    # dfdx1
    20 * (x[1] + 10*x[2]) + 4 * (x[2] - 2 * x[3])^3,# dfdx2
    10 * (x[3] - x[4]) - 8 * (x[2] - 2*x[3])^3,     # dfdx3
    -(10 * (x[3] - x[4]) + 40 * (x[1] - x[4])^3))   # dfdx4
}

# Lower tolerances from default
rnewton(c(1, 1, 1, 1), fn, gr, control = list(mu0 = 1, tol.g = 1e-10, tol.obj = 0))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
