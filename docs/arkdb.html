<!DOCTYPE html><html><head><title>Help for package arkdb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {arkdb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#arkdb-package'><p>arkdb: Archive and Unarchive Databases Using Flat Files</p></a></li>
<li><a href='#ark'><p>Archive tables from a database as flat files</p></a></li>
<li><a href='#arkdb_delete_db'><p>delete the local arkdb database</p></a></li>
<li><a href='#local_db'><p>Connect to a local stand-alone database</p></a></li>
<li><a href='#local_db_disconnect'><p>Disconnect from the arkdb database.</p></a></li>
<li><a href='#process_chunks'><p>process a table in chunks</p></a></li>
<li><a href='#streamable_base_csv'><p>streamable csv using base R functions</p></a></li>
<li><a href='#streamable_base_tsv'><p>streamable tsv using base R functions</p></a></li>
<li><a href='#streamable_parquet'><p>streamable chunked parquet using <code>arrow</code></p></a></li>
<li><a href='#streamable_readr_csv'><p>streamable csv using <code>readr</code></p></a></li>
<li><a href='#streamable_readr_tsv'><p>streamable tsv using <code>readr</code></p></a></li>
<li><a href='#streamable_table'><p>streamable table</p></a></li>
<li><a href='#streamable_vroom'><p>streamable tables using <code>vroom</code></p></a></li>
<li><a href='#unark'><p>Unarchive a list of compressed tsv files into a database</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.0.18</td>
</tr>
<tr>
<td>Title:</td>
<td>Archive and Unarchive Databases Using Flat Files</td>
</tr>
<tr>
<td>Description:</td>
<td>Flat text files provide a robust, compressible, and portable
  way to store tables from databases.  This package provides convenient
  functions for exporting tables from relational database connections
  into compressed text files and streaming those text files back into
  a database without requiring the whole table to fit in working memory.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ropensci/arkdb">https://github.com/ropensci/arkdb</a>,
<a href="https://docs.ropensci.org/arkdb/">https://docs.ropensci.org/arkdb/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/arkdb/issues">https://github.com/ropensci/arkdb/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>DBI, tools, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arrow, R.utils, progress, readr, spelling, dplyr, dbplyr,
nycflights13, testthat, knitr, covr, fs, rmarkdown, RSQLite,
duckdb, vroom, utf8, future.apply</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-15 23:36:34 UTC; cboettig</td>
</tr>
<tr>
<td>Author:</td>
<td>Carl Boettiger <a href="https://orcid.org/0000-0002-1642-628X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Richard FitzJohn [ctb],
  Brandon Bertelsen [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Carl Boettiger &lt;cboettig@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-16 00:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='arkdb-package'>arkdb: Archive and Unarchive Databases Using Flat Files</h2><span id='topic+arkdb'></span><span id='topic+arkdb-package'></span>

<h3>Description</h3>

<p>Flat text files provide a more robust, compressible,
and portable way to store tables.  This package provides convenient
functions for exporting tables from relational database connections
into compressed text files and streaming those text files back into
a database without requiring the whole table to fit in working memory.
</p>


<h3>Details</h3>

<p>It has two functions:
</p>

<ul>
<li> <p><code><a href="#topic+ark">ark()</a></code>: archive a database into flat files, chunk by chunk.
</p>
</li>
<li> <p><code><a href="#topic+unark">unark()</a></code>: Unarchive flat files back int a database connection.
</p>
</li></ul>

<p>arkdb will work with any <code>DBI</code> supported connection.  This makes it
a convenient and robust way to migrate between different databases
as well.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Carl Boettiger <a href="mailto:cboettig@gmail.com">cboettig@gmail.com</a> (<a href="https://orcid.org/0000-0002-1642-628X">ORCID</a>) [copyright holder]
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Richard FitzJohn [contributor]
</p>
</li>
<li><p> Brandon Bertelsen <a href="mailto:brandon@bertelsen.ca">brandon@bertelsen.ca</a> [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/ropensci/arkdb">https://github.com/ropensci/arkdb</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ropensci/arkdb/issues">https://github.com/ropensci/arkdb/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ark'>Archive tables from a database as flat files</h2><span id='topic+ark'></span>

<h3>Description</h3>

<p>Archive tables from a database as flat files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ark(
  db_con,
  dir,
  streamable_table = streamable_base_tsv(),
  lines = 50000L,
  compress = c("bzip2", "gzip", "xz", "none"),
  tables = list_tables(db_con),
  method = c("keep-open", "window", "window-parallel", "sql-window"),
  overwrite = "ask",
  filter_statement = NULL,
  filenames = NULL,
  callback = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ark_+3A_db_con">db_con</code></td>
<td>
<p>a database connection</p>
</td></tr>
<tr><td><code id="ark_+3A_dir">dir</code></td>
<td>
<p>a directory where we will write the compressed text files output</p>
</td></tr>
<tr><td><code id="ark_+3A_streamable_table">streamable_table</code></td>
<td>
<p>interface for serializing/deserializing in chunks</p>
</td></tr>
<tr><td><code id="ark_+3A_lines">lines</code></td>
<td>
<p>the number of lines to use in each single chunk</p>
</td></tr>
<tr><td><code id="ark_+3A_compress">compress</code></td>
<td>
<p>file compression algorithm. Should be one of &quot;bzip2&quot; (default),
&quot;gzip&quot; (faster write times, a bit less compression), &quot;xz&quot;, or &quot;none&quot;, for
no compression.</p>
</td></tr>
<tr><td><code id="ark_+3A_tables">tables</code></td>
<td>
<p>a list of tables from the database that should be
archived.  By default, will archive all tables. Table list should specify
schema if appropriate, see examples.</p>
</td></tr>
<tr><td><code id="ark_+3A_method">method</code></td>
<td>
<p>method to use to query the database, see details.</p>
</td></tr>
<tr><td><code id="ark_+3A_overwrite">overwrite</code></td>
<td>
<p>should any existing text files of the same name be overwritten?
default is &quot;ask&quot;, which will ask for confirmation in an interactive session, and
overwrite in a non-interactive script.  TRUE will always overwrite, FALSE will
always skip such tables.</p>
</td></tr>
<tr><td><code id="ark_+3A_filter_statement">filter_statement</code></td>
<td>
<p>Typically an SQL &quot;WHERE&quot; clause, specific to your
dataset. (e.g., <code style="white-space: pre;">&#8288;WHERE year = 2013&#8288;</code>)</p>
</td></tr>
<tr><td><code id="ark_+3A_filenames">filenames</code></td>
<td>
<p>An optional vector of names that will be used to name the
files instead of using the tablename from the <code>tables</code> parameter.</p>
</td></tr>
<tr><td><code id="ark_+3A_callback">callback</code></td>
<td>
<p>An optional function that acts on the data.frame before it is
written to disk by <code>streamable_table</code>. It is recommended to use this on a single
table at a time. Callback functions must return a data.frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ark</code> will archive tables from a database as (compressed) tsv files.
Or other formats that have a <code style="white-space: pre;">&#8288;streamtable_table method&#8288;</code>, like parquet.
<code>ark</code> does this by reading only chunks at a time into memory, allowing it to
process tables that would be too large to read into memory all at once (which
is probably why you are using a database in the first place!)  Compressed
text files will likely take up much less space, making them easier to store and
transfer over networks.  Compressed plain-text files are also more archival
friendly, as they rely on widely available and long-established open source compression
algorithms and plain text, making them less vulnerable to loss by changes in
database technology and formats.
</p>
<p>In almost all cases, the default method should be the best choice.
If the <code><a href="DBI.html#topic+dbSendQuery">DBI::dbSendQuery()</a></code> implementation for your database platform returns the
full results to the client immediately rather than supporting chunking with <code>n</code>
parameter, you may want to use &quot;window&quot; method, which is the most generic.  The
&quot;sql-window&quot; method provides a faster alternative for databases like PostgreSQL that
support windowing natively (i.e. <code>BETWEEN</code> queries). Note that &quot;window-parallel&quot;
only works with <code>streamable_parquet</code>.
</p>


<h3>Value</h3>

<p>the path to <code>dir</code> where output files are created (invisibly), for piping.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# setup
library(dplyr)
dir &lt;- tempdir()
db &lt;- dbplyr::nycflights13_sqlite(tempdir())

## And here we go:
ark(db, dir)

## Not run: 

## For a Postgres DB with schema, we can append schema names first
## to each of the table names, like so:
schema_tables &lt;- dbGetQuery(db, sqlInterpolate(db,
  "SELECT table_name FROM information_schema.tables
WHERE table_schema = ?schema",
  schema = "schema_name"
))

ark(db, dir, tables = paste0("schema_name", ".", schema_tables$table_name))

## End(Not run)

</code></pre>

<hr>
<h2 id='arkdb_delete_db'>delete the local arkdb database</h2><span id='topic+arkdb_delete_db'></span>

<h3>Description</h3>

<p>delete the local arkdb database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arkdb_delete_db(db_dir = arkdb_dir(), ask = interactive())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arkdb_delete_db_+3A_db_dir">db_dir</code></td>
<td>
<p>neon database location</p>
</td></tr>
<tr><td><code id="arkdb_delete_db_+3A_ask">ask</code></td>
<td>
<p>Ask for confirmation first?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just a helper function that deletes the database
files.  Usually unnecessary but can be
helpful in resetting a corrupt database.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a db
dir &lt;- tempfile()
db &lt;- local_db(dir)

# Delete it
arkdb_delete_db(dir, ask = FALSE)
</code></pre>

<hr>
<h2 id='local_db'>Connect to a local stand-alone database</h2><span id='topic+local_db'></span>

<h3>Description</h3>

<p>This function will provide a connection to the best available database.
This function is a drop-in replacement for <code style="white-space: pre;">&#8288;[DBI::dbConnect]&#8288;</code> with behaviour
that makes it more subtle for R packages that need a database backend with
minimal complexity, as described in details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_db(
  dbdir = arkdb_dir(),
  driver = Sys.getenv("ARKDB_DRIVER", "duckdb"),
  readonly = FALSE,
  cache_connection = TRUE,
  memory_limit = getOption("duckdb_memory_limit", NA),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_db_+3A_dbdir">dbdir</code></td>
<td>
<p>Path to the database.</p>
</td></tr>
<tr><td><code id="local_db_+3A_driver">driver</code></td>
<td>
<p>Default driver, one of &quot;duckdb&quot;, &quot;MonetDBLite&quot;, &quot;RSQLite&quot;.
It will select the first one of those it finds available if a
driver is not set. This fallback can be overwritten either by explicit
argument or by setting the environmental variable <code>ARKDB_DRIVER</code>.</p>
</td></tr>
<tr><td><code id="local_db_+3A_readonly">readonly</code></td>
<td>
<p>Should the database be opened read-only? (duckdb only).
This allows multiple concurrent connections (e.g. from different R sessions)</p>
</td></tr>
<tr><td><code id="local_db_+3A_cache_connection">cache_connection</code></td>
<td>
<p>should we preserve a cache of the connection? allows
faster load times and prevents connection from being garbage-collected.  However,
keeping open a read-write connection to duckdb or MonetDBLite will block access of
other R sessions to the database.</p>
</td></tr>
<tr><td><code id="local_db_+3A_memory_limit">memory_limit</code></td>
<td>
<p>Set a memory limit for duckdb, in GB.  This can
also be set for the session by using options, e.g.
<code>options(duckdb_memory_limit=10)</code> for a limit of 10GB.  On most systems
duckdb will automatically set a limit to 80% of machine capacity if not
set explicitly.</p>
</td></tr>
<tr><td><code id="local_db_+3A_...">...</code></td>
<td>
<p>additional arguments (not used at this time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides several abstractions to <code style="white-space: pre;">&#8288;[DBI::dbConnect]&#8288;</code> to
provide a seamless backend for use inside other R packages.
</p>
<p>First, this  provides a generic method that allows the use of a <code style="white-space: pre;">&#8288;[RSQLite::SQLite]`` connection if nothing else is available, while being able to automatically select a much faster, more powerful backend from &#8288;</code><a href="duckdb.html#topic+duckdb">duckdb::duckdb</a>'
if available.  An argument or environmental variable can be used to override this
to manually set a database endpoint for testing purposes.
</p>
<p>Second, this function will cache the database connection in an R environment and
load that cache.  That means you can call <code>local_db()</code> as fast/frequently as you
like without causing errors that would occur by rapid calls to <code style="white-space: pre;">&#8288;[DBI::dbConnect]&#8288;</code>
</p>
<p>Third, this function defaults to persistent storage location set by <code style="white-space: pre;">&#8288;[tools::R_user_dir]&#8288;</code>
and configurable by setting the environmental variable <code>ARKDB_HOME</code>.  This allows
a package to provide persistent storage out-of-the-box, and easily switch that storage
to a temporary directory (e.g. for testing purposes, or custom user configuration) without
having to edit database calls directly.
</p>


<h3>Value</h3>

<p>Returns a <code style="white-space: pre;">&#8288;[DBIconnection]&#8288;</code> connection to the default database
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## OPTIONAL: you can first set an alternative home location,
## such as a temporary directory:
Sys.setenv(ARKDB_HOME = tempdir())

## Connect to the database:
db &lt;- local_db()

</code></pre>

<hr>
<h2 id='local_db_disconnect'>Disconnect from the arkdb database.</h2><span id='topic+local_db_disconnect'></span>

<h3>Description</h3>

<p>Disconnect from the arkdb database.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_db_disconnect(db = local_db(), env = arkdb_cache)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_db_disconnect_+3A_db">db</code></td>
<td>
<p>a DBI connection. By default, will call <a href="#topic+local_db">local_db</a> for the default connection.</p>
</td></tr>
<tr><td><code id="local_db_disconnect_+3A_env">env</code></td>
<td>
<p>The environment where the function looks for a connection.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function manually closes a connection to the <code>arkdb</code> database.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Disconnect from the database:
local_db_disconnect()

</code></pre>

<hr>
<h2 id='process_chunks'>process a table in chunks</h2><span id='topic+process_chunks'></span>

<h3>Description</h3>

<p>process a table in chunks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_chunks(
  file,
  process_fn,
  streamable_table = NULL,
  lines = 50000L,
  encoding = Sys.getenv("encoding", "UTF-8"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_chunks_+3A_file">file</code></td>
<td>
<p>path to a file</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_process_fn">process_fn</code></td>
<td>
<p>a function of a <code>chunk</code></p>
</td></tr>
<tr><td><code id="process_chunks_+3A_streamable_table">streamable_table</code></td>
<td>
<p>interface for serializing/deserializing in chunks</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_lines">lines</code></td>
<td>
<p>number of lines to read in a chunk.</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input files.</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>streamable_table$read</code> method.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>con &lt;- system.file("extdata/mtcars.tsv.gz", package = "arkdb")
dummy &lt;- function(x) message(paste(dim(x), collapse = " x "))
process_chunks(con, dummy, lines = 8)
</code></pre>

<hr>
<h2 id='streamable_base_csv'>streamable csv using base R functions</h2><span id='topic+streamable_base_csv'></span>

<h3>Description</h3>

<p>streamable csv using base R functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_base_csv()
</code></pre>


<h3>Details</h3>

<p>Follows the comma-separate-values standard using <code><a href="utils.html#topic+read.table">utils::read.table()</a></code>
</p>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+read.table">utils::read.table()</a></code>, <code><a href="utils.html#topic+write.table">utils::write.table()</a></code>
</p>

<hr>
<h2 id='streamable_base_tsv'>streamable tsv using base R functions</h2><span id='topic+streamable_base_tsv'></span>

<h3>Description</h3>

<p>streamable tsv using base R functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_base_tsv()
</code></pre>


<h3>Details</h3>

<p>Follows the tab-separate-values standard using <code><a href="utils.html#topic+read.table">utils::read.table()</a></code>,
see IANA specification at:
<a href="https://www.iana.org/assignments/media-types/text/tab-separated-values">https://www.iana.org/assignments/media-types/text/tab-separated-values</a>
</p>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+read.table">utils::read.table()</a></code>, <code><a href="utils.html#topic+write.table">utils::write.table()</a></code>
</p>

<hr>
<h2 id='streamable_parquet'>streamable chunked parquet using <code>arrow</code></h2><span id='topic+streamable_parquet'></span>

<h3>Description</h3>

<p>streamable chunked parquet using <code>arrow</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_parquet()
</code></pre>


<h3>Details</h3>

<p>Parquet files are streamed to disk by breaking them into chunks that are
equal to the <code>nlines</code> parameter in the initial call to <code>ark</code>. For each <code>tablename</code>, a
folder is created and the chunks are placed in the folder in the form <code style="white-space: pre;">&#8288;part-000000.parquet&#8288;</code>.
The software looks at the folder, and increments the name appropriately for the next
chunk. This is done intentionally so that users can take advantage of <code>arrow::open_dataset</code>
in the future, when coming back to review or perform analysis of these data.
</p>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+read_parquet">arrow::read_parquet()</a></code>, <code><a href="arrow.html#topic+write_parquet">arrow::write_parquet()</a></code>
</p>

<hr>
<h2 id='streamable_readr_csv'>streamable csv using <code>readr</code></h2><span id='topic+streamable_readr_csv'></span>

<h3>Description</h3>

<p>streamable csv using <code>readr</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_readr_csv()
</code></pre>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="readr.html#topic+read_delim">readr::read_csv()</a></code>, <code><a href="readr.html#topic+write_delim">readr::write_csv()</a></code>
</p>

<hr>
<h2 id='streamable_readr_tsv'>streamable tsv using <code>readr</code></h2><span id='topic+streamable_readr_tsv'></span>

<h3>Description</h3>

<p>streamable tsv using <code>readr</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_readr_tsv()
</code></pre>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="readr.html#topic+read_delim">readr::read_tsv()</a></code>, <code><a href="readr.html#topic+write_delim">readr::write_tsv()</a></code>
</p>

<hr>
<h2 id='streamable_table'>streamable table</h2><span id='topic+streamable_table'></span>

<h3>Description</h3>

<p>streamable table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_table(read, write, extension)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="streamable_table_+3A_read">read</code></td>
<td>
<p>read function. Arguments should be &quot;<code>file</code>&quot;
(must be able to take a <code><a href="base.html#topic+connection">connection()</a></code> object) and &quot;<code>...</code>&quot; (for)
additional arguments.</p>
</td></tr>
<tr><td><code id="streamable_table_+3A_write">write</code></td>
<td>
<p>write function. Arguments should be &quot;<code>data</code>&quot; (a data.frame),
<code>file</code> (must be able to take a <code><a href="base.html#topic+connection">connection()</a></code> object), and &quot;<code>omit_header</code>&quot;
logical, include header (initial write) or not (for appending subsequent
chunks)</p>
</td></tr>
<tr><td><code id="streamable_table_+3A_extension">extension</code></td>
<td>
<p>file extension to use (e.g. &quot;tsv&quot;, &quot;csv&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note several constraints on this design. The write method must be able
to take a generic R <code>connection</code> object (which will allow it to handle
the compression methods used, if any), and the read method must be able
to take a <code>textConnection</code> object.  <code>readr</code> functions handle these cases
out of the box, so the above method is easy to write.  Also note that
the write method must be able to <code>omit_header</code>. See the built-in methods
for more examples.
</p>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
streamable_readr_tsv &lt;- function() {
  streamable_table(
    function(file, ...) readr::read_tsv(file, ...),
    function(x, path, omit_header) {
      readr::write_tsv(x = x, path = path, omit_header = omit_header)
    },
    "tsv"
  )
}
</code></pre>

<hr>
<h2 id='streamable_vroom'>streamable tables using <code>vroom</code></h2><span id='topic+streamable_vroom'></span>

<h3>Description</h3>

<p>streamable tables using <code>vroom</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>streamable_vroom()
</code></pre>


<h3>Value</h3>

<p>a <code>streamable_table</code> object (S3)
</p>


<h3>See Also</h3>

<p><code><a href="readr.html#topic+read_delim">readr::read_tsv()</a></code>, <code><a href="readr.html#topic+write_delim">readr::write_tsv()</a></code>
</p>

<hr>
<h2 id='unark'>Unarchive a list of compressed tsv files into a database</h2><span id='topic+unark'></span>

<h3>Description</h3>

<p>Unarchive a list of compressed tsv files into a database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unark(
  files,
  db_con,
  streamable_table = NULL,
  lines = 50000L,
  overwrite = "ask",
  encoding = Sys.getenv("encoding", "UTF-8"),
  tablenames = NULL,
  try_native = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unark_+3A_files">files</code></td>
<td>
<p>vector of filenames to be read in. Must be <code>tsv</code>
format, optionally compressed using <code>bzip2</code>, <code>gzip</code>, <code>zip</code>,
or <code>xz</code> format at present.</p>
</td></tr>
<tr><td><code id="unark_+3A_db_con">db_con</code></td>
<td>
<p>a database src (<code>src_dbi</code> object from <code>dplyr</code>)</p>
</td></tr>
<tr><td><code id="unark_+3A_streamable_table">streamable_table</code></td>
<td>
<p>interface for serializing/deserializing in chunks</p>
</td></tr>
<tr><td><code id="unark_+3A_lines">lines</code></td>
<td>
<p>number of lines to read in a chunk.</p>
</td></tr>
<tr><td><code id="unark_+3A_overwrite">overwrite</code></td>
<td>
<p>should any existing text files of the same name be overwritten?
default is &quot;ask&quot;, which will ask for confirmation in an interactive session, and
overwrite in a non-interactive script.  TRUE will always overwrite, FALSE will
always skip such tables.</p>
</td></tr>
<tr><td><code id="unark_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input files.</p>
</td></tr>
<tr><td><code id="unark_+3A_tablenames">tablenames</code></td>
<td>
<p>vector of tablenames to be used for corresponding files.
By default, tables will be named using lowercase names from file basename with
special characters replaced with underscores (for SQL compatibility).</p>
</td></tr>
<tr><td><code id="unark_+3A_try_native">try_native</code></td>
<td>
<p>logical, default TRUE. Should we try to use a native bulk
import method for the database connection?  This can substantially speed up
read times and will fall back on the DBI method for any table that fails
to import.  Currently only MonetDBLite connections support this.</p>
</td></tr>
<tr><td><code id="unark_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>streamable_table$read</code> method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>unark</code> will read in a files in chunks and
write them into a database.  This is essential for processing
large compressed tables which may be too large to read into
memory before writing into a database.  In general, increasing
the <code>lines</code> parameter will result in a faster total transfer
but require more free memory for working with these larger chunks.
</p>
<p>If using <code>readr</code>-based streamable-table, you can suppress the progress bar
by using <code>options(readr.show_progress = FALSE)</code> when reading in large
files.
</p>


<h3>Value</h3>

<p>the database connection (invisibly)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Setup: create an archive.
library(dplyr)
dir &lt;- tempdir()
db &lt;- dbplyr::nycflights13_sqlite(tempdir())

## database -&gt; .tsv.bz2
ark(db, dir)

## list all files in archive (full paths)
files &lt;- list.files(dir, "bz2$", full.names = TRUE)

## Read archived files into a new database (another sqlite in this case)
new_db &lt;- DBI::dbConnect(RSQLite::SQLite())
unark(files, new_db)

## Prove table is returned successfully.
tbl(new_db, "flights")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
