<!DOCTYPE html><html><head><title>Help for package EESPCA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {EESPCA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#EESPCA-package'>
<p>Eigenvectors</p></a></li>
<li><a href='#computeApproxNormSquaredEigenvector'><p>Approximates the normed squared eigenvector loadings</p></a></li>
<li><a href='#computeResidualMatrix'><p>Calculates the residual matrix from the reduced rank reconstruction</p></a></li>
<li><a href='#eespca'><p>Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</p></a></li>
<li><a href='#eespcaCV'><p>Cross-validation for Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</p></a></li>
<li><a href='#eespcaForK'><p>Multi-PC version of Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</p></a></li>
<li><a href='#powerIteration'><p>Power iteration method for calculating principal eigenvector and eigenvalue.</p></a></li>
<li><a href='#reconstruct'><p>Calculates the reduced rank reconstruction</p></a></li>
<li><a href='#reconstructionError'><p>Calculates the reduced rank reconstruction error</p></a></li>
<li><a href='#rifleInit'><p>Computes the initial eigenvector for the rifle method of Tan et al.</p></a></li>
<li><a href='#riflePCACV'><p>Sparsity parameter selection via cross-validation for rifle method of Tan et al.</p></a></li>
<li><a href='#tpower'><p>Implementation of the Yuan and Zhang TPower method.</p></a></li>
<li><a href='#tpowerPCACV'><p>Sparsity parameter selection for the Yuan and Zhang TPower method using cross-validation.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Eigenvectors from Eigenvalues Sparse Principal Component
Analysis (EESPCA)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.0</td>
</tr>
<tr>
<td>Author:</td>
<td>H. Robert Frost</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>H. Robert Frost &lt;rob.frost@dartmouth.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains logic for computing sparse principal components via the EESPCA method, 
    which is based on an approximation of the eigenvector/eigenvalue identity. 
    Includes logic to support execution of the TPower and rifle sparse PCA methods,
    as well as logic to estimate the sparsity parameters used by EESPCA, TPower and rifle
    via cross-validation to minimize the out-of-sample reconstruction error.
    H. Robert Frost (2021) &lt;<a href="https://doi.org/10.1080%2F10618600.2021.1987254">doi:10.1080/10618600.2021.1987254</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), rifle (&ge; 1.0.0), MASS, PMA</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Copyright:</td>
<td>Dartmouth College</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-15 19:58:05 UTC; d37329b</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-15 20:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='EESPCA-package'>
Eigenvectors
</h2><span id='topic+EESPCA-package'></span>

<h3>Description</h3>

<p>Implementation of Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA).
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> EESPCA</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.7.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Note</h3>

<p>This work was supported by the National Institutes of Health grants K01LM012426, R21CA253408, P20GM130454,and P30CA023108.
</p>


<h3>Author(s)</h3>

<p>H. Robert Frost
</p>


<h3>References</h3>


<ul>
<li><p> Frost, H. R. (2022). Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA). Journal of Computational and Graphical Statistics.
</p>
</li></ul>


<hr>
<h2 id='computeApproxNormSquaredEigenvector'>Approximates the normed squared eigenvector loadings</h2><span id='topic+computeApproxNormSquaredEigenvector'></span>

<h3>Description</h3>

<p>Approximates the normed squared eigenvector loadings using a simplified version of the formula
associating normed squared eigenvector loadings with the eigenvalues of the full matrix and sub-matrices. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    computeApproxNormSquaredEigenvector(cov.X, v1, lambda1, max.iter=5, 
        lambda.diff.threshold=1e-6, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_cov.x">cov.X</code></td>
<td>
<p>Covariance matrix.</p>
</td></tr>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_v1">v1</code></td>
<td>
<p>Principal eigenvector of <code>cov.X</code>, i.e, the loadings of the first PC.</p>
</td></tr>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_lambda1">lambda1</code></td>
<td>
<p>Largest eigenvalue of <code>cov.X</code>.</p>
</td></tr>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for power iteration method when computing sub-matrix eigenvalues.
See description <code><a href="#topic+powerIteration">powerIteration</a></code>.</p>
</td></tr>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>Threshold for exiting the power iteration calculation. 
See description <code><a href="#topic+powerIteration">powerIteration</a></code>.</p>
</td></tr>
<tr><td><code id="computeApproxNormSquaredEigenvector_+3A_trace">trace</code></td>
<td>
<p>True if debugging messages should be displayed during execution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of approximate normed squared eigenvector loadings.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eespca">eespca</a></code>,<code><a href="#topic+powerIteration">powerIteration</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Estimate covariance matrix
    cov.X = cov(X)
    # Compute eigenvectors/values
    eigen.out = eigen(cov.X)
    v1 = eigen.out$vectors[,1]
    lambda1 = eigen.out$values[1]
    # Print true squared loadings
    v1^2
    # Compute approximate normed squared eigenvector loadings
    computeApproxNormSquaredEigenvector(cov.X=cov.X, v1=v1,
        lambda1=lambda1)  
</code></pre>

<hr>
<h2 id='computeResidualMatrix'>Calculates the residual matrix from the reduced rank reconstruction</h2><span id='topic+computeResidualMatrix'></span>

<h3>Description</h3>

<p>Utility function for computing the residual matrix formed by 
subtracting from <code>X</code> a reduced rank approximation of matrix <code>X</code> generated from 
the top k principal components contained in matrix <code>V</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    computeResidualMatrix(X,V,center=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeResidualMatrix_+3A_x">X</code></td>
<td>
<p>An n-by-p data matrix whose top k principal components are contained in the p-by-k matrix <code>V</code>.</p>
</td></tr>
<tr><td><code id="computeResidualMatrix_+3A_v">V</code></td>
<td>
<p>A p-by-k matrix containing the loadings for the top k principal components of <code>X</code>.</p>
</td></tr>
<tr><td><code id="computeResidualMatrix_+3A_center">center</code></td>
<td>
<p>If true (the default), <code>X</code> will be mean-centered before the residual matrix is computed.
If the PCs in <code>V</code> were computed via SVD on 
a mean-centered matrix or via eigen-decomposition of the sample covariance matrix, 
this should be set to true.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Residual matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Perform PCA
    prcomp.out = prcomp(X)
    # Get rank 2 residual matrix
    computeResidualMatrix(X=X, V=prcomp.out$rotation[,1:2])
</code></pre>

<hr>
<h2 id='eespca'>Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</h2><span id='topic+eespca'></span>

<h3>Description</h3>

<p>Computes the first sparse principal component of the specified data matrix using 
the Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA) method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    eespca(X, max.iter=20, sparse.threshold, lambda.diff.threshold=1e-6, 
        compute.sparse.lambda=FALSE, sub.mat.max.iter=5, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eespca_+3A_x">X</code></td>
<td>
<p>An n-by-p data matrix for which the first sparse PC will be computed.</p>
</td></tr>
<tr><td><code id="eespca_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for power iteration method. See <code><a href="#topic+powerIteration">powerIteration</a></code>.</p>
</td></tr>
<tr><td><code id="eespca_+3A_sparse.threshold">sparse.threshold</code></td>
<td>
<p>Threshold on loadings used to induce sparsity. 
Loadings below this value are set to 0. If not specified, defaults to <code>1/sqrt(p)</code>.</p>
</td></tr>
<tr><td><code id="eespca_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>Threshold for exiting the power iteration calculation. 
If the absolute relative difference in lambda is less than this threshold between subsequent iterations,
the power iteration method is terminated. See <code><a href="#topic+powerIteration">powerIteration</a></code>.</p>
</td></tr> 
<tr><td><code id="eespca_+3A_compute.sparse.lambda">compute.sparse.lambda</code></td>
<td>
<p>If true, the sparse loadings will be used to compute the sparse eigenvalue.</p>
</td></tr>
<tr><td><code id="eespca_+3A_sub.mat.max.iter">sub.mat.max.iter</code></td>
<td>
<p>Maximum iterations for computation of sub-matrix eigenvalues using
the power iteration method. To maximize performance, set to 1. Uses the same lambda.diff.threshold.</p>
</td></tr>
<tr><td><code id="eespca_+3A_trace">trace</code></td>
<td>
<p>True if debugging messages should be displayed during execution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the following elements:
</p>

<ul>
<li><p> &quot;v1&quot;: The first non-sparse PC as calculated via power iteration. 
</p>
</li>
<li><p> &quot;lambda1&quot;: The variance of the first non-sparse PC as calculated via power iteration.
</p>
</li>
<li><p> &quot;v1.sparse&quot;: First sparse PC.
</p>
</li>
<li><p> &quot;lambda1.sparse&quot;: Variance of the first sparse PC. NA if compute.sparse.lambda is FALSE.
</p>
</li>
<li><p> &quot;ratio&quot;: Vector of ratios of the sparse to non-sparse PC loadings.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Frost, H. R. (2021). Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA). arXiv e-prints. https://arxiv.org/abs/2006.01924  
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+eespcaForK">eespcaForK</a></code>,<code><a href="#topic+computeApproxNormSquaredEigenvector">computeApproxNormSquaredEigenvector</a></code>, <code><a href="#topic+powerIteration">powerIteration</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Compute first sparse PC loadings using default threshold
    eespca(X=X)
</code></pre>

<hr>
<h2 id='eespcaCV'>Cross-validation for Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</h2><span id='topic+eespcaCV'></span>

<h3>Description</h3>

<p>Performs cross-validation of EESPCA to determine the optimal 
sparsity threshold. Selection is based on the minimization of reconstruction error.
Based on the cross-validation approach of Witten et al. as implemented by the <code>SPC.cv</code> method in the PMA package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    eespcaCV(X, max.iter=20, sparse.threshold.values, nfolds=5, 
        lambda.diff.threshold=1e-6, compute.sparse.lambda=FALSE, 
        sub.mat.max.iter=5, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eespcaCV_+3A_x">X</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_max.iter">max.iter</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_sparse.threshold.values">sparse.threshold.values</code></td>
<td>
<p>Vector of threshold values to evaluate via cross-validation. 
See description for <code><a href="#topic+eespca">eespca</a></code> for details.</p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of cross-validation folds.</p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_compute.sparse.lambda">compute.sparse.lambda</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_sub.mat.max.iter">sub.mat.max.iter</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaCV_+3A_trace">trace</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the following elements:
</p>

<ul>
<li><p> &quot;cv&quot;: The mean of the out-of-sample reconstruction error computed for each threshold.
</p>
</li>
<li><p> &quot;cv.error&quot;: The standard deviations of the means of the out-of-sample reconstruction error computed for each
threshold.
</p>
</li>
<li><p> &quot;best.sparsity&quot;: Threshold value with the lowest mean reconstruction error.
</p>
</li>
<li><p> &quot;best.sparsity.1se&quot;: Threshold value whose mean reconstruction error is within 1 standard error of the lowest.      
</p>
</li>
<li><p> &quot;nonzerovs&quot;: Mean number of nonzero values for each threshold.
</p>
</li>
<li><p> &quot;sparse.threshold.values&quot;: Tested threshold values.
</p>
</li>
<li><p> &quot;nfolds&quot;: Number of cross-validation folds.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Frost, H. R. (2021). Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA). arXiv e-prints. https://arxiv.org/abs/2006.01924  
</p>
</li>
<li><p> Witten, D. M., Tibshirani, R., and Hastie, T. (2009). 
A penalized matrix decomposition, with applications to sparse principal components and 
canonical correlation analysis. Biostatistics, 10(3), 515-534.            
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+eespca">eespca</a></code>, <code><a href="PMA.html#topic+PMA">PMA</a>{SPC.cv}</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Generate range of threshold values to evaluate
    default.threshold = 1/sqrt(5)
    threshold.values = seq(from=.5*default.threshold, to=1.5*default.threshold, length.out=10)
    # Use 5-fold cross-validation to estimate optimal sparsity threshold
    eespcaCV(X=X, sparse.threshold.values=threshold.values)
</code></pre>

<hr>
<h2 id='eespcaForK'>Multi-PC version of Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA)</h2><span id='topic+eespcaForK'></span>

<h3>Description</h3>

<p>Computes multiple sparse principal components of the specified data matrix via sequential application of
the Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA) algorithm.
After computing the first sparse PC via the <code><a href="#topic+eespca">eespca</a></code> function, 
subsequent sparse PCs are computing by repeatedly applying <code><a href="#topic+eespca">eespca</a></code> to the residual matrix formed
by subtracting the reconstruction of <code>X</code> from the original <code>X</code>. 
Multiple sparse PCs are not guaranteed to be orthogonal. 
</p>
<p>Note that the accuracy of the sparse approximation declines substantially for PCs with very small
variances. To avoid this issue, <code>k</code> should not be set higher than the number of statistically 
significant PCs according to a Tracey-Widom test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    eespcaForK(X, k=2, max.iter=20, sparse.threshold, lambda.diff.threshold=1e-6, 
        compute.sparse.lambda=FALSE, sub.mat.max.iter=5, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eespcaForK_+3A_x">X</code></td>
<td>
<p>An n-by-p data matrix for which the first <code>k</code> sparse PCs will be computed.</p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_k">k</code></td>
<td>
<p>The number of sparse PCs to compute. The specified k must be 2 or greater (for k=1, use
the <code><a href="#topic+eespca">eespca</a></code> method). A check is made that k is not greater than the maximum theoretical
rank of X but, for performance reasons, a check is NOT made that
k is less than or equal to the actual rank of X. </p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_max.iter">max.iter</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_sparse.threshold">sparse.threshold</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_compute.sparse.lambda">compute.sparse.lambda</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_sub.mat.max.iter">sub.mat.max.iter</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
<tr><td><code id="eespcaForK_+3A_trace">trace</code></td>
<td>
<p>See description for <code><a href="#topic+eespca">eespca</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the following elements:
</p>

<ul>
<li><p> &quot;V&quot;: Matrix of sparse loadings for the first k PCs. 
</p>
</li>
<li><p> &quot;lambdas&quot;: Vector of variances of the first k sparse PCs.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Frost, H. R. (2021). Eigenvectors from Eigenvalues Sparse Principal Component Analysis (EESPCA). arXiv e-prints. https://arxiv.org/abs/2006.01924  
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+eespca">eespca</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Get first two sparse PCs
    eespcaForK(X=X, sparse.threshold=1/sqrt(5), k=2)
</code></pre>

<hr>
<h2 id='powerIteration'>Power iteration method for calculating principal eigenvector and eigenvalue.</h2><span id='topic+powerIteration'></span>

<h3>Description</h3>

<p>Computes the principal eigenvector and eigenvalue of the specified matrix using the power iteration method.
Includes support for truncating the estimated eigenvector on each iteration to retain just the
k eigenvector loadings with the largest absolute values with all other values set to 0,
i.e., the the TPower method by Yuan &amp; Zhang.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    powerIteration(X, k, v1.init, max.iter=10, lambda.diff.threshold=1e-6, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="powerIteration_+3A_x">X</code></td>
<td>
<p>Matrix for which the largest eigenvector and eigenvalue will be computed.</p>
</td></tr>
<tr><td><code id="powerIteration_+3A_k">k</code></td>
<td>
<p>If specified, the estimated eigenvector is truncated on each iteration to retain
only the k loadings with the largest absolute values, all other loadings are set to 0. 
Must be an integer between 1 and ncol(X).</p>
</td></tr> 
<tr><td><code id="powerIteration_+3A_v1.init">v1.init</code></td>
<td>
<p>If specified, the power iteration calculation will be initialized using this vector, otherwise,
the calculation will be initialized using a unit vector with equal values.</p>
</td></tr>
<tr><td><code id="powerIteration_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for power iteration method.</p>
</td></tr>
<tr><td><code id="powerIteration_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>Threshold for exiting the power iteration calculation. 
If the absolute relative difference in computed eigenvalue is less than this threshold between subsequent iterations,
the power iteration method is terminated.</p>
</td></tr>
<tr><td><code id="powerIteration_+3A_trace">trace</code></td>
<td>
<p>True if debugging messages should be displayed during execution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the following elements:
</p>

<ul>
<li><p> &quot;v1&quot;: The principal eigenvector of <code>X</code>.
</p>
</li>
<li><p> &quot;lambda&quot;: The largest eigenvalue of <code>X</code>.
</p>
</li>
<li><p> &quot;num.iter&quot;: Number of iterations of the power iteration method before termination.
</p>
</li></ul>



<h3>References</h3>

    
<ul>
<li><p> Yuan, X.-T. and Zhang, T. (2013). Truncated power method for sparse eigenvalue problems. 
J. Mach. Learn. Res., 14(1), 899-925.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+eespca">eespca</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Compute sample covariance matrix
    cov.X = cov(X)
    # Use power iteration to get first PC loadings using default initial vector
    powerIteration(X=cov.X)
</code></pre>

<hr>
<h2 id='reconstruct'>Calculates the reduced rank reconstruction</h2><span id='topic+reconstruct'></span>

<h3>Description</h3>

<p>Utility function for computing the reduced rank reconstruction of <code>X</code> using the PC 
loadings in <code>V</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    reconstruct(X,V,center=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstruct_+3A_x">X</code></td>
<td>
<p>An n-by-p data matrix whose top k principal components are contained the p-by-k matrix <code>V</code>.</p>
</td></tr>
<tr><td><code id="reconstruct_+3A_v">V</code></td>
<td>
<p>A p-by-k matrix containing the loadings for the top k principal components of <code>X</code>.</p>
</td></tr>
<tr><td><code id="reconstruct_+3A_center">center</code></td>
<td>
<p>If true (the default), <code>X</code> will be mean-centered before the reconstruction
is computed. If the PCs in <code>V</code> were computed via SVD on 
a mean-centered matrix or via eigen-decomposition of the sample covariance matrix, 
this should be set to true.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Reduced rank reconstruction of X.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Perform PCA
    prcomp.out = prcomp(X)
    # Get rank 2 reconstruction
    reconstruct(X, prcomp.out$rotation[,1:2])
</code></pre>

<hr>
<h2 id='reconstructionError'>Calculates the reduced rank reconstruction error</h2><span id='topic+reconstructionError'></span>

<h3>Description</h3>

<p>Utility function for computing the squared Frobenius norm of the residual matrix formed by 
subtracting from <code>X</code> a reduced rank approximation of matrix <code>X</code> generated from 
the top k principal components contained in matrix <code>V</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    reconstructionError(X,V,center=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstructionError_+3A_x">X</code></td>
<td>
<p>An n-by-p data matrix whose top k principal components are contained the p-by-k matrix <code>V</code>.</p>
</td></tr>
<tr><td><code id="reconstructionError_+3A_v">V</code></td>
<td>
<p>A p-by-k matrix containing the loadings for the top k principal components of <code>X</code>.</p>
</td></tr>
<tr><td><code id="reconstructionError_+3A_center">center</code></td>
<td>
<p>If true (the default), <code>X</code> will be mean-centered before the reconstruction
error is computed. If the PCs in <code>V</code> were computed via SVD on 
a mean-centered matrix or via eigen-decomposition of the sample covariance matrix, 
this should be set to true.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The squared Frobenius norm of the residual matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Perform PCA
    prcomp.out = prcomp(X)
    # Get rank 2 reconstruction error, which will be the minimum since the first 2 PCs are used
    reconstructionError(X, prcomp.out$rotation[,1:2])
    # Use all PCs to get approximately 0 reconstruction error
    reconstructionError(X, prcomp.out$rotation)    
</code></pre>

<hr>
<h2 id='rifleInit'>Computes the initial eigenvector for the rifle method of Tan et al.</h2><span id='topic+rifleInit'></span>

<h3>Description</h3>

<p>Computes the initial eigenvector for the rifle method of Tan et al. (as implemented by the
<code>rifle</code> method in the rifle R package) using the <code>initial.convex</code> method 
from the rifle package with lambda=sqrt(log(p)/n) and K=1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    rifleInit(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rifleInit_+3A_x">X</code></td>
<td>
<p>n-by-p data matrix to be evaluated via PCA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Initial eigenvector to use with rifle method.
</p>


<h3>References</h3>

    
<ul>
<li><p> Tan, K. M., Wang, Z., Liu, H., and Zhang, T. (2018). 
Sparse generalized eigenvalue problem: optimal statistical rates via truncated rayleigh flow. 
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(5), 1057-1086.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+riflePCACV">riflePCACV</a></code>, <code><a href="rifle.html#topic+rifle">rifle</a>{rifle}</code>, <code><a href="rifle.html#topic+rifle">rifle</a>{initial.convex}</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Compute initial eigenvector to use with rifle method
    v1.init = rifleInit(X)
    # Use with rifle method to get first PC loadings with 2 non-zero elements
    rifle(A=cov(X), B=diag(5), init=v1.init, k=2)
</code></pre>

<hr>
<h2 id='riflePCACV'>Sparsity parameter selection via cross-validation for rifle method of Tan et al.</h2><span id='topic+riflePCACV'></span>

<h3>Description</h3>

<p>Sparsity parameter selection for PCA-based rifle (as implemented by the
<code>rifle</code> method in the rifle package) using the cross-validation 
approach of Witten et al. as implemented by the <code>SPC.cv</code> method in the PMA package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    riflePCACV(X, k.values, nfolds=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="riflePCACV_+3A_x">X</code></td>
<td>
<p>n-by-p data matrix being evaluated via PCA.</p>
</td></tr>
<tr><td><code id="riflePCACV_+3A_k.values">k.values</code></td>
<td>
<p>Set of truncation parameter values to evaluate via cross-validation. 
Values must be between 1 and p.</p>
</td></tr>
<tr><td><code id="riflePCACV_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds for cross-validation</p>
</td></tr>    
</table>


<h3>Value</h3>

<p>k value that generated the smallest cross-validation error.
</p>


<h3>References</h3>

    
<ul>
<li><p> Tan, K. M., Wang, Z., Liu, H., and Zhang, T. (2018). 
Sparse generalized eigenvalue problem: optimal statistical rates via truncated rayleigh flow. 
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(5), 1057-1086.
</p>
</li>
<li><p> Witten, D. M., Tibshirani, R., and Hastie, T. (2009). 
A penalized matrix decomposition, with applications to sparse principal components and 
canonical correlation analysis. Biostatistics, 10(3), 515-534.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+rifleInit">rifleInit</a></code>, <code><a href="rifle.html#topic+rifle">rifle</a>{rifle}</code>, <code><a href="PMA.html#topic+PMA">PMA</a>{SPC.cv}</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Generate range of k values to evaluate
    k.values = 1:5
    # Use 5-fold cross-validation to estimate optimal k value
    riflePCACV(X=X, k.values=k.values)
</code></pre>

<hr>
<h2 id='tpower'>Implementation of the Yuan and Zhang TPower method.</h2><span id='topic+tpower'></span>

<h3>Description</h3>

<p>Implements the TPower method by Yuan and Zhang. 
Specifically, it computes the sparse principal eigenvector using power iteration method 
where the estimated eigenvector is truncated on each iteration to retain just the
k eigenvector loadings with the largest absolute values with all other values set to 0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    tpower(X, k, v1.init, max.iter=10, lambda.diff.threshold=1e-6, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tpower_+3A_x">X</code></td>
<td>
<p>Matrix for which the largest eigenvector and eigenvalue will be computed.</p>
</td></tr>
<tr><td><code id="tpower_+3A_k">k</code></td>
<td>
<p>Must be an integer between 1 and ncol(X). The estimated eigenvector is truncated on each iteration to retain
only the k loadings with the largest absolute values, all other loadings are set to 0.</p>
</td></tr> 
<tr><td><code id="tpower_+3A_v1.init">v1.init</code></td>
<td>
<p>If specified, the power iteration calculation will be initialized using this vector, otherwise,
the calculation will be initialized using a unit vector with equal values.</p>
</td></tr>
<tr><td><code id="tpower_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for power iteration method.</p>
</td></tr>
<tr><td><code id="tpower_+3A_lambda.diff.threshold">lambda.diff.threshold</code></td>
<td>
<p>Threshold for exiting the power iteration calculation. 
If the absolute relative difference in computed eigenvalues is less than this threshold between subsequent iterations,
the power iteration method is terminated.</p>
</td></tr>
<tr><td><code id="tpower_+3A_trace">trace</code></td>
<td>
<p>True if debugging messages should be displayed during execution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The estimated sparse principal eigenvector.</p>


<h3>References</h3>

    
<ul>
<li><p> Yuan, X.-T. and Zhang, T. (2013). Truncated power method for sparse eigenvalue problems. J. Mach. Learn. Res., 14(1), 899-925.    
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+powerIteration">powerIteration</a></code>,<code><a href="#topic+tpowerPCACV">tpowerPCACV</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Compute first sparse PC loadings with 2 non-zero elements
    tpower(X=cov(X), k=2)
</code></pre>

<hr>
<h2 id='tpowerPCACV'>Sparsity parameter selection for the Yuan and Zhang TPower method using cross-validation.</h2><span id='topic+tpowerPCACV'></span>

<h3>Description</h3>

<p>Sparsity parameter selection for PCA-based TPower using the cross-validation 
approach of Witten et al. as implemented by the <code>SPC.cv</code> method in the PMA package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    tpowerPCACV(X, k.values, nfolds=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tpowerPCACV_+3A_x">X</code></td>
<td>
<p>n-by-p data matrix being evaluated via PCA.</p>
</td></tr>
<tr><td><code id="tpowerPCACV_+3A_k.values">k.values</code></td>
<td>
<p>Set of truncation parameter values to evaluate via cross-validation. 
Values must be between 1 and p.</p>
</td></tr>
<tr><td><code id="tpowerPCACV_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds for cross-validation</p>
</td></tr>    
</table>


<h3>Value</h3>

<p>k value that generated the smallest cross-validation error.
</p>


<h3>References</h3>

    
<ul>
<li><p> Yuan, X.-T. and Zhang, T. (2013). Truncated power method for sparse eigenvalue problems. 
J. Mach. Learn. Res., 14(1), 899-925.
</p>
</li>
<li><p> Witten, D. M., Tibshirani, R., and Hastie, T. (2009). 
A penalized matrix decomposition, with applications to sparse principal components and 
canonical correlation analysis. Biostatistics, 10(3), 515-534.            
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+tpower">tpower</a></code>,<code><a href="PMA.html#topic+PMA">PMA</a>{SPC.cv}</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1)
    # Simulate 10x5 MVN data matrix
    X=matrix(rnorm(50), nrow=10)
    # Generate range of k values to evaluate
    k.values = 1:5
    # Use 5-fold cross-validation to estimate optimal k value
    tpowerPCACV(X=X, k.values=k.values)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
