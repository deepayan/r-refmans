<!DOCTYPE html><html><head><title>Help for package Rtnmin</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rtnmin}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tn'><p>Truncated Newton minimization of an unconstrained function.</p></a></li>
<li><a href='#tnbc'><p>Truncated Newton function minimization with bounds constraints</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Truncated Newton Function Minimization with Bounds Constraints</td>
</tr>
<tr>
<td>Version:</td>
<td>2016-7.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-07-07</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>John C Nash &lt;nashjc@uottawa.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Truncated Newton function minimization with bounds constraints
	based on the 'Matlab'/'Octave' codes of Stephen Nash.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-07-07 20:47:11 UTC; john</td>
</tr>
<tr>
<td>Author:</td>
<td>John C Nash [aut, cre, cph],
  Stephen G Nash [aut, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-07-07 23:56:42</td>
</tr>
</table>
<hr>
<h2 id='tn'>Truncated Newton minimization of an unconstrained function.</h2><span id='topic+tn'></span>

<h3>Description</h3>

<p>An R implementation of the Truncated Newton method
of Stephen Nash for  driver to call the unconstrained function
minimization. The algorithm is based on Nash (1979) 
</p>
<p>This set of codes is entirely in R to allow users to explore and
understand the method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   tn(x, fgfun, trace, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tn_+3A_x">x</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="tn_+3A_fgfun">fgfun</code></td>
<td>
<p>A function that returns the value of the objective at
the supplied set of parameters <code>par</code> using auxiliary data in
.... The gradient is returned as attribute &quot;gradient&quot;. 
The first argument of <code>fgfun</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="tn_+3A_trace">trace</code></td>
<td>
<p>TRUE if progress output is to be presented. (Not yet verified.)</p>
</td></tr>
<tr><td><code id="tn_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>fgfun</code> must return a numeric value in list item <code>f</code>
and a numeric vector in list item <code>g</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>xstar</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>The gradient of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>ierror</code></td>
<td>
<p>An integer indicating the situation on termination. <code>0</code>
indicates that the method believes it has succeeded; <code>2</code> that
more than <code>maxfun</code> (default 150*n, where there are n parameters);
<code>3</code> if the line search appears to have failed (which may not be serious);
and <code>-1</code> if there appears to be an error in the input parameters.</p>
</td></tr>
<tr><td><code>nfngr</code></td>
<td>
<p>A number giving a measure of how many conjugate gradient solutions
were used during the minimization process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Stephen G. Nash (1984) &quot;Newton-type minimization via the Lanczos method&quot;,
SIAM J Numerical Analysis, vol. 21, no. 4, pages 770-788.
</p>
<p>For Matlab code, see http://www.netlib.org/opt/tn
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
## All examples are in this .Rd file
##
## Rosenbrock Banana function
fr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
gr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    g1 &lt;- -400 * (x2 - x1*x1) * x1 - 2*(1-x1)
    g2 &lt;- 200*(x2 - x1*x1) 
    gg&lt;-c(g1, g2)
}

rosefg&lt;-function(x){
   f&lt;-fr(x)
   g&lt;-gr(x)
   attr(f, "gradient") &lt;- g
   f
}

x&lt;-c(-1.2, 1)

ansrosenbrock &lt;- tn(x, rosefg)
print(ansrosenbrock) # use print to allow copy to separate file that 
cat("Compare to optim\n")
ansoptrose &lt;- optim(x, fr, gr)
print(ansoptrose)


genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}
genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	gg
}

grosefg&lt;-function(x, gs=100.0) {
    f&lt;-genrose.f(x, gs)
    g&lt;-genrose.g(x, gs)
    attr(f, "gradient") &lt;- g
    f
}

n &lt;- 100
x &lt;- (1:100)/20
groseu&lt;-tn(x, grosefg, gs=10)
print(groseu)

groseuo &lt;- optim(x, fn=genrose.f, gr=genrose.g, method="BFGS",
      control=list(maxit=1000), gs=10)
cat("compare optim BFGS\n")
print(groseuo)


lower&lt;-1+(1:n)/100
upper&lt;-5-(1:n)/100
xmid&lt;-0.5*(lower+upper)

grosec&lt;-tnbc(xmid, grosefg, lower, upper)
print(grosec)

cat("compare L-BFGS-B\n")
grosecl &lt;- optim(par=xmid, fn=genrose.f, gr=genrose.g, 
     lower=lower, upper=upper, method="L-BFGS-B")
print(grosecl)


</code></pre>

<hr>
<h2 id='tnbc'>Truncated Newton function minimization with bounds constraints</h2><span id='topic+tnbc'></span>

<h3>Description</h3>

<p>A bounds-constarined R implementation of a truncated Newton method
for minimization of nonlinear functions subject to bounds (box) constraints. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   tnbc(x, fgfun, lower, upper, trace=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnbc_+3A_x">x</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_fgfun">fgfun</code></td>
<td>
<p>A function that returns the value of the objective at
the supplied set of parameters <code>par</code> using auxiliary data in
.... The gradient is returned as attribute &quot;gradient&quot;. 
The first argument of <code>fgfun</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="tnbc_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_trace">trace</code></td>
<td>
<p>Set TRUE to cause intermediate output to allow progress
to be followed.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>fgfun</code> must return a numeric value in list item <code>f</code>
and a numeric vector in list item <code>g</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>xstar</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>The gradient of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>ierror</code></td>
<td>
<p>An integer indicating the situation on termination. <code>0</code>
indicates that the method believes it has succeeded; <code>2</code> that
more than <code>maxfun</code> (default 150*n, where there are n parameters);
<code>3</code> if the line search appears to have failed (which may not be serious);
and <code>-1</code> if there appears to be an error in the input parameters.</p>
</td></tr>
<tr><td><code>nfngr</code></td>
<td>
<p>A number giving a measure of how many conjugate gradient solutions
were used during the minimization process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Stephen G. Nash (1984) &quot;Newton-type minimization via the Lanczos method&quot;,
SIAM J Numerical Analysis, vol. 21, no. 4, pages 770-788.
</p>
<p>For Matlab code, see http://www.netlib.org/opt/tn
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## See tn.Rd

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
