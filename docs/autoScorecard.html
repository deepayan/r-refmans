<!DOCTYPE html><html><head><title>Help for package autoScorecard</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {autoScorecard}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#auto_scorecard'><p>Functions to Automatically Generate Scorecards</p></a></li>
<li><a href='#best_iv'><p>Calculate the Best IV Value for the Binned Data</p></a></li>
<li><a href='#best_vs'><p>The Combination of Two Bins Produces the Best Binning Result</p></a></li>
<li><a href='#binning_eqfreq'><p>Equal Frequency Binning</p></a></li>
<li><a href='#binning_eqwid'><p>Equal Width Binning</p></a></li>
<li><a href='#binning_kmean'><p>The K-means Binning</p>
The k-means binning method first gives the center number, classifies the observation points using the Euclidean distance calculation and the distance from the center point,
and then recalculates the center point until the center point no longer changes, and uses the classification result as the binning of the result.</a></li>
<li><a href='#bins_chim'><p>Chi-Square Binning</p>
Chi-square binning, using the ChiMerge algorithm for bottom-up merging based on the chi-square test.</a></li>
<li><a href='#bins_tree'><p>Automatic Binning Based on Decision Tree</p>
Automatic Binning Based on  Decision Tree(rpart).</a></li>
<li><a href='#bins_unsupervised'><p>Unsupervised Automatic Binning Function</p>
By setting bin_nums, perform three unsupervised automatic binning</a></li>
<li><a href='#comparison_two'><p>Compare the Distribution of the Two Variable</p>
Draw box plots, cdf plot , QQ plots and histograms for two data.</a></li>
<li><a href='#comparison_two_data'><p>Compare the Distribution of the Two Data</p></a></li>
<li><a href='#data_detect'><p>Data Description Function</p></a></li>
<li><a href='#filter_var'><p>Data Filtering</p></a></li>
<li><a href='#get_IV'><p>Function to Calculate IV Value</p></a></li>
<li><a href='#noauto_scorecard'><p>Manually Input Parameters to Generate Scorecards</p></a></li>
<li><a href='#noauto_scorecard2'><p>Manually Input Parameters to Generate Scorecards</p>
The basic score is dispersed into each feature score</a></li>
<li><a href='#plot_board'><p>Data Painter Function</p>
Draw K-S diagram, Lorenz diagram, lift diagram and  AUC diagram.</a></li>
<li><a href='#psi_cal'><p>PSI Calculation Function</p></a></li>
<li><a href='#rep_woe'><p>Replace Feature Data by Binning Template</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fully Automatic Generation of Scorecards</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tai-Sen Zheng &lt;jc3802201@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Provides an efficient suite of R tools for scorecard modeling, analysis, and visualization. 
    Including equal frequency binning, equidistant binning, 
    K-means binning, chi-square binning, decision tree binning, data screening, manual parameter modeling, 
    fully automatic generation of scorecards, etc. 
    This package is designed to make scorecard development easier and faster.
    References include:
    1. <a href="http://shichen.name/posts/">http://shichen.name/posts/</a>. 
    2. Dong-feng Li(Peking University),Class PPT.
    3. <a href="https://zhuanlan.zhihu.com/p/389710022">https://zhuanlan.zhihu.com/p/389710022</a>. 
    4. <a href="https://www.zhangshengrong.com/p/281oqR9JNw/">https://www.zhangshengrong.com/p/281oqR9JNw/</a>. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/AGPL-3">AGPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>infotheo, ROCR, rpart, discretization, stats, graphics,
grDevices, corrplot, ggplot2</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-07 18:54:46 UTC; admin</td>
</tr>
<tr>
<td>Author:</td>
<td>Tai-Sen Zheng [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-13 08:50:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='auto_scorecard'>Functions to Automatically Generate Scorecards</h2><span id='topic+auto_scorecard'></span>

<h3>Description</h3>

<p>Functions to Automatically Generate Scorecards
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_scorecard(
  feature = accepts,
  key_var = "application_id",
  y_var = "bad_ind",
  sample_rate = 0.7,
  base0 = FALSE,
  points0 = 600,
  odds0 = 1/20,
  pdo = 50,
  k = 2,
  max_depth = 3,
  tree_p = 0.1,
  missing_rate = 0,
  single_var_rate = 1,
  iv_set = 0.02,
  char_to_number = TRUE,
  na.omit = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_scorecard_+3A_feature">feature</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_sample_rate">sample_rate</code></td>
<td>
<p>Training set sampling percentage.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_base0">base0</code></td>
<td>
<p>Whether the scorecard base score is 0.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_points0">points0</code></td>
<td>
<p>Base point.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_odds0">odds0</code></td>
<td>
<p>odds.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_pdo">pdo</code></td>
<td>
<p>Point-to Double Odds.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_k">k</code></td>
<td>
<p>Each scale doubles the probability of default several times.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_max_depth">max_depth</code></td>
<td>
<p>Set the maximum depth of any node of the final tree, with the root node counted as depth 0. Values greater than 30 rpart will give nonsense results on 32-bit machines.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_tree_p">tree_p</code></td>
<td>
<p>Meet the following conversion formula: minbucket = round(  p*nrow( df )).Smallest bucket(rpart):Minimum number of observations in any terminal &lt;leaf&gt; node.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_missing_rate">missing_rate</code></td>
<td>
<p>Data missing rate, variables smaller than this setting will be deleted.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_single_var_rate">single_var_rate</code></td>
<td>
<p>The maximum proportion of a single variable, the variable greater than the setting will be deleted.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_iv_set">iv_set</code></td>
<td>
<p>IV value minimum threshold, variable IV value less than the setting will be deleted.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_char_to_number">char_to_number</code></td>
<td>
<p>Whether to convert character variables to numeric.</p>
</td></tr>
<tr><td><code id="auto_scorecard_+3A_na.omit">na.omit</code></td>
<td>
<p>na.omit returns the object with incomplete cases removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing data, bins, scorecards and models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv(system.file("extdata", "accepts.csv", package = "autoScorecard" ))
auto_scorecard1 &lt;- auto_scorecard( feature = accepts[1:2000,], key_var= "application_id",
y_var = "bad_ind",sample_rate = 0.7, points0 = 600, odds0=1/20, pdo = 50, max_depth = 3,
tree_p = 0.1, missing_rate = 0, single_var_rate = 1, iv_set = 0.02,
char_to_number = TRUE , na.omit = TRUE)
</code></pre>

<hr>
<h2 id='best_iv'>Calculate the Best IV Value for the Binned Data</h2><span id='topic+best_iv'></span>

<h3>Description</h3>

<p>Calculate the Best IV Value for the Binned Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_iv(df, variable, bin, method, label_iv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_iv_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="best_iv_+3A_variable">variable</code></td>
<td>
<p>Name of variable.</p>
</td></tr>
<tr><td><code id="best_iv_+3A_bin">bin</code></td>
<td>
<p>Name of bins.</p>
</td></tr>
<tr><td><code id="best_iv_+3A_method">method</code></td>
<td>
<p>Name of method.</p>
</td></tr>
<tr><td><code id="best_iv_+3A_label_iv">label_iv</code></td>
<td>
<p>Name of IV.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of best IV, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata" , "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
f_1 &lt;-bins_unsupervised(  df = feature , id="application_id" , label="bad_ind" ,
methods = c("k_means", "equal_width","equal_freq"  )  ,  bin_nums=10  )
best1 &lt;- best_iv( df=f_1 ,bin=c('bins') ,  method = c('method') ,
variable= c( "variable" )  ,label_iv='miv'  )
</code></pre>

<hr>
<h2 id='best_vs'>The Combination of Two Bins Produces the Best Binning Result</h2><span id='topic+best_vs'></span>

<h3>Description</h3>

<p>The Combination of Two Bins Produces the Best Binning Result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_vs(df1, df2, variable = "variable", label_iv = "miv")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_vs_+3A_df1">df1</code></td>
<td>
<p>A binned data.</p>
</td></tr>
<tr><td><code id="best_vs_+3A_df2">df2</code></td>
<td>
<p>A binned data.</p>
</td></tr>
<tr><td><code id="best_vs_+3A_variable">variable</code></td>
<td>
<p>A name of X variable.</p>
</td></tr>
<tr><td><code id="best_vs_+3A_label_iv">label_iv</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of best IV.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv(system.file( "extdata", "accepts.csv", package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
all2 &lt;- bins_tree(df = feature, key_var= "application_id", y_var= "bad_ind"
, max_depth = 3, p = 0.1 )
f_1 &lt;-bins_unsupervised(  df = feature , id="application_id" , label="bad_ind" ,
methods = c("k_means", "equal_width","equal_freq"  )  ,  bin_nums=10  )
best1 &lt;- best_iv( df=f_1 ,bin=c('bins') ,  method = c('method') ,
variable= c( "variable" )  ,label_iv='miv'  )
vs1 &lt;- best_vs( df1 = all2[,-c(3)], df2 = best1[,-c(1:2)] ,variable="variable" ,label_iv='miv' )
</code></pre>

<hr>
<h2 id='binning_eqfreq'>Equal Frequency Binning</h2><span id='topic+binning_eqfreq'></span>

<h3>Description</h3>

<p>Equal Frequency Binning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binning_eqfreq(df, feat, label, nbins = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binning_eqfreq_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="binning_eqfreq_+3A_feat">feat</code></td>
<td>
<p>A name of dependent variable.</p>
</td></tr>
<tr><td><code id="binning_eqfreq_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="binning_eqfreq_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins,default:3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv", package ="autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
binning_eqfreq1 &lt;- binning_eqfreq( df= feature, feat= 'tot_derog', label = 'bad_ind', nbins = 3)
</code></pre>

<hr>
<h2 id='binning_eqwid'>Equal Width Binning</h2><span id='topic+binning_eqwid'></span>

<h3>Description</h3>

<p>Equal Width Binning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binning_eqwid(df, feat, label, nbins = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binning_eqwid_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="binning_eqwid_+3A_feat">feat</code></td>
<td>
<p>A name of dependent variable.</p>
</td></tr>
<tr><td><code id="binning_eqwid_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="binning_eqwid_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins,default:3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
binning_eqwid1 &lt;- binning_eqwid( df = feature, feat = 'tot_derog', label = 'bad_ind', nbins = 3 )
</code></pre>

<hr>
<h2 id='binning_kmean'>The K-means Binning
The k-means binning method first gives the center number, classifies the observation points using the Euclidean distance calculation and the distance from the center point,
and then recalculates the center point until the center point no longer changes, and uses the classification result as the binning of the result.</h2><span id='topic+binning_kmean'></span>

<h3>Description</h3>

<p>The K-means Binning
The k-means binning method first gives the center number, classifies the observation points using the Euclidean distance calculation and the distance from the center point,
and then recalculates the center point until the center point no longer changes, and uses the classification result as the binning of the result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binning_kmean(df, feat, label, nbins = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binning_kmean_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="binning_kmean_+3A_feat">feat</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="binning_kmean_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="binning_kmean_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins,default:3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata" , "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
ddd &lt;- binning_kmean( df = feature, feat= 'loan_term', label = 'bad_ind', nbins = 3)
</code></pre>

<hr>
<h2 id='bins_chim'>Chi-Square Binning
Chi-square binning, using the ChiMerge algorithm for bottom-up merging based on the chi-square test.</h2><span id='topic+bins_chim'></span>

<h3>Description</h3>

<p>Chi-Square Binning
Chi-square binning, using the ChiMerge algorithm for bottom-up merging based on the chi-square test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bins_chim(df, key_var, y_var, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bins_chim_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="bins_chim_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="bins_chim_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="bins_chim_+3A_alpha">alpha</code></td>
<td>
<p>Significance level(discretization);</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature2 &lt;- stats::na.omit( accepts[1:200,c(1,3,7:23)] )
all3 &lt;- bins_chim( df = feature2 , key_var = "application_id", y_var = "bad_ind" , alpha=0.1 )
</code></pre>

<hr>
<h2 id='bins_tree'>Automatic Binning Based on Decision Tree
Automatic Binning Based on  Decision Tree(rpart).</h2><span id='topic+bins_tree'></span>

<h3>Description</h3>

<p>Automatic Binning Based on Decision Tree
Automatic Binning Based on  Decision Tree(rpart).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bins_tree(df, key_var, y_var, max_depth = 3, p = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bins_tree_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="bins_tree_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="bins_tree_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="bins_tree_+3A_max_depth">max_depth</code></td>
<td>
<p>Set the maximum depth of any node of the final tree, with the root node counted as depth 0. Values greater than 30 rpart will give nonsense results on 32-bit machines.</p>
</td></tr>
<tr><td><code id="bins_tree_+3A_p">p</code></td>
<td>
<p>Meet the following conversion formula: minbucket = round(p*nrow(df)).Smallest bucket(rpart):Minimum number of observations in any terminal &lt;leaf&gt; node.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv(system.file( "extdata", "accepts.csv", package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
all2 &lt;- bins_tree(df = feature, key_var= "application_id", y_var= "bad_ind"
, max_depth = 3, p = 0.1 )
</code></pre>

<hr>
<h2 id='bins_unsupervised'>Unsupervised Automatic Binning Function
By setting bin_nums, perform three unsupervised automatic binning</h2><span id='topic+bins_unsupervised'></span>

<h3>Description</h3>

<p>Unsupervised Automatic Binning Function
By setting bin_nums, perform three unsupervised automatic binning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bins_unsupervised(
  df,
  id,
  label,
  methods = c("k_means", "equal_width", "equal_freq"),
  bin_nums
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bins_unsupervised_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="bins_unsupervised_+3A_id">id</code></td>
<td>
<p>A name of index.</p>
</td></tr>
<tr><td><code id="bins_unsupervised_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="bins_unsupervised_+3A_methods">methods</code></td>
<td>
<p>Simultaneously calculate three kinds of unsupervised binning(&quot;k_means&quot;,&quot;equal_width&quot;,&quot;equal_freq&quot; ), the parameters only determine the final output result.</p>
</td></tr>
<tr><td><code id="bins_unsupervised_+3A_bin_nums">bin_nums</code></td>
<td>
<p>Number of bins.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, including the contents of the bin, the upper bound of the bin, the lower bound of the bin, and all the contents returned by the get_IV function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata" , "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
f_1 &lt;-bins_unsupervised(  df = feature , id="application_id" , label="bad_ind" ,
methods = c("k_means", "equal_width","equal_freq"  )  ,  bin_nums=10  )
</code></pre>

<hr>
<h2 id='comparison_two'>Compare the Distribution of the Two Variable
Draw box plots, cdf plot , QQ plots and histograms for two data.</h2><span id='topic+comparison_two'></span>

<h3>Description</h3>

<p>Compare the Distribution of the Two Variable
Draw box plots, cdf plot , QQ plots and histograms for two data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comparison_two(var_A, var_B, name_A, name_B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comparison_two_+3A_var_a">var_A</code></td>
<td>
<p>A variable.</p>
</td></tr>
<tr><td><code id="comparison_two_+3A_var_b">var_B</code></td>
<td>
<p>A variable.</p>
</td></tr>
<tr><td><code id="comparison_two_+3A_name_a">name_A</code></td>
<td>
<p>The name of data A.</p>
</td></tr>
<tr><td><code id="comparison_two_+3A_name_b">name_B</code></td>
<td>
<p>The name of data B.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv(system.file("extdata", "accepts.csv", package = "autoScorecard" ))
comparison_two( var_A = accepts$purch_price ,var_B = accepts$tot_rev_line ,
name_A = 'purch_price' , name_B = "tot_rev_line"  )
</code></pre>

<hr>
<h2 id='comparison_two_data'>Compare the Distribution of the Two Data</h2><span id='topic+comparison_two_data'></span>

<h3>Description</h3>

<p>Compare the Distribution of the Two Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comparison_two_data(df1, df2, key_var, y_var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comparison_two_data_+3A_df1">df1</code></td>
<td>
<p>A data.</p>
</td></tr>
<tr><td><code id="comparison_two_data_+3A_df2">df2</code></td>
<td>
<p>A data.</p>
</td></tr>
<tr><td><code id="comparison_two_data_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="comparison_two_data_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
d = sort( sample( nrow( feature ), nrow( feature )*0.7))
train &lt;- feature[d,]
test &lt;- feature[-d,]
comparison_two_data( df1 = train , df2 =  test ,
key_var = c("application_id","account_number"), y_var="bad_ind"   )
</code></pre>

<hr>
<h2 id='data_detect'>Data Description Function</h2><span id='topic+data_detect'></span>

<h3>Description</h3>

<p>Data Description Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_detect(df, key_var, y_var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_detect_+3A_df">df</code></td>
<td>
<p>A data.</p>
</td></tr>
<tr><td><code id="data_detect_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="data_detect_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of data description.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv(system.file("extdata", "accepts.csv", package = "autoScorecard" ))
aaa &lt;- data_detect( df = accepts, key_var = c("application_id","account_number") ,
 y_var = "bad_ind" )
</code></pre>

<hr>
<h2 id='filter_var'>Data Filtering</h2><span id='topic+filter_var'></span>

<h3>Description</h3>

<p>Data Filtering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_var(
  df,
  key_var,
  y_var,
  missing_rate,
  single_var_rate,
  iv_set,
  char_to_number = TRUE,
  na.omit = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_var_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_missing_rate">missing_rate</code></td>
<td>
<p>Data missing rate, variables smaller than this setting will be deleted.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_single_var_rate">single_var_rate</code></td>
<td>
<p>The maximum proportion of a single variable, the variable greater than the setting will be deleted.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_iv_set">iv_set</code></td>
<td>
<p>IV value minimum threshold, variable IV value less than the setting will be deleted.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_char_to_number">char_to_number</code></td>
<td>
<p>Whether to convert character variables to numeric.</p>
</td></tr>
<tr><td><code id="filter_var_+3A_na.omit">na.omit</code></td>
<td>
<p>na.omit returns the object with incomplete cases removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata" , "accepts.csv",package = "autoScorecard" ))
fff1 &lt;- filter_var( df = accepts, key_var = "application_id", y_var = "bad_ind", missing_rate = 0,
single_var_rate = 1, iv_set = 0.02 )
</code></pre>

<hr>
<h2 id='get_IV'>Function to Calculate IV Value</h2><span id='topic+get_IV'></span>

<h3>Description</h3>

<p>Function to Calculate IV Value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_IV(df, feat, label, E = 0, woeInf.rep = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_IV_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="get_IV_+3A_feat">feat</code></td>
<td>
<p>A name of dependent variable.</p>
</td></tr>
<tr><td><code id="get_IV_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="get_IV_+3A_e">E</code></td>
<td>
<p>Constant, should be set to [0,1], used to prevent calculation overflow due to no data in binning.</p>
</td></tr>
<tr><td><code id="get_IV_+3A_woeinf.rep">woeInf.rep</code></td>
<td>
<p>Woe replaces the constant, and when woe is positive or negative infinity, it is replaced by a constant.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame including counts, proportions, odds, woe, and IV values for each stratum.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv", package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
iv1 = get_IV( df= feature ,feat ='tot_derog' , label ='bad_ind'  )
</code></pre>

<hr>
<h2 id='noauto_scorecard'>Manually Input Parameters to Generate Scorecards</h2><span id='topic+noauto_scorecard'></span>

<h3>Description</h3>

<p>Manually Input Parameters to Generate Scorecards
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noauto_scorecard(
  bins_card,
  fit,
  bins_woe,
  points0 = 600,
  odds0 = 1/19,
  pdo = 50,
  k = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noauto_scorecard_+3A_bins_card">bins_card</code></td>
<td>
<p>Binning template.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_fit">fit</code></td>
<td>
<p>See glm stats.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_bins_woe">bins_woe</code></td>
<td>
<p>A data frame of woe with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_points0">points0</code></td>
<td>
<p>Base point.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_odds0">odds0</code></td>
<td>
<p>odds.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_pdo">pdo</code></td>
<td>
<p>Point-to Double Odds.</p>
</td></tr>
<tr><td><code id="noauto_scorecard_+3A_k">k</code></td>
<td>
<p>Each scale doubles the probability of default several times.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with score ratings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
d = sort( sample( nrow( feature ), nrow( feature )*0.7))
train &lt;- feature[d,]
test &lt;- feature[-d,]
treebins_train &lt;- bins_tree( df = train, key_var = "application_id", y_var="bad_ind",
max_depth=3, p=0.1)
woe_train &lt;- rep_woe( df= train , key_var = "application_id", y_var = "bad_ind" ,
tool = treebins_train ,var_label = "variable",col_woe = 'woe', lower = 'lower' , upper = 'upper')
woe_test &lt;- rep_woe(  df = test , key_var ="application_id", y_var= "bad_ind",
tool = treebins_train ,var_label= "variable",
    col_woe = 'woe', lower = 'lower' ,upper = 'upper'  )
lg &lt;- stats::glm( bad_ind~. , family = stats::binomial( link = 'logit' ) , data = woe_train )
lg_both &lt;- stats::step( lg , direction = "both")
Score1 &lt;- noauto_scorecard( bins_card= woe_test , fit =lg_both , bins_woe = treebins_train ,
points0 = 600 , odds0 = 1/20 , pdo = 50 )
</code></pre>

<hr>
<h2 id='noauto_scorecard2'>Manually Input Parameters to Generate Scorecards
The basic score is dispersed into each feature score</h2><span id='topic+noauto_scorecard2'></span>

<h3>Description</h3>

<p>Manually Input Parameters to Generate Scorecards
The basic score is dispersed into each feature score
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noauto_scorecard2(
  bins_card,
  fit,
  bins_woe,
  points0 = 600,
  odds0 = 1/19,
  pdo = 50,
  k = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noauto_scorecard2_+3A_bins_card">bins_card</code></td>
<td>
<p>Binning template.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_fit">fit</code></td>
<td>
<p>See glm stats.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_bins_woe">bins_woe</code></td>
<td>
<p>Base point.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_points0">points0</code></td>
<td>
<p>odds.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_odds0">odds0</code></td>
<td>
<p>Point-to Double Odds.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_pdo">pdo</code></td>
<td>
<p>A data frame of woe with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="noauto_scorecard2_+3A_k">k</code></td>
<td>
<p>Each scale doubles the probability of default several times.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with score ratings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
d = sort( sample( nrow( feature ), nrow( feature )*0.7))
train &lt;- feature[d,]
test &lt;- feature[-d,]
treebins_train &lt;- bins_tree( df = train, key_var = "application_id", y_var="bad_ind",
max_depth=3, p=0.1)
woe_train &lt;- rep_woe( df= train , key_var = "application_id", y_var = "bad_ind" ,
tool = treebins_train ,var_label = "variable",col_woe = 'woe', lower = 'lower' , upper = 'upper')
woe_test &lt;- rep_woe(  df = test , key_var ="application_id", y_var= "bad_ind",
tool = treebins_train ,var_label= "variable",
    col_woe = 'woe', lower = 'lower' ,upper = 'upper'  )
lg &lt;- stats::glm( bad_ind~. , family = stats::binomial( link = 'logit' ) , data = woe_train )
lg_both &lt;- stats::step( lg , direction = "both")
Score2 &lt;- noauto_scorecard2( bins_card= woe_test , fit =lg_both , bins_woe = treebins_train ,
points0 = 600 , odds0 = 1/20 , pdo = 50 )
</code></pre>

<hr>
<h2 id='plot_board'>Data Painter Function
Draw K-S diagram, Lorenz diagram, lift diagram and  AUC diagram.</h2><span id='topic+plot_board'></span>

<h3>Description</h3>

<p>Data Painter Function
Draw K-S diagram, Lorenz diagram, lift diagram and  AUC diagram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_board(label, pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_board_+3A_label">label</code></td>
<td>
<p>A target variable.</p>
</td></tr>
<tr><td><code id="plot_board_+3A_pred">pred</code></td>
<td>
<p>A predictor variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
d = sort( sample( nrow( feature ), nrow( feature )*0.7))
train &lt;- feature[d,]
test &lt;- feature[-d,]
treebins_train &lt;- bins_tree( df = train, key_var = "application_id", y_var="bad_ind",
max_depth=3, p=0.1)
woe_train &lt;- rep_woe( df= train , key_var = "application_id", y_var = "bad_ind" ,
tool = treebins_train ,var_label = "variable",col_woe = 'woe', lower = 'lower' , upper = 'upper')
woe_test &lt;- rep_woe(  df = test , key_var ="application_id", y_var= "bad_ind",
tool = treebins_train ,var_label= "variable",
    col_woe = 'woe', lower = 'lower' ,upper = 'upper'  )
lg&lt;-stats::glm(bad_ind~.,family=stats::binomial(link='logit'),data= woe_train)
lg_both&lt;-stats::step(lg,direction = "both")
logit&lt;-stats::predict(lg_both,woe_test)
woe_test$lg_both_p&lt;-exp(logit)/(1+exp(logit))
plot_board( label= woe_test$bad_ind, pred = woe_test$lg_both_p   )
</code></pre>

<hr>
<h2 id='psi_cal'>PSI Calculation Function</h2><span id='topic+psi_cal'></span>

<h3>Description</h3>

<p>PSI Calculation Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psi_cal(df_train, df_test, feat, label, nbins = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psi_cal_+3A_df_train">df_train</code></td>
<td>
<p>Train data.</p>
</td></tr>
<tr><td><code id="psi_cal_+3A_df_test">df_test</code></td>
<td>
<p>Test data.</p>
</td></tr>
<tr><td><code id="psi_cal_+3A_feat">feat</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="psi_cal_+3A_label">label</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="psi_cal_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of PSI.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv" , package = "autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
d = sort( sample( nrow( feature ), nrow( feature )*0.7))
train &lt;- feature[d,]
test &lt;- feature[-d,]
treebins_train &lt;- bins_tree( df = train, key_var = "application_id", y_var="bad_ind",
max_depth=3, p=0.1)
woe_train &lt;- rep_woe( df= train , key_var = "application_id", y_var = "bad_ind" ,
tool = treebins_train ,var_label = "variable",col_woe = 'woe', lower = 'lower' , upper = 'upper')
woe_test &lt;- rep_woe(  df = test , key_var ="application_id", y_var= "bad_ind",
tool = treebins_train ,var_label= "variable",
    col_woe = 'woe', lower = 'lower' ,upper = 'upper'  )
lg &lt;- stats::glm( bad_ind~. , family = stats::binomial( link = 'logit' ) , data = woe_train )
lg_both &lt;- stats::step( lg , direction = "both")
Score_2 &lt;- noauto_scorecard( bins_card= woe_test , fit =lg_both , bins_woe = treebins_train ,
points0 = 600 , odds0 = 1/20 , pdo = 50 )

Score_1&lt;- noauto_scorecard( bins_card = woe_train, fit = lg_both, bins_woe = treebins_train,
points0 = 600, odds0 = 1/20, pdo = 50 )
psi_1&lt;- psi_cal( df_train = Score_1$data_score , df_test = Score_2$data_score,
feat = 'Score',label ='bad_ind' , nbins =10 )
</code></pre>

<hr>
<h2 id='rep_woe'>Replace Feature Data by Binning Template</h2><span id='topic+rep_woe'></span>

<h3>Description</h3>

<p>Replace Feature Data by Binning Template
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rep_woe(df, key_var, y_var, tool, var_label, col_woe, lower, upper)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rep_woe_+3A_df">df</code></td>
<td>
<p>A data.frame with independent variables and target variable.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_key_var">key_var</code></td>
<td>
<p>A name of index variable name.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_y_var">y_var</code></td>
<td>
<p>A name of target variable.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_tool">tool</code></td>
<td>
<p>Binning template.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_var_label">var_label</code></td>
<td>
<p>The name of the characteristic variable.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_col_woe">col_woe</code></td>
<td>
<p>The name of the woe variable</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_lower">lower</code></td>
<td>
<p>The name of the binning lower bound.</p>
</td></tr>
<tr><td><code id="rep_woe_+3A_upper">upper</code></td>
<td>
<p>The name of the binning upper bound.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of woe
</p>


<h3>Examples</h3>

<pre><code class='language-R'>accepts &lt;- read.csv( system.file( "extdata", "accepts.csv", package ="autoScorecard" ))
feature &lt;- stats::na.omit( accepts[,c(1,3,7:23)] )
all2 &lt;- bins_tree( df = feature, key_var = "application_id", y_var = "bad_ind",
max_depth = 3, p= 0.1)
re2 &lt;- rep_woe(  df= feature ,key_var = "application_id", y_var = "bad_ind",
tool = all2, var_label = "variable",col_woe ='woe', lower ='lower',upper ='upper')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
