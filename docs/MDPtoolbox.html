<!DOCTYPE html><html lang="en"><head><title>Help for package MDPtoolbox</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MDPtoolbox}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#MDPtoolbox-package'>
<p>Markov Decision Processes Toolbox</p></a></li>
<li><a href='#mdp_bellman_operator'>
<p>Applies the Bellman operator</p></a></li>
<li><a href='#mdp_check'>
<p>Checks the validity of a MDP</p></a></li>
<li><a href='#mdp_check_square_stochastic'>
<p>Checks if a matrix is square and stochastic</p></a></li>
<li><a href='#mdp_computePpolicyPRpolicy'>
<p>Computes the transition matrix and the reward matrix for a fixed policy</p></a></li>
<li><a href='#mdp_computePR'>
<p>Computes a reward matrix for any form of transition and reward functions</p></a></li>
<li><a href='#mdp_eval_policy_iterative'>
<p>Evaluates a policy using an iterative method</p></a></li>
<li><a href='#mdp_eval_policy_matrix'>
<p>Evaluates a policy using matrix inversion and product</p></a></li>
<li><a href='#mdp_eval_policy_optimality'>
<p>Computes sets of 'near optimal' actions for each state</p></a></li>
<li><a href='#mdp_eval_policy_TD_0'>
<p>Evaluates a policy using the TD(0) algorithm</p></a></li>
<li><a href='#mdp_example_forest'>
<p>Generates a MDP for a simple forest management problem</p></a></li>
<li><a href='#mdp_example_rand'>
<p>Generates a random MDP problem</p></a></li>
<li><a href='#mdp_finite_horizon'>
<p>Solves finite-horizon MDP using backwards induction algorithm</p></a></li>
<li><a href='#mdp_LP'>
<p>Solves discounted MDP using linear programming algorithm</p></a></li>
<li><a href='#mdp_policy_iteration'>
<p>Solves discounted MDP using policy iteration algorithm</p></a></li>
<li><a href='#mdp_policy_iteration_modified'>
<p>Solves discounted MDP using modified policy iteration algorithm</p></a></li>
<li><a href='#mdp_Q_learning'>
<p>Solves discounted MDP using the Q-learning algorithm (Reinforcement Learning)</p></a></li>
<li><a href='#mdp_relative_value_iteration'>
<p>Solves MDP with average reward using relative value iteration algorithm</p></a></li>
<li><a href='#mdp_span'>
<p>Evaluates the span of a vector</p></a></li>
<li><a href='#mdp_value_iteration'>
<p>Solves discounted MDP using value iteration algorithm</p></a></li>
<li><a href='#mdp_value_iteration_bound_iter'>
<p>Computes a bound for the number of iterations for the value iteration algorithm</p></a></li>
<li><a href='#mdp_value_iterationGS'>
<p>Solves discounted MDP using Gauss-Seidel's value iteration algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Markov Decision Processes Toolbox</td>
</tr>
<tr>
<td>Version:</td>
<td>4.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-03-02</td>
</tr>
<tr>
<td>Author:</td>
<td>Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia, Regis Sabbadin</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Guillaume Chapron &lt;gchapron@carnivoreconservation.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The Markov Decision Processes (MDP) toolbox proposes functions related to the resolution of discrete-time Markov Decision Processes: finite horizon, value iteration, policy iteration, linear programming algorithms with some variants and also proposes some functions related to Reinforcement Learning.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-3-Clause">BSD_3_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>Matrix, linprog</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-03-03 13:50:01 UTC; Guillaume</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-03-03 18:01:56</td>
</tr>
</table>
<hr>
<h2 id='MDPtoolbox-package'>
Markov Decision Processes Toolbox
</h2><span id='topic+MDPtoolbox-package'></span><span id='topic+MDPtoolbox'></span>

<h3>Description</h3>

<p>The Markov Decision Processes (MDP) toolbox proposes functions related to the resolution of discrete-time Markov Decision Processes: finite horizon, value iteration, policy iteration, linear programming algorithms with some variants  and also proposes some functions related to Reinforcement Learning.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> MDPtoolbox</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 4.0.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2017-03-02</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> BSD (4.4)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Iadine Chadès &lt;Iadine.Chades@csiro.au&gt;<br />
Guillaume Chapron &lt;gchapron@carnivoreconservation.org&gt;<br />
Marie-Josée Cros &lt;Marie-Josee.Cros@toulouse.inra.fr&gt;<br />
Fredérick Garcia &lt;fgarcia@toulouse.inra.fr&gt;<br />
Régis Sabbadin &lt;Regis.Sabbadin@toulouse.inra.fr&gt; <br />
</p>


<h3>References</h3>

<p>Chadès, I., Chapron, G., Cros, M.-J., Garcia, F. &amp; Sabbadin, R. 2014. MDPtoolbox: a multi-platform toolbox to solve stochastic dynamic programming problems. Ecography DOI:10.1111/ecog.00888 <br />
Puterman, M. L. 1994. Markov Decision Processes. John Wiley &amp; Sons, New-York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generates a random MDP problem
set.seed(0)
mdp_example_rand(2, 2)
mdp_example_rand(2, 2, FALSE)
mdp_example_rand(2, 2, TRUE)
mdp_example_rand(2, 2, FALSE, matrix(c(1,0,1,1),2,2))

# Generates a MDP for a simple forest management problem
MDP &lt;- mdp_example_forest()

# Find an optimal policy
results &lt;- mdp_policy_iteration(MDP$P, MDP$R, 0.9)

# Visualise the policy
results$policy

</code></pre>

<hr>
<h2 id='mdp_bellman_operator'>
Applies the Bellman operator
</h2><span id='topic+mdp_bellman_operator'></span>

<h3>Description</h3>

<p>Applies the Bellman operator to a value function Vprev and returns a new value function and a Vprev-improving policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_bellman_operator(P, PR, discount, Vprev)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_bellman_operator_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_bellman_operator_+3A_pr">PR</code></td>
<td>

<p>reward array. PR can be a 2 dimension array [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_bellman_operator_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number belonging to ]0; 1].
</p>
</td></tr>
<tr><td><code id="mdp_bellman_operator_+3A_vprev">Vprev</code></td>
<td>

<p>value fonction. Vprev is a vector of length S.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_bellman_operator applies the Bellman operator: PR + discount*P*Vprev to the value function Vprev. Returns a new value function and a Vprev-improving policy.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>new value fonction. V is a vector of length S.</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>policy is a vector of length S. Each element is an integer corresponding to an action.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_bellman_operator(P, R, 0.9, c(0,0))

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_bellman_operator(P, R, 0.9, c(0,0))

</code></pre>

<hr>
<h2 id='mdp_check'>
Checks the validity of a MDP
</h2><span id='topic+mdp_check'></span>

<h3>Description</h3>

<p>Checks the validity of a MDP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_check(P, R)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_check_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_check_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_check checks whether the MDP defined by the transition probability array (P) and the reward matrix (R) is valid. If P and R are correct, the function returns an empty error message. In the opposite case, the function returns an error message describing the problem.
</p>


<h3>Value</h3>

<p>Returns a character string which is empty if the MDP is valid. In the opposite case, the variable contains problem information
</p>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_check(P, R)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_check(P, R)
</code></pre>

<hr>
<h2 id='mdp_check_square_stochastic'>
Checks if a matrix is square and stochastic
</h2><span id='topic+mdp_check_square_stochastic'></span>

<h3>Description</h3>

<p>Checks whether a matrix is square and stochastic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_check_square_stochastic(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_check_square_stochastic_+3A_x">X</code></td>
<td>
<p>a matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_check_square_stochastic checks if the matrix (X) is square and stochastic (sums of rows equal to 1).
If it is the case, the function returns an empty error message. In the opposite case, the function returns an error message describing the problem.
</p>


<h3>Value</h3>

<p>Returns a character string which is empty if the matrix is square and stochastic. In the opposite case, the variable contains problem information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M &lt;- matrix(c(0.6116, 0.3884, 0, 1.0000), 2, 2, byrow=TRUE)

mdp_check_square_stochastic(M)
</code></pre>

<hr>
<h2 id='mdp_computePpolicyPRpolicy'>
Computes the transition matrix and the reward matrix for a fixed policy
</h2><span id='topic+mdp_computePpolicyPRpolicy'></span>

<h3>Description</h3>

<p>Computes the transition matrix and the reward matrix for a given policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_computePpolicyPRpolicy(P, R, policy)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_computePpolicyPRpolicy_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_computePpolicyPRpolicy_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_computePpolicyPRpolicy_+3A_policy">policy</code></td>
<td>

<p>a policy. policy is a length S vector of integer representing actions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_computePpolicyPRpolicy computes the state transition matrix and the reward matrix of a policy, given a probability matrix P and a reward matrix.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Ppolicy</code></td>
<td>
<p>transition probability array of the policy. Ppolicy is a [S,S] matrix.</p>
</td></tr>
<tr><td><code>PRpolicy</code></td>
<td>
<p>reward matrix of the policy. PRpolicy is a vector of length S.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.6116, 0.3884, 0, 1.0000), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0.6674, 0.3326, 0, 1.0000), 2, 2, byrow=TRUE)
R &lt;- array(0, c(2,2,2))
R[,,1] &lt;- matrix(c(-0.2433, 0.7073, 0, 0.1871), 2, 2, byrow=TRUE)
R[,,2] &lt;- matrix(c(-0.0069, 0.6433, 0, 0.2898), 2, 2, byrow=TRUE)
policy &lt;- c(2,2)
mdp_computePpolicyPRpolicy(P, R, policy)

# With a sparse matrix (P)
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.6116, 0.3884, 0, 1.0000), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0.6674, 0.3326, 0, 1.0000), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_computePpolicyPRpolicy(P, R, policy)
</code></pre>

<hr>
<h2 id='mdp_computePR'>
Computes a reward matrix for any form of transition and reward functions
</h2><span id='topic+mdp_computePR'></span>

<h3>Description</h3>

<p>Computes the reward associated to a state/action pair.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_computePR(P, R)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_computePR_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_computePR_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_computePR computes the reward of a state/action pair, given a probability array P and a reward array possibly depending on arrival state.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>PR</code></td>
<td>
<p>reward matrix. PR is a [S,A] matrix.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.6116, 0.3884, 0, 1.0000), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0.6674, 0.3326, 0, 1.0000), 2, 2, byrow=TRUE)
R &lt;- array(0, c(2,2,2))
R[,,1] &lt;- matrix(c(-0.2433, 0.7073, 0, 0.1871), 2, 2, byrow=TRUE)
R[,,2] &lt;- matrix(c(-0.0069, 0.6433, 0, 0.2898), 2, 2, byrow=TRUE)
mdp_computePR(P, R)

# With a sparse matrix (P)
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.6116, 0.3884, 0, 1.0000), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0.6674, 0.3326, 0, 1.0000), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_computePR(P, R)

</code></pre>

<hr>
<h2 id='mdp_eval_policy_iterative'>
Evaluates a policy using an iterative method
</h2><span id='topic+mdp_eval_policy_iterative'></span>

<h3>Description</h3>

<p>Evaluates a policy using iterations of the Bellman operator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_eval_policy_iterative(P, R, discount, policy, V0, epsilon, max_iter)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_eval_policy_iterative_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_policy">policy</code></td>
<td>

<p>a policy. policy is a S length vector. Each element is an integer corresponding
to an action.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_v0">V0</code></td>
<td>

<p>(optional) starting point. V0 is a S length vector representing an inital guess of the value function. By default, V0 is only composed of 0 elements.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_epsilon">epsilon</code></td>
<td>

<p>(optional) search for an epsilon-optimal policy. epsilon is a real greater than 0. By default, epsilon = 0.01.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_iterative_+3A_max_iter">max_iter</code></td>
<td>

<p>(optional) maximum number of iterations. max_iter is an integer greater than 0. If the value given in argument is greater than a computed bound, a warning informs that the computed bound will be used instead. By default, max_iter = 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_eval_policy_iterative evaluates the value fonction associated to a policy applying iteratively the Bellman operator.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Vpolicy</code></td>
<td>
<p>value fonction. Vpolicy is a S length vector.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
policy &lt;- c(2,1)
mdp_eval_policy_iterative(P, R, 0.8, policy)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_eval_policy_iterative(P, R, 0.8, policy)

</code></pre>

<hr>
<h2 id='mdp_eval_policy_matrix'>
Evaluates a policy using matrix inversion and product
</h2><span id='topic+mdp_eval_policy_matrix'></span>

<h3>Description</h3>

<p>Evaluates a policy using matrix operation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_eval_policy_matrix(P, R, discount, policy)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_eval_policy_matrix_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_matrix_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_matrix_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_matrix_+3A_policy">policy</code></td>
<td>

<p>a policy. policy is a S length vector. Each element is an integer corresponding
to an action.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_eval_policy_matrix evaluates the value fonction associated with a policy
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Vpolicy</code></td>
<td>
<p>value fonction. Vpolicy is a S length vector</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_eval_policy_matrix(P, R, 0.9, c(1,2))

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_eval_policy_matrix(P, R, 0.9, c(1,2))

</code></pre>

<hr>
<h2 id='mdp_eval_policy_optimality'>
Computes sets of 'near optimal' actions for each state
</h2><span id='topic+mdp_eval_policy_optimality'></span>

<h3>Description</h3>

<p>Determines sets of 'near optimal' actions for all states
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_eval_policy_optimality(P, R, discount, Vpolicy)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_eval_policy_optimality_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_optimality_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_optimality_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_optimality_+3A_vpolicy">Vpolicy</code></td>
<td>

<p>value function of the optimal policy. Vpolicy is a S length vector.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For some states, the evaluation of the value function may give close results for different actions. It is interesting to identify those states for which several actions have a value function very close the optimal one (i.e. less than 0.01 different). We called this the search for near optimal actions in each state.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>multiple</code></td>
<td>
<p>existence of at least two 'nearly' optimal actions for a state.
multiple is egal to true when at least one state has several epsilon-optimal actions, false if not.</p>
</td></tr>
<tr><td><code>optimal_actions</code></td>
<td>
<p>actions 'nearly' optimal for each state. optimal_actions is a [S,A] boolean matrix whose element optimal_actions(s, a) is true if the action a is 'nearly' optimal being in state s and false if not.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
Vpolicy &lt;- c(42.4419, 36.0465)
mdp_eval_policy_optimality(P, R, 0.9, Vpolicy)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_eval_policy_optimality(P, R, 0.9, Vpolicy)

</code></pre>

<hr>
<h2 id='mdp_eval_policy_TD_0'>
Evaluates a policy using the TD(0) algorithm
</h2><span id='topic+mdp_eval_policy_TD_0'></span>

<h3>Description</h3>

<p>Evaluates a policy using the TD(0) algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_eval_policy_TD_0(P, R, discount, policy, N)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_eval_policy_TD_0_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_TD_0_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_TD_0_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_TD_0_+3A_policy">policy</code></td>
<td>

<p>a policy. policy is a S length vector. Each element is an integer corresponding
to an action.
</p>
</td></tr>
<tr><td><code id="mdp_eval_policy_TD_0_+3A_n">N</code></td>
<td>
<p>(optional) number of iterations to perform. N is an integer greater than the de- fault value. By default, N is set to 10000
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_eval_policy_TD_0 evaluates the value fonction associated to a policy using the TD(0) algorithm (Reinforcement Learning).
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Vpolicy</code></td>
<td>
<p>value fonction. Vpolicy is a length S vector.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_eval_policy_TD_0(P, R, 0.9, c(1,2))

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_eval_policy_TD_0(P, R, 0.9, c(1,2))

</code></pre>

<hr>
<h2 id='mdp_example_forest'>
Generates a MDP for a simple forest management problem
</h2><span id='topic+mdp_example_forest'></span>

<h3>Description</h3>

<p>Generates a simple MDP example of forest management problem
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_example_forest(S, r1, r2, p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_example_forest_+3A_s">S</code></td>
<td>

<p>(optional) number of states. S is an integer greater than 0. By default, S is set to 3.
</p>
</td></tr>
<tr><td><code id="mdp_example_forest_+3A_r1">r1</code></td>
<td>

<p>(optional) reward when forest is in the oldest state and action Wait is performed. r1 is a real greater than 0. By default, r1 is set to 4.
</p>
</td></tr>
<tr><td><code id="mdp_example_forest_+3A_r2">r2</code></td>
<td>

<p>(optional) reward when forest is in the oldest state and action Cut is performed
r2 is a real greater than 0. By default, r2 is set to 2.
</p>
</td></tr>
<tr><td><code id="mdp_example_forest_+3A_p">p</code></td>
<td>

<p>(optional) probability of wildfire occurence. p is a real in ]0, 1[. By default, p is set to 0.1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_example_forest generates a transition probability (SxSxA) array P and a reward (SxA) matrix R that model the following problem.
A forest is managed by two actions: Wait and Cut.
An action is decided each year with first the objective to maintain an old forest for wildlife and second to make money selling cut wood. 
Each year there is a probability p that a fire burns the forest.
</p>
<p>Here is the modelisation of this problem.
Let 1, ... S be the states of the forest. the Sth state being the oldest.
Let Wait be action 1 and Cut action 2.
After a fire, the forest is in the youngest state, that is state 1.
</p>
<p>The transition matrix P of the problem can then be defined as follows:
</p>
<p style="text-align: center;"><code class="reqn"> P(,,1) = \left[
					\begin{array}{llllll}
					p &amp; 1-p &amp; 0 &amp; \ldots &amp; \ldots &amp; 0 \\
					p &amp; 1-p &amp; 0 &amp; \ldots &amp; \ldots &amp; 0 \\
					\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
					\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
					\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; 1-p \\
					p &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 1-p \\
					\end{array}
				\right] </code>
</p>

<p style="text-align: center;"><code class="reqn"> P(,,2) = \left[
					\begin{array}{lllll}
					1 &amp; 0 &amp; \ldots &amp; \ldots &amp; 0 \\
					\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
					\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
					1 &amp; 0 &amp; \ldots &amp; \ldots &amp; 0 \\
					\end{array}
				\right] </code>
</p>

<p>The reward matrix R is defined as follows:
</p>
<p style="text-align: center;"><code class="reqn"> R(,1) = \left[
					\begin{array}{l}
					0 \\
					\vdots \\
					\vdots \\
					\vdots \\
					0 \\
					r1 \\
					\end{array}
				\right] </code>
</p>

<p style="text-align: center;"><code class="reqn"> R(,2) = \left[
					\begin{array}{l}
					0 \\
					1 \\
					\vdots \\
					\vdots \\
					1 \\
					r2 \\
					\end{array}
				\right] </code>
</p>



<h3>Value</h3>

<table role = "presentation">
<tr><td><code>P</code></td>
<td>
<p>transition probability array. P is a [S,S,A] array.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>reward matrix. R is a [S,A] matrix</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>mdp_example_forest()
</code></pre>

<hr>
<h2 id='mdp_example_rand'>
Generates a random MDP problem
</h2><span id='topic+mdp_example_rand'></span>

<h3>Description</h3>

<p>Generates a random MDP problem
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_example_rand(S, A, is_sparse, mask)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_example_rand_+3A_s">S</code></td>
<td>

<p>number of states. S is an integer greater than 0
</p>
</td></tr>
<tr><td><code id="mdp_example_rand_+3A_a">A</code></td>
<td>

<p>number of actions. A is an integer greater than 0
</p>
</td></tr>
<tr><td><code id="mdp_example_rand_+3A_is_sparse">is_sparse</code></td>
<td>

<p>(optional) used to generate sparse matrices. is_sparse is a boolean. If it is set to true, sparse matrices are generated. By default, it is set to false.
</p>
</td></tr>
<tr><td><code id="mdp_example_rand_+3A_mask">mask</code></td>
<td>

<p>(optional) indicates the possible transitions between states. mask is a [S,S] ma- trix composed of 0 and 1 elements (0 indicates a transition probability always equal to zero). By default, mask is only composed of 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_example_rand generates a transition probability matrix (P) and a reward matrix (R). Optional arguments allow to define sparse matrices and pairs of states with impossible transitions.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>P</code></td>
<td>
<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S]. Elements of R are in ]-1; 1[</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>mdp_example_rand(2, 2)
mdp_example_rand(2, 2, FALSE)
mdp_example_rand(2, 2, TRUE)
mdp_example_rand(2, 2, FALSE, matrix(c(1,0,1,1),2,2))

</code></pre>

<hr>
<h2 id='mdp_finite_horizon'>
Solves finite-horizon MDP using backwards induction algorithm
</h2><span id='topic+mdp_finite_horizon'></span>

<h3>Description</h3>

<p>Solves finite-horizon MDP with backwards induction algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_finite_horizon(P, R, discount, N, h)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_finite_horizon_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_finite_horizon_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_finite_horizon_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_finite_horizon_+3A_n">N</code></td>
<td>

<p>number of stages. N is an integer greater than 0.
</p>
</td></tr>
<tr><td><code id="mdp_finite_horizon_+3A_h">h</code></td>
<td>
<p>(optional) terminal reward. h is a S length vector. By default, h = numeric(S).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_finite_horizon applies backwards induction algorithm for finite-horizon MDP. The optimality equations allow to recursively evaluate function values starting from the terminal stage. This function uses verbose and silent modes. In verbose mode, the function displays the current stage and the corresponding optimal policy.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>value fonction. V is a [S,(N+1)] matrix. Each column n is the optimal value fonction at stage n, with n = 1, ... N. V[,N+1] is the terminal reward.</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>optimal policy. policy is a [S,N] matrix. Each element is an integer correspond- ing to an action and each column n is the optimal policy at stage n.</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_finite_horizon(P, R, 0.9, 3)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_finite_horizon(P, R, 0.9, 3)
</code></pre>

<hr>
<h2 id='mdp_LP'>
Solves discounted MDP using linear programming algorithm
</h2><span id='topic+mdp_LP'></span>

<h3>Description</h3>

<p>Solves discounted MDP with linear programming
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_LP(P, R, discount)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_LP_+3A_p">P</code></td>
<td>

<p>transition probability array. P is a 3 dimensions array [S,S,A]. Sparse matrix are not supported.
</p>
</td></tr>
<tr><td><code id="mdp_LP_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_LP_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real which belongs to ]0; 1[
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_LP applies linear programming to solve discounted MDP for non-sparse matrix only.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>optimal value fonction. V is a S length vector</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>optimal policy. policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Only with a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_LP(P, R, 0.9)
</code></pre>

<hr>
<h2 id='mdp_policy_iteration'>
Solves discounted MDP using policy iteration algorithm
</h2><span id='topic+mdp_policy_iteration'></span>

<h3>Description</h3>

<p>Solves discounted MDP with policy iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_policy_iteration(P, R, discount, policy0, max_iter, eval_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_policy_iteration_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_+3A_discount">discount</code></td>
<td>

<p>discount factor.
discount is a real which belongs to ]0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_+3A_policy0">policy0</code></td>
<td>
<p>(optional) starting policy. policy0 is a S length vector. By default, policy0 is the policy which maximizes the expected immediate reward.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_+3A_max_iter">max_iter</code></td>
<td>
<p>(optional)
maximum number of iterations to be done.
max_iter is an integer greater than 0. 
By default, max_iter is set to 1000.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_+3A_eval_type">eval_type</code></td>
<td>
<p>(optional)
define function used to evaluate a policy.
eval_type is 0 for mdp_eval_policy_matrix use, mdp_eval_policy_iterative is used in all other cases. 
By default, eval_type is set to 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_policy_iteration applies the policy iteration algorithm to solve discounted MDP. The algorithm consists in improving the policy iteratively, using the evaluation of the current policy. Iterating is stopped when two successive policies are identical or when a specified number (max_iter) of iterations have been performed.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>optimal value fonction. V is a S length vector</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>optimal policy. policy is a S length vector. Each element is an integer corre-
sponding to an action which maximizes the value function</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_policy_iteration(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_policy_iteration(P, R, 0.9)

</code></pre>

<hr>
<h2 id='mdp_policy_iteration_modified'>
Solves discounted MDP using modified policy iteration algorithm
</h2><span id='topic+mdp_policy_iteration_modified'></span>

<h3>Description</h3>

<p>Solves discounted MDP using modified policy iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_policy_iteration_modified(P, R, discount, epsilon, max_iter)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_policy_iteration_modified_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_modified_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_modified_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[. For discount equals to 1, a warning recalls to check conditions of convergence.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_modified_+3A_epsilon">epsilon</code></td>
<td>
<p>(optional)
search for an epsilon-optimal policy. epsilon is a real in ]0; 1]. By default, epsilon = 0.01.
</p>
</td></tr>
<tr><td><code id="mdp_policy_iteration_modified_+3A_max_iter">max_iter</code></td>
<td>
<p>(optional)
maximum number of iterations to be done. max_iter is an integer greater than 0. By default, max_iter = 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_policy_iteration_modified applies the modified policy iteration algorithm to solve discounted MDP. The algorithm consists, like policy iteration one, in improving the policy iteratively but in policy evaluation few iterations (max_iter) of value function updates done. Iterating is stopped when an epsilon-optimal policy is found.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>V</code></td>
<td>
<p>optimal value fonction. V is a S length vector</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>optimal policy. policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R&lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_policy_iteration_modified(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_policy_iteration_modified(P, R, 0.9)

</code></pre>

<hr>
<h2 id='mdp_Q_learning'>
Solves discounted MDP using the Q-learning algorithm (Reinforcement Learning)
</h2><span id='topic+mdp_Q_learning'></span>

<h3>Description</h3>

<p>Solves discounted MDP with the Q-learning algorithm (Reinforcement learning)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_Q_learning(P, R, discount, N)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_Q_learning_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_Q_learning_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_Q_learning_+3A_discount">discount</code></td>
<td>

<p>discount factor.
discount is a real which belongs to ]0; 1[
</p>
</td></tr>
<tr><td><code id="mdp_Q_learning_+3A_n">N</code></td>
<td>

<p>(optional) : number of iterations to perform.
N is an integer that must be greater than the default value.
By default, N is set to 10000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_Q_learning computes the Q matrix, the mean discrepancy and gives the optimal value function and the optimal policy when allocated enough iterations. It uses an iterative method.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Q</code></td>
<td>
<p>an action-value function that gives the expected utility of taking a given action in a given state and following an optimal policy thereafter. Q is a [S,A] matrix.</p>
</td></tr>
<tr><td><code>mean_discrepancy</code></td>
<td>
<p>discrepancy means over 100 iterations. mean_discrepancy is a vector of V discrepancy mean over 100 iterations. Then the length of the vector for the default value of N is 100.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>value function. V is a S length vector.</p>
</td></tr>
<tr><td><code>policy</code></td>
<td>
<p>policy. policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
# Not run
# mdp_Q_learning(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
# Not run
# mdp_Q_learning(P, R, 0.9)

</code></pre>

<hr>
<h2 id='mdp_relative_value_iteration'>
Solves MDP with average reward using relative value iteration algorithm
</h2><span id='topic+mdp_relative_value_iteration'></span>

<h3>Description</h3>

<p>Solves MDP with average reward using relative value iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_relative_value_iteration(P, R, epsilon, max_iter)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_relative_value_iteration_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_relative_value_iteration_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_relative_value_iteration_+3A_epsilon">epsilon</code></td>
<td>

<p>(optional) : search for an epsilon-optimal policy.
epsilon is a real in [0; 1]. 
By default, epsilon is set to 0.01
</p>
</td></tr>
<tr><td><code id="mdp_relative_value_iteration_+3A_max_iter">max_iter</code></td>
<td>

<p>(optional) : maximum number of iterations.
max_iter is an integer greater than 0. 
By default, max_iter is set to 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_relative_value_iteration applies the relative value iteration algorithm to solve MDP with average reward. The algorithm consists in solving optimality equations iteratively. Iterating is stopped when an epsilon-optimal policy is found or after a specified number (max_iter) of iterations is done.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>policy</code></td>
<td>
<p>optimal policy. policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function.</p>
</td></tr>
<tr><td><code>average_reward</code></td>
<td>
<p>average reward of the optimal policy. average_reward is a real.</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_relative_value_iteration(P, R)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_relative_value_iteration(P, R)

</code></pre>

<hr>
<h2 id='mdp_span'>
Evaluates the span of a vector
</h2><span id='topic+mdp_span'></span>

<h3>Description</h3>

<p>Computes the span of a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_span(W)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_span_+3A_w">W</code></td>
<td>

<p>a vector.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_span computes the span of the W vector: max W(s) - min W(s)
</p>


<h3>Value</h3>

<p>the value of the span of the W vector.
</p>

<hr>
<h2 id='mdp_value_iteration'>
Solves discounted MDP using value iteration algorithm
</h2><span id='topic+mdp_value_iteration'></span>

<h3>Description</h3>

<p>Solves discounted MDP with value iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_value_iteration(P, R, discount, epsilon, max_iter, V0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_value_iteration_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_+3A_discount">discount</code></td>
<td>

<p>discount factor. discount is a real number which belongs to [0; 1[.
For discount equals to 1, a warning recalls to check conditions of convergence.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_+3A_epsilon">epsilon</code></td>
<td>

<p>(optional) : search for an epsilon-optimal policy.
epsilon is a real in ]0; 1].
By default, epsilon = 0.01.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_+3A_max_iter">max_iter</code></td>
<td>

<p>(optional) : maximum number of iterations.
max_iter is an integer greater than 0. If the value given in argument is greater than a computed bound, a warning informs that the computed bound will be considered.
By default, if discount is not egal to 1, a bound for max_iter is computed, if not max_iter = 1000.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_+3A_v0">V0</code></td>
<td>

<p>(optional) : starting value function.
V0 is a (Sx1) vector.
By default, V0 is only composed of 0 elements.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_value_iteration applies the value iteration algorithm to solve discounted MDP. The algorithm consists in solving Bellman's equation iteratively. Iterating is stopped when an epsilon-optimal policy is found or after a specified number (max_iter) of iterations.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>policy</code></td>
<td>
<p>optimal policy.
policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of done iterations.</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R&lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_value_iteration(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_value_iteration(P, R, 0.9)

</code></pre>

<hr>
<h2 id='mdp_value_iteration_bound_iter'>
Computes a bound for the number of iterations for the value iteration algorithm
</h2><span id='topic+mdp_value_iteration_bound_iter'></span>

<h3>Description</h3>

<p>Computes a bound on the number of iterations for the value iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_value_iteration_bound_iter(P, R, discount, epsilon, V0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_value_iteration_bound_iter_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_bound_iter_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_bound_iter_+3A_discount">discount</code></td>
<td>

<p>discount factor.
discount is a real which belongs to ]0; 1[.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_bound_iter_+3A_epsilon">epsilon</code></td>
<td>

<p>(optional) : search for an epsilon-optimal policy
epsilon is a real in ]0; 1]. 
By default, epsilon is set to 0.01.
</p>
</td></tr>
<tr><td><code id="mdp_value_iteration_bound_iter_+3A_v0">V0</code></td>
<td>

<p>(optional) : starting value function.
V0 is a S length vector.
By default, V0 is only composed of 0 elements.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_value_iteration_bound_iter computes a bound on the number of iterations for the value iteration algorithm to find an epsilon-optimal policy with use of span for the stopping criterion.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>max_iter</code></td>
<td>
<p>maximum number of iterations to be done.
max_iter is an integer greater than 0.</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_value_iteration_bound_iter(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_value_iteration_bound_iter(P, R, 0.9)

</code></pre>

<hr>
<h2 id='mdp_value_iterationGS'>
Solves discounted MDP using Gauss-Seidel's value iteration algorithm
</h2><span id='topic+mdp_value_iterationGS'></span>

<h3>Description</h3>

<p>Solves discounted MDP with Gauss-Seidel's value iteration algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdp_value_iterationGS(P, R, discount, epsilon, max_iter, V0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdp_value_iterationGS_+3A_p">P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td></tr>
<tr><td><code id="mdp_value_iterationGS_+3A_r">R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td></tr>
<tr><td><code id="mdp_value_iterationGS_+3A_discount">discount</code></td>
<td>

<p>discount factor.
discount is a real which belongs to ]0; 1].
For discount equals to 1, a warning recalls to check conditions of convergence.
</p>
</td></tr>
<tr><td><code id="mdp_value_iterationGS_+3A_epsilon">epsilon</code></td>
<td>

<p>(optional) : search of an epsilon-optimal policy.
epsilon is a real in ]0; 1].
By default, epsilon is set to 0.01.
</p>
</td></tr>
<tr><td><code id="mdp_value_iterationGS_+3A_max_iter">max_iter</code></td>
<td>

<p>(optional) : maximum number of iterations to be done.
max_iter is an integer greater than 0. If the value given in argument is greater than a computed bound, a warning informs that the computed bound will be considered.
By default, if discount is not equal to 1, a bound for max_iter is computed, if not max_iter is set to 1000.
</p>
</td></tr>
<tr><td><code id="mdp_value_iterationGS_+3A_v0">V0</code></td>
<td>

<p>(optional) : starting value function.
V0 is a S length vector.
By default, V0 is only composed of 0 elements.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mdp_value_iterationGS applies Gauss-Seidel's value iteration algorithm to solve discounted MDP. The algorithm consists, like value iteration, in solving Bellman's equation iteratively Vn+1(s) calculation is modified. The algorithm uses Vn+1(s) instead of Vn(s) whenever this value has been calculated. In this way, convergence speed is improved. Iterating is stopped when an epsilon-optimal policy is found or after a specified number (max_iter) of iterations.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>policy</code></td>
<td>
<p>epsilon-optimal policy.
policy is a S length vector. Each element is an integer corresponding to an action which maximizes the value function.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of done iterations.</p>
</td></tr>
<tr><td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_value_iterationGS(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_value_iterationGS(P, R, 0.9)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
