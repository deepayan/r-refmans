<!DOCTYPE html><html><head><title>Help for package pedmod</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pedmod}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biconnected_components'><p>Finds the Biconnected Components</p></a></li>
<li><a href='#block_cut_tree'><p>Creates a Block-cut Tree Like Object</p></a></li>
<li><a href='#eval_pedigree_ll'><p>Approximate the Log Marginal Likelihood</p></a></li>
<li><a href='#max_balanced_partition'><p>Finds an Approximately Balanced Connected Partition</p></a></li>
<li><a href='#mvndst'><p>Multivariate Normal Distribution CDF and Its Derivative</p></a></li>
<li><a href='#pedigree_ll_terms'><p>Get a C++ Object for Log Marginal Likelihood Approximations</p></a></li>
<li><a href='#pedmod_opt'><p>Optimize the Log Marginal Likelihood</p></a></li>
<li><a href='#pedmod_profile'><p>Computes Profile Likelihood Based Confidence Intervals</p></a></li>
<li><a href='#pedmod_profile_nleq'><p>Computes Profile Likelihood Based Confidence Intervals for a Nonlinear</p>
Transformation of the Variables</a></li>
<li><a href='#pedmod_profile_prop'><p>Computes Profile Likelihood Based Confidence Intervals for the Proportion</p>
of Variance</a></li>
<li><a href='#pedmod_sqn'><p>Optimize the Log Marginal Likelihood Using a Stochastic Quasi-Newton Method</p></a></li>
<li><a href='#standardized_to_direct'><p>Transform Between Parameterizations</p></a></li>
<li><a href='#unconnected_partition'><p>Finds an Approximately Balanced Partition</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Pedigree Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.4</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Benjamin Christoffersen &lt;boennecd@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions to estimate mixed probit models using, for 
    instance, pedigree data like in &lt;<a href="https://doi.org/10.1002%2Fsim.1603">doi:10.1002/sim.1603</a>&gt;. The models are also 
    commonly called liability threshold models. The approximation is 
    based on direct log marginal likelihood approximations like the randomized 
    Quasi-Monte Carlo suggested by &lt;<a href="https://doi.org/10.1198%2F106186002394">doi:10.1198/106186002394</a>&gt; with a similar 
    procedure to approximate the derivatives. The minimax tilting method 
    suggested by &lt;<a href="https://doi.org/10.1111%2Frssb.12162">doi:10.1111/rssb.12162</a>&gt; is also supported. Graph-based methods 
    are also provided that can be used to simplify pedigrees.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.0</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/boennecd/pedmod">https://github.com/boennecd/pedmod</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/boennecd/pedmod/issues">https://github.com/boennecd/pedmod/issues</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, BH, testthat, psqn</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, alabama</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, mvtnorm, xml2, knitr, rmarkdown, R.rsp, abind,
kinship2, igraph, TruncatedNormal, numDeriv</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-11 09:37:02 UTC; boennecd</td>
</tr>
<tr>
<td>Author:</td>
<td>Benjamin Christoffersen
    <a href="https://orcid.org/0000-0002-7182-1346"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cre, aut],
  Alan Genz [cph],
  Frank Bretz [cph],
  Bjoern Bornkamp [cph],
  Torsten Hothorn [cph],
  Christophe Dutang [cph],
  Diethelm Wuertz [cph],
  R-core [cph],
  Leo Belzile [cph],
  Zdravko Botev [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-11 10:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='biconnected_components'>Finds the Biconnected Components</h2><span id='topic+biconnected_components'></span><span id='topic+biconnected_components_pedigree'></span>

<h3>Description</h3>

<p>Finds the biconnected components and the cut vertices (articulation points)
using the methods suggested by Hopcroft et al. (1973).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biconnected_components(from, to)

biconnected_components_pedigree(id, father.id, mother.id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biconnected_components_+3A_from">from</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="biconnected_components_+3A_to">to</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="biconnected_components_+3A_id">id</code></td>
<td>
<p>integer vector with the child id.</p>
</td></tr>
<tr><td><code id="biconnected_components_+3A_father.id">father.id</code></td>
<td>
<p>integer vector with the father id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="biconnected_components_+3A_mother.id">mother.id</code></td>
<td>
<p>integer vector with the mother id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with vectors of vertices in each biconnected component. An
attribute called <code>"cut_verices"</code> contains the cut vertices in each
biconnected component.
</p>


<h3>References</h3>

<p>Hopcroft, J., &amp; Tarjan, R. (1973).
<em>Algorithm 447: efficient algorithms for graph manipulation</em>.
Communications of the ACM, 16(6), 372-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+block_cut_tree">block_cut_tree</a></code> and
<code><a href="#topic+max_balanced_partition">max_balanced_partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example of a data set in pedigree and graph form
library(pedmod)
dat_pedigree &lt;- data.frame(
  id = 1:48,
  mom = c(
    NA, NA, 2L, 2L, 2L, NA, NA, 7L, 7L, 7L, 3L, 3L, 3L, 3L, NA, 15L, 15L, 43L,
    18L, NA, NA, 21L, 21L, 9L, 9L, 9L, 9L, NA, NA, 29L, 29L, 29L, 30L, 30L, NA,
    NA, 36L, 36L, 36L, 38L, 38L, NA, NA, 43L, 43L, 43L, 32L, 32L),
  dad = c(NA, NA, 1L, 1L, 1L, NA, NA, 6L, 6L, 6L, 8L, 8L, 8L, 8L, NA, 4L, 4L,
          42L, 5L, NA, NA, 20L, 20L, 22L, 22L, 22L, 22L, NA, NA, 28L, 28L, 28L,
          23L, 23L, NA, NA, 35L, 35L, 35L, 31L, 31L, NA, NA, 42L, 42L, 42L,
          45L, 45L),
  sex = c(1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L,
          2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L,
          1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L))

dat &lt;- list(
  to = c(
    3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L, 18L, 19L, 22L, 23L,
    24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L, 39L, 40L, 41L, 44L,
    45L, 46L, 47L, 48L, 3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L,
    18L, 19L, 22L, 23L, 24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L,
    39L, 40L, 41L, 44L, 45L, 46L, 47L, 48L),
  from = c(
    1L, 1L, 1L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 4L, 4L, 42L, 5L, 20L, 20L, 22L, 22L,
    22L, 22L, 28L, 28L, 28L, 23L, 23L, 35L, 35L, 35L, 31L, 31L, 42L, 42L, 42L,
    45L, 45L, 2L, 2L, 2L, 7L, 7L, 7L, 3L, 3L, 3L, 3L, 15L, 15L, 43L, 18L, 21L,
    21L, 9L, 9L, 9L, 9L, 29L, 29L, 29L, 30L, 30L, 36L, 36L, 36L, 38L, 38L, 43L,
    43L, 43L, 32L, 32L))

# they give the same
out_pedigree &lt;- biconnected_components_pedigree(
  id = dat_pedigree$id, father.id = dat_pedigree$dad,
  mother.id = dat_pedigree$mom)
out &lt;- biconnected_components(dat$to, dat$from)
all.equal(out_pedigree, out)

</code></pre>

<hr>
<h2 id='block_cut_tree'>Creates a Block-cut Tree Like Object</h2><span id='topic+block_cut_tree'></span><span id='topic+block_cut_tree_pedigree'></span>

<h3>Description</h3>

<p>Creates a block-cut tree like structure computed using the method suggested
by Hopcroft et al. (1973).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>block_cut_tree(from, to)

block_cut_tree_pedigree(id, father.id, mother.id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="block_cut_tree_+3A_from">from</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="block_cut_tree_+3A_to">to</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="block_cut_tree_+3A_id">id</code></td>
<td>
<p>integer vector with the child id.</p>
</td></tr>
<tr><td><code id="block_cut_tree_+3A_father.id">father.id</code></td>
<td>
<p>integer vector with the father id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="block_cut_tree_+3A_mother.id">mother.id</code></td>
<td>
<p>integer vector with the mother id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tree structure where each node is represented as list that
contains the vertices in the biconnected component, the cut_vertices,
and the node's leafs.
</p>


<h3>References</h3>

<p>Hopcroft, J., &amp; Tarjan, R. (1973).
<em>Algorithm 447: efficient algorithms for graph manipulation</em>.
Communications of the ACM, 16(6), 372-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biconnected_components">biconnected_components</a></code> and
<code><a href="#topic+max_balanced_partition">max_balanced_partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example of a data set in pedigree and graph form
library(pedmod)
dat_pedigree &lt;- data.frame(
  id = 1:48,
  mom = c(
    NA, NA, 2L, 2L, 2L, NA, NA, 7L, 7L, 7L, 3L, 3L, 3L, 3L, NA, 15L, 15L, 43L,
    18L, NA, NA, 21L, 21L, 9L, 9L, 9L, 9L, NA, NA, 29L, 29L, 29L, 30L, 30L, NA,
    NA, 36L, 36L, 36L, 38L, 38L, NA, NA, 43L, 43L, 43L, 32L, 32L),
  dad = c(NA, NA, 1L, 1L, 1L, NA, NA, 6L, 6L, 6L, 8L, 8L, 8L, 8L, NA, 4L, 4L,
          42L, 5L, NA, NA, 20L, 20L, 22L, 22L, 22L, 22L, NA, NA, 28L, 28L, 28L,
          23L, 23L, NA, NA, 35L, 35L, 35L, 31L, 31L, NA, NA, 42L, 42L, 42L,
          45L, 45L),
  sex = c(1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L,
          2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L,
          1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L))

dat &lt;- list(
  to = c(
    3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L, 18L, 19L, 22L, 23L,
    24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L, 39L, 40L, 41L, 44L,
    45L, 46L, 47L, 48L, 3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L,
    18L, 19L, 22L, 23L, 24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L,
    39L, 40L, 41L, 44L, 45L, 46L, 47L, 48L),
  from = c(
    1L, 1L, 1L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 4L, 4L, 42L, 5L, 20L, 20L, 22L, 22L,
    22L, 22L, 28L, 28L, 28L, 23L, 23L, 35L, 35L, 35L, 31L, 31L, 42L, 42L, 42L,
    45L, 45L, 2L, 2L, 2L, 7L, 7L, 7L, 3L, 3L, 3L, 3L, 15L, 15L, 43L, 18L, 21L,
    21L, 9L, 9L, 9L, 9L, 29L, 29L, 29L, 30L, 30L, 36L, 36L, 36L, 38L, 38L, 43L,
    43L, 43L, 32L, 32L))

# they give the same
out_pedigree &lt;- block_cut_tree_pedigree(
  id = dat_pedigree$id, father.id = dat_pedigree$dad,
  mother.id = dat_pedigree$mom)
out &lt;- block_cut_tree(dat$to, dat$from)
all.equal(out_pedigree, out)

</code></pre>

<hr>
<h2 id='eval_pedigree_ll'>Approximate the Log Marginal Likelihood</h2><span id='topic+eval_pedigree_ll'></span><span id='topic+eval_pedigree_grad'></span><span id='topic+eval_pedigree_hess'></span>

<h3>Description</h3>

<p>Approximate the log marginal likelihood and the derivatives with
respect to the model parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_pedigree_ll(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  indices = NULL,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  standardized = FALSE,
  method = 0L,
  use_tilting = FALSE,
  vls_scales = NULL
)

eval_pedigree_grad(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  indices = NULL,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  standardized = FALSE,
  method = 0L,
  use_tilting = FALSE,
  vls_scales = NULL
)

eval_pedigree_hess(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  indices = NULL,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  standardized = FALSE,
  method = 0L,
  use_tilting = FALSE,
  vls_scales = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eval_pedigree_ll_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> or
<code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code>.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_par">par</code></td>
<td>
<p>numeric vector with parameters. For an object from
<code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> these are the fixed effect coefficients and
log scale parameters. The log scale parameters should be last. For an object
from <code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code> these are the fixed effects
and the coefficients for scale parameters.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_rel_eps">rel_eps</code></td>
<td>
<p>relative convergence threshold.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_standardized">standardized</code></td>
<td>
<p>logical for whether to use the standardized or direct
parameterization. See <code><a href="#topic+standardized_to_direct">standardized_to_direct</a></code> and the vignette
at <code>vignette("pedmod", package = "pedmod")</code>.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="eval_pedigree_ll_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>eval_pedigree_hess</code> is only implemented for objects from
<code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code>.
</p>


<h3>Value</h3>

<p><code>eval_pedigree_ll</code>:
a scalar with the log marginal likelihood approximation.
It has an attribute called <code>"n_fails"</code> which shows the number of
log marginal likelihood term approximations which do not satisfy
the <code>abs_eps</code> and <code>rel_eps</code> criteria and an attribute called
<code>std</code> with a standard error estimate based on the delta rule.
</p>
<p><code>eval_pedigree_grad</code>: a vector with the derivatives with
respect to <code>par</code>. An attribute called <code>"logLik"</code> contains the
log marginal likelihood approximation. There will also be <code>"n_fails"</code>
attribute like for <code>eval_pedigree_ll</code> and an attribute called
<code>"std"</code> which first element is the standard error estimate of the
log likelihood based on the delta method and the last elements are the
standard error estimates of the gradient. The latter ignores the Monte Carlo
error from the likelihood approximation.
</p>
<p><code>eval_pedigree_hess</code>: a matrix with the hessian with
respect to <code>par</code>.
An attribute called <code>"logLik"</code> contains the
log marginal likelihood approximation and an attribute called <code>"grad"</code>
contains the gradient·
The attribute <code>"hess_org"</code> contains the Hessian with the scale
parameter on the identity scale rather than the log scale.
<code>"vcov"</code> and <code>"vcov_org"</code> are
the covariance matrices from the hessian and <code>"hess_org"</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three families as an example
fam_dat &lt;- list(
  list(
    y = c(FALSE, TRUE, FALSE, FALSE),
    X = structure(c(
      1, 1, 1, 1, 1.2922654151273, 0.358134905909256, -0.734963997107464,
      0.855235473516044, -1.16189500386223, -0.387298334620742,
      0.387298334620742, 1.16189500386223),
      .Dim = 4:3, .Dimnames = list( NULL, c("(Intercept)", "X1", ""))),
    rel_mat = structure(c(
      1, 0.5, 0.5, 0.125, 0.5, 1, 0.5, 0.125, 0.5, 0.5,
      1, 0.125, 0.125, 0.125, 0.125, 1), .Dim = c(4L, 4L)),
    met_mat = structure(c(1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1),
                        .Dim = c(4L, 4L))),
  list(
    y = c(FALSE, FALSE, FALSE),
    X = structure(c(
      1, 1, 1, -0.0388728997202442, -0.0913782435233639,
      -0.0801619722392612, -1, 0, 1), .Dim = c(3L, 3L)),
    rel_mat = structure(c(
      1, 0.5, 0.125, 0.5, 1, 0.125, 0.125, 0.125, 1), .Dim = c(3L, 3L)),
    met_mat = structure(c(
      1, 1, 0, 1, 1, 0, 0, 0, 1), .Dim = c(3L, 3L))),
  list(
    y = c(TRUE, FALSE),
    X = structure(c(
      1, 1, 0.305275750370738, -1.49482995913648,  -0.707106781186547,
      0.707106781186547),
      .Dim = 2:3, .Dimnames = list( NULL, c("(Intercept)", "X1", ""))),
    rel_mat = structure(c(1, 0.5,  0.5, 1), .Dim = c(2L, 2L)),
    met_mat = structure(c(1, 1, 1, 1), .Dim = c(2L,  2L))))

# get the data into the format needed for the package
dat_arg &lt;- lapply(fam_dat, function(x){
  # we need the following for each family:
  #   y: the zero-one outcomes.
  #   X: the design matrix for the fixed effects.
  #   scale_mats: list with the scale matrices for each type of effect.
  list(y = as.numeric(x$y), X = x$X,
       scale_mats = list(x$rel_mat, x$met_mat))
})

# get a pointer to the C++ object
ptr &lt;- pedigree_ll_terms(dat_arg, max_threads = 1L)

# approximate the log marginal likelihood
beta &lt;- c(-1, 0.3, 0.2) # fixed effect coefficients
scs &lt;- c(0.5, 0.33)     # scales parameters

set.seed(44492929)
system.time(ll1 &lt;- eval_pedigree_ll(
  ptr = ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e5,
  rel_eps = 1e-5, minvls = 2000, use_aprx = FALSE))
ll1 # the approximation

# with the approximation of pnorm and qnorm
system.time(ll2 &lt;- eval_pedigree_ll(
  ptr = ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e5,
  rel_eps = 1e-5, minvls = 2000, use_aprx = TRUE))
all.equal(ll1, ll2, tolerance = 1e-5)

# cluster weights can be used as follows to repeat the second family three
# times and remove the third
system.time(deriv_w_weight &lt;- eval_pedigree_grad(
  ptr = ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e6,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE,
  cluster_weights = c(1, 3, 0)))

# the same as manually repeating second cluster and not including the third
dum_dat &lt;- dat_arg[c(1, 2, 2, 2)]
dum_ptr &lt;- pedigree_ll_terms(dum_dat, 1L)
system.time(deriv_dum &lt;- eval_pedigree_grad(
  ptr = dum_ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e6,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE))
all.equal(deriv_dum, deriv_w_weight, tolerance = 1e-3)

# the hessian is computed on the scale parameter scale rather than on the
# log of the scale parameters
system.time(hess_w_weight &lt;- eval_pedigree_hess(
  ptr = ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e6,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE,
  cluster_weights = c(1, 3, 0)))

system.time(hess_dum &lt;- eval_pedigree_hess(
  ptr = dum_ptr, par = c(beta, log(scs)), abs_eps = -1, maxvls = 1e6,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE))
attr(hess_w_weight, "n_fails") &lt;- attr(hess_dum, "n_fails") &lt;- NULL
all.equal(hess_w_weight, hess_dum, tolerance = 1e-3)

# the results are consistent with the gradient output
all.equal(attr(deriv_dum, "logLik"), attr(hess_dum, "logLik"),
          tolerance = 1e-5)

hess_grad &lt;- attr(hess_dum, "grad")
all.equal(hess_grad, deriv_dum, check.attributes = FALSE,
          tolerance = 1e-3)

# with loadings
dat_arg_loadings &lt;- lapply(fam_dat, function(x){
  list(y = as.numeric(x$y), X = x$X, Z = x$X[, 1:2],
       scale_mats = list(x$rel_mat, x$met_mat))
})

ptr_loadings &lt;-
  pedigree_ll_terms_loadings(dat_arg_loadings, max_threads = 1L)

scs &lt;- c(log(0.5) / 2, 0.1, log(0.33) / 2, 0.2) # got more scales parameters
eval_pedigree_ll(
  ptr = ptr_loadings, par = c(beta, scs), abs_eps = -1, maxvls = 1e4,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE)
eval_pedigree_grad(
  ptr = ptr_loadings, par = c(beta, scs), abs_eps = -1, maxvls = 1e4,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE)

# can recover the result from before
scs &lt;- c(log(0.5) / 2, 0, log(0.33) / 2, 0)
ll3 &lt;- eval_pedigree_ll(
  ptr = ptr_loadings, par = c(beta, scs), abs_eps = -1, maxvls = 1e4,
  rel_eps = 1e-3, minvls = 2000, use_aprx = TRUE)
all.equal(ll1, ll3, tolerance = 1e-5)

</code></pre>

<hr>
<h2 id='max_balanced_partition'>Finds an Approximately Balanced Connected Partition</h2><span id='topic+max_balanced_partition'></span><span id='topic+max_balanced_partition_pedigree'></span>

<h3>Description</h3>

<p>Uses the method suggested by Chlebíková (1996) to construct an approximate
maximally balanced connected partition. A further refinement step can be made
to reduce the cost of the cut edges. See
<code>vignette("pedigree_partitioning", package = "pedmod")</code> for further
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>max_balanced_partition(
  from,
  to,
  weight_data = NULL,
  edge_weights = NULL,
  slack = 0,
  max_kl_it_inner = 50L,
  max_kl_it = 10000L,
  trace = 0L,
  check_weights = TRUE,
  do_reorder = FALSE
)

max_balanced_partition_pedigree(
  id,
  father.id,
  mother.id,
  id_weight = NULL,
  father_weight = NULL,
  mother_weight = NULL,
  slack = 0,
  max_kl_it_inner = 50L,
  max_kl_it = 10000L,
  trace = 0L,
  check_weights = TRUE,
  do_reorder = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="max_balanced_partition_+3A_from">from</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_to">to</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_weight_data">weight_data</code></td>
<td>
<p>list with two elements called <code>"id"</code> for the id and
<code>"weight"</code> for the vertex weight. All vertices that are not in this list
have a weight of one. Use <code>NULL</code> if all vertices have a weight of one.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_edge_weights">edge_weights</code></td>
<td>
<p>numeric vector with weights for each edge. Needs to have
the same length as <code>from</code> and <code>to</code>. Use <code>NULL</code> if all edges
should have a weight of one.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_slack">slack</code></td>
<td>
<p>fraction between zero and 0.5 for the allowed amount of
deviation from the balance criterion that is allowed to reduce the cost of
the cut edges.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_max_kl_it_inner">max_kl_it_inner</code></td>
<td>
<p>maximum number of moves to consider in each
iteration when <code>slack &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_max_kl_it">max_kl_it</code></td>
<td>
<p>maximum number of iterations to use when reducing the
cost of the cut edges. Typically the method converges quickly and this
argument is not needed.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_trace">trace</code></td>
<td>
<p>integer where larger values yields more information printed to
the console during the procedure.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_check_weights">check_weights</code></td>
<td>
<p>logical for whether to check the weights in each
biconnected component. This may fail if the graph is not connected in which
case the results will likely be wrong. It may also fail for large graphs
because of floating-point arithmetic. The latter is not an error and the
reason for this argument.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_do_reorder">do_reorder</code></td>
<td>
<p>logical for whether the implementation should reorder the
vertices. This may reduce the computation time for some data sets.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_id">id</code></td>
<td>
<p>integer vector with the child id.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_father.id">father.id</code></td>
<td>
<p>integer vector with the father id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_mother.id">mother.id</code></td>
<td>
<p>integer vector with the mother id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_id_weight">id_weight</code></td>
<td>
<p>numeric vector with the weight to use for each vertex
(individual). <code>NULL</code> yields a weight of one for all.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_father_weight">father_weight</code></td>
<td>
<p>weights of the edges created between the fathers
and the children. Use <code>NULL</code> if all should have a weight of one.</p>
</td></tr>
<tr><td><code id="max_balanced_partition_+3A_mother_weight">mother_weight</code></td>
<td>
<p>weights of the edges created between the mothers
and the children. Use <code>NULL</code> if all should have a weight of one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>balance_criterion</code></td>
<td>
<p>value of the balance criterion.</p>
</td></tr>
<tr><td><code>removed_edges</code></td>
<td>
<p>2D integer matrix with the removed edges.</p>
</td></tr>
<tr><td><code>set_1</code>, <code>set_2</code></td>
<td>
<p>The two sets in the partition.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chlebíková, J. (1996).
<em>Approximating the maximally balanced connected partition problem in
graphs</em>. Information Processing Letters, 60(5), 225-230.
</p>
<p>Hopcroft, J., &amp; Tarjan, R. (1973).
<em>Algorithm 447: efficient algorithms for graph manipulation</em>.
Communications of the ACM, 16(6), 372-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biconnected_components">biconnected_components</a></code>,
<code><a href="#topic+block_cut_tree">block_cut_tree</a></code>, and
<code><a href="#topic+unconnected_partition">unconnected_partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example of a data set in pedigree and graph form
library(pedmod)
dat_pedigree &lt;- data.frame(
  id = 1:48,
  mom = c(
    NA, NA, 2L, 2L, 2L, NA, NA, 7L, 7L, 7L, 3L, 3L, 3L, 3L, NA, 15L, 15L, 43L,
    18L, NA, NA, 21L, 21L, 9L, 9L, 9L, 9L, NA, NA, 29L, 29L, 29L, 30L, 30L, NA,
    NA, 36L, 36L, 36L, 38L, 38L, NA, NA, 43L, 43L, 43L, 32L, 32L),
  dad = c(NA, NA, 1L, 1L, 1L, NA, NA, 6L, 6L, 6L, 8L, 8L, 8L, 8L, NA, 4L, 4L,
          42L, 5L, NA, NA, 20L, 20L, 22L, 22L, 22L, 22L, NA, NA, 28L, 28L, 28L,
          23L, 23L, NA, NA, 35L, 35L, 35L, 31L, 31L, NA, NA, 42L, 42L, 42L,
          45L, 45L),
  sex = c(1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L,
          2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L,
          1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L))

dat &lt;- list(
  to = c(
    3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L, 18L, 19L, 22L, 23L,
    24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L, 39L, 40L, 41L, 44L,
    45L, 46L, 47L, 48L, 3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L,
    18L, 19L, 22L, 23L, 24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L,
    39L, 40L, 41L, 44L, 45L, 46L, 47L, 48L),
  from = c(
    1L, 1L, 1L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 4L, 4L, 42L, 5L, 20L, 20L, 22L, 22L,
    22L, 22L, 28L, 28L, 28L, 23L, 23L, 35L, 35L, 35L, 31L, 31L, 42L, 42L, 42L,
    45L, 45L, 2L, 2L, 2L, 7L, 7L, 7L, 3L, 3L, 3L, 3L, 15L, 15L, 43L, 18L, 21L,
    21L, 9L, 9L, 9L, 9L, 29L, 29L, 29L, 30L, 30L, 36L, 36L, 36L, 38L, 38L, 43L,
    43L, 43L, 32L, 32L))

# the results may be different because of different orders!
out_pedigree &lt;- max_balanced_partition_pedigree(
  id = dat_pedigree$id, father.id = dat_pedigree$dad,
  mother.id = dat_pedigree$mom)
out &lt;- max_balanced_partition(dat$to, dat$from)

all.equal(out_pedigree$balance_criterion, out$balance_criterion)
all.equal(out_pedigree$removed_edges, out$removed_edges)

</code></pre>

<hr>
<h2 id='mvndst'>Multivariate Normal Distribution CDF and Its Derivative</h2><span id='topic+mvndst'></span><span id='topic+mvndst_grad'></span>

<h3>Description</h3>

<p>Provides an approximation of the multivariate normal distribution CDF
over a hyperrectangle and the derivative with respect to the mean vector
and the covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvndst(
  lower,
  upper,
  mu,
  sigma,
  maxvls = 25000L,
  abs_eps = 0.001,
  rel_eps = 0L,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  method = 0L,
  n_sequences = 8L,
  use_tilting = FALSE
)

mvndst_grad(
  lower,
  upper,
  mu,
  sigma,
  maxvls = 25000L,
  abs_eps = 0.001,
  rel_eps = 0L,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  method = 0L,
  n_sequences = 8L,
  use_tilting = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvndst_+3A_lower">lower</code></td>
<td>
<p>numeric vector with lower bounds.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_upper">upper</code></td>
<td>
<p>numeric vector with upper bounds.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_mu">mu</code></td>
<td>
<p>numeric vector with means.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_sigma">sigma</code></td>
<td>
<p>covariance matrix.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_rel_eps">rel_eps</code></td>
<td>
<p>relative convergence threshold.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_n_sequences">n_sequences</code></td>
<td>
<p>number of randomized quasi-Monte Carlo sequences to use.
More samples yields a better estimate of the error but a worse
approximation. Eight is used in the original Fortran code. If one is
used then the error will be set to zero because it cannot be estimated.</p>
</td></tr>
<tr><td><code id="mvndst_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mvndst:</code>
An approximation of the CDF. The <code>"n_it"</code> attribute shows the number of
integrand evaluations, the <code>"inform"</code> attribute is zero if the
requested precision is achieved, and the <code>"abserr"</code> attribute
shows 3.5 times the estimated standard error.
</p>
<p><code>mvndst_grad:</code>
A list with
</p>

<ul>
<li> <p><code>likelihood</code>: the likelihood approximation.
</p>
</li>
<li> <p><code>d_mu</code>: the derivative with respect to the the mean vector.
</p>
</li>
<li> <p><code>d_sigma</code>: the derivative with respect to the covariance matrix
ignoring the symmetry (i.e. working the <code class="reqn">n^2</code> parameters with
<code class="reqn">n</code> being the dimension rather than the <code class="reqn">n(n + 1) / 2</code>
free parameters).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># simulate covariance matrix and the upper bound
set.seed(1)
n &lt;- 10L
S &lt;- drop(rWishart(1L, 2 * n, diag(n) / 2 / n))
u &lt;- rnorm(n)

system.time(pedmod_res &lt;- mvndst(
    lower = rep(-Inf, n), upper = u, sigma = S, mu = numeric(n),
    maxvls = 1e6, abs_eps = 0, rel_eps = 1e-4, use_aprx = TRUE))
pedmod_res

# compare with mvtnorm
if(require(mvtnorm)){
    mvtnorm_time &lt;- system.time(mvtnorm_res &lt;- mvtnorm::pmvnorm(
        upper = u, sigma = S, algorithm = GenzBretz(
            maxpts = 1e6, abseps = 0, releps = 1e-4)))
    cat("mvtnorm_res:\n")
    print(mvtnorm_res)

    cat("mvtnorm_time:\n")
    print(mvtnorm_time)
}

# with titling
system.time(pedmod_res &lt;- mvndst(
    lower = rep(-Inf, n), upper = u, sigma = S, mu = numeric(n),
    maxvls = 1e6, abs_eps = 0, rel_eps = 1e-4, use_tilting = TRUE))
pedmod_res

# compare with TruncatedNormal
if(require(TruncatedNormal)){
    TruncatedNormal_time &lt;- system.time(
        TruncatedNormal_res &lt;- TruncatedNormal::pmvnorm(
            lb = rep(-Inf, n), ub = u, sigma = S,
            B = attr(pedmod_res, "n_it"), type = "qmc"))
    cat("TruncatedNormal_res:\n")
    print(TruncatedNormal_res)

    cat("TruncatedNormal_time:\n")
    print(TruncatedNormal_time)
}

# check the gradient
system.time(pedmod_res &lt;- mvndst_grad(
  lower = rep(-Inf, n), upper = u, sigma = S, mu = numeric(n),
  maxvls = 1e5, minvls = 1e5, abs_eps = 0, rel_eps = 1e-4, use_aprx = TRUE))
pedmod_res

# compare with numerical differentiation. Should give the same up to Monte
# Carlo and finite difference error
if(require(numDeriv)){
  num_res &lt;- grad(
    function(par){
      set.seed(1)
      mu &lt;- head(par, n)
      S[upper.tri(S, TRUE)] &lt;- tail(par, -n)
      S[lower.tri(S)] &lt;- t(S)[lower.tri(S)]
      mvndst(
        lower = rep(-Inf, n), upper = u, sigma = S, mu = mu,
        maxvls = 1e4, minvls = 1e4, abs_eps = 0, rel_eps = 1e-4,
        use_aprx = TRUE)
    }, c(numeric(n), S[upper.tri(S, TRUE)]),
    method.args = list(d = .01, r = 2))

  d_mu &lt;- head(num_res, n)
  d_sigma &lt;- matrix(0, n, n)
  d_sigma[upper.tri(d_sigma, TRUE)] &lt;- tail(num_res, -n)
  d_sigma[upper.tri(d_sigma)] &lt;- d_sigma[upper.tri(d_sigma)] / 2
  d_sigma[lower.tri(d_sigma)] &lt;- t(d_sigma)[lower.tri(d_sigma)]

  cat("numerical derivatives\n")
  print(rbind(numDeriv = d_mu,
              pedmod = pedmod_res$d_mu))
  print(d_sigma)
  cat("\nd_sigma from pedmod\n")
  print(pedmod_res$d_sigma) # for comparison
}

</code></pre>

<hr>
<h2 id='pedigree_ll_terms'>Get a C++ Object for Log Marginal Likelihood Approximations</h2><span id='topic+pedigree_ll_terms'></span><span id='topic+pedigree_ll_terms_loadings'></span>

<h3>Description</h3>

<p>Constructs an object needed for <code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and
<code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedigree_ll_terms(data, max_threads = 1L, n_sequences = 8L)

pedigree_ll_terms_loadings(data, max_threads = 1L, n_sequences = 8L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedigree_ll_terms_+3A_data">data</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> where each element is a list for a cluster
with an:
</p>

<ul>
<li><p><code>"X"</code> element with the design matrix for the fixed effect,
</p>
</li>
<li><p><code>"Z"</code> element with the design matrix for the loadings of the effects (only needed for <code>pedigree_ll_terms_loadings</code>),
</p>
</li>
<li><p><code>"y"</code> element with the zero-one outcomes, and
</p>
</li>
<li><p><code>"scale_mats"</code> element with a list where each element is a
scale/correlation matrix for a particular type of effect.
</p>
</li></ul>
</td></tr>
<tr><td><code id="pedigree_ll_terms_+3A_max_threads">max_threads</code></td>
<td>
<p>maximum number of threads to use.</p>
</td></tr>
<tr><td><code id="pedigree_ll_terms_+3A_n_sequences">n_sequences</code></td>
<td>
<p>number of randomized quasi-Monte Carlo sequences to use.
More samples yields a better estimate of the error but a worse
approximation. Eight is used in the original Fortran code. If one is
used then the error will be set to zero because it cannot be estimated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An intercept column is not added to the <code>X</code> matrices
like what <code><a href="stats.html#topic+lm.fit">lm.fit</a></code> and <code><a href="stats.html#topic+glm.fit">glm.fit</a></code> do.
Thus, it is often important that the user adds an intercept column
to these matrices as it is hardly ever justified to not include the
intercept (the exceptions being e.g. when splines are used which include
the intercept and with certain dummy designs). This equally holds for
the <code>Z</code> matrices with <code>pedigree_ll_terms_loadings</code>.
</p>
<p><code>pedigree_ll_terms_loadings</code> relax the assumption that the scale
parameter is the same for all individuals. <code>pedigree_ll_terms_loadings</code>
and <code>pedigree_ll_terms</code> yield the same model if <code>"Z"</code> is an
intercept column for all families but with a different parameterization.
In this case, <code>pedigree_ll_terms</code> will be
faster. See <code>vignette("pedmod", "pedmod")</code> for examples of using
<code>pedigree_ll_terms_loadings</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three families as an example
fam_dat &lt;- list(
  list(
    y = c(FALSE, TRUE, FALSE, FALSE),
    X = structure(c(
      1, 1, 1, 1, 1.2922654151273, 0.358134905909256, -0.734963997107464,
      0.855235473516044, -1.16189500386223, -0.387298334620742,
      0.387298334620742, 1.16189500386223),
      .Dim = 4:3, .Dimnames = list( NULL, c("(Intercept)", "X1", ""))),
    rel_mat = structure(c(
      1, 0.5, 0.5, 0.125, 0.5, 1, 0.5, 0.125, 0.5, 0.5,
      1, 0.125, 0.125, 0.125, 0.125, 1), .Dim = c(4L, 4L)),
    met_mat = structure(c(1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1),
                        .Dim = c(4L, 4L))),
  list(
    y = c(FALSE, FALSE, FALSE),
    X = structure(c(
      1, 1, 1, -0.0388728997202442, -0.0913782435233639,
      -0.0801619722392612, -1, 0, 1), .Dim = c(3L, 3L)),
    rel_mat = structure(c(
      1, 0.5, 0.125, 0.5, 1, 0.125, 0.125, 0.125, 1), .Dim = c(3L, 3L)),
    met_mat = structure(c(
      1, 1, 0, 1, 1, 0, 0, 0, 1), .Dim = c(3L, 3L))),
  list(
    y = c(TRUE, FALSE),
    X = structure(c(
      1, 1, 0.305275750370738, -1.49482995913648,  -0.707106781186547,
      0.707106781186547),
      .Dim = 2:3, .Dimnames = list( NULL, c("(Intercept)", "X1", ""))),
    rel_mat = structure(c(1, 0.5,  0.5, 1), .Dim = c(2L, 2L)),
    met_mat = structure(c(1, 1, 1, 1), .Dim = c(2L,  2L))))

# get the data into the format needed for the package
dat_arg &lt;- lapply(fam_dat, function(x){
  # we need the following for each family:
  #   y: the zero-one outcomes.
  #   X: the design matrix for the fixed effects.
  #   scale_mats: list with the scale matrices for each type of effect.
  list(y = as.numeric(x$y), X = x$X,
       scale_mats = list(x$rel_mat, x$met_mat))
})

# get a pointer to the C++ object
ptr &lt;- pedigree_ll_terms(dat_arg, max_threads = 1L)

# get the argument for a the version with loadings
dat_arg_loadings &lt;- lapply(fam_dat, function(x){
  list(y = as.numeric(x$y), X = x$X, Z = x$X[, 1:2],
       scale_mats = list(x$rel_mat, x$met_mat))
})

ptr &lt;- pedigree_ll_terms_loadings(dat_arg_loadings, max_threads = 1L)

</code></pre>

<hr>
<h2 id='pedmod_opt'>Optimize the Log Marginal Likelihood</h2><span id='topic+pedmod_opt'></span><span id='topic+pedmod_start'></span><span id='topic+pedmod_start_loadings'></span>

<h3>Description</h3>

<p>Optimizes <code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>
using a passed optimization function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedmod_opt(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  opt_func = NULL,
  seed = 1L,
  indices = NULL,
  minvls = -1L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  fix = NULL,
  standardized = FALSE,
  method = 0L,
  use_tilting = FALSE,
  vls_scales = NULL,
  ...
)

pedmod_start(
  ptr,
  data,
  maxvls = 1000L,
  abs_eps = 0,
  rel_eps = 0.01,
  seed = 1L,
  indices = NULL,
  scale_max = 9,
  minvls = 100L,
  do_reorder = TRUE,
  use_aprx = TRUE,
  n_threads = 1L,
  cluster_weights = NULL,
  standardized = FALSE,
  method = 0L,
  sc_start = NULL,
  use_tilting = FALSE,
  vls_scales = NULL
)

pedmod_start_loadings(
  ptr,
  data,
  indices = NULL,
  cluster_weights = NULL,
  sc_start_invariant = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedmod_opt_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> or
<code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_par">par</code></td>
<td>
<p>starting values passed to <code>opt_func</code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_rel_eps">rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_opt_func">opt_func</code></td>
<td>
<p>function to perform minimization with arguments like
<code><a href="stats.html#topic+optim">optim</a></code>. BFGS is used with <code><a href="stats.html#topic+optim">optim</a></code> if this argument
is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_seed">seed</code></td>
<td>
<p>seed to pass to <code><a href="base.html#topic+set.seed">set.seed</a></code> before each gradient and
function evaluation. Use <code>NULL</code> if the seed should not be fixed.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_fix">fix</code></td>
<td>
<p>integer vector with indices of <code>par</code> to fix. This is useful
for computing profile likelihoods. <code>NULL</code> yields all parameters.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_standardized">standardized</code></td>
<td>
<p>logical for whether to use the standardized or direct
parameterization. See <code><a href="#topic+standardized_to_direct">standardized_to_direct</a></code> and the vignette
at <code>vignette("pedmod", package = "pedmod")</code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>opt_func</code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_data">data</code></td>
<td>
<p>the <code><a href="base.html#topic+list">list</a></code> that was passed to
<code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> or <code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_scale_max">scale_max</code></td>
<td>
<p>the maximum value for the scale parameters. Sometimes, the
optimization method tends to find large scale parameters and get stuck.
Setting a maximum solves this.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_sc_start">sc_start</code></td>
<td>
<p>starting value for the scale parameters. Use <code>NULL</code> if
you have no value to start with.</p>
</td></tr>
<tr><td><code id="pedmod_opt_+3A_sc_start_invariant">sc_start_invariant</code></td>
<td>
<p>scale parameter(s) like sc_start. It is the value
that all individuals should have (i.e. not one that varies by individual).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>pedmod_start</code> and <code>pedmod_start_loadings</code>
yield starting values which can be used for
<code>pedmod_opt</code>. The methods are based on a heuristics.
</p>


<h3>Value</h3>

<p><code>pedmod_opt</code>: The output from the <code>opt_func</code> argument. Thus, if
<code>fix</code> is supplied then this is optimal values of only <code>par[-fix]</code>
with
<code>par[fix]</code> being fixed to the inputs. Thus, the length is only the
number of non-fixed parameters.
</p>
<p><code>pedmod_start</code>: A <code>list</code> with:
</p>

<ul>
<li><p> par: the starting value.
</p>
</li>
<li><p> beta_no_rng: the fixed effects MLEs without random effects.
</p>
</li>
<li><p> logLik_no_rng: the log maximum likelihood without random effects.
</p>
</li>
<li><p> logLik_est: the likelihood at par.
</p>
</li></ul>

<p><code>pedmod_start_loadings</code>: A <code>list</code> with:
</p>

<ul>
<li><p> par: the starting value.
</p>
</li>
<li><p> beta_no_rng: the fixed effects MLEs without random effects.
</p>
</li>
<li><p> logLik_no_rng: the log maximum likelihood without random effects.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+pedmod_sqn">pedmod_sqn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# we simulate outcomes with an additive genetic effect. The kinship matrix is
# the same for all families and given by
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq * Cmat
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(100L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 1L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 1L)
fit &lt;- pedmod_opt(ptr = ptr, par = start$par, n_threads = 1L, use_aprx = TRUE,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3)
fit$par # the estimate
-fit$value # the log maximum likelihood
start$logLik_no_rng # the log maximum likelihood without the random effects

</code></pre>

<hr>
<h2 id='pedmod_profile'>Computes Profile Likelihood Based Confidence Intervals</h2><span id='topic+pedmod_profile'></span>

<h3>Description</h3>

<p>Computes likelihood ratio based confidence intervals for one the parameters
in the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedmod_profile(
  ptr,
  par,
  delta,
  maxvls,
  minvls = -1L,
  alpha = 0.05,
  abs_eps,
  rel_eps,
  which_prof,
  indices = NULL,
  maxvls_start = max(100L, as.integer(ceiling(maxvls/5))),
  minvls_start = if (minvls &lt; 0) minvls else minvls/5,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  method = 0L,
  seed = 1L,
  verbose = FALSE,
  max_step = 15L,
  standardized = FALSE,
  use_tilting = FALSE,
  vls_scales = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedmod_profile_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> or
<code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_par">par</code></td>
<td>
<p>numeric vector with the maximum likelihood estimator e.g. from
<code><a href="#topic+pedmod_opt">pedmod_opt</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_delta">delta</code></td>
<td>
<p>numeric scalar with an initial step to take. Subsequent steps
are taken by <code>2^(&lt;iteration number&gt; - 1) * delta</code>. Two times the
standard error is a good value or a guess thereof. Hessian approximations are
not implemented as of this writing and therefore the user needs to provide
some guess.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_alpha">alpha</code></td>
<td>
<p>numeric scalar with the confidence level required.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_rel_eps">rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_which_prof">which_prof</code></td>
<td>
<p>integer scalar with index of the parameter which the
profile likelihood curve should be computed for.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_maxvls_start">maxvls_start</code>, <code id="pedmod_profile_+3A_minvls_start">minvls_start</code></td>
<td>
<p>number of samples to use when finding the
initial values for the optimization.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_seed">seed</code></td>
<td>
<p>seed to pass to <code><a href="base.html#topic+set.seed">set.seed</a></code> before each gradient and
function evaluation. Use <code>NULL</code> if the seed should not be fixed.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_verbose">verbose</code></td>
<td>
<p>logical for whether output should be printed to the console
during the estimation of the profile likelihood curve.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_max_step">max_step</code></td>
<td>
<p>integer scalar with the maximum number of steps to take in
either directions.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_standardized">standardized</code></td>
<td>
<p>logical for whether to use the standardized or direct
parameterization. See <code><a href="#topic+standardized_to_direct">standardized_to_direct</a></code> and the vignette
at <code>vignette("pedmod", package = "pedmod")</code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="#topic+pedmod_opt">pedmod_opt</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>confs</code></td>
<td>
<p>2D numeric vector with the profile likelihood based confidence
interval.</p>
</td></tr>
<tr><td><code>xs</code></td>
<td>
<p>the points at which the profile likelihood is evaluated.</p>
</td></tr>
<tr><td><code>p_log_Lik</code></td>
<td>
<p>the log profile likelihood values at <code>xs</code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>list with the returned objects from <code><a href="#topic+pedmod_opt">pedmod_opt</a></code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pedmod_opt">pedmod_opt</a></code>, <code><a href="#topic+pedmod_sqn">pedmod_sqn</a></code>,
<code><a href="#topic+pedmod_profile_prop">pedmod_profile_prop</a></code>, and <code><a href="#topic+pedmod_profile_nleq">pedmod_profile_nleq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# we simulate outcomes with an additive genetic effect. The kinship matrix is
# the same for all families and given by
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq * Cmat
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(100L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 1L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 1L)
fit &lt;- pedmod_opt(ptr = ptr, par = start$par, n_threads = 1L, use_aprx = TRUE,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3)
fit$par # the estimate

# 90% likelihood ratio based confidence interval for the log of the scale
# parameter
prof_out &lt;- pedmod_profile(ptr = ptr, fit$par, delta = .4, maxvls = 5000L,
                           minvls = 1000L, alpha = .1, which_prof = 4L,
                           abs_eps = 0, rel_eps = 1e-3, verbose = TRUE)
exp(prof_out$confs) # the confidence interval

# plot the log profile likelihood
plot(exp(prof_out$xs), prof_out$p_log_Lik, pch = 16,
     xlab = expression(sigma), ylab = "log profile likelihood")
abline(v = exp(prof_out$confs), lty = 2)


</code></pre>

<hr>
<h2 id='pedmod_profile_nleq'>Computes Profile Likelihood Based Confidence Intervals for a Nonlinear
Transformation of the Variables</h2><span id='topic+pedmod_profile_nleq'></span>

<h3>Description</h3>

<p>Computes Profile Likelihood Based Confidence Intervals for a Nonlinear
Transformation of the Variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedmod_profile_nleq(
  ptr,
  par,
  maxvls,
  minvls = -1L,
  alpha = 0.05,
  abs_eps,
  rel_eps,
  heq,
  heq_bounds = c(-Inf, Inf),
  delta,
  indices = NULL,
  maxvls_start = max(100L, as.integer(ceiling(maxvls/5))),
  minvls_start = if (minvls &lt; 0) minvls else minvls/5,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  method = 0L,
  seed = 1L,
  verbose = FALSE,
  max_step = 15L,
  use_tilting = FALSE,
  vls_scales = NULL,
  control.outer = list(itmax = 100L, method = "BFGS", kkt2.check = FALSE, trace =
    FALSE),
  control.optim = list(fnscale = get_n_terms(ptr)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedmod_profile_nleq_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code> or
<code><a href="#topic+pedigree_ll_terms_loadings">pedigree_ll_terms_loadings</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_par">par</code></td>
<td>
<p>numeric vector with the maximum likelihood estimator e.g. from
<code><a href="#topic+pedmod_opt">pedmod_opt</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_alpha">alpha</code></td>
<td>
<p>numeric scalar with the confidence level required.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_rel_eps">rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_heq">heq</code></td>
<td>
<p>function that returns a one dimensional numerical vector which
should be profiled. It does not need to evaluate to zero at the maximum
likelihood estimator.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_heq_bounds">heq_bounds</code></td>
<td>
<p>two dimensional numerical vector with bounds for
<code>heq</code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_delta">delta</code></td>
<td>
<p>numeric scalar with an initial step to take. Subsequent steps
are taken by <code>2^(&lt;iteration number&gt; - 1) * delta</code>. Two times the
standard error is a good value or a guess thereof. Hessian approximations are
not implemented as of this writing and therefore the user needs to provide
some guess.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_maxvls_start">maxvls_start</code>, <code id="pedmod_profile_nleq_+3A_minvls_start">minvls_start</code></td>
<td>
<p>number of samples to use when finding the
initial values for the optimization.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_seed">seed</code></td>
<td>
<p>seed to pass to <code><a href="base.html#topic+set.seed">set.seed</a></code> before each gradient and
function evaluation. Use <code>NULL</code> if the seed should not be fixed.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_verbose">verbose</code></td>
<td>
<p>logical for whether output should be printed to the console
during the estimation of the profile likelihood curve.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_max_step">max_step</code></td>
<td>
<p>integer scalar with the maximum number of steps to take in
either directions.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_nleq_+3A_control.outer">control.outer</code>, <code id="pedmod_profile_nleq_+3A_control.optim">control.optim</code>, <code id="pedmod_profile_nleq_+3A_...">...</code></td>
<td>
<p>arguments passed to
<code><a href="alabama.html#topic+auglag">auglag</a></code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pedmod_opt">pedmod_opt</a></code>, <code><a href="#topic+pedmod_sqn">pedmod_sqn</a></code>,
<code><a href="#topic+pedmod_profile">pedmod_profile</a></code>, and <code><a href="#topic+pedmod_profile_prop">pedmod_profile_prop</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# similar examples to that in help("pedmod_profile_prop")
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

C &lt;- matrix(c(
  1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
  0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
  0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
  0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1
), 10L)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameters.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = c(3, 1)){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq[1] * Cmat + sig_sq[2] * C
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X,
           scale_mats = list(genetic = Cmat, environment = C))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(200L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 2L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 2L)
fit &lt;- pedmod_opt(ptr = ptr, par = start$par, use_aprx = TRUE, n_threads = 2L,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3)
fit$par # the estimate

# 90% likelihood ratio based confidence interval for the proportion of variance
# of the genetic effect
heq &lt;- function(par){
 vars &lt;- exp(tail(par, 2))
 vars[1] / (1 + sum(vars))
}
heq(fit$par)
prof_out_nleq &lt;- pedmod_profile_nleq(
  ptr = ptr, fit$par, maxvls = 2500L, minvls = 500L, alpha = .1,
  abs_eps = 0, rel_eps = 1e-3, verbose = TRUE, use_aprx = TRUE,
  heq = heq, heq_bounds = c(0, Inf), delta = .2, n_threads = 2L)
prof_out_nleq$confs # the confidence interval for the proportion

# plot the log profile likelihood
plot(prof_out_nleq$xs, prof_out_nleq$p_log_Lik, pch = 16,
     xlab = "proportion of variance", ylab = "log profile likelihood")
abline(v = prof_out_nleq$confs, lty = 2)


</code></pre>

<hr>
<h2 id='pedmod_profile_prop'>Computes Profile Likelihood Based Confidence Intervals for the Proportion
of Variance</h2><span id='topic+pedmod_profile_prop'></span>

<h3>Description</h3>

<p>Constructs a likelihood ratio based confidence intervals for the
proportion of variance for one of the effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedmod_profile_prop(
  ptr,
  par,
  maxvls,
  minvls = -1L,
  alpha = 0.05,
  abs_eps,
  rel_eps,
  which_prof,
  indices = NULL,
  maxvls_start = max(100L, as.integer(ceiling(maxvls/5))),
  minvls_start = if (minvls &lt; 0) minvls else minvls/5,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  method = 0L,
  seed = 1L,
  verbose = FALSE,
  max_step = 15L,
  opt_func = NULL,
  use_tilting = FALSE,
  vls_scales = NULL,
  bound = c(0.01, 0.99),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedmod_profile_prop_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_par">par</code></td>
<td>
<p>numeric vector with the maximum likelihood estimator e.g. from
<code><a href="#topic+pedmod_opt">pedmod_opt</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_alpha">alpha</code></td>
<td>
<p>numeric scalar with the confidence level required.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_rel_eps">rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_which_prof">which_prof</code></td>
<td>
<p>the index of the random effect which proportion of
variance should be profiled.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_maxvls_start">maxvls_start</code>, <code id="pedmod_profile_prop_+3A_minvls_start">minvls_start</code></td>
<td>
<p>number of samples to use when finding the
initial values for the optimization.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_seed">seed</code></td>
<td>
<p>seed to pass to <code><a href="base.html#topic+set.seed">set.seed</a></code> before each gradient and
function evaluation. Use <code>NULL</code> if the seed should not be fixed.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_verbose">verbose</code></td>
<td>
<p>logical for whether output should be printed to the console
during the estimation of the profile likelihood curve.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_max_step">max_step</code></td>
<td>
<p>integer scalar with the maximum number of steps to take in
either directions.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_opt_func">opt_func</code></td>
<td>
<p>function to perform minimization with arguments like
<code><a href="stats.html#topic+optim">optim</a></code>. BFGS is used with <code><a href="stats.html#topic+optim">optim</a></code> if this argument
is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_bound">bound</code></td>
<td>
<p>boundaries for the limits of the proportion. Has to be in
between <code class="reqn">(0,1)</code>. This is useful particularly if the optimization fails to
work on the default values.</p>
</td></tr>
<tr><td><code id="pedmod_profile_prop_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>opt_func</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is only useful when there is more than one type of random
effect. If not, then <code><a href="#topic+pedmod_profile">pedmod_profile</a></code> can be used because of
the scale invariance of the likelihood ratio.
</p>


<h3>Value</h3>

<p>A list like <code><a href="#topic+pedmod_profile">pedmod_profile</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pedmod_opt">pedmod_opt</a></code>, <code><a href="#topic+pedmod_sqn">pedmod_sqn</a></code>,
<code><a href="#topic+pedmod_profile">pedmod_profile</a></code>, and <code><a href="#topic+pedmod_profile_nleq">pedmod_profile_nleq</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# we simulate outcomes with an additive genetic effect and a childhood
# environment effect. The kinship matrix is the same for all families and
# given by
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

# the scale matrix for the childhood environment effect is also the same and
# given by
C &lt;- matrix(c(
  1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
  0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
  0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
  0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1
), 10L)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameters.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = c(3, 1)){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq[1] * Cmat + sig_sq[2] * C
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X,
           scale_mats = list(genetic = Cmat, environment = C))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(200L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 1L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 1L)
fit &lt;- pedmod_opt(ptr = ptr, par = start$par, n_threads = 1L, use_aprx = TRUE,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3)
fit$par # the estimate

# 90% likelihood ratio based confidence interval for the proportion of variance
# of the genetic effect
prof_out &lt;- pedmod_profile_prop(
  ptr = ptr, fit$par, maxvls = 5000L, minvls = 1000L, alpha = .1,
  which_prof = 1L, abs_eps = 0, rel_eps = 1e-3, verbose = TRUE)
prof_out$confs # the confidence interval for the proportion

# plot the log profile likelihood
keep &lt;- c(-1L, -length(prof_out$xs))
plot(prof_out$xs[keep], prof_out$p_log_Lik[keep], pch = 16,
     xlab = "proportion of variance", ylab = "log profile likelihood")
abline(v = prof_out$confs, lty = 2)


</code></pre>

<hr>
<h2 id='pedmod_sqn'>Optimize the Log Marginal Likelihood Using a Stochastic Quasi-Newton Method</h2><span id='topic+pedmod_sqn'></span>

<h3>Description</h3>

<p>Optimizes <code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>
using a stochastic quasi-Newton method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pedmod_sqn(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  step_factor,
  n_it,
  n_grad_steps,
  indices = NULL,
  minvls = -1L,
  n_grad = 50L,
  n_hess = 500L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  fix = NULL,
  standardized = FALSE,
  minvls_hess = minvls,
  maxvls_hess = maxvls,
  abs_eps_hess = abs_eps,
  rel_eps_hess = rel_eps,
  verbose = FALSE,
  method = 0L,
  check_every = 2L * n_grad_steps,
  use_tilting = FALSE,
  vls_scales = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pedmod_sqn_+3A_ptr">ptr</code></td>
<td>
<p>object from <code><a href="#topic+pedigree_ll_terms">pedigree_ll_terms</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_par">par</code></td>
<td>
<p>starting values.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_maxvls">maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_abs_eps">abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_rel_eps">rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code><a href="#topic+eval_pedigree_ll">eval_pedigree_ll</a></code> and <code><a href="#topic+eval_pedigree_grad">eval_pedigree_grad</a></code>.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_step_factor">step_factor</code></td>
<td>
<p>factor used for the step size. The step size is
<code>step_factor</code> divided by the iteration number.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_n_it">n_it</code></td>
<td>
<p>number of stochastic gradient steps to make.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_n_grad_steps">n_grad_steps</code></td>
<td>
<p>number of stochastic gradient steps to make between each
Hessian approximation update.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_indices">indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_minvls">minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_n_grad">n_grad</code></td>
<td>
<p>number of log marginal likelihood terms to include in the
stochastic gradient step.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_n_hess">n_hess</code></td>
<td>
<p>number of log marginal likelihood terms to include in the
gradients used for the Hessian approximation update. This is set to the
entire sample (or <code>indices</code>) if this is greater than or equal to half
the number of log marginal likelihood terms.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_do_reorder">do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_use_aprx">use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code><a href="stats.html#topic+pnorm">pnorm</a></code> and <code><a href="stats.html#topic+qnorm">qnorm</a></code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_n_threads">n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_cluster_weights">cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_fix">fix</code></td>
<td>
<p>integer vector with indices of <code>par</code> to fix. This is useful
for computing profile likelihoods. <code>NULL</code> yields all parameters.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_standardized">standardized</code></td>
<td>
<p>logical for whether to use the standardized or direct
parameterization. See <code><a href="#topic+standardized_to_direct">standardized_to_direct</a></code> and the vignette
at <code>vignette("pedmod", package = "pedmod")</code>.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_minvls_hess">minvls_hess</code></td>
<td>
<p><code>minvls</code> argument to use when updating the Hessian
approximation.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_maxvls_hess">maxvls_hess</code></td>
<td>
<p><code>maxvls</code> argument to use when updating the Hessian
approximation.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_abs_eps_hess">abs_eps_hess</code></td>
<td>
<p><code>abs_eps</code> argument to use when updating the Hessian
approximation.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_rel_eps_hess">rel_eps_hess</code></td>
<td>
<p><code>rel_eps</code> argument to use when updating the Hessian
approximation.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_verbose">verbose</code></td>
<td>
<p>logical for whether to print output during the estimation.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_method">method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_check_every">check_every</code></td>
<td>
<p>integer for the number of gradient steps between checking
that the likelihood did increase. If not, the iterations are reset and the
step-size is halved.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_use_tilting">use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td></tr>
<tr><td><code id="pedmod_sqn_+3A_vls_scales">vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses a stochastic quasi-Newton method like suggested by
Byrd et al. (2016) with a few differences: Differences in gradients are
used rather than Hessian-vector products, BFGS rather than L-BFGS is used
because the problem is typically low dimensional, and damped BFGS updates
are used (see e.g. chapter 18 of Nocedal and Wright, 2006).
</p>
<p>Separate arguments for the gradient approximation in the Hessian update are
provided as one may want a more precise approximation for these gradients.
<code>step_factor</code> likely depends on the other parameters and the data set
and should be altered.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>estimated parameters.</p>
</td></tr>
<tr><td><code>omegas</code></td>
<td>
<p>parameter estimates after each iteration.</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hessian approximation in the quasi-Newton method. It should not
be treated as the Hessian.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Byrd, R. H., Hansen, S. L., Nocedal, J., &amp; Singer, Y. (2016).
<em>A stochastic quasi-Newton method for large-scale optimization</em>.
SIAM Journal on Optimization, 26(2), 1008-1031.
</p>
<p>Nocedal, J., &amp; Wright, S. (2006). <em>Numerical optimization</em>.
Springer Science &amp; Business Media.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pedmod_opt">pedmod_opt</a></code> and <code><a href="#topic+pedmod_start">pedmod_start</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# we simulate outcomes with an additive genetic effect. The kinship matrix is
# the same for all families and given by
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq * Cmat
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(100L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 1L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 1L)
fit &lt;- pedmod_sqn(ptr = ptr, par = start$par, n_threads = 1L, use_aprx = TRUE,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3,
                  n_grad_steps = 20L, step_factor = 1, n_grad = 10L,
                  n_hess = 50L, check_every = 50L, n_it = 1000L)
fit$par # maximum likelihood estimate
# the maximum likelihood
eval_pedigree_ll(ptr = ptr, fit$par, maxvls = 5000L, abs_eps = 0,
                 rel_eps = 1e-3, minvls = 1000L)

</code></pre>

<hr>
<h2 id='standardized_to_direct'>Transform Between Parameterizations</h2><span id='topic+standardized_to_direct'></span><span id='topic+direct_to_standardized'></span>

<h3>Description</h3>

<p>Transform the parameters between the parameterizations that are used in the
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardized_to_direct(par, n_scales, jacobian = FALSE)

direct_to_standardized(par, n_scales)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardized_to_direct_+3A_par">par</code></td>
<td>
<p>concatenated vector with the fixed effect slopes and the scale
parameters that should be transformed.</p>
</td></tr>
<tr><td><code id="standardized_to_direct_+3A_n_scales">n_scales</code></td>
<td>
<p>integer with the number of scale parameters.</p>
</td></tr>
<tr><td><code id="standardized_to_direct_+3A_jacobian">jacobian</code></td>
<td>
<p>logical indicating if the Jacobian matrix of transformation
should be computed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>standardized_to_direct</code>:
returns the parameters using the direct parameterizations. See
<code>vignette("pedmod", package = "pedmod")</code> for the definition. There is
an attribute called 'variance proportions' with the proportion of variance
of each effect assuming that all the scale matrices are correlation matrices.
There is an attribute called jacobian with the Jacobian matrix if
<code>jacobian</code> is <code>TRUE</code>.
</p>
<p><code>direct_to_standardized</code>:
the parameters using the standardized parameterizations. See
<code>vignette("pedmod", package = "pedmod")</code> for the definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># transform backwards and forwards
set.seed(1)
smp &lt;- runif(10, -1, 1)
res &lt;- standardized_to_direct(smp, 2L, jacobian = TRUE)
back_val &lt;- direct_to_standardized(res, 2L)

all.equal(smp, back_val, check.attributes = FALSE)
res

</code></pre>

<hr>
<h2 id='unconnected_partition'>Finds an Approximately Balanced Partition</h2><span id='topic+unconnected_partition'></span><span id='topic+unconnected_partition_pedigree'></span>

<h3>Description</h3>

<p>Finds an Approximately Balanced Partition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unconnected_partition(
  from,
  to,
  weight_data = NULL,
  edge_weights = NULL,
  slack = 0,
  max_kl_it_inner = 50L,
  max_kl_it = 10000L,
  trace = 0L,
  init = integer()
)

unconnected_partition_pedigree(
  id,
  father.id,
  mother.id,
  id_weight = NULL,
  father_weight = NULL,
  mother_weight = NULL,
  slack = 0,
  max_kl_it_inner = 50L,
  max_kl_it = 10000L,
  trace = 0L,
  init = integer()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unconnected_partition_+3A_from">from</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_to">to</code></td>
<td>
<p>integer vector with one of the vertex ids.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_weight_data">weight_data</code></td>
<td>
<p>list with two elements called <code>"id"</code> for the id and
<code>"weight"</code> for the vertex weight. All vertices that are not in this list
have a weight of one. Use <code>NULL</code> if all vertices have a weight of one.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_edge_weights">edge_weights</code></td>
<td>
<p>numeric vector with weights for each edge. Needs to have
the same length as <code>from</code> and <code>to</code>. Use <code>NULL</code> if all edges
should have a weight of one.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_slack">slack</code></td>
<td>
<p>fraction between zero and 0.5 for the allowed amount of
deviation from the balance criterion that is allowed to reduce the cost of
the cut edges.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_max_kl_it_inner">max_kl_it_inner</code></td>
<td>
<p>maximum number of moves to consider in each
iteration when <code>slack &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_max_kl_it">max_kl_it</code></td>
<td>
<p>maximum number of iterations to use when reducing the
cost of the cut edges. Typically the method converges quickly and this
argument is not needed.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_trace">trace</code></td>
<td>
<p>integer where larger values yields more information printed to
the console during the procedure.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_init">init</code></td>
<td>
<p>integer vector with ids that one of the two sets in the partition
should start out with.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_id">id</code></td>
<td>
<p>integer vector with the child id.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_father.id">father.id</code></td>
<td>
<p>integer vector with the father id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_mother.id">mother.id</code></td>
<td>
<p>integer vector with the mother id. May be <code>NA</code> if
it is missing.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_id_weight">id_weight</code></td>
<td>
<p>numeric vector with the weight to use for each vertex
(individual). <code>NULL</code> yields a weight of one for all.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_father_weight">father_weight</code></td>
<td>
<p>weights of the edges created between the fathers
and the children. Use <code>NULL</code> if all should have a weight of one.</p>
</td></tr>
<tr><td><code id="unconnected_partition_+3A_mother_weight">mother_weight</code></td>
<td>
<p>weights of the edges created between the mothers
and the children. Use <code>NULL</code> if all should have a weight of one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>balance_criterion</code></td>
<td>
<p>value of the balance criterion.</p>
</td></tr>
<tr><td><code>removed_edges</code></td>
<td>
<p>2D integer matrix with the removed edges.</p>
</td></tr>
<tr><td><code>set_1</code>, <code>set_2</code></td>
<td>
<p>The two sets in the partition.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+max_balanced_partition">max_balanced_partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example of a data set in pedigree and graph form
library(pedmod)
dat_pedigree &lt;- data.frame(
  id = 1:48,
  mom = c(
    NA, NA, 2L, 2L, 2L, NA, NA, 7L, 7L, 7L, 3L, 3L, 3L, 3L, NA, 15L, 15L, 43L,
    18L, NA, NA, 21L, 21L, 9L, 9L, 9L, 9L, NA, NA, 29L, 29L, 29L, 30L, 30L, NA,
    NA, 36L, 36L, 36L, 38L, 38L, NA, NA, 43L, 43L, 43L, 32L, 32L),
  dad = c(NA, NA, 1L, 1L, 1L, NA, NA, 6L, 6L, 6L, 8L, 8L, 8L, 8L, NA, 4L, 4L,
          42L, 5L, NA, NA, 20L, 20L, 22L, 22L, 22L, 22L, NA, NA, 28L, 28L, 28L,
          23L, 23L, NA, NA, 35L, 35L, 35L, 31L, 31L, NA, NA, 42L, 42L, 42L,
          45L, 45L),
  sex = c(1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L,
          2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L,
          1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L))

dat &lt;- list(
  to = c(
    3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L, 18L, 19L, 22L, 23L,
    24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L, 39L, 40L, 41L, 44L,
    45L, 46L, 47L, 48L, 3L, 4L, 5L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 16L, 17L,
    18L, 19L, 22L, 23L, 24L, 25L, 26L, 27L, 30L, 31L, 32L, 33L, 34L, 37L, 38L,
    39L, 40L, 41L, 44L, 45L, 46L, 47L, 48L),
  from = c(
    1L, 1L, 1L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 4L, 4L, 42L, 5L, 20L, 20L, 22L, 22L,
    22L, 22L, 28L, 28L, 28L, 23L, 23L, 35L, 35L, 35L, 31L, 31L, 42L, 42L, 42L,
    45L, 45L, 2L, 2L, 2L, 7L, 7L, 7L, 3L, 3L, 3L, 3L, 15L, 15L, 43L, 18L, 21L,
    21L, 9L, 9L, 9L, 9L, 29L, 29L, 29L, 30L, 30L, 36L, 36L, 36L, 38L, 38L, 43L,
    43L, 43L, 32L, 32L))

# the results may be different because of different orders!
out_pedigree &lt;- unconnected_partition_pedigree(
  id = dat_pedigree$id, father.id = dat_pedigree$dad,
  mother.id = dat_pedigree$mom)
out &lt;- unconnected_partition(dat$to, dat$from)

all.equal(out_pedigree$balance_criterion, out$balance_criterion)
all.equal(out_pedigree$removed_edges, out$removed_edges)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
