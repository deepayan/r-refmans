<!DOCTYPE html><html><head><title>Help for package torch</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {torch}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#as_array'><p>Converts to array</p></a></li>
<li><a href='#autograd_backward'><p>Computes the sum of gradients of given tensors w.r.t. graph leaves.</p></a></li>
<li><a href='#autograd_function'><p>Records operation history and defines formulas for differentiating ops.</p></a></li>
<li><a href='#autograd_grad'><p>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</p></a></li>
<li><a href='#autograd_set_grad_mode'><p>Set grad mode</p></a></li>
<li><a href='#AutogradContext'><p>Class representing the context.</p></a></li>
<li><a href='#backends_cudnn_is_available'><p>CuDNN is available</p></a></li>
<li><a href='#backends_cudnn_version'><p>CuDNN version</p></a></li>
<li><a href='#backends_mkl_is_available'><p>MKL is available</p></a></li>
<li><a href='#backends_mkldnn_is_available'><p>MKLDNN is available</p></a></li>
<li><a href='#backends_mps_is_available'><p>MPS is available</p></a></li>
<li><a href='#backends_openmp_is_available'><p>OpenMP is available</p></a></li>
<li><a href='#broadcast_all'><p>Given a list of values (possibly containing numbers), returns a list where each</p>
value is broadcasted based on the following rules:</a></li>
<li><a href='#call_torch_function'><p>Call a (Potentially Unexported) Torch Function</p></a></li>
<li><a href='#Constraint'><p>Abstract base class for constraints.</p></a></li>
<li><a href='#contrib_sort_vertices'><p>Contrib sort vertices</p></a></li>
<li><a href='#cuda_amp_grad_scaler'><p>Creates a gradient scaler</p></a></li>
<li><a href='#cuda_current_device'><p>Returns the index of a currently selected device.</p></a></li>
<li><a href='#cuda_device_count'><p>Returns the number of GPUs available.</p></a></li>
<li><a href='#cuda_empty_cache'><p>Empty cache</p></a></li>
<li><a href='#cuda_get_device_capability'><p>Returns the major and minor CUDA capability of <code>device</code></p></a></li>
<li><a href='#cuda_is_available'><p>Returns a bool indicating if CUDA is currently available.</p></a></li>
<li><a href='#cuda_memory_stats'><p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p></a></li>
<li><a href='#cuda_runtime_version'><p>Returns the CUDA runtime version</p></a></li>
<li><a href='#cuda_synchronize'><p>Waits for all kernels in all streams on a CUDA device to complete.</p></a></li>
<li><a href='#dataloader'><p>Data loader. Combines a dataset and a sampler, and provides</p>
single- or multi-process iterators over the dataset.</a></li>
<li><a href='#dataloader_make_iter'><p>Creates an iterator from a DataLoader</p></a></li>
<li><a href='#dataloader_next'><p>Get the next element of a dataloader iterator</p></a></li>
<li><a href='#dataset'><p>Helper function to create an function that generates R6 instances of class <code>dataset</code></p></a></li>
<li><a href='#dataset_subset'><p>Dataset Subset</p></a></li>
<li><a href='#distr_bernoulli'><p>Creates a Bernoulli distribution parameterized by <code>probs</code></p>
or <code>logits</code> (but not both).
Samples are binary (0 or 1). They take the value <code>1</code> with probability <code>p</code>
and <code>0</code> with probability <code>1 - p</code>.</a></li>
<li><a href='#distr_categorical'><p>Creates a categorical distribution parameterized by either <code>probs</code> or</p>
<code>logits</code> (but not both).</a></li>
<li><a href='#distr_chi2'><p>Creates a Chi2 distribution parameterized by shape parameter <code>df</code>.</p>
This is exactly equivalent to <code>distr_gamma(alpha=0.5*df, beta=0.5)</code></a></li>
<li><a href='#distr_gamma'><p>Creates a Gamma distribution parameterized by shape <code>concentration</code> and <code>rate</code>.</p></a></li>
<li><a href='#distr_mixture_same_family'><p>Mixture of components in the same family</p></a></li>
<li><a href='#distr_multivariate_normal'><p>Gaussian distribution</p></a></li>
<li><a href='#distr_normal'><p>Creates a normal (also called Gaussian) distribution parameterized by</p>
<code>loc</code> and <code>scale</code>.</a></li>
<li><a href='#distr_poisson'><p>Creates a Poisson distribution parameterized by <code>rate</code>, the rate parameter.</p></a></li>
<li><a href='#Distribution'><p>Generic R6 class representing distributions</p></a></li>
<li><a href='#enumerate'><p>Enumerate an iterator</p></a></li>
<li><a href='#enumerate.dataloader'><p>Enumerate an iterator</p></a></li>
<li><a href='#get_install_libs_url'><p>Install Torch from files</p></a></li>
<li><a href='#install_torch'><p>Install Torch</p></a></li>
<li><a href='#is_dataloader'><p>Checks if the object is a dataloader</p></a></li>
<li><a href='#is_nn_buffer'><p>Checks if the object is a nn_buffer</p></a></li>
<li><a href='#is_nn_module'><p>Checks if the object is an nn_module</p></a></li>
<li><a href='#is_nn_parameter'><p>Checks if an object is a nn_parameter</p></a></li>
<li><a href='#is_optimizer'><p>Checks if the object is a torch optimizer</p></a></li>
<li><a href='#is_torch_device'><p>Checks if object is a device</p></a></li>
<li><a href='#is_torch_dtype'><p>Check if object is a torch data type</p></a></li>
<li><a href='#is_torch_layout'><p>Check if an object is a torch layout.</p></a></li>
<li><a href='#is_torch_memory_format'><p>Check if an object is a memory format</p></a></li>
<li><a href='#is_torch_qscheme'><p>Checks if an object is a QScheme</p></a></li>
<li><a href='#is_undefined_tensor'><p>Checks if a tensor is undefined</p></a></li>
<li><a href='#iterable_dataset'><p>Creates an iterable dataset</p></a></li>
<li><a href='#jit_compile'><p>Compile TorchScript code into a graph</p></a></li>
<li><a href='#jit_load'><p>Loads a <code>script_function</code> or <code>script_module</code> previously saved with <code>jit_save</code></p></a></li>
<li><a href='#jit_ops'><p>Enable idiomatic access to JIT operators from R.</p></a></li>
<li><a href='#jit_save'><p>Saves a <code>script_function</code> to a path</p></a></li>
<li><a href='#jit_save_for_mobile'><p>Saves a <code>script_function</code> or <code>script_module</code> in bytecode form,</p>
to be loaded on a mobile device</a></li>
<li><a href='#jit_scalar'><p>Adds the 'jit_scalar' class to the input</p></a></li>
<li><a href='#jit_trace'><p>Trace a function and return an executable <code>script_function</code>.</p></a></li>
<li><a href='#jit_trace_module'><p>Trace a module</p></a></li>
<li><a href='#jit_tuple'><p>Adds the 'jit_tuple' class to the input</p></a></li>
<li><a href='#linalg_cholesky'><p>Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.</p></a></li>
<li><a href='#linalg_cholesky_ex'><p>Computes the Cholesky decomposition of a complex Hermitian or real</p>
symmetric positive-definite matrix.</a></li>
<li><a href='#linalg_cond'><p>Computes the condition number of a matrix with respect to a matrix norm.</p></a></li>
<li><a href='#linalg_det'><p>Computes the determinant of a square matrix.</p></a></li>
<li><a href='#linalg_eig'><p>Computes the eigenvalue decomposition of a square matrix if it exists.</p></a></li>
<li><a href='#linalg_eigh'><p>Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.</p></a></li>
<li><a href='#linalg_eigvals'><p>Computes the eigenvalues of a square matrix.</p></a></li>
<li><a href='#linalg_eigvalsh'><p>Computes the eigenvalues of a complex Hermitian or real symmetric matrix.</p></a></li>
<li><a href='#linalg_householder_product'><p>Computes the first <code>n</code> columns of a product of Householder matrices.</p></a></li>
<li><a href='#linalg_inv'><p>Computes the inverse of a square matrix if it exists.</p></a></li>
<li><a href='#linalg_inv_ex'><p>Computes the inverse of a square matrix if it is invertible.</p></a></li>
<li><a href='#linalg_lstsq'><p>Computes a solution to the least squares problem of a system of linear equations.</p></a></li>
<li><a href='#linalg_matrix_norm'><p>Computes a matrix norm.</p></a></li>
<li><a href='#linalg_matrix_power'><p>Computes the <code>n</code>-th power of a square matrix for an integer <code>n</code>.</p></a></li>
<li><a href='#linalg_matrix_rank'><p>Computes the numerical rank of a matrix.</p></a></li>
<li><a href='#linalg_multi_dot'><p>Efficiently multiplies two or more matrices</p></a></li>
<li><a href='#linalg_norm'><p>Computes a vector or matrix norm.</p></a></li>
<li><a href='#linalg_pinv'><p>Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.</p></a></li>
<li><a href='#linalg_qr'><p>Computes the QR decomposition of a matrix.</p></a></li>
<li><a href='#linalg_slogdet'><p>Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.</p></a></li>
<li><a href='#linalg_solve'><p>Computes the solution of a square system of linear equations with a unique solution.</p></a></li>
<li><a href='#linalg_svd'><p>Computes the singular value decomposition (SVD) of a matrix.</p></a></li>
<li><a href='#linalg_svdvals'><p>Computes the singular values of a matrix.</p></a></li>
<li><a href='#linalg_tensorinv'><p>Computes the multiplicative inverse of <code>torch_tensordot()</code></p></a></li>
<li><a href='#linalg_tensorsolve'><p>Computes the solution <code>X</code> to the system <code>torch_tensordot(A, X) = B</code>.</p></a></li>
<li><a href='#linalg_vector_norm'><p>Computes a vector norm.</p></a></li>
<li><a href='#load_state_dict'><p>Load a state dict file</p></a></li>
<li><a href='#local_autocast'><p>Autocast context manager</p></a></li>
<li><a href='#local_device'><p>Device contexts</p></a></li>
<li><a href='#lr_cosine_annealing'><p>Set the learning rate of each parameter group using a cosine annealing schedule</p></a></li>
<li><a href='#lr_lambda'><p>Sets the learning rate of each parameter group to the initial lr</p>
times a given function. When last_epoch=-1, sets initial lr as lr.</a></li>
<li><a href='#lr_multiplicative'><p>Multiply the learning rate of each parameter group by the factor given</p>
in the specified function. When last_epoch=-1, sets initial lr as lr.</a></li>
<li><a href='#lr_one_cycle'><p>Once cycle learning rate</p></a></li>
<li><a href='#lr_reduce_on_plateau'><p>Reduce learning rate on plateau</p></a></li>
<li><a href='#lr_scheduler'><p>Creates learning rate schedulers</p></a></li>
<li><a href='#lr_step'><p>Step learning rate decay</p></a></li>
<li><a href='#nn_adaptive_avg_pool1d'><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_adaptive_avg_pool2d'><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_adaptive_avg_pool3d'><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_adaptive_log_softmax_with_loss'><p>AdaptiveLogSoftmaxWithLoss module</p></a></li>
<li><a href='#nn_adaptive_max_pool1d'><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_adaptive_max_pool2d'><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_adaptive_max_pool3d'><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_avg_pool1d'><p>Applies a 1D average pooling over an input signal composed of several</p>
input planes.</a></li>
<li><a href='#nn_avg_pool2d'><p>Applies a 2D average pooling over an input signal composed of several input</p>
planes.</a></li>
<li><a href='#nn_avg_pool3d'><p>Applies a 3D average pooling over an input signal composed of several input</p>
planes.</a></li>
<li><a href='#nn_batch_norm1d'><p>BatchNorm1D module</p></a></li>
<li><a href='#nn_batch_norm2d'><p>BatchNorm2D</p></a></li>
<li><a href='#nn_batch_norm3d'><p>BatchNorm3D</p></a></li>
<li><a href='#nn_bce_loss'><p>Binary cross entropy loss</p></a></li>
<li><a href='#nn_bce_with_logits_loss'><p>BCE with logits loss</p></a></li>
<li><a href='#nn_bilinear'><p>Bilinear module</p></a></li>
<li><a href='#nn_buffer'><p>Creates a nn_buffer</p></a></li>
<li><a href='#nn_celu'><p>CELU module</p></a></li>
<li><a href='#nn_contrib_sparsemax'><p>Sparsemax activation</p></a></li>
<li><a href='#nn_conv_transpose1d'><p>ConvTranspose1D</p></a></li>
<li><a href='#nn_conv_transpose2d'><p>ConvTranpose2D module</p></a></li>
<li><a href='#nn_conv_transpose3d'><p>ConvTranpose3D module</p></a></li>
<li><a href='#nn_conv1d'><p>Conv1D module</p></a></li>
<li><a href='#nn_conv2d'><p>Conv2D module</p></a></li>
<li><a href='#nn_conv3d'><p>Conv3D module</p></a></li>
<li><a href='#nn_cosine_embedding_loss'><p>Cosine embedding loss</p></a></li>
<li><a href='#nn_cross_entropy_loss'><p>CrossEntropyLoss module</p></a></li>
<li><a href='#nn_ctc_loss'><p>The Connectionist Temporal Classification loss.</p></a></li>
<li><a href='#nn_dropout'><p>Dropout module</p></a></li>
<li><a href='#nn_dropout2d'><p>Dropout2D module</p></a></li>
<li><a href='#nn_dropout3d'><p>Dropout3D module</p></a></li>
<li><a href='#nn_elu'><p>ELU module</p></a></li>
<li><a href='#nn_embedding'><p>Embedding module</p></a></li>
<li><a href='#nn_embedding_bag'><p>Embedding bag module</p></a></li>
<li><a href='#nn_flatten'><p>Flattens a contiguous range of dims into a tensor.</p></a></li>
<li><a href='#nn_fractional_max_pool2d'><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_fractional_max_pool3d'><p>Applies a 3D fractional max pooling over an input signal composed of several input planes.</p></a></li>
<li><a href='#nn_gelu'><p>GELU module</p></a></li>
<li><a href='#nn_glu'><p>GLU module</p></a></li>
<li><a href='#nn_group_norm'><p>Group normalization</p></a></li>
<li><a href='#nn_gru'><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></a></li>
<li><a href='#nn_hardshrink'><p>Hardshwink module</p></a></li>
<li><a href='#nn_hardsigmoid'><p>Hardsigmoid module</p></a></li>
<li><a href='#nn_hardswish'><p>Hardswish module</p></a></li>
<li><a href='#nn_hardtanh'><p>Hardtanh module</p></a></li>
<li><a href='#nn_hinge_embedding_loss'><p>Hinge embedding loss</p></a></li>
<li><a href='#nn_identity'><p>Identity module</p></a></li>
<li><a href='#nn_init_calculate_gain'><p>Calculate gain</p></a></li>
<li><a href='#nn_init_constant_'><p>Constant initialization</p></a></li>
<li><a href='#nn_init_dirac_'><p>Dirac initialization</p></a></li>
<li><a href='#nn_init_eye_'><p>Eye initialization</p></a></li>
<li><a href='#nn_init_kaiming_normal_'><p>Kaiming normal initialization</p></a></li>
<li><a href='#nn_init_kaiming_uniform_'><p>Kaiming uniform initialization</p></a></li>
<li><a href='#nn_init_normal_'><p>Normal initialization</p></a></li>
<li><a href='#nn_init_ones_'><p>Ones initialization</p></a></li>
<li><a href='#nn_init_orthogonal_'><p>Orthogonal initialization</p></a></li>
<li><a href='#nn_init_sparse_'><p>Sparse initialization</p></a></li>
<li><a href='#nn_init_trunc_normal_'><p>Truncated normal initialization</p></a></li>
<li><a href='#nn_init_uniform_'><p>Uniform initialization</p></a></li>
<li><a href='#nn_init_xavier_normal_'><p>Xavier normal initialization</p></a></li>
<li><a href='#nn_init_xavier_uniform_'><p>Xavier uniform initialization</p></a></li>
<li><a href='#nn_init_zeros_'><p>Zeros initialization</p></a></li>
<li><a href='#nn_kl_div_loss'><p>Kullback-Leibler divergence loss</p></a></li>
<li><a href='#nn_l1_loss'><p>L1 loss</p></a></li>
<li><a href='#nn_layer_norm'><p>Layer normalization</p></a></li>
<li><a href='#nn_leaky_relu'><p>LeakyReLU module</p></a></li>
<li><a href='#nn_linear'><p>Linear module</p></a></li>
<li><a href='#nn_log_sigmoid'><p>LogSigmoid module</p></a></li>
<li><a href='#nn_log_softmax'><p>LogSoftmax module</p></a></li>
<li><a href='#nn_lp_pool1d'><p>Applies a 1D power-average pooling over an input signal composed of several input</p>
planes.</a></li>
<li><a href='#nn_lp_pool2d'><p>Applies a 2D power-average pooling over an input signal composed of several input</p>
planes.</a></li>
<li><a href='#nn_lstm'><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input</p>
sequence.</a></li>
<li><a href='#nn_margin_ranking_loss'><p>Margin ranking loss</p></a></li>
<li><a href='#nn_max_pool1d'><p>MaxPool1D module</p></a></li>
<li><a href='#nn_max_pool2d'><p>MaxPool2D module</p></a></li>
<li><a href='#nn_max_pool3d'><p>Applies a 3D max pooling over an input signal composed of several input</p>
planes.</a></li>
<li><a href='#nn_max_unpool1d'><p>Computes a partial inverse of <code>MaxPool1d</code>.</p></a></li>
<li><a href='#nn_max_unpool2d'><p>Computes a partial inverse of <code>MaxPool2d</code>.</p></a></li>
<li><a href='#nn_max_unpool3d'><p>Computes a partial inverse of <code>MaxPool3d</code>.</p></a></li>
<li><a href='#nn_module'><p>Base class for all neural network modules.</p></a></li>
<li><a href='#nn_module_dict'><p>Container that allows named values</p></a></li>
<li><a href='#nn_module_list'><p>Holds submodules in a list.</p></a></li>
<li><a href='#nn_mse_loss'><p>MSE loss</p></a></li>
<li><a href='#nn_multi_margin_loss'><p>Multi margin loss</p></a></li>
<li><a href='#nn_multihead_attention'><p>MultiHead attention</p></a></li>
<li><a href='#nn_multilabel_margin_loss'><p>Multilabel margin loss</p></a></li>
<li><a href='#nn_multilabel_soft_margin_loss'><p>Multi label soft margin loss</p></a></li>
<li><a href='#nn_nll_loss'><p>Nll loss</p></a></li>
<li><a href='#nn_pairwise_distance'><p>Pairwise distance</p></a></li>
<li><a href='#nn_parameter'><p>Creates an <code>nn_parameter</code></p></a></li>
<li><a href='#nn_poisson_nll_loss'><p>Poisson NLL loss</p></a></li>
<li><a href='#nn_prelu'><p>PReLU module</p></a></li>
<li><a href='#nn_prune_head'><p>Prune top layer(s) of a network</p></a></li>
<li><a href='#nn_relu'><p>ReLU module</p></a></li>
<li><a href='#nn_relu6'><p>ReLu6 module</p></a></li>
<li><a href='#nn_rnn'><p>RNN module</p></a></li>
<li><a href='#nn_rrelu'><p>RReLU module</p></a></li>
<li><a href='#nn_selu'><p>SELU module</p></a></li>
<li><a href='#nn_sequential'><p>A sequential container</p></a></li>
<li><a href='#nn_sigmoid'><p>Sigmoid module</p></a></li>
<li><a href='#nn_silu'><p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</p>
The SiLU function is also known as the swish function.</a></li>
<li><a href='#nn_smooth_l1_loss'><p>Smooth L1 loss</p></a></li>
<li><a href='#nn_soft_margin_loss'><p>Soft margin loss</p></a></li>
<li><a href='#nn_softmax'><p>Softmax module</p></a></li>
<li><a href='#nn_softmax2d'><p>Softmax2d module</p></a></li>
<li><a href='#nn_softmin'><p>Softmin</p></a></li>
<li><a href='#nn_softplus'><p>Softplus module</p></a></li>
<li><a href='#nn_softshrink'><p>Softshrink module</p></a></li>
<li><a href='#nn_softsign'><p>Softsign module</p></a></li>
<li><a href='#nn_tanh'><p>Tanh module</p></a></li>
<li><a href='#nn_tanhshrink'><p>Tanhshrink module</p></a></li>
<li><a href='#nn_threshold'><p>Threshold module</p></a></li>
<li><a href='#nn_triplet_margin_loss'><p>Triplet margin loss</p></a></li>
<li><a href='#nn_triplet_margin_with_distance_loss'><p>Triplet margin with distance loss</p></a></li>
<li><a href='#nn_unflatten'><p>Unflattens a tensor dim expanding it to a desired shape.</p>
For use with [nn_sequential.</a></li>
<li><a href='#nn_upsample'><p>Upsample module</p></a></li>
<li><a href='#nn_utils_clip_grad_norm_'><p>Clips gradient norm of an iterable of parameters.</p></a></li>
<li><a href='#nn_utils_clip_grad_value_'><p>Clips gradient of an iterable of parameters at specified value.</p></a></li>
<li><a href='#nn_utils_rnn_pack_padded_sequence'><p>Packs a Tensor containing padded sequences of variable length.</p></a></li>
<li><a href='#nn_utils_rnn_pack_sequence'><p>Packs a list of variable length Tensors</p></a></li>
<li><a href='#nn_utils_rnn_pad_packed_sequence'><p>Pads a packed batch of variable length sequences.</p></a></li>
<li><a href='#nn_utils_rnn_pad_sequence'><p>Pad a list of variable length Tensors with <code>padding_value</code></p></a></li>
<li><a href='#nn_utils_weight_norm'><p>nn_utils_weight_norm</p></a></li>
<li><a href='#nnf_adaptive_avg_pool1d'><p>Adaptive_avg_pool1d</p></a></li>
<li><a href='#nnf_adaptive_avg_pool2d'><p>Adaptive_avg_pool2d</p></a></li>
<li><a href='#nnf_adaptive_avg_pool3d'><p>Adaptive_avg_pool3d</p></a></li>
<li><a href='#nnf_adaptive_max_pool1d'><p>Adaptive_max_pool1d</p></a></li>
<li><a href='#nnf_adaptive_max_pool2d'><p>Adaptive_max_pool2d</p></a></li>
<li><a href='#nnf_adaptive_max_pool3d'><p>Adaptive_max_pool3d</p></a></li>
<li><a href='#nnf_affine_grid'><p>Affine_grid</p></a></li>
<li><a href='#nnf_alpha_dropout'><p>Alpha_dropout</p></a></li>
<li><a href='#nnf_avg_pool1d'><p>Avg_pool1d</p></a></li>
<li><a href='#nnf_avg_pool2d'><p>Avg_pool2d</p></a></li>
<li><a href='#nnf_avg_pool3d'><p>Avg_pool3d</p></a></li>
<li><a href='#nnf_batch_norm'><p>Batch_norm</p></a></li>
<li><a href='#nnf_bilinear'><p>Bilinear</p></a></li>
<li><a href='#nnf_binary_cross_entropy'><p>Binary_cross_entropy</p></a></li>
<li><a href='#nnf_binary_cross_entropy_with_logits'><p>Binary_cross_entropy_with_logits</p></a></li>
<li><a href='#nnf_celu'><p>Celu</p></a></li>
<li><a href='#nnf_contrib_sparsemax'><p>Sparsemax</p></a></li>
<li><a href='#nnf_conv_tbc'><p>Conv_tbc</p></a></li>
<li><a href='#nnf_conv_transpose1d'><p>Conv_transpose1d</p></a></li>
<li><a href='#nnf_conv_transpose2d'><p>Conv_transpose2d</p></a></li>
<li><a href='#nnf_conv_transpose3d'><p>Conv_transpose3d</p></a></li>
<li><a href='#nnf_conv1d'><p>Conv1d</p></a></li>
<li><a href='#nnf_conv2d'><p>Conv2d</p></a></li>
<li><a href='#nnf_conv3d'><p>Conv3d</p></a></li>
<li><a href='#nnf_cosine_embedding_loss'><p>Cosine_embedding_loss</p></a></li>
<li><a href='#nnf_cosine_similarity'><p>Cosine_similarity</p></a></li>
<li><a href='#nnf_cross_entropy'><p>Cross_entropy</p></a></li>
<li><a href='#nnf_ctc_loss'><p>Ctc_loss</p></a></li>
<li><a href='#nnf_dropout'><p>Dropout</p></a></li>
<li><a href='#nnf_dropout2d'><p>Dropout2d</p></a></li>
<li><a href='#nnf_dropout3d'><p>Dropout3d</p></a></li>
<li><a href='#nnf_elu'><p>Elu</p></a></li>
<li><a href='#nnf_embedding'><p>Embedding</p></a></li>
<li><a href='#nnf_embedding_bag'><p>Embedding_bag</p></a></li>
<li><a href='#nnf_fold'><p>Fold</p></a></li>
<li><a href='#nnf_fractional_max_pool2d'><p>Fractional_max_pool2d</p></a></li>
<li><a href='#nnf_fractional_max_pool3d'><p>Fractional_max_pool3d</p></a></li>
<li><a href='#nnf_gelu'><p>Gelu</p></a></li>
<li><a href='#nnf_glu'><p>Glu</p></a></li>
<li><a href='#nnf_grid_sample'><p>Grid_sample</p></a></li>
<li><a href='#nnf_group_norm'><p>Group_norm</p></a></li>
<li><a href='#nnf_gumbel_softmax'><p>Gumbel_softmax</p></a></li>
<li><a href='#nnf_hardshrink'><p>Hardshrink</p></a></li>
<li><a href='#nnf_hardsigmoid'><p>Hardsigmoid</p></a></li>
<li><a href='#nnf_hardswish'><p>Hardswish</p></a></li>
<li><a href='#nnf_hardtanh'><p>Hardtanh</p></a></li>
<li><a href='#nnf_hinge_embedding_loss'><p>Hinge_embedding_loss</p></a></li>
<li><a href='#nnf_instance_norm'><p>Instance_norm</p></a></li>
<li><a href='#nnf_interpolate'><p>Interpolate</p></a></li>
<li><a href='#nnf_kl_div'><p>Kl_div</p></a></li>
<li><a href='#nnf_l1_loss'><p>L1_loss</p></a></li>
<li><a href='#nnf_layer_norm'><p>Layer_norm</p></a></li>
<li><a href='#nnf_leaky_relu'><p>Leaky_relu</p></a></li>
<li><a href='#nnf_linear'><p>Linear</p></a></li>
<li><a href='#nnf_local_response_norm'><p>Local_response_norm</p></a></li>
<li><a href='#nnf_log_softmax'><p>Log_softmax</p></a></li>
<li><a href='#nnf_logsigmoid'><p>Logsigmoid</p></a></li>
<li><a href='#nnf_lp_pool1d'><p>Lp_pool1d</p></a></li>
<li><a href='#nnf_lp_pool2d'><p>Lp_pool2d</p></a></li>
<li><a href='#nnf_margin_ranking_loss'><p>Margin_ranking_loss</p></a></li>
<li><a href='#nnf_max_pool1d'><p>Max_pool1d</p></a></li>
<li><a href='#nnf_max_pool2d'><p>Max_pool2d</p></a></li>
<li><a href='#nnf_max_pool3d'><p>Max_pool3d</p></a></li>
<li><a href='#nnf_max_unpool1d'><p>Max_unpool1d</p></a></li>
<li><a href='#nnf_max_unpool2d'><p>Max_unpool2d</p></a></li>
<li><a href='#nnf_max_unpool3d'><p>Max_unpool3d</p></a></li>
<li><a href='#nnf_mse_loss'><p>Mse_loss</p></a></li>
<li><a href='#nnf_multi_head_attention_forward'><p>Multi head attention forward</p></a></li>
<li><a href='#nnf_multi_margin_loss'><p>Multi_margin_loss</p></a></li>
<li><a href='#nnf_multilabel_margin_loss'><p>Multilabel_margin_loss</p></a></li>
<li><a href='#nnf_multilabel_soft_margin_loss'><p>Multilabel_soft_margin_loss</p></a></li>
<li><a href='#nnf_nll_loss'><p>Nll_loss</p></a></li>
<li><a href='#nnf_normalize'><p>Normalize</p></a></li>
<li><a href='#nnf_one_hot'><p>One_hot</p></a></li>
<li><a href='#nnf_pad'><p>Pad</p></a></li>
<li><a href='#nnf_pairwise_distance'><p>Pairwise_distance</p></a></li>
<li><a href='#nnf_pdist'><p>Pdist</p></a></li>
<li><a href='#nnf_pixel_shuffle'><p>Pixel_shuffle</p></a></li>
<li><a href='#nnf_poisson_nll_loss'><p>Poisson_nll_loss</p></a></li>
<li><a href='#nnf_prelu'><p>Prelu</p></a></li>
<li><a href='#nnf_relu'><p>Relu</p></a></li>
<li><a href='#nnf_relu6'><p>Relu6</p></a></li>
<li><a href='#nnf_rrelu'><p>Rrelu</p></a></li>
<li><a href='#nnf_selu'><p>Selu</p></a></li>
<li><a href='#nnf_sigmoid'><p>Sigmoid</p></a></li>
<li><a href='#nnf_silu'><p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</p>
See <code>nn_silu()</code> for more information.</a></li>
<li><a href='#nnf_smooth_l1_loss'><p>Smooth_l1_loss</p></a></li>
<li><a href='#nnf_soft_margin_loss'><p>Soft_margin_loss</p></a></li>
<li><a href='#nnf_softmax'><p>Softmax</p></a></li>
<li><a href='#nnf_softmin'><p>Softmin</p></a></li>
<li><a href='#nnf_softplus'><p>Softplus</p></a></li>
<li><a href='#nnf_softshrink'><p>Softshrink</p></a></li>
<li><a href='#nnf_softsign'><p>Softsign</p></a></li>
<li><a href='#nnf_tanhshrink'><p>Tanhshrink</p></a></li>
<li><a href='#nnf_threshold'><p>Threshold</p></a></li>
<li><a href='#nnf_triplet_margin_loss'><p>Triplet_margin_loss</p></a></li>
<li><a href='#nnf_triplet_margin_with_distance_loss'><p>Triplet margin with distance loss</p></a></li>
<li><a href='#nnf_unfold'><p>Unfold</p></a></li>
<li><a href='#optim_adadelta'><p>Adadelta optimizer</p></a></li>
<li><a href='#optim_adagrad'><p>Adagrad optimizer</p></a></li>
<li><a href='#optim_adam'><p>Implements Adam algorithm.</p></a></li>
<li><a href='#optim_adamw'><p>Implements AdamW algorithm</p></a></li>
<li><a href='#optim_asgd'><p>Averaged Stochastic Gradient Descent optimizer</p></a></li>
<li><a href='#optim_lbfgs'><p>LBFGS optimizer</p></a></li>
<li><a href='#optim_required'><p>Dummy value indicating a required value.</p></a></li>
<li><a href='#optim_rmsprop'><p>RMSprop optimizer</p></a></li>
<li><a href='#optim_rprop'><p>Implements the resilient backpropagation algorithm.</p></a></li>
<li><a href='#optim_sgd'><p>SGD optimizer</p></a></li>
<li><a href='#optimizer'><p>Creates a custom optimizer</p></a></li>
<li><a href='#reexports'><p>Re-exporting the as_iterator function.</p></a></li>
<li><a href='#sampler'><p>Creates a new Sampler</p></a></li>
<li><a href='#slc'><p>Creates a slice</p></a></li>
<li><a href='#tensor_dataset'><p>Dataset wrapping tensors.</p></a></li>
<li><a href='#threads'><p>Number of threads</p></a></li>
<li><a href='#torch_abs'><p>Abs</p></a></li>
<li><a href='#torch_absolute'><p>Absolute</p></a></li>
<li><a href='#torch_acos'><p>Acos</p></a></li>
<li><a href='#torch_acosh'><p>Acosh</p></a></li>
<li><a href='#torch_adaptive_avg_pool1d'><p>Adaptive_avg_pool1d</p></a></li>
<li><a href='#torch_add'><p>Add</p></a></li>
<li><a href='#torch_addbmm'><p>Addbmm</p></a></li>
<li><a href='#torch_addcdiv'><p>Addcdiv</p></a></li>
<li><a href='#torch_addcmul'><p>Addcmul</p></a></li>
<li><a href='#torch_addmm'><p>Addmm</p></a></li>
<li><a href='#torch_addmv'><p>Addmv</p></a></li>
<li><a href='#torch_addr'><p>Addr</p></a></li>
<li><a href='#torch_allclose'><p>Allclose</p></a></li>
<li><a href='#torch_amax'><p>Amax</p></a></li>
<li><a href='#torch_amin'><p>Amin</p></a></li>
<li><a href='#torch_angle'><p>Angle</p></a></li>
<li><a href='#torch_arange'><p>Arange</p></a></li>
<li><a href='#torch_arccos'><p>Arccos</p></a></li>
<li><a href='#torch_arccosh'><p>Arccosh</p></a></li>
<li><a href='#torch_arcsin'><p>Arcsin</p></a></li>
<li><a href='#torch_arcsinh'><p>Arcsinh</p></a></li>
<li><a href='#torch_arctan'><p>Arctan</p></a></li>
<li><a href='#torch_arctanh'><p>Arctanh</p></a></li>
<li><a href='#torch_argmax'><p>Argmax</p></a></li>
<li><a href='#torch_argmin'><p>Argmin</p></a></li>
<li><a href='#torch_argsort'><p>Argsort</p></a></li>
<li><a href='#torch_as_strided'><p>As_strided</p></a></li>
<li><a href='#torch_asin'><p>Asin</p></a></li>
<li><a href='#torch_asinh'><p>Asinh</p></a></li>
<li><a href='#torch_atan'><p>Atan</p></a></li>
<li><a href='#torch_atan2'><p>Atan2</p></a></li>
<li><a href='#torch_atanh'><p>Atanh</p></a></li>
<li><a href='#torch_atleast_1d'><p>Atleast_1d</p></a></li>
<li><a href='#torch_atleast_2d'><p>Atleast_2d</p></a></li>
<li><a href='#torch_atleast_3d'><p>Atleast_3d</p></a></li>
<li><a href='#torch_avg_pool1d'><p>Avg_pool1d</p></a></li>
<li><a href='#torch_baddbmm'><p>Baddbmm</p></a></li>
<li><a href='#torch_bartlett_window'><p>Bartlett_window</p></a></li>
<li><a href='#torch_bernoulli'><p>Bernoulli</p></a></li>
<li><a href='#torch_bincount'><p>Bincount</p></a></li>
<li><a href='#torch_bitwise_and'><p>Bitwise_and</p></a></li>
<li><a href='#torch_bitwise_not'><p>Bitwise_not</p></a></li>
<li><a href='#torch_bitwise_or'><p>Bitwise_or</p></a></li>
<li><a href='#torch_bitwise_xor'><p>Bitwise_xor</p></a></li>
<li><a href='#torch_blackman_window'><p>Blackman_window</p></a></li>
<li><a href='#torch_block_diag'><p>Block_diag</p></a></li>
<li><a href='#torch_bmm'><p>Bmm</p></a></li>
<li><a href='#torch_broadcast_tensors'><p>Broadcast_tensors</p></a></li>
<li><a href='#torch_bucketize'><p>Bucketize</p></a></li>
<li><a href='#torch_can_cast'><p>Can_cast</p></a></li>
<li><a href='#torch_cartesian_prod'><p>Cartesian_prod</p></a></li>
<li><a href='#torch_cat'><p>Cat</p></a></li>
<li><a href='#torch_cdist'><p>Cdist</p></a></li>
<li><a href='#torch_ceil'><p>Ceil</p></a></li>
<li><a href='#torch_celu'><p>Celu</p></a></li>
<li><a href='#torch_celu_'><p>Celu_</p></a></li>
<li><a href='#torch_chain_matmul'><p>Chain_matmul</p></a></li>
<li><a href='#torch_channel_shuffle'><p>Channel_shuffle</p></a></li>
<li><a href='#torch_cholesky'><p>Cholesky</p></a></li>
<li><a href='#torch_cholesky_inverse'><p>Cholesky_inverse</p></a></li>
<li><a href='#torch_cholesky_solve'><p>Cholesky_solve</p></a></li>
<li><a href='#torch_chunk'><p>Chunk</p></a></li>
<li><a href='#torch_clamp'><p>Clamp</p></a></li>
<li><a href='#torch_clip'><p>Clip</p></a></li>
<li><a href='#torch_clone'><p>Clone</p></a></li>
<li><a href='#torch_combinations'><p>Combinations</p></a></li>
<li><a href='#torch_complex'><p>Complex</p></a></li>
<li><a href='#torch_conj'><p>Conj</p></a></li>
<li><a href='#torch_conv_tbc'><p>Conv_tbc</p></a></li>
<li><a href='#torch_conv_transpose1d'><p>Conv_transpose1d</p></a></li>
<li><a href='#torch_conv_transpose2d'><p>Conv_transpose2d</p></a></li>
<li><a href='#torch_conv_transpose3d'><p>Conv_transpose3d</p></a></li>
<li><a href='#torch_conv1d'><p>Conv1d</p></a></li>
<li><a href='#torch_conv2d'><p>Conv2d</p></a></li>
<li><a href='#torch_conv3d'><p>Conv3d</p></a></li>
<li><a href='#torch_cos'><p>Cos</p></a></li>
<li><a href='#torch_cosh'><p>Cosh</p></a></li>
<li><a href='#torch_cosine_similarity'><p>Cosine_similarity</p></a></li>
<li><a href='#torch_count_nonzero'><p>Count_nonzero</p></a></li>
<li><a href='#torch_cross'><p>Cross</p></a></li>
<li><a href='#torch_cummax'><p>Cummax</p></a></li>
<li><a href='#torch_cummin'><p>Cummin</p></a></li>
<li><a href='#torch_cumprod'><p>Cumprod</p></a></li>
<li><a href='#torch_cumsum'><p>Cumsum</p></a></li>
<li><a href='#torch_deg2rad'><p>Deg2rad</p></a></li>
<li><a href='#torch_dequantize'><p>Dequantize</p></a></li>
<li><a href='#torch_det'><p>Det</p></a></li>
<li><a href='#torch_device'><p>Create a Device object</p></a></li>
<li><a href='#torch_diag'><p>Diag</p></a></li>
<li><a href='#torch_diag_embed'><p>Diag_embed</p></a></li>
<li><a href='#torch_diagflat'><p>Diagflat</p></a></li>
<li><a href='#torch_diagonal'><p>Diagonal</p></a></li>
<li><a href='#torch_diff'><p>Computes the n-th forward difference along the given dimension.</p></a></li>
<li><a href='#torch_digamma'><p>Digamma</p></a></li>
<li><a href='#torch_dist'><p>Dist</p></a></li>
<li><a href='#torch_div'><p>Div</p></a></li>
<li><a href='#torch_divide'><p>Divide</p></a></li>
<li><a href='#torch_dot'><p>Dot</p></a></li>
<li><a href='#torch_dstack'><p>Dstack</p></a></li>
<li><a href='#torch_dtype'><p>Torch data types</p></a></li>
<li><a href='#torch_eig'><p>Eig</p></a></li>
<li><a href='#torch_einsum'><p>Einsum</p></a></li>
<li><a href='#torch_empty'><p>Empty</p></a></li>
<li><a href='#torch_empty_like'><p>Empty_like</p></a></li>
<li><a href='#torch_empty_strided'><p>Empty_strided</p></a></li>
<li><a href='#torch_eq'><p>Eq</p></a></li>
<li><a href='#torch_equal'><p>Equal</p></a></li>
<li><a href='#torch_erf'><p>Erf</p></a></li>
<li><a href='#torch_erfc'><p>Erfc</p></a></li>
<li><a href='#torch_erfinv'><p>Erfinv</p></a></li>
<li><a href='#torch_exp'><p>Exp</p></a></li>
<li><a href='#torch_exp2'><p>Exp2</p></a></li>
<li><a href='#torch_expm1'><p>Expm1</p></a></li>
<li><a href='#torch_eye'><p>Eye</p></a></li>
<li><a href='#torch_fft_fft'><p>Fft</p></a></li>
<li><a href='#torch_fft_fftfreq'><p>fftfreq</p></a></li>
<li><a href='#torch_fft_ifft'><p>Ifft</p></a></li>
<li><a href='#torch_fft_irfft'><p>Irfft</p></a></li>
<li><a href='#torch_fft_rfft'><p>Rfft</p></a></li>
<li><a href='#torch_finfo'><p>Floating point type info</p></a></li>
<li><a href='#torch_fix'><p>Fix</p></a></li>
<li><a href='#torch_flatten'><p>Flatten</p></a></li>
<li><a href='#torch_flip'><p>Flip</p></a></li>
<li><a href='#torch_fliplr'><p>Fliplr</p></a></li>
<li><a href='#torch_flipud'><p>Flipud</p></a></li>
<li><a href='#torch_floor'><p>Floor</p></a></li>
<li><a href='#torch_floor_divide'><p>Floor_divide</p></a></li>
<li><a href='#torch_fmod'><p>Fmod</p></a></li>
<li><a href='#torch_frac'><p>Frac</p></a></li>
<li><a href='#torch_full'><p>Full</p></a></li>
<li><a href='#torch_full_like'><p>Full_like</p></a></li>
<li><a href='#torch_gather'><p>Gather</p></a></li>
<li><a href='#torch_gcd'><p>Gcd</p></a></li>
<li><a href='#torch_ge'><p>Ge</p></a></li>
<li><a href='#torch_generator'><p>Create a Generator object</p></a></li>
<li><a href='#torch_geqrf'><p>Geqrf</p></a></li>
<li><a href='#torch_ger'><p>Ger</p></a></li>
<li><a href='#torch_get_rng_state'><p>RNG state management</p></a></li>
<li><a href='#torch_greater'><p>Greater</p></a></li>
<li><a href='#torch_greater_equal'><p>Greater_equal</p></a></li>
<li><a href='#torch_gt'><p>Gt</p></a></li>
<li><a href='#torch_hamming_window'><p>Hamming_window</p></a></li>
<li><a href='#torch_hann_window'><p>Hann_window</p></a></li>
<li><a href='#torch_heaviside'><p>Heaviside</p></a></li>
<li><a href='#torch_histc'><p>Histc</p></a></li>
<li><a href='#torch_hstack'><p>Hstack</p></a></li>
<li><a href='#torch_hypot'><p>Hypot</p></a></li>
<li><a href='#torch_i0'><p>I0</p></a></li>
<li><a href='#torch_iinfo'><p>Integer type info</p></a></li>
<li><a href='#torch_imag'><p>Imag</p></a></li>
<li><a href='#torch_index'><p>Index torch tensors</p></a></li>
<li><a href='#torch_index_put'><p>Modify values selected by <code>indices</code>.</p></a></li>
<li><a href='#torch_index_put_'><p>In-place version of <code>torch_index_put</code>.</p></a></li>
<li><a href='#torch_index_select'><p>Index_select</p></a></li>
<li><a href='#torch_install_path'><p>A simple exported version of install_path</p>
Returns the torch installation path.</a></li>
<li><a href='#torch_inverse'><p>Inverse</p></a></li>
<li><a href='#torch_is_complex'><p>Is_complex</p></a></li>
<li><a href='#torch_is_floating_point'><p>Is_floating_point</p></a></li>
<li><a href='#torch_is_installed'><p>Verifies if torch is installed</p></a></li>
<li><a href='#torch_is_nonzero'><p>Is_nonzero</p></a></li>
<li><a href='#torch_isclose'><p>Isclose</p></a></li>
<li><a href='#torch_isfinite'><p>Isfinite</p></a></li>
<li><a href='#torch_isinf'><p>Isinf</p></a></li>
<li><a href='#torch_isnan'><p>Isnan</p></a></li>
<li><a href='#torch_isneginf'><p>Isneginf</p></a></li>
<li><a href='#torch_isposinf'><p>Isposinf</p></a></li>
<li><a href='#torch_isreal'><p>Isreal</p></a></li>
<li><a href='#torch_istft'><p>Istft</p></a></li>
<li><a href='#torch_kaiser_window'><p>Kaiser_window</p></a></li>
<li><a href='#torch_kron'><p>Kronecker product</p></a></li>
<li><a href='#torch_kthvalue'><p>Kthvalue</p></a></li>
<li><a href='#torch_layout'><p>Creates the corresponding layout</p></a></li>
<li><a href='#torch_lcm'><p>Lcm</p></a></li>
<li><a href='#torch_le'><p>Le</p></a></li>
<li><a href='#torch_lerp'><p>Lerp</p></a></li>
<li><a href='#torch_less'><p>Less</p></a></li>
<li><a href='#torch_less_equal'><p>Less_equal</p></a></li>
<li><a href='#torch_lgamma'><p>Lgamma</p></a></li>
<li><a href='#torch_linspace'><p>Linspace</p></a></li>
<li><a href='#torch_load'><p>Loads a saved object</p></a></li>
<li><a href='#torch_log'><p>Log</p></a></li>
<li><a href='#torch_log10'><p>Log10</p></a></li>
<li><a href='#torch_log1p'><p>Log1p</p></a></li>
<li><a href='#torch_log2'><p>Log2</p></a></li>
<li><a href='#torch_logaddexp'><p>Logaddexp</p></a></li>
<li><a href='#torch_logaddexp2'><p>Logaddexp2</p></a></li>
<li><a href='#torch_logcumsumexp'><p>Logcumsumexp</p></a></li>
<li><a href='#torch_logdet'><p>Logdet</p></a></li>
<li><a href='#torch_logical_and'><p>Logical_and</p></a></li>
<li><a href='#torch_logical_not'><p>Logical_not</p></a></li>
<li><a href='#torch_logical_or'><p>Logical_or</p></a></li>
<li><a href='#torch_logical_xor'><p>Logical_xor</p></a></li>
<li><a href='#torch_logit'><p>Logit</p></a></li>
<li><a href='#torch_logspace'><p>Logspace</p></a></li>
<li><a href='#torch_logsumexp'><p>Logsumexp</p></a></li>
<li><a href='#torch_lstsq'><p>Lstsq</p></a></li>
<li><a href='#torch_lt'><p>Lt</p></a></li>
<li><a href='#torch_lu'><p>LU</p></a></li>
<li><a href='#torch_lu_solve'><p>Lu_solve</p></a></li>
<li><a href='#torch_lu_unpack'><p>Lu_unpack</p></a></li>
<li><a href='#torch_manual_seed'><p>Sets the seed for generating random numbers.</p></a></li>
<li><a href='#torch_masked_select'><p>Masked_select</p></a></li>
<li><a href='#torch_matmul'><p>Matmul</p></a></li>
<li><a href='#torch_matrix_exp'><p>Matrix_exp</p></a></li>
<li><a href='#torch_matrix_power'><p>Matrix_power</p></a></li>
<li><a href='#torch_matrix_rank'><p>Matrix_rank</p></a></li>
<li><a href='#torch_max'><p>Max</p></a></li>
<li><a href='#torch_maximum'><p>Maximum</p></a></li>
<li><a href='#torch_mean'><p>Mean</p></a></li>
<li><a href='#torch_median'><p>Median</p></a></li>
<li><a href='#torch_memory_format'><p>Memory format</p></a></li>
<li><a href='#torch_meshgrid'><p>Meshgrid</p></a></li>
<li><a href='#torch_min'><p>Min</p></a></li>
<li><a href='#torch_minimum'><p>Minimum</p></a></li>
<li><a href='#torch_mm'><p>Mm</p></a></li>
<li><a href='#torch_mode'><p>Mode</p></a></li>
<li><a href='#torch_movedim'><p>Movedim</p></a></li>
<li><a href='#torch_mul'><p>Mul</p></a></li>
<li><a href='#torch_multinomial'><p>Multinomial</p></a></li>
<li><a href='#torch_multiply'><p>Multiply</p></a></li>
<li><a href='#torch_mv'><p>Mv</p></a></li>
<li><a href='#torch_mvlgamma'><p>Mvlgamma</p></a></li>
<li><a href='#torch_nanquantile'><p>Nanquantile</p></a></li>
<li><a href='#torch_nansum'><p>Nansum</p></a></li>
<li><a href='#torch_narrow'><p>Narrow</p></a></li>
<li><a href='#torch_ne'><p>Ne</p></a></li>
<li><a href='#torch_neg'><p>Neg</p></a></li>
<li><a href='#torch_negative'><p>Negative</p></a></li>
<li><a href='#torch_nextafter'><p>Nextafter</p></a></li>
<li><a href='#torch_nonzero'><p>Nonzero</p></a></li>
<li><a href='#torch_norm'><p>Norm</p></a></li>
<li><a href='#torch_normal'><p>Normal</p></a></li>
<li><a href='#torch_not_equal'><p>Not_equal</p></a></li>
<li><a href='#torch_ones'><p>Ones</p></a></li>
<li><a href='#torch_ones_like'><p>Ones_like</p></a></li>
<li><a href='#torch_orgqr'><p>Orgqr</p></a></li>
<li><a href='#torch_ormqr'><p>Ormqr</p></a></li>
<li><a href='#torch_outer'><p>Outer</p></a></li>
<li><a href='#torch_pdist'><p>Pdist</p></a></li>
<li><a href='#torch_pinverse'><p>Pinverse</p></a></li>
<li><a href='#torch_pixel_shuffle'><p>Pixel_shuffle</p></a></li>
<li><a href='#torch_poisson'><p>Poisson</p></a></li>
<li><a href='#torch_polar'><p>Polar</p></a></li>
<li><a href='#torch_polygamma'><p>Polygamma</p></a></li>
<li><a href='#torch_pow'><p>Pow</p></a></li>
<li><a href='#torch_prod'><p>Prod</p></a></li>
<li><a href='#torch_promote_types'><p>Promote_types</p></a></li>
<li><a href='#torch_qr'><p>Qr</p></a></li>
<li><a href='#torch_qscheme'><p>Creates the corresponding Scheme object</p></a></li>
<li><a href='#torch_quantile'><p>Quantile</p></a></li>
<li><a href='#torch_quantize_per_channel'><p>Quantize_per_channel</p></a></li>
<li><a href='#torch_quantize_per_tensor'><p>Quantize_per_tensor</p></a></li>
<li><a href='#torch_rad2deg'><p>Rad2deg</p></a></li>
<li><a href='#torch_rand'><p>Rand</p></a></li>
<li><a href='#torch_rand_like'><p>Rand_like</p></a></li>
<li><a href='#torch_randint'><p>Randint</p></a></li>
<li><a href='#torch_randint_like'><p>Randint_like</p></a></li>
<li><a href='#torch_randn'><p>Randn</p></a></li>
<li><a href='#torch_randn_like'><p>Randn_like</p></a></li>
<li><a href='#torch_randperm'><p>Randperm</p></a></li>
<li><a href='#torch_range'><p>Range</p></a></li>
<li><a href='#torch_real'><p>Real</p></a></li>
<li><a href='#torch_reciprocal'><p>Reciprocal</p></a></li>
<li><a href='#torch_reduction'><p>Creates the reduction objet</p></a></li>
<li><a href='#torch_relu'><p>Relu</p></a></li>
<li><a href='#torch_relu_'><p>Relu_</p></a></li>
<li><a href='#torch_remainder'><p>Remainder</p></a></li>
<li><a href='#torch_renorm'><p>Renorm</p></a></li>
<li><a href='#torch_repeat_interleave'><p>Repeat_interleave</p></a></li>
<li><a href='#torch_reshape'><p>Reshape</p></a></li>
<li><a href='#torch_result_type'><p>Result_type</p></a></li>
<li><a href='#torch_roll'><p>Roll</p></a></li>
<li><a href='#torch_rot90'><p>Rot90</p></a></li>
<li><a href='#torch_round'><p>Round</p></a></li>
<li><a href='#torch_rrelu_'><p>Rrelu_</p></a></li>
<li><a href='#torch_rsqrt'><p>Rsqrt</p></a></li>
<li><a href='#torch_save'><p>Saves an object to a disk file.</p></a></li>
<li><a href='#torch_scalar_tensor'><p>Scalar tensor</p></a></li>
<li><a href='#torch_searchsorted'><p>Searchsorted</p></a></li>
<li><a href='#torch_selu'><p>Selu</p></a></li>
<li><a href='#torch_selu_'><p>Selu_</p></a></li>
<li><a href='#torch_serialize'><p>Serialize a torch object returning a raw object</p></a></li>
<li><a href='#torch_set_default_dtype'><p>Gets and sets the default floating point dtype.</p></a></li>
<li><a href='#torch_sgn'><p>Sgn</p></a></li>
<li><a href='#torch_sigmoid'><p>Sigmoid</p></a></li>
<li><a href='#torch_sign'><p>Sign</p></a></li>
<li><a href='#torch_signbit'><p>Signbit</p></a></li>
<li><a href='#torch_sin'><p>Sin</p></a></li>
<li><a href='#torch_sinh'><p>Sinh</p></a></li>
<li><a href='#torch_slogdet'><p>Slogdet</p></a></li>
<li><a href='#torch_sort'><p>Sort</p></a></li>
<li><a href='#torch_sparse_coo_tensor'><p>Sparse_coo_tensor</p></a></li>
<li><a href='#torch_split'><p>Split</p></a></li>
<li><a href='#torch_sqrt'><p>Sqrt</p></a></li>
<li><a href='#torch_square'><p>Square</p></a></li>
<li><a href='#torch_squeeze'><p>Squeeze</p></a></li>
<li><a href='#torch_stack'><p>Stack</p></a></li>
<li><a href='#torch_std'><p>Std</p></a></li>
<li><a href='#torch_std_mean'><p>Std_mean</p></a></li>
<li><a href='#torch_stft'><p>Stft</p></a></li>
<li><a href='#torch_sub'><p>Sub</p></a></li>
<li><a href='#torch_subtract'><p>Subtract</p></a></li>
<li><a href='#torch_sum'><p>Sum</p></a></li>
<li><a href='#torch_svd'><p>Svd</p></a></li>
<li><a href='#torch_t'><p>T</p></a></li>
<li><a href='#torch_take'><p>Take</p></a></li>
<li><a href='#torch_tan'><p>Tan</p></a></li>
<li><a href='#torch_tanh'><p>Tanh</p></a></li>
<li><a href='#torch_tensor'><p>Converts R objects to a torch tensor</p></a></li>
<li><a href='#torch_tensor_from_buffer'><p>Creates a tensor from a buffer of memory</p></a></li>
<li><a href='#torch_tensordot'><p>Tensordot</p></a></li>
<li><a href='#torch_threshold_'><p>Threshold_</p></a></li>
<li><a href='#torch_topk'><p>Topk</p></a></li>
<li><a href='#torch_trace'><p>Trace</p></a></li>
<li><a href='#torch_transpose'><p>Transpose</p></a></li>
<li><a href='#torch_trapz'><p>Trapz</p></a></li>
<li><a href='#torch_triangular_solve'><p>Triangular_solve</p></a></li>
<li><a href='#torch_tril'><p>Tril</p></a></li>
<li><a href='#torch_tril_indices'><p>Tril_indices</p></a></li>
<li><a href='#torch_triu'><p>Triu</p></a></li>
<li><a href='#torch_triu_indices'><p>Triu_indices</p></a></li>
<li><a href='#torch_true_divide'><p>TRUE_divide</p></a></li>
<li><a href='#torch_trunc'><p>Trunc</p></a></li>
<li><a href='#torch_unbind'><p>Unbind</p></a></li>
<li><a href='#torch_unique_consecutive'><p>Unique_consecutive</p></a></li>
<li><a href='#torch_unsafe_chunk'><p>Unsafe_chunk</p></a></li>
<li><a href='#torch_unsafe_split'><p>Unsafe_split</p></a></li>
<li><a href='#torch_unsqueeze'><p>Unsqueeze</p></a></li>
<li><a href='#torch_vander'><p>Vander</p></a></li>
<li><a href='#torch_var'><p>Var</p></a></li>
<li><a href='#torch_var_mean'><p>Var_mean</p></a></li>
<li><a href='#torch_vdot'><p>Vdot</p></a></li>
<li><a href='#torch_view_as_complex'><p>View_as_complex</p></a></li>
<li><a href='#torch_view_as_real'><p>View_as_real</p></a></li>
<li><a href='#torch_vstack'><p>Vstack</p></a></li>
<li><a href='#torch_where'><p>Where</p></a></li>
<li><a href='#torch_zeros'><p>Zeros</p></a></li>
<li><a href='#torch_zeros_like'><p>Zeros_like</p></a></li>
<li><a href='#with_detect_anomaly'><p>Context-manager that enable anomaly detection for the autograd engine.</p></a></li>
<li><a href='#with_enable_grad'><p>Enable grad</p></a></li>
<li><a href='#with_no_grad'><p>Temporarily modify gradient recording.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tensors and Neural Networks with 'GPU' Acceleration</td>
</tr>
<tr>
<td>Version:</td>
<td>0.12.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functionality to define and train neural networks similar to
    'PyTorch' by Paszke et al (2019) &lt;<a href="https://doi.org/10.48550/arXiv.1912.01703">doi:10.48550/arXiv.1912.01703</a>&gt; but written entirely in R
    using the 'libtorch' library. Also supports low-level tensor operations and
    'GPU' acceleration.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://torch.mlverse.org/docs">https://torch.mlverse.org/docs</a>, <a href="https://github.com/mlverse/torch">https://github.com/mlverse/torch</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/torch/issues">https://github.com/mlverse/torch/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>LibTorch (https://pytorch.org/); Only x86_64
platforms are currently supported except for ARM system running
macOS.</td>
</tr>
<tr>
<td>Config/build/copy-method:</td>
<td>copy</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, R6, withr, rlang, methods, utils, stats, bit64,
magrittr, tools, coro (&ge; 1.0.2), callr, cli (&ge; 3.0.0), glue,
ellipsis, desc, safetensors (&ge; 0.1.1), jsonlite</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), covr, knitr (&ge; 1.36), rmarkdown,
palmerpenguins, mvtnorm, numDeriv, katex</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Collate:</td>
<td>'R7.R' 'RcppExports.R' 'autocast.R' 'tensor.R' 'autograd.R'
'backends.R' 'call_torch_function.R' 'codegen-utils.R'
'compat-purrr.R' 'compilation_unit.R' 'conditions.R'
'contrib.R' 'creation-ops.R' 'cuda.R' 'device.R'
'dimname_list.R' 'utils.R' 'distributions-constraints.R'
'distributions-utils.R' 'distributions-exp-family.R'
'distributions.R' 'distributions-bernoulli.R'
'distributions-categorical.R' 'distributions-gamma.R'
'distributions-chi2.R' 'distributions-mixture_same_family.R'
'distributions-multivariate_normal.R' 'distributions-normal.R'
'distributions-poisson.R' 'dtype.R' 'gen-method.R'
'gen-namespace-docs.R' 'gen-namespace-examples.R'
'gen-namespace.R' 'generator.R' 'help.R' 'indexing.R'
'install.R' 'ivalue.R' 'jit-compile.R' 'jit-execute.R'
'jit-ops.R' 'lantern_load.R' 'lantern_sync.R' 'layout.R'
'linalg.R' 'memory_format.R' 'utils-data.R' 'nn.R'
'nn-activation.R' 'nn-batchnorm.R' 'nn-conv.R' 'nn-distance.R'
'nn-dropout.R' 'nn-flatten.R' 'nn-init.R' 'nn-linear.R'
'nn-loss.R' 'nn-normalization.R' 'nn-pooling.R' 'nn-rnn.R'
'nn-sparse.R' 'nn-upsampling.R' 'nn-utils-clip-grad.R'
'nn-utils-rnn.R' 'nn-utils-weight-norm.R' 'nn-utils.R'
'nn_adaptive.R' 'nnf-activation.R' 'nnf-batchnorm.R'
'nnf-conv.R' 'nnf-distance.R' 'nnf-dropout.R' 'nnf-embedding.R'
'nnf-fold.R' 'nnf-instancenorm.R' 'nnf-linear.R' 'nnf-loss.R'
'nnf-normalization.R' 'nnf-padding.R' 'nnf-pixelshuffle.R'
'nnf-pooling.R' 'nnf-upsampling.R' 'nnf-vision.R' 'operators.R'
'optim.R' 'optim-adadelta.R' 'optim-adagrad.R' 'optim-adam.R'
'optim-adamw.R' 'optim-asgd.R' 'optim-lbfgs.R'
'optim-lr_scheduler.R' 'optim-rmsprop.R' 'optim-rprop.R'
'optim-sgd.R' 'package.R' 'qscheme.R' 'quantization.R'
'reduction.R' 'save.R' 'scalar.R' 'script_module.R' 'stack.R'
'storage.R' 'threads.R' 'trace.R' 'translate.R' 'type-info.R'
'utils-data-collate.R' 'utils-data-dataloader.R'
'utils-data-enum.R' 'utils-data-fetcher.R'
'utils-data-sampler.R' 'utils-pipe.R' 'variable_list.R'
'with-indices.R' 'wrapers.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-15 12:08:27 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut, cre, cph],
  Javier Luraschi [aut],
  Dmitriy Selivanov [ctb],
  Athos Damiani [ctb],
  Christophe Regouby [ctb],
  Krzysztof Joachimiak [ctb],
  Hamada S. Badr [ctb],
  RStudio [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-15 16:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='as_array'>Converts to array</h2><span id='topic+as_array'></span>

<h3>Description</h3>

<p>Converts to array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_array(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_array_+3A_x">x</code></td>
<td>
<p>object to be converted into an array</p>
</td></tr>
</table>

<hr>
<h2 id='autograd_backward'>Computes the sum of gradients of given tensors w.r.t. graph leaves.</h2><span id='topic+autograd_backward'></span>

<h3>Description</h3>

<p>The graph is differentiated using the chain rule. If any of tensors are
non-scalar (i.e. their data has more than one element) and require gradient,
then the Jacobian-vector product would be computed, in this case the function
additionally requires specifying <code>grad_tensors</code>. It should be a sequence of
matching length, that contains the vector in the Jacobian-vector product,
usually the gradient of the differentiated function w.r.t. corresponding
tensors (None is an acceptable value for all tensors that dont need gradient
tensors).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autograd_backward(
  tensors,
  grad_tensors = NULL,
  retain_graph = create_graph,
  create_graph = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autograd_backward_+3A_tensors">tensors</code></td>
<td>
<p>(list of Tensor)  Tensors of which the derivative will
be computed.</p>
</td></tr>
<tr><td><code id="autograd_backward_+3A_grad_tensors">grad_tensors</code></td>
<td>
<p>(list of (Tensor or <code style="white-space: pre;">&#8288;NULL))  The vector in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. &#8288;</code>NULL<code style="white-space: pre;">&#8288;values can be specified for scalar Tensors or ones that dont require grad. If a&#8288;</code>NULL' value would be acceptable for all
grad_tensors, then this argument is optional.</p>
</td></tr>
<tr><td><code id="autograd_backward_+3A_retain_graph">retain_graph</code></td>
<td>
<p>(bool, optional)  If <code>FALSE</code>, the graph used to compute
the grad will be freed. Note that in nearly all cases setting this option to
<code>TRUE</code> is not needed and often can be worked around in a much more efficient
way. Defaults to the value of <code>create_graph</code>.</p>
</td></tr>
<tr><td><code id="autograd_backward_+3A_create_graph">create_graph</code></td>
<td>
<p>(bool, optional)  If <code>TRUE</code>, graph of the derivative will
be constructed, allowing to compute higher order derivative products.
Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function accumulates gradients in the leaves - you might need to zero
them before calling it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_tensor(1, requires_grad = TRUE)
y &lt;- 2 * x

a &lt;- torch_tensor(1, requires_grad = TRUE)
b &lt;- 3 * a

autograd_backward(list(y, b))
}
</code></pre>

<hr>
<h2 id='autograd_function'>Records operation history and defines formulas for differentiating ops.</h2><span id='topic+autograd_function'></span>

<h3>Description</h3>

<p>Every operation performed on Tensor's creates a new function object, that
performs the computation, and records that it happened. The history is
retained in the form of a DAG of functions, with edges denoting data
dependencies (input &lt;- output). Then, when backward is called, the graph is
processed in the topological ordering, by calling <code>backward()</code> methods of each
Function object, and passing returned gradients on to next Function's.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autograd_function(forward, backward)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autograd_function_+3A_forward">forward</code></td>
<td>
<p>Performs the operation. It must accept a context <code>ctx</code> as the first argument,
followed by any number of arguments (tensors or other types). The context can be
used to store tensors that can be then retrieved during the backward pass.
See <a href="#topic+AutogradContext">AutogradContext</a> for more information about context methods.</p>
</td></tr>
<tr><td><code id="autograd_function_+3A_backward">backward</code></td>
<td>
<p>Defines a formula for differentiating the operation. It must accept
a context <code>ctx</code> as the first argument, followed by as many outputs ad <code>forward()</code>
returned (as a <code>list()</code>). The names of the arguments don't matter and they are passed
in the order in which they were returned by <code>forward()</code>. The function should
return a named list, where each argument is the gradient w.r.t
the given output, and each element in the returned list should be the gradient
w.r.t. the corresponding input. The context can be used to retrieve tensors saved
during the forward pass. It also has an attribute <code>ctx$needs_input_grad</code> as a
named list of booleans representing whether each input needs gradient.
E.g., <code>backward()</code> will have <code>ctx$needs_input_grad$input = TRUE</code> if the <code>input</code>
argument to <code>forward()</code> needs gradient computated w.r.t. the output.
See <a href="#topic+AutogradContext">AutogradContext</a> for more information about context methods.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

exp2 &lt;- autograd_function(
  forward = function(ctx, i) {
    result &lt;- i$exp()
    ctx$save_for_backward(result = result)
    result
  },
  backward = function(ctx, grad_output) {
    list(i = grad_output * ctx$saved_variable$result)
  }
)
}
</code></pre>

<hr>
<h2 id='autograd_grad'>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</h2><span id='topic+autograd_grad'></span>

<h3>Description</h3>

<p><code>grad_outputs</code> should be a list of length matching output containing the vector
in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of
the outputs. If an output doesnt require_grad, then the gradient can be None).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autograd_grad(
  outputs,
  inputs,
  grad_outputs = NULL,
  retain_graph = create_graph,
  create_graph = FALSE,
  allow_unused = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autograd_grad_+3A_outputs">outputs</code></td>
<td>
<p>(sequence of Tensor)  outputs of the differentiated function.</p>
</td></tr>
<tr><td><code id="autograd_grad_+3A_inputs">inputs</code></td>
<td>
<p>(sequence of Tensor)  Inputs w.r.t. which the gradient will be
returned (and not accumulated into .grad).</p>
</td></tr>
<tr><td><code id="autograd_grad_+3A_grad_outputs">grad_outputs</code></td>
<td>
<p>(sequence of Tensor)  The vector in the Jacobian-vector
product. Usually gradients w.r.t. each output. None values can be specified for
scalar Tensors or ones that dont require grad. If a None value would be acceptable
for all <code>grad_tensors</code>, then this argument is optional. Default: None.</p>
</td></tr>
<tr><td><code id="autograd_grad_+3A_retain_graph">retain_graph</code></td>
<td>
<p>(bool, optional)  If <code>FALSE</code>, the graph used to compute the
grad will be freed. Note that in nearly all cases setting this option to <code>TRUE</code> is
not needed and often can be worked around in a much more efficient way.
Defaults to the value of <code>create_graph</code>.</p>
</td></tr>
<tr><td><code id="autograd_grad_+3A_create_graph">create_graph</code></td>
<td>
<p>(bool, optional)  If <code style="white-space: pre;">&#8288;TRUE, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: &#8288;</code>FALSE'.</p>
</td></tr>
<tr><td><code id="autograd_grad_+3A_allow_unused">allow_unused</code></td>
<td>
<p>(bool, optional)  If <code>FALSE</code>, specifying inputs that were
not used when computing outputs (and therefore their grad is always zero) is an
error. Defaults to <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If only_inputs is <code>TRUE</code>, the function will only return a list of gradients w.r.t
the specified inputs. If its <code>FALSE</code>, then gradient w.r.t. all remaining leaves
will still be computed, and will be accumulated into their <code>.grad</code> attribute.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_tensor(0.5, requires_grad = TRUE)
b &lt;- torch_tensor(0.9, requires_grad = TRUE)
x &lt;- torch_tensor(runif(100))
y &lt;- 2 * x + 1
loss &lt;- (y - (w * x + b))^2
loss &lt;- loss$mean()

o &lt;- autograd_grad(loss, list(w, b))
o
}
</code></pre>

<hr>
<h2 id='autograd_set_grad_mode'>Set grad mode</h2><span id='topic+autograd_set_grad_mode'></span>

<h3>Description</h3>

<p>Sets or disables gradient history.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autograd_set_grad_mode(enabled)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autograd_set_grad_mode_+3A_enabled">enabled</code></td>
<td>
<p>bool wether to enable or disable the gradient recording.</p>
</td></tr>
</table>

<hr>
<h2 id='AutogradContext'>Class representing the context.</h2><span id='topic+AutogradContext'></span>

<h3>Description</h3>

<p>Class representing the context.
</p>
<p>Class representing the context.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>ptr</code></dt><dd><p>(Dev related) pointer to the context c++ object.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>needs_input_grad</code></dt><dd><p>boolean listing arguments of <code>forward</code> and whether they require_grad.</p>
</dd>
<dt><code>saved_variables</code></dt><dd><p>list of objects that were saved for backward via <code>save_for_backward</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-torch_autograd_context-new"><code>AutogradContext$new()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_autograd_context-save_for_backward"><code>AutogradContext$save_for_backward()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_autograd_context-mark_non_differentiable"><code>AutogradContext$mark_non_differentiable()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_autograd_context-mark_dirty"><code>AutogradContext$mark_dirty()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_autograd_context-clone"><code>AutogradContext$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-torch_autograd_context-new"></a>



<h4>Method <code>new()</code></h4>

<p>(Dev related) Initializes the context. Not user related.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutogradContext$new(
  ptr,
  env,
  argument_names = NULL,
  argument_needs_grad = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ptr</code></dt><dd><p>pointer to the c++ object</p>
</dd>
<dt><code>env</code></dt><dd><p>environment that encloses both forward and backward</p>
</dd>
<dt><code>argument_names</code></dt><dd><p>names of forward arguments</p>
</dd>
<dt><code>argument_needs_grad</code></dt><dd><p>whether each argument in forward needs grad.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_autograd_context-save_for_backward"></a>



<h4>Method <code>save_for_backward()</code></h4>

<p>Saves given objects for a future call to backward().
</p>
<p>This should be called at most once, and only from inside the <code>forward()</code>
method.
</p>
<p>Later, saved objects can be accessed through the <code>saved_variables</code> attribute.
Before returning them to the user, a check is made to ensure they werent used
in any in-place operation that modified their content.
</p>
<p>Arguments can also be any kind of R object.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutogradContext$save_for_backward(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>any kind of R object that will be saved for the backward pass.
It's common to pass named arguments.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_autograd_context-mark_non_differentiable"></a>



<h4>Method <code>mark_non_differentiable()</code></h4>

<p>Marks outputs as non-differentiable.
</p>
<p>This should be called at most once, only from inside the <code>forward()</code> method,
and all arguments should be outputs.
</p>
<p>This will mark outputs as not requiring gradients, increasing the efficiency
of backward computation. You still need to accept a gradient for each output
in <code>backward()</code>, but its always going to be a zero tensor with the same
shape as the shape of a corresponding output.
</p>
<p>This is used e.g. for indices returned from a max Function.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutogradContext$mark_non_differentiable(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>non-differentiable outputs.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_autograd_context-mark_dirty"></a>



<h4>Method <code>mark_dirty()</code></h4>

<p>Marks given tensors as modified in an in-place operation.
</p>
<p>This should be called at most once, only from inside the <code>forward()</code> method,
and all arguments should be inputs.
</p>
<p>Every tensor thats been modified in-place in a call to <code>forward()</code> should
be given to this function, to ensure correctness of our checks. It doesnt
matter whether the function is called before or after modification.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutogradContext$mark_dirty(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>tensors that are modified in-place.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_autograd_context-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutogradContext$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='backends_cudnn_is_available'>CuDNN is available</h2><span id='topic+backends_cudnn_is_available'></span>

<h3>Description</h3>

<p>CuDNN is available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_cudnn_is_available()
</code></pre>

<hr>
<h2 id='backends_cudnn_version'>CuDNN version</h2><span id='topic+backends_cudnn_version'></span>

<h3>Description</h3>

<p>CuDNN version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_cudnn_version()
</code></pre>

<hr>
<h2 id='backends_mkl_is_available'>MKL is available</h2><span id='topic+backends_mkl_is_available'></span>

<h3>Description</h3>

<p>MKL is available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_mkl_is_available()
</code></pre>


<h3>Value</h3>

<p>Returns whether LibTorch is built with MKL support.
</p>

<hr>
<h2 id='backends_mkldnn_is_available'>MKLDNN is available</h2><span id='topic+backends_mkldnn_is_available'></span>

<h3>Description</h3>

<p>MKLDNN is available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_mkldnn_is_available()
</code></pre>


<h3>Value</h3>

<p>Returns whether LibTorch is built with MKL-DNN support.
</p>

<hr>
<h2 id='backends_mps_is_available'>MPS is available</h2><span id='topic+backends_mps_is_available'></span>

<h3>Description</h3>

<p>MPS is available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_mps_is_available()
</code></pre>


<h3>Value</h3>

<p>Returns whether LibTorch is built with MPS support.
</p>

<hr>
<h2 id='backends_openmp_is_available'>OpenMP is available</h2><span id='topic+backends_openmp_is_available'></span>

<h3>Description</h3>

<p>OpenMP is available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backends_openmp_is_available()
</code></pre>


<h3>Value</h3>

<p>Returns whether LibTorch is built with OpenMP support.
</p>

<hr>
<h2 id='broadcast_all'>Given a list of values (possibly containing numbers), returns a list where each
value is broadcasted based on the following rules:</h2><span id='topic+broadcast_all'></span>

<h3>Description</h3>

<p>Raises value_error: if any of the values is not a <code>numeric</code> instance,
a <code>torch.*Tensor</code> instance, or an instance implementing <strong>torch_function</strong>
TODO: add has_torch_function((v,))
See: https://github.com/pytorch/pytorch/blob/master/torch/distributions/utils.py
</p>


<h3>Usage</h3>

<pre><code class='language-R'>broadcast_all(values)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="broadcast_all_+3A_values">values</code></td>
<td>
<p>List of:
</p>

<ul>
<li> <p><code>torch.*Tensor</code> instances are broadcasted as per <code style="white-space: pre;">&#8288;_broadcasting-semantics&#8288;</code>.
</p>
</li>
<li> <p><code>numeric</code> instances (scalars) are upcast to tensors having
the same size and type as the first tensor passed to <code>values</code>.  If all the
values are scalars, then they are upcasted to scalar Tensors.
values (list of <code>numeric</code>, <code>torch.*Tensor</code> or objects implementing <strong>torch_function</strong>)
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='call_torch_function'>Call a (Potentially Unexported) Torch Function</h2><span id='topic+call_torch_function'></span>

<h3>Description</h3>

<p>This function allows calling a function prefixed with <code>torch_</code>, including unexported
functions which could have potentially valuable uses but which do not yet have
a user-friendly R wrapper function. Therefore, this function should be used with
extreme caution. Make sure you understand what the function expects as input. It
may be helpful to read the <code>torch</code> source code for help with this, as well as
the documentation for the corresponding function in the Pytorch C++ API. Generally
for development and advanced use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>call_torch_function(name, ..., quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="call_torch_function_+3A_name">name</code></td>
<td>
<p>Name of the function to call as a string. Should start with &quot;torch_&quot;</p>
</td></tr>
<tr><td><code id="call_torch_function_+3A_...">...</code></td>
<td>
<p>A list of arguments to pass to the function. Argument splicing with
<code style="white-space: pre;">&#8288;!!!&#8288;</code> is supported.</p>
</td></tr>
<tr><td><code id="call_torch_function_+3A_quiet">quiet</code></td>
<td>
<p>If TRUE, suppress warnings with valuable information about the dangers of
this function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The return value from calling the function <code>name</code> with arguments <code>...</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## many unexported functions do 'backward' calculations (e.g. derivatives)
## These could be used as a part of custom autograd functions for example.
x &lt;- torch_randn(10, requires_grad = TRUE)
y &lt;- torch_tanh(x)
## calculate backwards gradient using standard torch method
y$backward(torch_ones_like(x))
x$grad
## we can get the same result by calling the unexported `torch_tanh_backward()`
## function. The first argument is 1 to setup the Jacobian-vector product.
## see https://pytorch.org/blog/overview-of-pytorch-autograd-engine/ for details.
call_torch_function("torch_tanh_backward", 1, y)
all.equal(call_torch_function("torch_tanh_backward", 1, y, quiet = TRUE), x$grad)

}
</code></pre>

<hr>
<h2 id='Constraint'>Abstract base class for constraints.</h2><span id='topic+Constraint'></span>

<h3>Description</h3>

<p>Abstract base class for constraints.
</p>
<p>Abstract base class for constraints.
</p>


<h3>Details</h3>

<p>A constraint object represents a region over which a variable is valid,
e.g. within which a variable can be optimized.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-torch_Constraint-check"><code>Constraint$check()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Constraint-print"><code>Constraint$print()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Constraint-clone"><code>Constraint$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-torch_Constraint-check"></a>



<h4>Method <code>check()</code></h4>

<p>Returns a byte tensor of <code>sample_shape + batch_shape</code> indicating
whether each event in value satisfies this constraint.
</p>


<h5>Usage</h5>

<div class="r"><pre>Constraint$check(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>each event in value will be checked.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Constraint-print"></a>



<h4>Method <code>print()</code></h4>

<p>Define the print method for constraints,
</p>


<h5>Usage</h5>

<div class="r"><pre>Constraint$print()</pre></div>


<hr>
<a id="method-torch_Constraint-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Constraint$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='contrib_sort_vertices'>Contrib sort vertices</h2><span id='topic+contrib_sort_vertices'></span>

<h3>Description</h3>

<p>Based on the implementation from <a href="https://github.com/lilanxiao/Rotated_IoU">Rotated_IoU</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrib_sort_vertices(vertices, mask, num_valid)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrib_sort_vertices_+3A_vertices">vertices</code></td>
<td>
<p>A Tensor with the vertices.</p>
</td></tr>
<tr><td><code id="contrib_sort_vertices_+3A_mask">mask</code></td>
<td>
<p>A tensors containing the masks.</p>
</td></tr>
<tr><td><code id="contrib_sort_vertices_+3A_num_valid">num_valid</code></td>
<td>
<p>A integer tensors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All tensors should be on a CUDA device so this function can be used.
</p>


<h3>Note</h3>

<p>This function does not make part of the official torch API.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
if (cuda_is_available()) {
  v &lt;- torch_randn(8, 1024, 24, 2)$cuda()
  mean &lt;- torch_mean(v, dim = 2, keepdim = TRUE)
  v &lt;- v - mean
  m &lt;- (torch_rand(8, 1024, 24) &gt; 0.8)$cuda()
  nv &lt;- torch_sum(m$to(dtype = torch_int()), dim = -1)$to(dtype = torch_int())$cuda()
  result &lt;- contrib_sort_vertices(v, m, nv)
}
}
</code></pre>

<hr>
<h2 id='cuda_amp_grad_scaler'>Creates a gradient scaler</h2><span id='topic+cuda_amp_grad_scaler'></span>

<h3>Description</h3>

<p>A gradient scaler instance is used to perform dynamic gradient scaling
to avoid gradient underflow when training with mixed precision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_amp_grad_scaler(
  init_scale = 2^16,
  growth_factor = 2,
  backoff_factor = 0.5,
  growth_interval = 2000,
  enabled = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_amp_grad_scaler_+3A_init_scale">init_scale</code></td>
<td>
<p>a numeric value indicating the initial scale factor.</p>
</td></tr>
<tr><td><code id="cuda_amp_grad_scaler_+3A_growth_factor">growth_factor</code></td>
<td>
<p>a numeric value indicating the growth factor.</p>
</td></tr>
<tr><td><code id="cuda_amp_grad_scaler_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>a numeric value indicating the backoff factor.</p>
</td></tr>
<tr><td><code id="cuda_amp_grad_scaler_+3A_growth_interval">growth_interval</code></td>
<td>
<p>a numeric value indicating the growth interval.</p>
</td></tr>
<tr><td><code id="cuda_amp_grad_scaler_+3A_enabled">enabled</code></td>
<td>
<p>a logical value indicating whether the gradient scaler should be enabled.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A gradient scaler object.
</p>

<hr>
<h2 id='cuda_current_device'>Returns the index of a currently selected device.</h2><span id='topic+cuda_current_device'></span>

<h3>Description</h3>

<p>Returns the index of a currently selected device.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_current_device()
</code></pre>

<hr>
<h2 id='cuda_device_count'>Returns the number of GPUs available.</h2><span id='topic+cuda_device_count'></span>

<h3>Description</h3>

<p>Returns the number of GPUs available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_device_count()
</code></pre>

<hr>
<h2 id='cuda_empty_cache'>Empty cache</h2><span id='topic+cuda_empty_cache'></span>

<h3>Description</h3>

<p>Releases all unoccupied cached memory currently held by the caching allocator
so that those can be used in other GPU application and visible in <code>nvidia-smi</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_empty_cache()
</code></pre>


<h3>Note</h3>

<p><code><a href="#topic+cuda_empty_cache">cuda_empty_cache()</a></code> doesnt increase the amount of GPU memory available
for torch. However, it may help reduce fragmentation of GPU memory in certain
cases. See Memory management article for more details about GPU memory management.
</p>

<hr>
<h2 id='cuda_get_device_capability'>Returns the major and minor CUDA capability of <code>device</code></h2><span id='topic+cuda_get_device_capability'></span>

<h3>Description</h3>

<p>Returns the major and minor CUDA capability of <code>device</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_get_device_capability(device = cuda_current_device())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_get_device_capability_+3A_device">device</code></td>
<td>
<p>Integer value of the CUDA device to return capabilities of.</p>
</td></tr>
</table>

<hr>
<h2 id='cuda_is_available'>Returns a bool indicating if CUDA is currently available.</h2><span id='topic+cuda_is_available'></span>

<h3>Description</h3>

<p>Returns a bool indicating if CUDA is currently available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_is_available()
</code></pre>

<hr>
<h2 id='cuda_memory_stats'>Returns a dictionary of CUDA memory allocator statistics for a given device.</h2><span id='topic+cuda_memory_stats'></span><span id='topic+cuda_memory_summary'></span>

<h3>Description</h3>

<p>The return value of this function is a dictionary of statistics, each of which
is a non-negative integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_memory_stats(device = cuda_current_device())

cuda_memory_summary(device = cuda_current_device())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_memory_stats_+3A_device">device</code></td>
<td>
<p>Integer value of the CUDA device to return capabilities of.</p>
</td></tr>
</table>


<h3>Core statistics</h3>


<ul>
<li><p> &quot;allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: number of allocation requests received by the memory allocator.
</p>
</li>
<li><p> &quot;allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: amount of allocated memory.
</p>
</li>
<li><p> &quot;segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: number of reserved segments from cudaMalloc().
</p>
</li>
<li><p> &quot;reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: amount of reserved memory.
</p>
</li>
<li><p> &quot;active.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: number of active memory blocks.
</p>
</li>
<li><p> &quot;active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: amount of active memory.
</p>
</li>
<li><p> &quot;inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: number of inactive, non-releasable memory blocks.
</p>
</li>
<li><p> &quot;inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;: amount of inactive, non-releasable memory.
</p>
</li></ul>

<p>For these core statistics, values are broken down as follows.
</p>
<p>Pool type:
</p>

<ul>
<li><p> all: combined statistics across all memory pools.
</p>
</li>
<li><p> large_pool: statistics for the large allocation pool (as of October 2019, for size &gt;= 1MB allocations).
</p>
</li>
<li><p> small_pool: statistics for the small allocation pool (as of October 2019, for size &lt; 1MB allocations).
</p>
</li></ul>

<p>Metric type:
</p>

<ul>
<li><p> current: current value of this metric.
</p>
</li>
<li><p> peak: maximum value of this metric.
</p>
</li>
<li><p> allocated: historical total increase in this metric.
</p>
</li>
<li><p> freed: historical total decrease in this metric.
</p>
</li></ul>



<h3>Additional metrics</h3>


<ul>
<li><p> &quot;num_alloc_retries&quot;: number of failed cudaMalloc calls that result in a cache flush and retry.
</p>
</li>
<li><p> &quot;num_ooms&quot;: number of out-of-memory errors thrown.
</p>
</li></ul>


<hr>
<h2 id='cuda_runtime_version'>Returns the CUDA runtime version</h2><span id='topic+cuda_runtime_version'></span>

<h3>Description</h3>

<p>Returns the CUDA runtime version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_runtime_version()
</code></pre>

<hr>
<h2 id='cuda_synchronize'>Waits for all kernels in all streams on a CUDA device to complete.</h2><span id='topic+cuda_synchronize'></span>

<h3>Description</h3>

<p>Waits for all kernels in all streams on a CUDA device to complete.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_synchronize(device = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_synchronize_+3A_device">device</code></td>
<td>
<p>device for which to synchronize. It uses the current device
given by <code><a href="#topic+cuda_current_device">cuda_current_device()</a></code> if no device is specified.</p>
</td></tr>
</table>

<hr>
<h2 id='dataloader'>Data loader. Combines a dataset and a sampler, and provides
single- or multi-process iterators over the dataset.</h2><span id='topic+dataloader'></span>

<h3>Description</h3>

<p>Data loader. Combines a dataset and a sampler, and provides
single- or multi-process iterators over the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataloader(
  dataset,
  batch_size = 1,
  shuffle = FALSE,
  sampler = NULL,
  batch_sampler = NULL,
  num_workers = 0,
  collate_fn = NULL,
  pin_memory = FALSE,
  drop_last = FALSE,
  timeout = -1,
  worker_init_fn = NULL,
  worker_globals = NULL,
  worker_packages = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataloader_+3A_dataset">dataset</code></td>
<td>
<p>(Dataset): dataset from which to load the data.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_batch_size">batch_size</code></td>
<td>
<p>(int, optional): how many samples per batch to load
(default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="dataloader_+3A_shuffle">shuffle</code></td>
<td>
<p>(bool, optional): set to <code>TRUE</code> to have the data reshuffled
at every epoch (default: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="dataloader_+3A_sampler">sampler</code></td>
<td>
<p>(Sampler, optional): defines the strategy to draw samples from
the dataset. If specified, <code>shuffle</code> must be False. Custom samplers can be
created with <code><a href="#topic+sampler">sampler()</a></code>.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_batch_sampler">batch_sampler</code></td>
<td>
<p>(Sampler, optional): like sampler, but returns a batch of
indices at a time. Mutually exclusive with <code>batch_size</code>,
<code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>. Custom samplers can be created with
<code><a href="#topic+sampler">sampler()</a></code>.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_num_workers">num_workers</code></td>
<td>
<p>(int, optional): how many subprocesses to use for data
loading. 0 means that the data will be loaded in the main process.
(default: <code>0</code>)</p>
</td></tr>
<tr><td><code id="dataloader_+3A_collate_fn">collate_fn</code></td>
<td>
<p>(callable, optional): merges a list of samples to form a mini-batch.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_pin_memory">pin_memory</code></td>
<td>
<p>(bool, optional): If <code>TRUE</code>, the data loader will copy tensors
into CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code>collate_fn</code> returns a batch that is a custom type
see the example below.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_drop_last">drop_last</code></td>
<td>
<p>(bool, optional): set to <code>TRUE</code> to drop the last incomplete batch,
if the dataset size is not divisible by the batch size. If <code>FALSE</code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="dataloader_+3A_timeout">timeout</code></td>
<td>
<p>(numeric, optional): if positive, the timeout value for collecting a batch
from workers. -1 means no timeout. (default: <code>-1</code>)</p>
</td></tr>
<tr><td><code id="dataloader_+3A_worker_init_fn">worker_init_fn</code></td>
<td>
<p>(callable, optional): If not <code>NULL</code>, this will be called on each
worker subprocess with the worker id (an int in <code style="white-space: pre;">&#8288;[1, num_workers]&#8288;</code>) as
input, after seeding and before data loading. (default: <code>NULL</code>)</p>
</td></tr>
<tr><td><code id="dataloader_+3A_worker_globals">worker_globals</code></td>
<td>
<p>(list or character vector, optional) only used when
<code>num_workers &gt; 0</code>. If a character vector, then objects with those names are
copied from the global environment to the workers. If a named list, then
this list is copied and attached to the worker global environment. Notice
that the objects are copied only once at the worker initialization.</p>
</td></tr>
<tr><td><code id="dataloader_+3A_worker_packages">worker_packages</code></td>
<td>
<p>(character vector, optional) Only used if <code>num_workers &gt; 0</code>
optional character vector naming packages that should be loaded in
each worker.</p>
</td></tr>
</table>


<h3>Parallel data loading</h3>

<p>When using <code>num_workers &gt; 0</code> data loading will happen in parallel for each
worker. Note that batches are taken in parallel and not observations.
</p>
<p>The worker initialization  process happens in the following order:
</p>

<ul>
<li> <p><code>num_workers</code> R sessions are initialized.
</p>
</li></ul>

<p>Then in each worker we perform the following actions:
</p>

<ul>
<li><p> the <code>torch</code> library is loaded.
</p>
</li>
<li><p> a random seed is set both using <code>set.seed()</code> and using <code>torch_manual_seed</code>.
</p>
</li>
<li><p> packages passed to the <code>worker_packages</code> argument are loaded.
</p>
</li>
<li><p> objects passed trough the <code>worker_globals</code> parameters are copied into the
global environment.
</p>
</li>
<li><p> the <code>worker_init</code> function is ran with an <code>id</code> argument.
</p>
</li>
<li><p> the dataset fetcher is copied to the worker.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+dataset">dataset()</a></code>, <code><a href="#topic+sampler">sampler()</a></code>
</p>

<hr>
<h2 id='dataloader_make_iter'>Creates an iterator from a DataLoader</h2><span id='topic+dataloader_make_iter'></span>

<h3>Description</h3>

<p>Creates an iterator from a DataLoader
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataloader_make_iter(dataloader)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataloader_make_iter_+3A_dataloader">dataloader</code></td>
<td>
<p>a dataloader object.</p>
</td></tr>
</table>

<hr>
<h2 id='dataloader_next'>Get the next element of a dataloader iterator</h2><span id='topic+dataloader_next'></span>

<h3>Description</h3>

<p>Get the next element of a dataloader iterator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataloader_next(iter, completed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataloader_next_+3A_iter">iter</code></td>
<td>
<p>a DataLoader iter created with <a href="#topic+dataloader_make_iter">dataloader_make_iter</a>.</p>
</td></tr>
<tr><td><code id="dataloader_next_+3A_completed">completed</code></td>
<td>
<p>the returned value when the iterator is exhausted.</p>
</td></tr>
</table>

<hr>
<h2 id='dataset'>Helper function to create an function that generates R6 instances of class <code>dataset</code></h2><span id='topic+dataset'></span>

<h3>Description</h3>

<p>All datasets that represent a map from keys to data samples should subclass this
class. All subclasses should overwrite the <code>.getitem()</code> method, which supports
fetching a data sample for a given key. Subclasses could also optionally
overwrite <code>.length()</code>, which is expected to return the size of the dataset
(e.g. number of samples) used by many sampler implementations
and the default options of <code><a href="#topic+dataloader">dataloader()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset(
  name = NULL,
  inherit = Dataset,
  ...,
  private = NULL,
  active = NULL,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_+3A_name">name</code></td>
<td>
<p>a name for the dataset. It it's also used as the class
for it.</p>
</td></tr>
<tr><td><code id="dataset_+3A_inherit">inherit</code></td>
<td>
<p>you can optionally inherit from a dataset when creating a
new dataset.</p>
</td></tr>
<tr><td><code id="dataset_+3A_...">...</code></td>
<td>
<p>public methods for the dataset class</p>
</td></tr>
<tr><td><code id="dataset_+3A_private">private</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="dataset_+3A_active">active</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="dataset_+3A_parent_env">parent_env</code></td>
<td>
<p>An environment to use as the parent of newly-created
objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output is a function <code>f</code> with class <code>dataset_generator</code>. Calling <code>f()</code>
creates a new instance of the R6 class <code>dataset</code>. The R6 class is stored in the
enclosing environment of <code>f</code> and can also be accessed through <code>f</code>s attribute
<code>Dataset</code>.
</p>


<h3>Get a batch of observations</h3>

<p>By default datasets are iterated by returning each observation/item individually.
Often it's possible to have an optimized implementation to take a batch
of observations (eg, subsetting a tensor by multiple indexes at once is faster than
subsetting once for each index), in this case you can implement a <code>.getbatch</code> method
that will be used instead of <code>.getitem</code> when getting a batch of observations within
the dataloader. <code>.getbatch</code> must work for batches of size larger or equal to 1 and
care must be taken so it doesn't drop the batch dimension when it's queried with
a length 1 batch index - for instance by using <code>drop=FALSE</code>. <code>.getitem()</code> is expected
to not include the batch dimension as it's added by the datalaoder.
For more on this see the the <code>vignette("loading-data")</code>.
</p>


<h3>Note</h3>

<p><code><a href="#topic+dataloader">dataloader()</a></code>  by default constructs a index
sampler that yields integral indices.  To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.
</p>

<hr>
<h2 id='dataset_subset'>Dataset Subset</h2><span id='topic+dataset_subset'></span>

<h3>Description</h3>

<p>Subset of a dataset at specified indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_subset(dataset, indices)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_subset_+3A_dataset">dataset</code></td>
<td>
<p>(Dataset): The whole Dataset</p>
</td></tr>
<tr><td><code id="dataset_subset_+3A_indices">indices</code></td>
<td>
<p>(sequence): Indices in the whole set selected for subset</p>
</td></tr>
</table>

<hr>
<h2 id='distr_bernoulli'>Creates a Bernoulli distribution parameterized by <code>probs</code>
or <code>logits</code> (but not both).
Samples are binary (0 or 1). They take the value <code>1</code> with probability <code>p</code>
and <code>0</code> with probability <code>1 - p</code>.</h2><span id='topic+distr_bernoulli'></span>

<h3>Description</h3>

<p>Creates a Bernoulli distribution parameterized by <code>probs</code>
or <code>logits</code> (but not both).
Samples are binary (0 or 1). They take the value <code>1</code> with probability <code>p</code>
and <code>0</code> with probability <code>1 - p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_bernoulli(probs = NULL, logits = NULL, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_bernoulli_+3A_probs">probs</code></td>
<td>
<p>(numeric or torch_tensor): the probability of sampling <code>1</code></p>
</td></tr>
<tr><td><code id="distr_bernoulli_+3A_logits">logits</code></td>
<td>
<p>(numeric or torch_tensor): the log-odds of sampling <code>1</code></p>
</td></tr>
<tr><td><code id="distr_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>whether to validate arguments or not.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_chi2">distr_chi2</a>()</code>,
<code><a href="#topic+distr_gamma">distr_gamma</a>()</code>,
<code><a href="#topic+distr_multivariate_normal">distr_multivariate_normal</a>()</code>,
<code><a href="#topic+distr_normal">distr_normal</a>()</code>,
<code><a href="#topic+distr_poisson">distr_poisson</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_bernoulli(0.3)
m$sample() # 30% chance 1; 70% chance 0
}
</code></pre>

<hr>
<h2 id='distr_categorical'>Creates a categorical distribution parameterized by either <code>probs</code> or
<code>logits</code> (but not both).</h2><span id='topic+distr_categorical'></span>

<h3>Description</h3>

<p>Creates a categorical distribution parameterized by either <code>probs</code> or
<code>logits</code> (but not both).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_categorical(probs = NULL, logits = NULL, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_categorical_+3A_probs">probs</code></td>
<td>
<p>(Tensor): event probabilities</p>
</td></tr>
<tr><td><code id="distr_categorical_+3A_logits">logits</code></td>
<td>
<p>(Tensor): event log probabilities (unnormalized)</p>
</td></tr>
<tr><td><code id="distr_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is equivalent to the distribution that <code><a href="#topic+torch_multinomial">torch_multinomial()</a></code>
samples from.
</p>
<p>Samples are integers from <code class="reqn">\{0, \ldots, K-1\}</code> where <code>K</code> is <code>probs$size(-1)</code>.
</p>
<p>If <code>probs</code> is 1-dimensional with length-<code>K</code>, each element is the relative probability
of sampling the class at that index.
</p>
<p>If <code>probs</code> is N-dimensional, the first N-1 dimensions are treated as a batch of
relative probability vectors.
</p>
<p>The <code>probs</code> argument must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1 along the last dimension. attr:<code>probs</code>
will return this normalized value.
The <code>logits</code> argument will be interpreted as unnormalized log probabilities
and can therefore be any real number. It will likewise be normalized so that
the resulting probabilities sum to 1 along the last dimension. attr:<code>logits</code>
will return this normalized value.
</p>
<p>See also: <code><a href="#topic+torch_multinomial">torch_multinomial()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_categorical(torch_tensor(c(0.25, 0.25, 0.25, 0.25)))
m$sample() # equal probability of 1,2,3,4
}
</code></pre>

<hr>
<h2 id='distr_chi2'>Creates a Chi2 distribution parameterized by shape parameter <code>df</code>.
This is exactly equivalent to <code>distr_gamma(alpha=0.5*df, beta=0.5)</code></h2><span id='topic+distr_chi2'></span>

<h3>Description</h3>

<p>Creates a Chi2 distribution parameterized by shape parameter <code>df</code>.
This is exactly equivalent to <code>distr_gamma(alpha=0.5*df, beta=0.5)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_chi2(df, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_chi2_+3A_df">df</code></td>
<td>
<p>(float or torch_tensor): shape parameter of the distribution</p>
</td></tr>
<tr><td><code id="distr_chi2_+3A_validate_args">validate_args</code></td>
<td>
<p>whether to validate arguments or not.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_bernoulli">distr_bernoulli</a>()</code>,
<code><a href="#topic+distr_gamma">distr_gamma</a>()</code>,
<code><a href="#topic+distr_multivariate_normal">distr_multivariate_normal</a>()</code>,
<code><a href="#topic+distr_normal">distr_normal</a>()</code>,
<code><a href="#topic+distr_poisson">distr_poisson</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_chi2(torch_tensor(1.0))
m$sample() # Chi2 distributed with shape df=1
torch_tensor(0.1046)
}
</code></pre>

<hr>
<h2 id='distr_gamma'>Creates a Gamma distribution parameterized by shape <code>concentration</code> and <code>rate</code>.</h2><span id='topic+distr_gamma'></span>

<h3>Description</h3>

<p>Creates a Gamma distribution parameterized by shape <code>concentration</code> and <code>rate</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_gamma(concentration, rate, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_gamma_+3A_concentration">concentration</code></td>
<td>
<p>(float or Tensor): shape parameter of the distribution
(often referred to as alpha)</p>
</td></tr>
<tr><td><code id="distr_gamma_+3A_rate">rate</code></td>
<td>
<p>(float or Tensor): rate = 1 / scale of the distribution
(often referred to as beta)</p>
</td></tr>
<tr><td><code id="distr_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>whether to validate arguments or not.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_bernoulli">distr_bernoulli</a>()</code>,
<code><a href="#topic+distr_chi2">distr_chi2</a>()</code>,
<code><a href="#topic+distr_multivariate_normal">distr_multivariate_normal</a>()</code>,
<code><a href="#topic+distr_normal">distr_normal</a>()</code>,
<code><a href="#topic+distr_poisson">distr_poisson</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_gamma(torch_tensor(1.0), torch_tensor(1.0))
m$sample() # Gamma distributed with concentration=1 and rate=1
}
</code></pre>

<hr>
<h2 id='distr_mixture_same_family'>Mixture of components in the same family</h2><span id='topic+distr_mixture_same_family'></span>

<h3>Description</h3>

<p>The <code>MixtureSameFamily</code> distribution implements a (batch of) mixture
distribution where all component are from different parameterizations of
the same distribution type. It is parameterized by a <code>Categorical</code>
selecting distribution&quot; (over <code>k</code> component) and a component
distribution, i.e., a <code>Distribution</code> with a rightmost batch shape
(equal to <code style="white-space: pre;">&#8288;[k]&#8288;</code>) which indexes each (batch of) component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_mixture_same_family(
  mixture_distribution,
  component_distribution,
  validate_args = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_mixture_same_family_+3A_mixture_distribution">mixture_distribution</code></td>
<td>
<p><code>torch_distributions.Categorical</code>-like
instance. Manages the probability of selecting component.
The number of categories must match the rightmost batch
dimension of the <code>component_distribution</code>. Must have either
scalar <code>batch_shape</code> or <code>batch_shape</code> matching
<code style="white-space: pre;">&#8288;component_distribution.batch_shape[:-1]&#8288;</code></p>
</td></tr>
<tr><td><code id="distr_mixture_same_family_+3A_component_distribution">component_distribution</code></td>
<td>
<p><code>torch_distributions.Distribution</code>-like
instance. Right-most batch dimension indexes component.</p>
</td></tr>
<tr><td><code id="distr_mixture_same_family_+3A_validate_args">validate_args</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# Construct Gaussian Mixture Model in 1D consisting of 5 equally
# weighted normal distributions
mix &lt;- distr_categorical(torch_ones(5))
comp &lt;- distr_normal(torch_randn(5), torch_rand(5))
gmm &lt;- distr_mixture_same_family(mix, comp)
}
</code></pre>

<hr>
<h2 id='distr_multivariate_normal'>Gaussian distribution</h2><span id='topic+distr_multivariate_normal'></span>

<h3>Description</h3>

<p>Creates a multivariate normal (also called Gaussian) distribution
parameterized by a mean vector and a covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_multivariate_normal(
  loc,
  covariance_matrix = NULL,
  precision_matrix = NULL,
  scale_tril = NULL,
  validate_args = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_multivariate_normal_+3A_loc">loc</code></td>
<td>
<p>(Tensor): mean of the distribution</p>
</td></tr>
<tr><td><code id="distr_multivariate_normal_+3A_covariance_matrix">covariance_matrix</code></td>
<td>
<p>(Tensor): positive-definite covariance matrix</p>
</td></tr>
<tr><td><code id="distr_multivariate_normal_+3A_precision_matrix">precision_matrix</code></td>
<td>
<p>(Tensor): positive-definite precision matrix</p>
</td></tr>
<tr><td><code id="distr_multivariate_normal_+3A_scale_tril">scale_tril</code></td>
<td>
<p>(Tensor): lower-triangular factor of covariance, with positive-valued diagonal</p>
</td></tr>
<tr><td><code id="distr_multivariate_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Bool wether to validate the arguments or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multivariate normal distribution can be parameterized either
in terms of a positive definite covariance matrix <code class="reqn">\mathbf{\Sigma}</code>
or a positive definite precision matrix <code class="reqn">\mathbf{\Sigma}^{-1}</code>
or a lower-triangular matrix <code class="reqn">\mathbf{L}</code> with positive-valued
diagonal entries, such that
<code class="reqn">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</code>. This triangular matrix
can be obtained via e.g. Cholesky decomposition of the covariance.
</p>


<h3>Note</h3>

<p>Only one of <code>covariance_matrix</code> or <code>precision_matrix</code> or
<code>scale_tril</code> can be specified.
Using <code>scale_tril</code> will be more efficient: all computations internally
are based on <code>scale_tril</code>. If <code>covariance_matrix</code> or
<code>precision_matrix</code> is passed instead, it is only used to compute
the corresponding lower triangular matrices using a Cholesky decomposition.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_bernoulli">distr_bernoulli</a>()</code>,
<code><a href="#topic+distr_chi2">distr_chi2</a>()</code>,
<code><a href="#topic+distr_gamma">distr_gamma</a>()</code>,
<code><a href="#topic+distr_normal">distr_normal</a>()</code>,
<code><a href="#topic+distr_poisson">distr_poisson</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_multivariate_normal(torch_zeros(2), torch_eye(2))
m$sample() # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
}
</code></pre>

<hr>
<h2 id='distr_normal'>Creates a normal (also called Gaussian) distribution parameterized by
<code>loc</code> and <code>scale</code>.</h2><span id='topic+distr_normal'></span>

<h3>Description</h3>

<p>Creates a normal (also called Gaussian) distribution parameterized by
<code>loc</code> and <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distr_normal(loc, scale, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_normal_+3A_loc">loc</code></td>
<td>
<p>(float or Tensor): mean of the distribution (often referred to as mu)</p>
</td></tr>
<tr><td><code id="distr_normal_+3A_scale">scale</code></td>
<td>
<p>(float or Tensor): standard deviation of the distribution (often referred to as sigma)</p>
</td></tr>
<tr><td><code id="distr_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of <code>torch_Normal</code> class
</p>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_bernoulli">distr_bernoulli</a>()</code>,
<code><a href="#topic+distr_chi2">distr_chi2</a>()</code>,
<code><a href="#topic+distr_gamma">distr_gamma</a>()</code>,
<code><a href="#topic+distr_multivariate_normal">distr_multivariate_normal</a>()</code>,
<code><a href="#topic+distr_poisson">distr_poisson</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_normal(loc = 0, scale = 1)
m$sample() # normally distributed with loc=0 and scale=1
}
</code></pre>

<hr>
<h2 id='distr_poisson'>Creates a Poisson distribution parameterized by <code>rate</code>, the rate parameter.</h2><span id='topic+distr_poisson'></span>

<h3>Description</h3>

<p>Samples are nonnegative integers, with a pmf given by
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{rate}^{k} \frac{e^{-\mbox{rate}}}{k!}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>distr_poisson(rate, validate_args = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distr_poisson_+3A_rate">rate</code></td>
<td>
<p>(numeric, torch_tensor): the rate parameter</p>
</td></tr>
<tr><td><code id="distr_poisson_+3A_validate_args">validate_args</code></td>
<td>
<p>whether to validate arguments or not.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Distribution">Distribution</a> for details on the available methods.
</p>
<p>Other distributions: 
<code><a href="#topic+distr_bernoulli">distr_bernoulli</a>()</code>,
<code><a href="#topic+distr_chi2">distr_chi2</a>()</code>,
<code><a href="#topic+distr_gamma">distr_gamma</a>()</code>,
<code><a href="#topic+distr_multivariate_normal">distr_multivariate_normal</a>()</code>,
<code><a href="#topic+distr_normal">distr_normal</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- distr_poisson(torch_tensor(4))
m$sample()
}
</code></pre>

<hr>
<h2 id='Distribution'>Generic R6 class representing distributions</h2><span id='topic+Distribution'></span>

<h3>Description</h3>

<p>Distribution is the abstract base class for probability distributions.
Note: in Python, adding torch.Size objects works as concatenation
Try for example: torch.Size((2, 1)) + torch.Size((1,))
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>.validate_args</code></dt><dd><p>whether to validate arguments</p>
</dd>
<dt><code>has_rsample</code></dt><dd><p>whether has an rsample</p>
</dd>
<dt><code>has_enumerate_support</code></dt><dd><p>whether has enumerate support</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>batch_shape</code></dt><dd><p>Returns the shape over which parameters are batched.</p>
</dd>
<dt><code>event_shape</code></dt><dd><p>Returns the shape of a single sample (without batching).
Returns a dictionary from argument names to
<code>torch_Constraint</code> objects that
should be satisfied by each argument of this distribution. Args that
are not tensors need not appear in this dict.</p>
</dd>
<dt><code>support</code></dt><dd><p>Returns a <code>torch_Constraint</code> object representing this distribution's
support.</p>
</dd>
<dt><code>mean</code></dt><dd><p>Returns the mean on of the distribution</p>
</dd>
<dt><code>variance</code></dt><dd><p>Returns the variance of the distribution</p>
</dd>
<dt><code>stddev</code></dt><dd><p>Returns the standard deviation of the distribution
TODO: consider different message</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-torch_Distribution-new"><code>Distribution$new()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-expand"><code>Distribution$expand()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-sample"><code>Distribution$sample()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-rsample"><code>Distribution$rsample()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-log_prob"><code>Distribution$log_prob()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-cdf"><code>Distribution$cdf()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-icdf"><code>Distribution$icdf()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-enumerate_support"><code>Distribution$enumerate_support()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-entropy"><code>Distribution$entropy()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-perplexity"><code>Distribution$perplexity()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-.extended_shape"><code>Distribution$.extended_shape()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-.validate_sample"><code>Distribution$.validate_sample()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-print"><code>Distribution$print()</code></a>
</p>
</li>
<li> <p><a href="#method-torch_Distribution-clone"><code>Distribution$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-torch_Distribution-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initializes a distribution class.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$new(batch_shape = NULL, event_shape = NULL, validate_args = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>batch_shape</code></dt><dd><p>the shape over which parameters are batched.</p>
</dd>
<dt><code>event_shape</code></dt><dd><p>the shape of a single sample (without batching).</p>
</dd>
<dt><code>validate_args</code></dt><dd><p>whether to validate the arguments or not. Validation
can be time consuming so you might want to disable it.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-expand"></a>



<h4>Method <code>expand()</code></h4>

<p>Returns a new distribution instance (or populates an existing instance
provided by a derived class) with batch dimensions expanded to batch_shape.
This method calls expand on the distributions parameters. As such, this
does not allocate new memory for the expanded distribution instance.
Additionally, this does not repeat any args checking or parameter
broadcasting in <code>initialize</code>, when an instance is first created.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$expand(batch_shape, .instance = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>batch_shape</code></dt><dd><p>the desired expanded size.</p>
</dd>
<dt><code>.instance</code></dt><dd><p>new instance provided by subclasses that need to
override <code>expand</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-sample"></a>



<h4>Method <code>sample()</code></h4>

<p>Generates a <code>sample_shape</code> shaped sample or <code>sample_shape</code> shaped batch of
samples if the distribution parameters are batched.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$sample(sample_shape = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sample_shape</code></dt><dd><p>the shape you want to sample.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-rsample"></a>



<h4>Method <code>rsample()</code></h4>

<p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$rsample(sample_shape = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sample_shape</code></dt><dd><p>the shape you want to sample.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-log_prob"></a>



<h4>Method <code>log_prob()</code></h4>

<p>Returns the log of the probability density/mass function evaluated at
<code>value</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$log_prob(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>values to evaluate the density on.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-cdf"></a>



<h4>Method <code>cdf()</code></h4>

<p>Returns the cumulative density/mass function evaluated at
<code>value</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$cdf(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>values to evaluate the density on.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-icdf"></a>



<h4>Method <code>icdf()</code></h4>

<p>Returns the inverse cumulative density/mass function evaluated at
<code>value</code>.
</p>
<p>@description
Returns tensor containing all values supported by a discrete
distribution. The result will enumerate over dimension 0, so the shape
of the result will be <code style="white-space: pre;">&#8288;(cardinality,) + batch_shape + event_shape (where &#8288;</code>event_shape = ()<code style="white-space: pre;">&#8288;for univariate distributions). Note that this enumerates over all batched tensors in lock-step&#8288;</code>list(c(0, 0), c(1, 1), ...)<code style="white-space: pre;">&#8288;. With &#8288;</code>expand=FALSE<code style="white-space: pre;">&#8288;, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, &#8288;</code>list(c(0), c(1), ...)'.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$icdf(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>values to evaluate the density on.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-enumerate_support"></a>



<h4>Method <code>enumerate_support()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Distribution$enumerate_support(expand = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>expand</code></dt><dd><p>(bool): whether to expand the support over the
batch dims to match the distribution's <code>batch_shape</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Tensor iterating over dimension 0.
</p>


<hr>
<a id="method-torch_Distribution-entropy"></a>



<h4>Method <code>entropy()</code></h4>

<p>Returns entropy of distribution, batched over batch_shape.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$entropy()</pre></div>



<h5>Returns</h5>

<p>Tensor of shape batch_shape.
</p>


<hr>
<a id="method-torch_Distribution-perplexity"></a>



<h4>Method <code>perplexity()</code></h4>

<p>Returns perplexity of distribution, batched over batch_shape.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$perplexity()</pre></div>



<h5>Returns</h5>

<p>Tensor of shape batch_shape.
</p>


<hr>
<a id="method-torch_Distribution-.extended_shape"></a>



<h4>Method <code>.extended_shape()</code></h4>

<p>Returns the size of the sample returned by the distribution, given
a <code>sample_shape</code>. Note, that the batch and event shapes of a distribution
instance are fixed at the time of construction. If this is empty, the
returned shape is upcast to (1,).
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$.extended_shape(sample_shape = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sample_shape</code></dt><dd><p>(torch_Size): the size of the sample to be drawn.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-.validate_sample"></a>



<h4>Method <code>.validate_sample()</code></h4>

<p>Argument validation for distribution methods such as <code>log_prob</code>,
<code>cdf</code> and <code>icdf</code>. The rightmost dimensions of a value to be
scored via these methods must agree with the distribution's batch
and event shapes.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$.validate_sample(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>(Tensor): the tensor whose log probability is to be
computed by the <code>log_prob</code> method.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-torch_Distribution-print"></a>



<h4>Method <code>print()</code></h4>

<p>Prints the distribution instance.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$print()</pre></div>


<hr>
<a id="method-torch_Distribution-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Distribution$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='enumerate'>Enumerate an iterator</h2><span id='topic+enumerate'></span>

<h3>Description</h3>

<p>Enumerate an iterator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enumerate(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enumerate_+3A_x">x</code></td>
<td>
<p>the generator to enumerate.</p>
</td></tr>
<tr><td><code id="enumerate_+3A_...">...</code></td>
<td>
<p>passed to specific methods.</p>
</td></tr>
</table>

<hr>
<h2 id='enumerate.dataloader'>Enumerate an iterator</h2><span id='topic+enumerate.dataloader'></span>

<h3>Description</h3>

<p>Enumerate an iterator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dataloader'
enumerate(x, max_len = 1e+06, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enumerate.dataloader_+3A_x">x</code></td>
<td>
<p>the generator to enumerate.</p>
</td></tr>
<tr><td><code id="enumerate.dataloader_+3A_max_len">max_len</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="enumerate.dataloader_+3A_...">...</code></td>
<td>
<p>passed to specific methods.</p>
</td></tr>
</table>

<hr>
<h2 id='get_install_libs_url'>Install Torch from files</h2><span id='topic+get_install_libs_url'></span><span id='topic+install_torch_from_file'></span>

<h3>Description</h3>

<p>List the Torch and Lantern libraries URLs to download as local files in order to proceed with  <code>install_torch_from_file()</code>.
</p>
<p>Installs Torch and its dependencies from files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_install_libs_url(version = NA, type = NA)

install_torch_from_file(version = NA, type = NA, libtorch, liblantern, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_install_libs_url_+3A_version">version</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="get_install_libs_url_+3A_type">type</code></td>
<td>
<p>Not used. This function is deprecated.</p>
</td></tr>
<tr><td><code id="get_install_libs_url_+3A_libtorch">libtorch</code></td>
<td>
<p>The installation archive file to use for Torch. Shall be a <code>"file://"</code> URL scheme.</p>
</td></tr>
<tr><td><code id="get_install_libs_url_+3A_liblantern">liblantern</code></td>
<td>
<p>The installation archive file to use for Lantern. Shall be a <code>"file://"</code> URL scheme.</p>
</td></tr>
<tr><td><code id="get_install_libs_url_+3A_...">...</code></td>
<td>
<p>other parameters to be passed to <code>"install_torch()"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>"install_torch()"</code> initiated download is not possible, but installation archive files are
present on local filesystem, <code>"install_torch_from_file()"</code> can be used as a workaround to installation issue.
<code>"libtorch"</code> is the archive containing all torch modules, and <code>"liblantern"</code> is the C interface to libtorch
that is used for the R package. Both are highly dependent, and should be checked through <code>"get_install_libs_url()"</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
# on a linux CPU platform 
get_install_libs_url()
# then after making both files available into /tmp/
Sys.setenv(TORCH_URL="/tmp/libtorch-v1.13.1.zip")
Sys.setenv(LANTERN_URL="/tmp/lantern-0.9.1.9001+cpu+arm64-Darwin.zip")
torch::install_torch()

## End(Not run)
}
</code></pre>

<hr>
<h2 id='install_torch'>Install Torch</h2><span id='topic+install_torch'></span>

<h3>Description</h3>

<p>Installs Torch and its dependencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_torch(reinstall = FALSE, ..., .inform_restart = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install_torch_+3A_reinstall">reinstall</code></td>
<td>
<p>Re-install Torch even if its already installed?</p>
</td></tr>
<tr><td><code id="install_torch_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
<tr><td><code id="install_torch_+3A_.inform_restart">.inform_restart</code></td>
<td>
<p>if <code>TRUE</code> and running in an <code>interactive()</code> session, after
installation it will print a message to inform the user that the session must
be restarted for torch to work correctly.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is mainly controlled by environment variables that can be used
to override the defaults:
</p>

<ul>
<li> <p><code>TORCH_HOME</code>: the installation path. By default dependencies are installed
within the package directory. Eg what's given by <code>system.file(package="torch")</code>.
</p>
</li>
<li> <p><code>TORCH_URL</code>: A URL, path to a ZIP file or a directory containing a LibTorch version.
Files will be installed/copied to the <code>TORCH_HOME</code> directory.
</p>
</li>
<li> <p><code>LANTERN_URL</code>: Same as <code>TORCH_URL</code> but for the Lantern library.
</p>
</li>
<li> <p><code>TORCH_INSTALL_DEBUG</code>: Setting it to 1, shows debug log messages during installation.
</p>
</li>
<li> <p><code>PRECXX11ABI</code>: Setting it to <code>1</code> will will trigger the installation of
a Pre-cxx11 ABI installation of LibTorch. This can be useful in environments with
older versions of GLIBC like CentOS7 and older Debian/Ubuntu versions.
</p>
</li>
<li> <p><code>LANTERN_BASE_URL</code>: The base URL for lantern files. This allows passing a directory
where lantern binaries are located. The filename is then constructed as usual.
</p>
</li>
<li> <p><code>TORCH_COMMIT_SHA</code>: torch repository commit sha to be used when querying lantern
uploads. Set it to <code>'none'</code> to avoid looking for build for that commit and
use the latest build for the branch.
</p>
</li>
<li> <p><code>CUDA</code>: We try to automatically detect the CUDA version installed in your system,
but you might want to manually set it here. You can also disable CUDA installation
by setting it to 'cpu'.
</p>
</li>
<li> <p><code>TORCH_R_VERSION</code>: The R torch version. It's unlikely that you need to change it,
but it can be useful if you don't have the R package installed, but want to
install the dependencies.
</p>
</li></ul>

<p>The <code>TORCH_INSTALL</code> environment
variable can be set to <code>0</code> to prevent auto-installing torch and <code>TORCH_LOAD</code> set to <code>0</code>
to avoid loading dependencies automatically. These environment variables are meant for advanced use
cases and troubleshooting only.
When timeout error occurs during library archive download, or length of downloaded files differ from
reported length, an increase of the <code>timeout</code> value should help.
</p>

<hr>
<h2 id='is_dataloader'>Checks if the object is a dataloader</h2><span id='topic+is_dataloader'></span>

<h3>Description</h3>

<p>Checks if the object is a dataloader
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_dataloader(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_dataloader_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_nn_buffer'>Checks if the object is a nn_buffer</h2><span id='topic+is_nn_buffer'></span>

<h3>Description</h3>

<p>Checks if the object is a nn_buffer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_nn_buffer(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_nn_buffer_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_nn_module'>Checks if the object is an nn_module</h2><span id='topic+is_nn_module'></span>

<h3>Description</h3>

<p>Checks if the object is an nn_module
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_nn_module(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_nn_module_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_nn_parameter'>Checks if an object is a nn_parameter</h2><span id='topic+is_nn_parameter'></span>

<h3>Description</h3>

<p>Checks if an object is a nn_parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_nn_parameter(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_nn_parameter_+3A_x">x</code></td>
<td>
<p>the object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_optimizer'>Checks if the object is a torch optimizer</h2><span id='topic+is_optimizer'></span>

<h3>Description</h3>

<p>Checks if the object is a torch optimizer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_optimizer(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_optimizer_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_torch_device'>Checks if object is a device</h2><span id='topic+is_torch_device'></span>

<h3>Description</h3>

<p>Checks if object is a device
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_torch_device(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_torch_device_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_torch_dtype'>Check if object is a torch data type</h2><span id='topic+is_torch_dtype'></span>

<h3>Description</h3>

<p>Check if object is a torch data type
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_torch_dtype(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_torch_dtype_+3A_x">x</code></td>
<td>
<p>object to check.</p>
</td></tr>
</table>

<hr>
<h2 id='is_torch_layout'>Check if an object is a torch layout.</h2><span id='topic+is_torch_layout'></span>

<h3>Description</h3>

<p>Check if an object is a torch layout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_torch_layout(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_torch_layout_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_torch_memory_format'>Check if an object is a memory format</h2><span id='topic+is_torch_memory_format'></span>

<h3>Description</h3>

<p>Check if an object is a memory format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_torch_memory_format(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_torch_memory_format_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_torch_qscheme'>Checks if an object is a QScheme</h2><span id='topic+is_torch_qscheme'></span>

<h3>Description</h3>

<p>Checks if an object is a QScheme
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_torch_qscheme(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_torch_qscheme_+3A_x">x</code></td>
<td>
<p>object to check</p>
</td></tr>
</table>

<hr>
<h2 id='is_undefined_tensor'>Checks if a tensor is undefined</h2><span id='topic+is_undefined_tensor'></span>

<h3>Description</h3>

<p>Checks if a tensor is undefined
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_undefined_tensor(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_undefined_tensor_+3A_x">x</code></td>
<td>
<p>tensor to check</p>
</td></tr>
</table>

<hr>
<h2 id='iterable_dataset'>Creates an iterable dataset</h2><span id='topic+iterable_dataset'></span>

<h3>Description</h3>

<p>Creates an iterable dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterable_dataset(
  name,
  inherit = IterableDataset,
  ...,
  private = NULL,
  active = NULL,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterable_dataset_+3A_name">name</code></td>
<td>
<p>a name for the dataset. It it's also used as the class
for it.</p>
</td></tr>
<tr><td><code id="iterable_dataset_+3A_inherit">inherit</code></td>
<td>
<p>you can optionally inherit from a dataset when creating a
new dataset.</p>
</td></tr>
<tr><td><code id="iterable_dataset_+3A_...">...</code></td>
<td>
<p>public methods for the dataset class</p>
</td></tr>
<tr><td><code id="iterable_dataset_+3A_private">private</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="iterable_dataset_+3A_active">active</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="iterable_dataset_+3A_parent_env">parent_env</code></td>
<td>
<p>An environment to use as the parent of newly-created
objects.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
ids &lt;- iterable_dataset(
  name = "hello",
  initialize = function(n = 5) {
    self$n &lt;- n
    self$i &lt;- 0
  },
  .iter = function() {
    i &lt;- 0
    function() {
      i &lt;&lt;- i + 1
      if (i &gt; self$n) {
        coro::exhausted()
      } else {
        i
      }
    }
  }
)
coro::collect(ids()$.iter())
}
</code></pre>

<hr>
<h2 id='jit_compile'>Compile TorchScript code into a graph</h2><span id='topic+jit_compile'></span>

<h3>Description</h3>

<p>See the <a href="https://pytorch.org/docs/stable/jit_language_reference.html#language-reference">TorchScript language reference</a> for
documentation on how to write TorchScript code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_compile(source)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_compile_+3A_source">source</code></td>
<td>
<p>valid TorchScript source code.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
comp &lt;- jit_compile("
def fn (x):
  return torch.abs(x)

def foo (x):
  return torch.sum(x)

")

comp$fn(torch_tensor(-1))
comp$foo(torch_randn(10))
}
</code></pre>

<hr>
<h2 id='jit_load'>Loads a <code>script_function</code> or <code>script_module</code> previously saved with <code>jit_save</code></h2><span id='topic+jit_load'></span>

<h3>Description</h3>

<p>Loads a <code>script_function</code> or <code>script_module</code> previously saved with <code>jit_save</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_load(path, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_load_+3A_path">path</code></td>
<td>
<p>a path to a <code>script_function</code> or <code>script_module</code> serialized with
<code><a href="#topic+jit_save">jit_save()</a></code>.</p>
</td></tr>
<tr><td><code id="jit_load_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='jit_ops'>Enable idiomatic access to JIT operators from R.</h2><span id='topic+jit_ops'></span>

<h3>Description</h3>

<p>Call JIT operators directly from R, keeping the familiar argument types and argument order.
Note, however, that:
</p>

<ul>
<li><p> all arguments are required (no defaults)
</p>
</li>
<li><p> axis numbering (as well as position numbers overall) starts from 0
</p>
</li>
<li><p> scalars have to be wrapped in <code>jit_scalar()</code>
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>jit_ops
</code></pre>


<h3>Format</h3>

<p>An object of class <code>torch_ops</code> of length 0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
t1 &lt;- torch::torch_rand(4, 5)
t2 &lt;- torch::torch_ones(5, 4)
# same as torch::torch_matmul(t1, t2)
jit_ops$aten$matmul(t1, t2)

# same as torch_split(torch::torch_arange(0, 3), 2, 1)
jit_ops$aten$split(torch::torch_arange(0, 3), torch::jit_scalar(2L), torch::jit_scalar(0L))

}
</code></pre>

<hr>
<h2 id='jit_save'>Saves a <code>script_function</code> to a path</h2><span id='topic+jit_save'></span>

<h3>Description</h3>

<p>Saves a <code>script_function</code> to a path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_save(obj, path, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_save_+3A_obj">obj</code></td>
<td>
<p>An <code>script_function</code> to save</p>
</td></tr>
<tr><td><code id="jit_save_+3A_path">path</code></td>
<td>
<p>The path to save the serialized function.</p>
</td></tr>
<tr><td><code id="jit_save_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
fn &lt;- function(x) {
  torch_relu(x)
}

input &lt;- torch_tensor(c(-1, 0, 1))
tr_fn &lt;- jit_trace(fn, input)

tmp &lt;- tempfile("tst", fileext = "pt")
jit_save(tr_fn, tmp)
}
</code></pre>

<hr>
<h2 id='jit_save_for_mobile'>Saves a <code>script_function</code> or <code>script_module</code> in bytecode form,
to be loaded on a mobile device</h2><span id='topic+jit_save_for_mobile'></span>

<h3>Description</h3>

<p>Saves a <code>script_function</code> or <code>script_module</code> in bytecode form,
to be loaded on a mobile device
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_save_for_mobile(obj, path, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_save_for_mobile_+3A_obj">obj</code></td>
<td>
<p>An <code>script_function</code> or <code>script_module</code> to save</p>
</td></tr>
<tr><td><code id="jit_save_for_mobile_+3A_path">path</code></td>
<td>
<p>The path to save the serialized function.</p>
</td></tr>
<tr><td><code id="jit_save_for_mobile_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
fn &lt;- function(x) {
  torch_relu(x)
}

input &lt;- torch_tensor(c(-1, 0, 1))
tr_fn &lt;- jit_trace(fn, input)

tmp &lt;- tempfile("tst", fileext = "pt")
jit_save_for_mobile(tr_fn, tmp)
}
</code></pre>

<hr>
<h2 id='jit_scalar'>Adds the 'jit_scalar' class to the input</h2><span id='topic+jit_scalar'></span>

<h3>Description</h3>

<p>Allows disambiguating length 1 vectors from scalars when passing
them to the jit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_scalar(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_scalar_+3A_x">x</code></td>
<td>
<p>a length 1 R vector.</p>
</td></tr>
</table>

<hr>
<h2 id='jit_trace'>Trace a function and return an executable <code>script_function</code>.</h2><span id='topic+jit_trace'></span>

<h3>Description</h3>

<p>Using <code>jit_trace</code>, you can turn an existing R function into a TorchScript
<code>script_function</code>. You must provide example inputs, and we run the function,
recording the operations performed on all the tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_trace(func, ..., strict = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_trace_+3A_func">func</code></td>
<td>
<p>An R function that will be run with <code>example_inputs</code>. func arguments
and return values must be tensors or (possibly nested) lists that contain tensors.
Can also be a <code><a href="#topic+nn_module">nn_module()</a></code>, in such case <code><a href="#topic+jit_trace_module">jit_trace_module()</a></code> is used to trace
that module.</p>
</td></tr>
<tr><td><code id="jit_trace_+3A_...">...</code></td>
<td>
<p>example inputs that will be passed to the function while
tracing. The resulting trace can be run with inputs of different types and
shapes assuming the traced operations support those types and shapes.
<code>example_inputs</code> may also be a single Tensor in which case it is automatically
wrapped in a list. Note that <code>...</code> <strong>can not</strong> be named, and the order is
respected.</p>
</td></tr>
<tr><td><code id="jit_trace_+3A_strict">strict</code></td>
<td>
<p>run the tracer in a strict mode or not (default: <code>TRUE</code>). Only
turn this off when you want the tracer to record your mutable container types
(currently list/dict) and you are sure that the container you are using in
your problem is a constant structure and does not get used as control flow
(<code>if</code>, <code>for</code>) conditions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The resulting recording of a standalone function produces a <code>script_function</code>.
In the future we will also support tracing <code>nn_modules</code>.
</p>


<h3>Value</h3>

<p>An <code>script_function</code> if <code>func</code> is a function and <code>script_module</code> if
<code>func</code> is a <code>nn_module()</code>.
</p>


<h3>Warning</h3>

<p>Tracing only correctly records functions and modules which are not data dependent
(e.g., do not have conditionals on data in tensors) and do not have any untracked
external dependencies (e.g., perform input/output or access global variables).
Tracing only records operations done when the given function is run on the given
tensors. Therefore, the returned <code>script_function</code> will always run the same traced
graph on any input. This has some important implications when your module is
expected to run different sets of operations, depending on the input and/or the
module state. For example,
</p>

<ul>
<li><p> Tracing will not record any control-flow like if-statements or loops. When
this control-flow is constant across your module, this is fine and it often
inlines the control-flow decisions. But sometimes the control-flow is actually
part of the model itself. For instance, a recurrent network is a loop over
the (possibly dynamic) length of an input sequence.
</p>
</li>
<li><p> In the returned <code>script_function</code>, operations that have different behaviors
in training and eval modes will always behave as if it is in the mode it was
in during tracing, no matter which mode the <code>script_function</code> is in.
</p>
</li></ul>

<p>In cases like these, tracing would not be appropriate and scripting is a better
choice. If you trace such models, you may silently get incorrect results on
subsequent invocations of the model. The tracer will try to emit warnings when
doing something that may cause an incorrect trace to be produced.
</p>


<h3>Note</h3>

<p>Scripting is not yet supported in R.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
fn &lt;- function(x) {
  torch_relu(x)
}
input &lt;- torch_tensor(c(-1, 0, 1))
tr_fn &lt;- jit_trace(fn, input)
tr_fn(input)
}
</code></pre>

<hr>
<h2 id='jit_trace_module'>Trace a module</h2><span id='topic+jit_trace_module'></span>

<h3>Description</h3>

<p>Trace a module and return an executable ScriptModule that will be optimized
using just-in-time compilation. When a module is passed to <code><a href="#topic+jit_trace">jit_trace()</a></code>, only
the forward method is run and traced. With <code><a href="#topic+jit_trace_module">jit_trace_module()</a></code>, you can specify
a named list of method names to example inputs to trace (see the inputs)
argument below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_trace_module(mod, ..., strict = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_trace_module_+3A_mod">mod</code></td>
<td>
<p>A torch <code>nn_module()</code>  containing methods whose names are specified
in inputs. The given methods will be compiled as a part of a single ScriptModule.</p>
</td></tr>
<tr><td><code id="jit_trace_module_+3A_...">...</code></td>
<td>
<p>A named list containing sample inputs indexed by method names
in mod. The inputs will be passed to methods whose names correspond to inputs
keys while tracing. <code>list('forward'=example_forward_input, 'method2'=example_method2_input)</code>.</p>
</td></tr>
<tr><td><code id="jit_trace_module_+3A_strict">strict</code></td>
<td>
<p>run the tracer in a strict mode or not (default: <code>TRUE</code>). Only
turn this off when you want the tracer to record your mutable container types
(currently list/dict) and you are sure that the container you are using in
your problem is a constant structure and does not get used as control flow
(<code>if</code>, <code>for</code>) conditions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <a href="#topic+jit_trace">jit_trace</a> for more information on tracing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
linear &lt;- nn_linear(10, 1)
tr_linear &lt;- jit_trace_module(linear, forward = list(torch_randn(10, 10)))

x &lt;- torch_randn(10, 10)
torch_allclose(linear(x), tr_linear(x))
}
</code></pre>

<hr>
<h2 id='jit_tuple'>Adds the 'jit_tuple' class to the input</h2><span id='topic+jit_tuple'></span>

<h3>Description</h3>

<p>Allows specifying that an output or input must be considered a jit
tuple and instead of a list or dictionary when tracing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jit_tuple(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jit_tuple_+3A_x">x</code></td>
<td>
<p>the list object that will be converted to a tuple.</p>
</td></tr>
</table>

<hr>
<h2 id='linalg_cholesky'>Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.</h2><span id='topic+linalg_cholesky'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>Cholesky decomposition</strong> of a complex Hermitian or real symmetric positive-definite matrix
<code class="reqn">A \in \mathbb{K}^{n \times n}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_cholesky(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_cholesky_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of symmetric or Hermitian positive-definite matrices.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">L</code> is a lower triangular matrix and
<code class="reqn">L^{H}</code> is the conjugate transpose when <code class="reqn">L</code> is complex, and the
transpose when <code class="reqn">L</code> is real-valued.
</p>
<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex()</a></code> for a version of this operation that
skips the (slow) error checking by default and instead returns the debug
information. This makes it a faster way to check if a matrix is
positive-definite.
<code><a href="#topic+linalg_eigh">linalg_eigh()</a></code> for a different decomposition of a Hermitian matrix.
The eigenvalue decomposition gives more information about the matrix but it
slower to compute than the Cholesky decomposition.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_eye(10)
linalg_cholesky(a)
}
</code></pre>

<hr>
<h2 id='linalg_cholesky_ex'>Computes the Cholesky decomposition of a complex Hermitian or real
symmetric positive-definite matrix.</h2><span id='topic+linalg_cholesky_ex'></span>

<h3>Description</h3>

<p>This function skips the (slow) error checking and error message construction
of <code><a href="#topic+linalg_cholesky">linalg_cholesky()</a></code>, instead directly returning the LAPACK
error codes as part of a named tuple <code style="white-space: pre;">&#8288;(L, info)&#8288;</code>. This makes this function
a faster way to check if a matrix is positive-definite, and it provides an
opportunity to handle decomposition errors more gracefully or performantly
than <code><a href="#topic+linalg_cholesky">linalg_cholesky()</a></code> does.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
If <code>A</code> is not a Hermitian positive-definite matrix, or if it's a batch of matrices
and one or more of them is not a Hermitian positive-definite matrix,
then <code>info</code> stores a positive integer for the corresponding matrix.
The positive integer indicates the order of the leading minor that is not positive-definite,
and the decomposition could not be completed.
<code>info</code> filled with zeros indicates that the decomposition was successful.
If <code>check_errors=TRUE</code> and <code>info</code> contains positive integers, then a RuntimeError is thrown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_cholesky_ex(A, check_errors = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_cholesky_ex_+3A_a">A</code></td>
<td>
<p>(Tensor): the Hermitian <code style="white-space: pre;">&#8288;n \times n&#8288;</code> matrix or the batch of such matrices of size
<code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is one or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_cholesky_ex_+3A_check_errors">check_errors</code></td>
<td>
<p>(bool, optional): controls whether to check the content of <code>infos</code>. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If <code>A</code> is on a CUDA device, this function may synchronize that device with the CPU.
</p>
<p>This function is &quot;experimental&quot; and it may change in a future PyTorch release.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_cholesky">linalg_cholesky()</a></code> is a NumPy compatible variant that always checks for errors.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(2, 2)
out &lt;- linalg_cholesky_ex(A)
out
}
</code></pre>

<hr>
<h2 id='linalg_cond'>Computes the condition number of a matrix with respect to a matrix norm.</h2><span id='topic+linalg_cond'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>condition number</strong> <code class="reqn">\kappa</code> of a matrix
<code class="reqn">A \in \mathbb{K}^{n \times n}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_cond(A, p = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_cond_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
for <code>p</code> in <code style="white-space: pre;">&#8288;(2, -2)&#8288;</code>, and of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where every matrix
is invertible for <code>p</code> in <code style="white-space: pre;">&#8288;('fro', 'nuc', inf, -inf, 1, -1)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="linalg_cond_+3A_p">p</code></td>
<td>
<p>(int, inf, -inf, 'fro', 'nuc', optional):
the type of the matrix norm to use in the computations (see above). Default: <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>The condition number of <code>A</code> measures the numerical stability of the linear system <code>AX = B</code>
with respect to a matrix norm.
</p>
<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>
<p><code>p</code> defines the matrix norm that is computed. See the table in 'Details' to
find the supported norms.
</p>
<p>For <code>p</code> is one of <code style="white-space: pre;">&#8288;('fro', 'nuc', inf, -inf, 1, -1)&#8288;</code>, this function uses
<code><a href="#topic+linalg_norm">linalg_norm()</a></code> and <code><a href="#topic+linalg_inv">linalg_inv()</a></code>.
</p>
<p>As such, in this case, the matrix (or every matrix in the batch) <code>A</code> has to be square
and invertible.
</p>
<p>For <code>p</code> in <code style="white-space: pre;">&#8288;(2, -2)&#8288;</code>, this function can be computed in terms of the singular values
<code class="reqn">\sigma_1 \geq \ldots \geq \sigma_n</code>
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>In these cases, it is computed using <code><a href="#topic+linalg_svd">linalg_svd()</a></code>. For these norms, the matrix
(or every matrix in the batch) <code>A</code> may have any shape.
</p>

<table>
<tr>
 <td style="text-align: left;">
   <code>p</code> </td><td style="text-align: left;"> matrix norm </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>NULL</code> </td><td style="text-align: left;"> <code>2</code>-norm (largest singular value) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>'fro'</code> </td><td style="text-align: left;"> Frobenius norm </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>'nuc'</code> </td><td style="text-align: left;"> nuclear norm </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>Inf</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=2))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-Inf</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=2))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>1</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=1))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-1</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=1))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>2</code> </td><td style="text-align: left;"> largest singular value </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-2</code> </td><td style="text-align: left;"> smallest singular value </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>A real-valued tensor, even when <code>A</code> is complex.
</p>


<h3>Note</h3>

<p>When inputs are on a CUDA device, this function synchronizes that device with the CPU if
if <code>p</code> is one of <code style="white-space: pre;">&#8288;('fro', 'nuc', inf, -inf, 1, -1)&#8288;</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_tensor(rbind(c(1., 0, -1), c(0, 1, 0), c(1, 0, 1)))
linalg_cond(a)
linalg_cond(a, "fro")
}
</code></pre>

<hr>
<h2 id='linalg_det'>Computes the determinant of a square matrix.</h2><span id='topic+linalg_det'></span>

<h3>Description</h3>

<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_det(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_det_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(3, 3)
linalg_det(a)

a &lt;- torch_randn(3, 3, 3)
linalg_det(a)
}
</code></pre>

<hr>
<h2 id='linalg_eig'>Computes the eigenvalue decomposition of a square matrix if it exists.</h2><span id='topic+linalg_eig'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>eigenvalue decomposition</strong> of a square matrix
<code class="reqn">A \in \mathbb{K}^{n \times n}</code> (if it exists) is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_eig(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_eig_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of diagonalizable matrices.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>This decomposition exists if and only if <code class="reqn">A</code> is <code>diagonalizable</code>_.
This is the case when all its eigenvalues are different.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(eigenvalues, eigenvectors)&#8288;</code> which corresponds to <code class="reqn">\Lambda</code> and <code class="reqn">V</code> above.
<code>eigenvalues</code> and <code>eigenvectors</code> will always be complex-valued, even when <code>A</code> is real. The eigenvectors
will be given by the columns of <code>eigenvectors</code>.
</p>


<h3>Warning</h3>


<ul>
<li><p> This function assumes that <code>A</code> is <code>diagonalizable</code>_ (for example, when all the
eigenvalues are different). If it is not diagonalizable, the returned
eigenvalues will be correct but <code class="reqn">A \neq V \operatorname{diag}(\Lambda)V^{-1}</code>.
</p>
</li>
<li><p> The eigenvectors of a matrix are not unique, nor are they continuous with respect to
<code>A</code>. Due to this lack of uniqueness, different hardware and software may compute
different eigenvectors.
This non-uniqueness is caused by the fact that multiplying an eigenvector by a
non-zero number produces another set of valid eigenvectors of the matrix.
In this implmentation, the returned eigenvectors are normalized to have norm
<code>1</code> and largest real component.
</p>
</li>
<li><p> Gradients computed using <code>V</code> will only be finite when <code>A</code> does not have repeated eigenvalues.
Furthermore, if the distance between any two eigenvalues is close to zero,
the gradient will be numerically unstable, as it depends on the eigenvalues
<code class="reqn">\lambda_i</code> through the computation of
<code class="reqn">\frac{1}{\min_{i \neq j} \lambda_i - \lambda_j}</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The eigenvalues and eigenvectors of a real matrix may be complex.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_eigvals">linalg_eigvals()</a></code> computes only the eigenvalues. Unlike <code><a href="#topic+linalg_eig">linalg_eig()</a></code>, the gradients of
<code><a href="#topic+linalg_eigvals">linalg_eigvals()</a></code> are always numerically stable.
</p>
</li>
<li> <p><code><a href="#topic+linalg_eigh">linalg_eigh()</a></code> for a (faster) function that computes the eigenvalue decomposition
for Hermitian and symmetric matrices.
</p>
</li>
<li> <p><code><a href="#topic+linalg_svd">linalg_svd()</a></code> for a function that computes another type of spectral
decomposition that works on matrices of any shape.
</p>
</li>
<li> <p><code><a href="#topic+linalg_qr">linalg_qr()</a></code> for another (much faster) decomposition that works on matrices of
any shape.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(2, 2)
wv &lt;- linalg_eig(a)
}
</code></pre>

<hr>
<h2 id='linalg_eigh'>Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.</h2><span id='topic+linalg_eigh'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>eigenvalue decomposition</strong> of a complex Hermitian or real symmetric matrix
<code class="reqn">A \in \mathbb{K}^{n \times n}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_eigh(A, UPLO = "L")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_eigh_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of symmetric or Hermitian matrices.</p>
</td></tr>
<tr><td><code id="linalg_eigh_+3A_uplo">UPLO</code></td>
<td>
<p>('L', 'U', optional): controls whether to use the upper or lower triangular part
of <code>A</code> in the computations. Default: <code>'L'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">Q^{H}</code> is the conjugate transpose when <code class="reqn">Q</code> is complex, and the transpose when <code class="reqn">Q</code> is real-valued.
<code class="reqn">Q</code> is orthogonal in the real case and unitary in the complex case.
</p>
<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>
<p><code>A</code> is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:
</p>

<ul>
<li><p> If <code>UPLO</code>\ <code style="white-space: pre;">&#8288;= 'L'&#8288;</code> (default), only the lower triangular part of the matrix is used in the computation.
</p>
</li>
<li><p> If <code>UPLO</code>\ <code style="white-space: pre;">&#8288;= 'U'&#8288;</code>, only the upper triangular part of the matrix is used.
The eigenvalues are returned in ascending order.
</p>
</li></ul>



<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(eigenvalues, eigenvectors)&#8288;</code> which corresponds to <code class="reqn">\Lambda</code> and <code class="reqn">Q</code> above.
<code>eigenvalues</code> will always be real-valued, even when <code>A</code> is complex.
</p>
<p>It will also be ordered in ascending order.
<code>eigenvectors</code> will have the same dtype as <code>A</code> and will contain the eigenvectors as its columns.
</p>


<h3>Warning</h3>


<ul>
<li><p> The eigenvectors of a symmetric matrix are not unique, nor are they continuous with
respect to <code>A</code>. Due to this lack of uniqueness, different hardware and
software may compute different eigenvectors.
This non-uniqueness is caused by the fact that multiplying an eigenvector by
<code>-1</code> in the real case or by <code class="reqn">e^{i \phi}, \phi \in \mathbb{R}</code> in the complex
case produces another set of valid eigenvectors of the matrix.
This non-uniqueness problem is even worse when the matrix has repeated eigenvalues.
In this case, one may multiply the associated eigenvectors spanning
the subspace by a rotation matrix and the resulting eigenvectors will be valid
eigenvectors.
</p>
</li>
<li><p> Gradients computed using the <code>eigenvectors</code> tensor will only be finite when
<code>A</code> has unique eigenvalues.
Furthermore, if the distance between any two eigvalues is close to zero,
the gradient will be numerically unstable, as it depends on the eigenvalues
<code class="reqn">\lambda_i</code> through the computation of
<code class="reqn">\frac{1}{\min_{i \neq j} \lambda_i - \lambda_j}</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The eigenvalues of real symmetric or complex Hermitian matrices are always real.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh()</a></code> computes only the eigenvalues values of a Hermitian matrix.
Unlike <code><a href="#topic+linalg_eigh">linalg_eigh()</a></code>, the gradients of <code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh()</a></code> are always
numerically stable.
</p>
</li>
<li> <p><code><a href="#topic+linalg_cholesky">linalg_cholesky()</a></code> for a different decomposition of a Hermitian matrix.
The Cholesky decomposition gives less information about the matrix but is much faster
to compute than the eigenvalue decomposition.
</p>
</li>
<li> <p><code><a href="#topic+linalg_eig">linalg_eig()</a></code> for a (slower) function that computes the eigenvalue decomposition
of a not necessarily Hermitian square matrix.
</p>
</li>
<li> <p><code><a href="#topic+linalg_svd">linalg_svd()</a></code> for a (slower) function that computes the more general SVD
decomposition of matrices of any shape.
</p>
</li>
<li> <p><code><a href="#topic+linalg_qr">linalg_qr()</a></code> for another (much faster) decomposition that works on general
matrices.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(2, 2)
linalg_eigh(a)
}
</code></pre>

<hr>
<h2 id='linalg_eigvals'>Computes the eigenvalues of a square matrix.</h2><span id='topic+linalg_eigvals'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>eigenvalues</strong> of a square matrix <code class="reqn">A \in \mathbb{K}^{n \times n}</code> are defined
as the roots (counted with multiplicity) of the polynomial <code>p</code> of degree <code>n</code> given by
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_eigvals(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_eigvals_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\mathrm{I}_n</code> is the <code>n</code>-dimensional identity matrix.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Note</h3>

<p>The eigenvalues of a real matrix may be complex, as the roots of a real polynomial may be complex.
The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_eig">linalg_eig()</a></code> computes the full eigenvalue decomposition.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(2, 2)
w &lt;- linalg_eigvals(a)
}
</code></pre>

<hr>
<h2 id='linalg_eigvalsh'>Computes the eigenvalues of a complex Hermitian or real symmetric matrix.</h2><span id='topic+linalg_eigvalsh'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>eigenvalues</strong> of a complex Hermitian or real symmetric  matrix <code class="reqn">A \in \mathbb{K}^{n \times n}</code>
are defined as the roots (counted with multiplicity) of the polynomial <code>p</code> of degree <code>n</code> given by
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_eigvalsh(A, UPLO = "L")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_eigvalsh_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of symmetric or Hermitian matrices.</p>
</td></tr>
<tr><td><code id="linalg_eigvalsh_+3A_uplo">UPLO</code></td>
<td>
<p>('L', 'U', optional): controls whether to use the upper or lower triangular part
of <code>A</code> in the computations. Default: <code>'L'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\mathrm{I}_n</code> is the <code>n</code>-dimensional identity matrix.
</p>
<p>The eigenvalues of a real symmetric or complex Hermitian matrix are always real.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
The eigenvalues are returned in ascending order.
</p>
<p><code>A</code> is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:
</p>

<ul>
<li><p> If <code>UPLO</code>\ <code style="white-space: pre;">&#8288;= 'L'&#8288;</code> (default), only the lower triangular part of the matrix is used in the computation.
</p>
</li>
<li><p> If <code>UPLO</code>\ <code style="white-space: pre;">&#8288;= 'U'&#8288;</code>, only the upper triangular part of the matrix is used.
</p>
</li></ul>



<h3>Value</h3>

<p>A real-valued tensor cointaining the eigenvalues even when <code>A</code> is complex.
The eigenvalues are returned in ascending order.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_eigh">linalg_eigh()</a></code> computes the full eigenvalue decomposition.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(2, 2)
linalg_eigvalsh(a)
}
</code></pre>

<hr>
<h2 id='linalg_householder_product'>Computes the first <code>n</code> columns of a product of Householder matrices.</h2><span id='topic+linalg_householder_product'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
for a matrix <code class="reqn">V \in \mathbb{K}^{m \times n}</code> with columns <code class="reqn">v_i \in \mathbb{K}^m</code>
with <code class="reqn">m \geq n</code> and a vector <code class="reqn">\tau \in \mathbb{K}^k</code> with <code class="reqn">k \leq n</code>,
this function computes the first <code class="reqn">n</code> columns of the matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_householder_product(A, tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_householder_product_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_householder_product_+3A_tau">tau</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, k)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\mathrm{I}_m</code> is the <code>m</code>-dimensional identity matrix and
<code class="reqn">v^{H}</code> is the conjugate transpose when <code class="reqn">v</code> is complex, and the transpose when <code class="reqn">v</code> is real-valued.
See <a href="https://netlib.org/lapack/lug/node128.html">Representation of Orthogonal or Unitary Matrices</a> for
further details.
</p>
<p>Supports inputs of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if the inputs are batches of matrices then
the output has the same batch dimensions.
</p>


<h3>Note</h3>

<p>This function only uses the values strictly below the main diagonal of <code>A</code>.
The other values are ignored.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+torch_geqrf">torch_geqrf()</a></code> can be used together with this function to form the <code>Q</code> from the
<code><a href="#topic+linalg_qr">linalg_qr()</a></code> decomposition.
</p>
</li>
<li> <p><code><a href="#topic+torch_ormqr">torch_ormqr()</a></code> is a related function that computes the matrix multiplication
of a product of Householder matrices with another matrix.
However, that function is not supported by autograd.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(2, 2)
h_tau &lt;- torch_geqrf(A)
Q &lt;- linalg_householder_product(h_tau[[1]], h_tau[[2]])
torch_allclose(Q, linalg_qr(A)[[1]])
}
</code></pre>

<hr>
<h2 id='linalg_inv'>Computes the inverse of a square matrix if it exists.</h2><span id='topic+linalg_inv'></span>

<h3>Description</h3>

<p>Throws a <code>runtime_error</code> if the matrix is not invertible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_inv(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_inv_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of invertible matrices.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
for a matrix <code class="reqn">A \in \mathbb{K}^{n \times n}</code>,
its <strong>inverse matrix</strong> <code class="reqn">A^{-1} \in \mathbb{K}^{n \times n}</code> (if it exists) is defined as
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\mathrm{I}_n</code> is the <code>n</code>-dimensional identity matrix.
</p>
<p>The inverse matrix exists if and only if <code class="reqn">A</code> is invertible. In this case,
the inverse is unique.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices
then the output has the same batch dimensions.
</p>
<p>Consider using <code><a href="#topic+linalg_solve">linalg_solve()</a></code> if possible for multiplying a matrix on the left by
the inverse, as <code>linalg_solve(A, B) == A$inv() %*% B</code>
It is always prefered to use <code><a href="#topic+linalg_solve">linalg_solve()</a></code> when possible, as it is faster and more
numerically stable than computing the inverse explicitly.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_pinv">linalg_pinv()</a></code> computes the pseudoinverse (Moore-Penrose inverse) of matrices
of any shape.
<code><a href="#topic+linalg_solve">linalg_solve()</a></code> computes <code>A$inv() %*% B</code> with a
numerically stable algorithm.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(4, 4)
linalg_inv(A)
}
</code></pre>

<hr>
<h2 id='linalg_inv_ex'>Computes the inverse of a square matrix if it is invertible.</h2><span id='topic+linalg_inv_ex'></span>

<h3>Description</h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(inverse, info)&#8288;</code>. <code>inverse</code> contains the result of
inverting <code>A</code> and <code>info</code> stores the LAPACK error codes.
If <code>A</code> is not an invertible matrix, or if it's a batch of matrices
and one or more of them is not an invertible matrix,
then <code>info</code> stores a positive integer for the corresponding matrix.
The positive integer indicates the diagonal element of the LU decomposition of
the input matrix that is exactly zero.
<code>info</code> filled with zeros indicates that the inversion was successful.
If <code>check_errors=TRUE</code> and <code>info</code> contains positive integers, then a RuntimeError is thrown.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_inv_ex(A, check_errors = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_inv_ex_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions
consisting of square matrices.</p>
</td></tr>
<tr><td><code id="linalg_inv_ex_+3A_check_errors">check_errors</code></td>
<td>
<p>(bool, optional): controls whether to check the content of <code>info</code>. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If <code>A</code> is on a CUDA device then this function may synchronize
that device with the CPU.
</p>
<p>This function is &quot;experimental&quot; and it may change in a future PyTorch release.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_inv">linalg_inv()</a></code> is a NumPy compatible variant that always checks for errors.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(3, 3)
out &lt;- linalg_inv_ex(A)
}
</code></pre>

<hr>
<h2 id='linalg_lstsq'>Computes a solution to the least squares problem of a system of linear equations.</h2><span id='topic+linalg_lstsq'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>least squares problem</strong> for a linear system <code class="reqn">AX = B</code> with
<code class="reqn">A \in \mathbb{K}^{m \times n}, B \in \mathbb{K}^{m \times k}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_lstsq(A, B, rcond = NULL, ..., driver = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_lstsq_+3A_a">A</code></td>
<td>
<p>(Tensor): lhs tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_lstsq_+3A_b">B</code></td>
<td>
<p>(Tensor): rhs tensor of shape <code style="white-space: pre;">&#8288;(*, m, k)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_lstsq_+3A_rcond">rcond</code></td>
<td>
<p>(float, optional): used to determine the effective rank of <code>A</code>.
If <code>rcond = NULL</code>, <code>rcond</code> is set to the machine
precision of the dtype of <code>A</code> times <code>max(m, n)</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="linalg_lstsq_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
<tr><td><code id="linalg_lstsq_+3A_driver">driver</code></td>
<td>
<p>(str, optional): name of the LAPACK/MAGMA method to be used.
If <code>NULL</code>, <code>'gelsy'</code> is used for CPU inputs and <code>'gels'</code> for CUDA inputs.
Default: <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\|-\|_F</code> denotes the Frobenius norm.
Supports inputs of float, double, cfloat and cdouble dtypes.
</p>
<p>Also supports batches of matrices, and if the inputs are batches of matrices then
the output has the same batch dimensions.
<code>driver</code> chooses the LAPACK/MAGMA function that will be used.
</p>
<p>For CPU inputs the valid values are <code>'gels'</code>, <code>'gelsy'</code>, <code style="white-space: pre;">&#8288;'gelsd&#8288;</code>, <code>'gelss'</code>.
For CUDA input, the only valid driver is <code>'gels'</code>, which assumes that <code>A</code> is full-rank.
</p>
<p>To choose the best driver on CPU consider:
</p>

<ul>
<li><p> If <code>A</code> is well-conditioned (its <a href="https://pytorch.org/docs/master/linalg.html#torch.linalg.cond">condition number</a> is not too large), or you do not mind some precision loss.
</p>
</li>
<li><p> For a general matrix: <code>'gelsy'</code> (QR with pivoting) (default)
</p>
</li>
<li><p> If <code>A</code> is full-rank: <code>'gels'</code> (QR)
</p>
</li>
<li><p> If <code>A</code> is not well-conditioned.
</p>
</li>
<li> <p><code>'gelsd'</code> (tridiagonal reduction and SVD)
</p>
</li>
<li><p> But if you run into memory issues: <code>'gelss'</code> (full SVD).
</p>
</li></ul>

<p>See also the <a href="https://netlib.org/lapack/lug/node27.html">full description of these drivers</a>
</p>
<p><code>rcond</code> is used to determine the effective rank of the matrices in <code>A</code>
when <code>driver</code> is one of (<code>'gelsy'</code>, <code>'gelsd'</code>, <code>'gelss'</code>).
In this case, if <code class="reqn">\sigma_i</code> are the singular values of <code>A</code> in decreasing order,
<code class="reqn">\sigma_i</code> will be rounded down to zero if <code class="reqn">\sigma_i \leq rcond \cdot \sigma_1</code>.
If <code>rcond = NULL</code> (default), <code>rcond</code> is set to the machine precision of the dtype of <code>A</code>.
</p>
<p>This function returns the solution to the problem and some extra information in a list of
four tensors <code style="white-space: pre;">&#8288;(solution, residuals, rank, singular_values)&#8288;</code>. For inputs <code>A</code>, <code>B</code>
of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code>, <code style="white-space: pre;">&#8288;(*, m, k)&#8288;</code> respectively, it cointains
</p>

<ul>
<li> <p><code>solution</code>: the least squares solution. It has shape <code style="white-space: pre;">&#8288;(*, n, k)&#8288;</code>.
</p>
</li>
<li> <p><code>residuals</code>: the squared residuals of the solutions, that is, <code class="reqn">\|AX - B\|_F^2</code>.
It has shape equal to the batch dimensions of <code>A</code>.
It is computed when <code>m &gt; n</code> and every matrix in <code>A</code> is full-rank,
otherwise, it is an empty tensor.
If <code>A</code> is a batch of matrices and any matrix in the batch is not full rank,
then an empty tensor is returned. This behavior may change in a future PyTorch release.
</p>
</li>
<li> <p><code>rank</code>: tensor of ranks of the matrices in <code>A</code>.
It has shape equal to the batch dimensions of <code>A</code>.
It is computed when <code>driver</code> is one of (<code>'gelsy'</code>, <code>'gelsd'</code>, <code>'gelss'</code>),
otherwise it is an empty tensor.
</p>
</li>
<li> <p><code>singular_values</code>: tensor of singular values of the matrices in <code>A</code>.
It has shape <code style="white-space: pre;">&#8288;(*, min(m, n))&#8288;</code>.
It is computed when <code>driver</code> is one of (<code>'gelsd'</code>, <code>'gelss'</code>),
otherwise it is an empty tensor.
</p>
</li></ul>



<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(solution, residuals, rank, singular_values)&#8288;</code>.
</p>


<h3>Warning</h3>

<p>The default value of <code>rcond</code> may change in a future PyTorch release.
It is therefore recommended to use a fixed value to avoid potential
breaking changes.
</p>


<h3>Note</h3>

<p>This function computes <code>X = A$pinverse() %*% B</code> in a faster and
more numerically stable way than performing the computations separately.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_tensor(rbind(c(10, 2, 3), c(3, 10, 5), c(5, 6, 12)))$unsqueeze(1) # shape (1, 3, 3)
B &lt;- torch_stack(list(
  rbind(c(2, 5, 1), c(3, 2, 1), c(5, 1, 9)),
  rbind(c(4, 2, 9), c(2, 0, 3), c(2, 5, 3))
), dim = 1) # shape (2, 3, 3)
X &lt;- linalg_lstsq(A, B)$solution # A is broadcasted to shape (2, 3, 3)
}
</code></pre>

<hr>
<h2 id='linalg_matrix_norm'>Computes a matrix norm.</h2><span id='topic+linalg_matrix_norm'></span>

<h3>Description</h3>

<p>If <code>A</code> is complex valued, it computes the norm of <code>A$abs()</code>
Support input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices: the norm will be computed over the
dimensions specified by the 2-tuple <code>dim</code> and the other dimensions will
be treated as batch dimensions. The output will have the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_matrix_norm(
  A,
  ord = "fro",
  dim = c(-2, -1),
  keepdim = FALSE,
  dtype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_matrix_norm_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor with two or more dimensions. By default its
shape is interpreted as <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more
batch dimensions, but this behavior can be controlled using <code>dim</code>.</p>
</td></tr>
<tr><td><code id="linalg_matrix_norm_+3A_ord">ord</code></td>
<td>
<p>(int, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: <code>'fro'</code></p>
</td></tr>
<tr><td><code id="linalg_matrix_norm_+3A_dim">dim</code></td>
<td>
<p>(int, Tuple<a href="rlang.html#topic+int">int</a>, optional): dimensions over which to compute
the vector or matrix norm. See above for the behavior when <code>dim=NULL</code>.
Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="linalg_matrix_norm_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool, optional): If set to <code>TRUE</code>, the reduced dimensions are retained
in the result as dimensions with size one. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="linalg_matrix_norm_+3A_dtype">dtype</code></td>
<td>
<p>dtype (<code>torch_dtype</code>, optional): If specified, the input tensor is cast to
<code>dtype</code> before performing the operation, and the returned tensor's type
will be <code>dtype</code>. Default: <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ord</code> defines the norm that is computed. The following norms are
supported:</p>

<table>
<tr>
 <td style="text-align: left;">
   <code>ord</code> </td><td style="text-align: left;"> norm for matrices </td><td style="text-align: left;"> norm for vectors </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>NULL</code> (default) </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;"> <code>2</code>-norm (see below) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"fro"</code> </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"nuc"</code> </td><td style="text-align: left;"> nuclear norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>Inf</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>max(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-Inf</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>min(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>0</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(x != 0)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>1</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-1</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>2</code> </td><td style="text-align: left;"> largest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-2</code> </td><td style="text-align: left;"> smallest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   other <code>int</code> or <code>float</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(abs(x)^{ord})^{(1 / ord)}</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_arange(0, 8, dtype = torch_float())$reshape(c(3, 3))
linalg_matrix_norm(a)
linalg_matrix_norm(a, ord = -1)
b &lt;- a$expand(c(2, -1, -1))
linalg_matrix_norm(b)
linalg_matrix_norm(b, dim = c(1, 3))
}
</code></pre>

<hr>
<h2 id='linalg_matrix_power'>Computes the <code>n</code>-th power of a square matrix for an integer <code>n</code>.</h2><span id='topic+linalg_matrix_power'></span>

<h3>Description</h3>

<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_matrix_power(A, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_matrix_power_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, m)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_matrix_power_+3A_n">n</code></td>
<td>
<p>(int): the exponent.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n=0</code>, it returns the identity matrix (or batch) of the same shape
as <code>A</code>. If <code>n</code> is negative, it returns the inverse of each matrix
(if invertible) raised to the power of <code>abs(n)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_solve">linalg_solve()</a></code> computes <code>A$inverse() %*% B</code> with a
numerically stable algorithm.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(3, 3)
linalg_matrix_power(A, 0)
}
</code></pre>

<hr>
<h2 id='linalg_matrix_rank'>Computes the numerical rank of a matrix.</h2><span id='topic+linalg_matrix_rank'></span>

<h3>Description</h3>

<p>The matrix rank is computed as the number of singular values
(or eigenvalues in absolute value when <code>hermitian = TRUE</code>)
that are greater than the specified <code>tol</code> threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_matrix_rank(
  A,
  ...,
  atol = NULL,
  rtol = NULL,
  tol = NULL,
  hermitian = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_matrix_rank_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more
batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_matrix_rank_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="linalg_matrix_rank_+3A_atol">atol</code></td>
<td>
<p>the absolute tolerance value. When <code>NULL</code> its considered to be zero.</p>
</td></tr>
<tr><td><code id="linalg_matrix_rank_+3A_rtol">rtol</code></td>
<td>
<p>the relative tolerance value. See above for the value it takes when <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="linalg_matrix_rank_+3A_tol">tol</code></td>
<td>
<p>(float, Tensor, optional): the tolerance value. See above for
the value it takes when <code>NULL</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="linalg_matrix_rank_+3A_hermitian">hermitian</code></td>
<td>
<p>(bool, optional): indicates whether <code>A</code> is Hermitian if complex
or symmetric if real. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>
<p>If <code>hermitian = TRUE</code>, <code>A</code> is assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations.
</p>
<p>If <code>tol</code> is not specified and <code>A</code> is a matrix of dimensions <code style="white-space: pre;">&#8288;(m, n)&#8288;</code>,
the tolerance is set to be
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\sigma_1</code> is the largest singular value
(or eigenvalue in absolute value when <code>hermitian = TRUE</code>), and
<code class="reqn">\varepsilon</code> is the epsilon value for the dtype of <code>A</code> (see <code><a href="#topic+torch_finfo">torch_finfo()</a></code>).
</p>
<p>If <code>A</code> is a batch of matrices, <code>tol</code> is computed this way for every element of
the batch.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_eye(10)
linalg_matrix_rank(a)
}
</code></pre>

<hr>
<h2 id='linalg_multi_dot'>Efficiently multiplies two or more matrices</h2><span id='topic+linalg_multi_dot'></span>

<h3>Description</h3>

<p>Efficiently multiplies two or more matrices by reordering the multiplications so that
the fewest arithmetic operations are performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_multi_dot(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_multi_dot_+3A_tensors">tensors</code></td>
<td>
<p>(<code>Sequence[Tensor]</code>): two or more tensors to multiply. The first and last
tensors may be 1D or 2D. Every other tensor must be 2D.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supports inputs of <code>float</code>, <code>double</code>, <code>cfloat</code> and <code>cdouble</code> dtypes.
This function does not support batched inputs.
</p>
<p>Every tensor in <code>tensors</code> must be 2D, except for the first and last which
may be 1D. If the first tensor is a 1D vector of shape <code style="white-space: pre;">&#8288;(n,)&#8288;</code> it is treated as a row vector
of shape <code style="white-space: pre;">&#8288;(1, n)&#8288;</code>, similarly if the last tensor is a 1D vector of shape <code style="white-space: pre;">&#8288;(n,)&#8288;</code> it is treated
as a column vector of shape <code style="white-space: pre;">&#8288;(n, 1)&#8288;</code>.
</p>
<p>If the first and last tensors are matrices, the output will be a matrix.
However, if either is a 1D vector, then the output will be a 1D vector.
</p>


<h3>Note</h3>

<p>This function is implemented by chaining <code><a href="#topic+torch_mm">torch_mm()</a></code> calls after
computing the optimal matrix multiplication order.
</p>
<p>The cost of multiplying two matrices with shapes <code style="white-space: pre;">&#8288;(a, b)&#8288;</code> and <code style="white-space: pre;">&#8288;(b, c)&#8288;</code> is
<code>a * b * c</code>. Given matrices <code>A</code>, <code>B</code>, <code>C</code> with shapes <code style="white-space: pre;">&#8288;(10, 100)&#8288;</code>,
<code style="white-space: pre;">&#8288;(100, 5)&#8288;</code>, <code style="white-space: pre;">&#8288;(5, 50)&#8288;</code> respectively, we can calculate the cost of different
multiplication orders as follows:
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>In this case, multiplying <code>A</code> and <code>B</code> first followed by <code>C</code> is 10 times faster.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

linalg_multi_dot(list(torch_tensor(c(1, 2)), torch_tensor(c(2, 3))))
}
</code></pre>

<hr>
<h2 id='linalg_norm'>Computes a vector or matrix norm.</h2><span id='topic+linalg_norm'></span>

<h3>Description</h3>

<p>If <code>A</code> is complex valued, it computes the norm of <code>A$abs()</code>
Supports input of float, double, cfloat and cdouble dtypes.
Whether this function computes a vector or matrix norm is determined as follows:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_norm(A, ord = NULL, dim = NULL, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_norm_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n)&#8288;</code> or <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions</p>
</td></tr>
<tr><td><code id="linalg_norm_+3A_ord">ord</code></td>
<td>
<p>(int, float, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="linalg_norm_+3A_dim">dim</code></td>
<td>
<p>(int, Tuple<a href="rlang.html#topic+int">int</a>, optional): dimensions over which to compute
the vector or matrix norm. See above for the behavior when <code>dim=NULL</code>.
Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="linalg_norm_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool, optional): If set to <code>TRUE</code>, the reduced dimensions are retained
in the result as dimensions with size one. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="linalg_norm_+3A_dtype">dtype</code></td>
<td>
<p>dtype (<code>torch_dtype</code>, optional): If specified, the input tensor is cast to
<code>dtype</code> before performing the operation, and the returned tensor's type
will be <code>dtype</code>. Default: <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If <code>dim</code> is an int, the vector norm will be computed.
</p>
</li>
<li><p> If <code>dim</code> is a 2-tuple, the matrix norm will be computed.
</p>
</li>
<li><p> If <code>dim=NULL</code> and <code>ord=NULL</code>, A will be flattened to 1D and the 2-norm of the resulting vector will be computed.
</p>
</li>
<li><p> If <code>dim=NULL</code> and <code>ord!=NULL</code>, A must be 1D or 2D.
</p>
</li></ul>

<p><code>ord</code> defines the norm that is computed. The following norms are
supported:</p>

<table>
<tr>
 <td style="text-align: left;">
   <code>ord</code> </td><td style="text-align: left;"> norm for matrices </td><td style="text-align: left;"> norm for vectors </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>NULL</code> (default) </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;"> <code>2</code>-norm (see below) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"fro"</code> </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"nuc"</code> </td><td style="text-align: left;"> nuclear norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>Inf</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>max(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-Inf</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>min(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>0</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(x != 0)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>1</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-1</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>2</code> </td><td style="text-align: left;"> largest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-2</code> </td><td style="text-align: left;"> smallest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   other <code>int</code> or <code>float</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(abs(x)^{ord})^{(1 / ord)}</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_arange(0, 8, dtype = torch_float()) - 4
a
b &lt;- a$reshape(c(3, 3))
b

linalg_norm(a)
linalg_norm(b)
}
</code></pre>

<hr>
<h2 id='linalg_pinv'>Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.</h2><span id='topic+linalg_pinv'></span>

<h3>Description</h3>

<p>The pseudoinverse may be <code style="white-space: pre;">&#8288;defined algebraically&#8288;</code>_
but it is more computationally convenient to understand it <code style="white-space: pre;">&#8288;through the SVD&#8288;</code>_
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_pinv(A, rcond = NULL, hermitian = FALSE, atol = NULL, rtol = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_pinv_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_pinv_+3A_rcond">rcond</code></td>
<td>
<p>(float or Tensor, optional): the tolerance value to determine when is a singular value zero
If it is a <code>torch_Tensor</code>, its shape must be
broadcastable to that of the singular values of
<code>A</code> as returned by <code><a href="#topic+linalg_svd">linalg_svd()</a></code>.
Alias for <code>rtol</code>.
Default: <code>0</code>.</p>
</td></tr>
<tr><td><code id="linalg_pinv_+3A_hermitian">hermitian</code></td>
<td>
<p>(bool, optional): indicates whether <code>A</code> is Hermitian if complex
or symmetric if real. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="linalg_pinv_+3A_atol">atol</code></td>
<td>
<p>the absolute tolerance value. When <code>NULL</code> its considered to be zero.</p>
</td></tr>
<tr><td><code id="linalg_pinv_+3A_rtol">rtol</code></td>
<td>
<p>the relative tolerance value. See above for the value it takes when <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>hermitian= TRUE</code>, <code>A</code> is assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations.
The singular values (or the norm of the eigenvalues when <code>hermitian= TRUE</code>)
that are below the specified <code>rcond</code> threshold are treated as zero and discarded
in the computation.
</p>


<h3>Note</h3>

<p>This function uses <code><a href="#topic+linalg_svd">linalg_svd()</a></code> if <code>hermitian= FALSE</code> and
<code><a href="#topic+linalg_eigh">linalg_eigh()</a></code> if <code>hermitian= TRUE</code>.
For CUDA inputs, this function synchronizes that device with the CPU.
</p>
<p>Consider using <code><a href="#topic+linalg_lstsq">linalg_lstsq()</a></code> if possible for multiplying a matrix on the left by
the pseudoinverse, as <code>linalg_lstsq(A, B)$solution == A$pinv() %*% B</code>
</p>
<p>It is always prefered to use <code><a href="#topic+linalg_lstsq">linalg_lstsq()</a></code> when possible, as it is faster and more
numerically stable than computing the pseudoinverse explicitly.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_inv">linalg_inv()</a></code> computes the inverse of a square matrix.
</p>
</li>
<li> <p><code><a href="#topic+linalg_lstsq">linalg_lstsq()</a></code> computes <code>A$pinv() %*% B</code> with a
numerically stable algorithm.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(3, 5)
linalg_pinv(A)
}
</code></pre>

<hr>
<h2 id='linalg_qr'>Computes the QR decomposition of a matrix.</h2><span id='topic+linalg_qr'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>full QR decomposition</strong> of a matrix
<code class="reqn">A \in \mathbb{K}^{m \times n}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_qr(A, mode = "reduced")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_qr_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_qr_+3A_mode">mode</code></td>
<td>
<p>(str, optional): one of <code>'reduced'</code>, <code>'complete'</code>, <code>'r'</code>.
Controls the shape of the returned tensors. Default: <code>'reduced'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">Q</code> is orthogonal in the real case and unitary in the complex case, and <code class="reqn">R</code> is upper triangular.
When <code>m &gt; n</code> (tall matrix), as <code>R</code> is upper triangular, its last <code>m - n</code> rows are zero.
In this case, we can drop the last <code>m - n</code> columns of <code>Q</code> to form the
<strong>reduced QR decomposition</strong>:
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>The reduced QR decomposition agrees with the full QR decomposition when <code>n &gt;= m</code> (wide matrix).
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
The parameter <code>mode</code> chooses between the full and reduced QR decomposition.
</p>
<p>If <code>A</code> has shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code>, denoting <code>k = min(m, n)</code>
</p>

<ul>
<li> <p><code>mode = 'reduced'</code> (default): Returns <code style="white-space: pre;">&#8288;(Q, R)&#8288;</code> of shapes <code style="white-space: pre;">&#8288;(*, m, k)&#8288;</code>, <code style="white-space: pre;">&#8288;(*, k, n)&#8288;</code> respectively.
</p>
</li>
<li> <p><code>mode = 'complete'</code>: Returns <code style="white-space: pre;">&#8288;(Q, R)&#8288;</code> of shapes <code style="white-space: pre;">&#8288;(*, m, m)&#8288;</code>, <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> respectively.
</p>
</li>
<li> <p><code>mode = 'r'</code>: Computes only the reduced <code>R</code>. Returns <code style="white-space: pre;">&#8288;(Q, R)&#8288;</code> with <code>Q</code> empty and <code>R</code> of shape <code style="white-space: pre;">&#8288;(*, k, n)&#8288;</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(Q, R)&#8288;</code>.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_tensor(rbind(c(12., -51, 4), c(6, 167, -68), c(-4, 24, -41)))
qr &lt;- linalg_qr(a)

torch_mm(qr[[1]], qr[[2]])$round()
torch_mm(qr[[1]]$t(), qr[[1]])$round()
}
</code></pre>

<hr>
<h2 id='linalg_slogdet'>Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.</h2><span id='topic+linalg_slogdet'></span>

<h3>Description</h3>

<p>For complex <code>A</code>, it returns the angle and the natural logarithm of the modulus of the
determinant, that is, a logarithmic polar decomposition of the determinant.
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_slogdet(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_slogdet_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(sign, logabsdet)&#8288;</code>.
<code>logabsdet</code> will always be real-valued, even when <code>A</code> is complex.
<code>sign</code> will have the same dtype as <code>A</code>.
</p>


<h3>Notes</h3>


<ul>
<li><p> The determinant can be recovered as <code>sign * exp(logabsdet)</code>.
</p>
</li>
<li><p> When a matrix has a determinant of zero, it returns <code style="white-space: pre;">&#8288;(0, -Inf)&#8288;</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_randn(3, 3)
linalg_slogdet(a)
}
</code></pre>

<hr>
<h2 id='linalg_solve'>Computes the solution of a square system of linear equations with a unique solution.</h2><span id='topic+linalg_solve'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
this function computes the solution <code class="reqn">X \in \mathbb{K}^{n \times k}</code> of the <strong>linear system</strong> associated to
<code class="reqn">A \in \mathbb{K}^{n \times n}, B \in \mathbb{K}^{m \times k}</code>, which is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_solve(A, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_solve_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_solve_+3A_b">B</code></td>
<td>
<p>(Tensor): right-hand side tensor of shape <code style="white-space: pre;">&#8288;(*, n)&#8288;</code> or  <code style="white-space: pre;">&#8288;(*, n, k)&#8288;</code> or <code style="white-space: pre;">&#8288;(n,)&#8288;</code> or <code style="white-space: pre;">&#8288;(n, k)&#8288;</code>
according to the rules described above</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  AX = B
</code>
</p>

<p>This system of linear equations has one solution if and only if <code class="reqn">A</code> is <code>invertible</code>_.
This function assumes that <code class="reqn">A</code> is invertible.
Supports inputs of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if the inputs are batches of matrices then
the output has the same batch dimensions.
</p>
<p>Letting <code>*</code> be zero or more batch dimensions,
</p>

<ul>
<li><p> If <code>A</code> has shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> and <code>B</code> has shape <code style="white-space: pre;">&#8288;(*, n)&#8288;</code> (a batch of vectors) or shape
<code style="white-space: pre;">&#8288;(*, n, k)&#8288;</code> (a batch of matrices or &quot;multiple right-hand sides&quot;), this function returns <code>X</code> of shape
<code style="white-space: pre;">&#8288;(*, n)&#8288;</code> or <code style="white-space: pre;">&#8288;(*, n, k)&#8288;</code> respectively.
</p>
</li>
<li><p> Otherwise, if <code>A</code> has shape <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> and  <code>B</code> has shape <code style="white-space: pre;">&#8288;(n,)&#8288;</code>  or <code style="white-space: pre;">&#8288;(n, k)&#8288;</code>, <code>B</code>
is broadcasted to have shape <code style="white-space: pre;">&#8288;(*, n)&#8288;</code> or <code style="white-space: pre;">&#8288;(*, n, k)&#8288;</code> respectively.
</p>
</li></ul>

<p>This function then returns the solution of the resulting batch of systems of linear equations.
</p>


<h3>Note</h3>

<p>This function computes <code>X = A$inverse() @ B</code> in a faster and
more numerically stable way than performing the computations separately.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(3, 3)
b &lt;- torch_randn(3)
x &lt;- linalg_solve(A, b)
torch_allclose(torch_matmul(A, x), b)
}
</code></pre>

<hr>
<h2 id='linalg_svd'>Computes the singular value decomposition (SVD) of a matrix.</h2><span id='topic+linalg_svd'></span>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>full SVD</strong> of a matrix
<code class="reqn">A \in \mathbb{K}^{m \times n}</code>, if <code>k = min(m,n)</code>, is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_svd(A, full_matrices = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_svd_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="linalg_svd_+3A_full_matrices">full_matrices</code></td>
<td>
<p>(bool, optional): controls whether to compute the full or reduced
SVD, and consequently, the shape of the returned tensors <code>U</code> and <code>V</code>. Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\operatorname{diag}(S) \in \mathbb{K}^{m \times n}</code>,
<code class="reqn">V^{H}</code> is the conjugate transpose when <code class="reqn">V</code> is complex, and the transpose when <code class="reqn">V</code> is real-valued.
</p>
<p>The matrices  <code class="reqn">U</code>, <code class="reqn">V</code> (and thus <code class="reqn">V^{H}</code>) are orthogonal in the real case, and unitary in the complex case.
When <code>m &gt; n</code> (resp. <code>m &lt; n</code>) we can drop the last <code>m - n</code> (resp. <code>n - m</code>) columns of <code>U</code> (resp. <code>V</code>) to form the <strong>reduced SVD</strong>:
</p>
Equation not displayed. Install 'katex' then re-install 'torch'.
<p>where <code class="reqn">\operatorname{diag}(S) \in \mathbb{K}^{k \times k}</code>.
</p>
<p>In this case, <code class="reqn">U</code> and <code class="reqn">V</code> also have orthonormal columns.
Supports input of float, double, cfloat and cdouble dtypes.
</p>
<p>Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
</p>
<p>The returned decomposition is a named tuple <code style="white-space: pre;">&#8288;(U, S, V)&#8288;</code>
which corresponds to <code class="reqn">U</code>, <code class="reqn">S</code>, <code class="reqn">V^{H}</code> above.
</p>
<p>The singular values are returned in descending order.
The parameter <code>full_matrices</code> chooses between the full (default) and reduced SVD.
</p>


<h3>Value</h3>

<p>A list <code style="white-space: pre;">&#8288;(U, S, V)&#8288;</code> which corresponds to <code class="reqn">U</code>, <code class="reqn">S</code>, <code class="reqn">V^{H}</code> above.
<code>S</code> will always be real-valued, even when <code>A</code> is complex.
It will also be ordered in descending order.
<code>U</code> and <code>V</code> will have the same dtype as <code>A</code>. The left / right singular vectors will be given by
the columns of <code>U</code> and the rows of <code>V</code> respectively.
</p>


<h3>Warnings</h3>

<p>The returned tensors <code>U</code> and <code>V</code> are not unique, nor are they continuous with
respect to <code>A</code>.
Due to this lack of uniqueness, different hardware and software may compute
different singular vectors.
This non-uniqueness is caused by the fact that multiplying any pair of singular
vectors <code class="reqn">u_k, v_k</code> by <code>-1</code> in the real case or by
<code class="reqn">e^{i \phi}, \phi \in \mathbb{R}</code> in the complex case produces another two
valid singular vectors of the matrix.
This non-uniqueness problem is even worse when the matrix has repeated singular values.
In this case, one may multiply the associated singular vectors of <code>U</code> and <code>V</code> spanning
the subspace by a rotation matrix and the resulting vectors will span the same subspace.
</p>
<p>Gradients computed using <code>U</code> or <code>V</code> will only be finite when
<code>A</code> does not have zero as a singular value or repeated singular values.
Furthermore, if the distance between any two singular values is close to zero,
the gradient will be numerically unstable, as it depends on the singular values
<code class="reqn">\sigma_i</code> through the computation of
<code class="reqn">\frac{1}{\min_{i \neq j} \sigma_i^2 - \sigma_j^2}</code>.
The gradient will also be numerically unstable when <code>A</code> has small singular
values, as it also depends on the computaiton of <code class="reqn">\frac{1}{\sigma_i}</code>.
</p>


<h3>Note</h3>

<p>When <code>full_matrices=TRUE</code>, the gradients with respect to <code style="white-space: pre;">&#8288;U[..., :, min(m, n):]&#8288;</code>
and <code style="white-space: pre;">&#8288;Vh[..., min(m, n):, :]&#8288;</code> will be ignored, as those vectors can be arbitrary bases
of the corresponding subspaces.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_svdvals">linalg_svdvals()</a></code> computes only the singular values.
Unlike <code><a href="#topic+linalg_svd">linalg_svd()</a></code>, the gradients of <code><a href="#topic+linalg_svdvals">linalg_svdvals()</a></code> are always
numerically stable.
</p>
</li>
<li> <p><code><a href="#topic+linalg_eig">linalg_eig()</a></code> for a function that computes another type of spectral
decomposition of a matrix. The eigendecomposition works just on on square matrices.
</p>
</li>
<li> <p><code><a href="#topic+linalg_eigh">linalg_eigh()</a></code> for a (faster) function that computes the eigenvalue decomposition
for Hermitian and symmetric matrices.
</p>
</li>
<li> <p><code><a href="#topic+linalg_qr">linalg_qr()</a></code> for another (much faster) decomposition that works on general
matrices.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(5, 3)
linalg_svd(a, full_matrices = FALSE)
}
</code></pre>

<hr>
<h2 id='linalg_svdvals'>Computes the singular values of a matrix.</h2><span id='topic+linalg_svdvals'></span>

<h3>Description</h3>

<p>Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if <code>A</code> is a batch of matrices then
the output has the same batch dimensions.
The singular values are returned in descending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_svdvals(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_svdvals_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor of shape <code style="white-space: pre;">&#8288;(*, m, n)&#8288;</code> where <code>*</code> is zero or more batch dimensions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A real-valued tensor, even when <code>A</code> is complex.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linalg_svd">linalg_svd()</a></code> computes the full singular value decomposition.
</p>
<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_randn(5, 3)
S &lt;- linalg_svdvals(A)
S
}
</code></pre>

<hr>
<h2 id='linalg_tensorinv'>Computes the multiplicative inverse of <code><a href="#topic+torch_tensordot">torch_tensordot()</a></code></h2><span id='topic+linalg_tensorinv'></span>

<h3>Description</h3>

<p>If <code>m</code> is the product of the first <code>ind</code> dimensions of <code>A</code> and <code>n</code> is the product of
the rest of the dimensions, this function expects <code>m</code> and <code>n</code> to be equal.
If this is the case, it computes a tensor <code>X</code> such that
<code>tensordot(A, X, ind)</code> is the identity matrix in dimension <code>m</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_tensorinv(A, ind = 3L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_tensorinv_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor to invert.</p>
</td></tr>
<tr><td><code id="linalg_tensorinv_+3A_ind">ind</code></td>
<td>
<p>(int): index at which to compute the inverse of <code><a href="#topic+torch_tensordot">torch_tensordot()</a></code>. Default: <code>3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supports input of float, double, cfloat and cdouble dtypes.
</p>


<h3>Note</h3>

<p>Consider using <code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve()</a></code> if possible for multiplying a tensor on the left
by the tensor inverse as <code style="white-space: pre;">&#8288;linalg_tensorsolve(A, B) == torch_tensordot(linalg_tensorinv(A), B))&#8288;</code>
</p>
<p>It is always prefered to use <code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve()</a></code> when possible, as it is faster and more
numerically stable than computing the pseudoinverse explicitly.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve()</a></code> computes <code style="white-space: pre;">&#8288;torch_tensordot(linalg_tensorinv(A), B))&#8288;</code>.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_eye(4 * 6)$reshape(c(4, 6, 8, 3))
Ainv &lt;- linalg_tensorinv(A, ind = 3)
Ainv$shape
B &lt;- torch_randn(4, 6)
torch_allclose(torch_tensordot(Ainv, B), linalg_tensorsolve(A, B))

A &lt;- torch_randn(4, 4)
Atensorinv &lt;- linalg_tensorinv(A, 2)
Ainv &lt;- linalg_inv(A)
torch_allclose(Atensorinv, Ainv)
}
</code></pre>

<hr>
<h2 id='linalg_tensorsolve'>Computes the solution <code>X</code> to the system <code>torch_tensordot(A, X) = B</code>.</h2><span id='topic+linalg_tensorsolve'></span>

<h3>Description</h3>

<p>If <code>m</code> is the product of the first <code>B</code>\ <code>.ndim</code>  dimensions of <code>A</code> and
<code>n</code> is the product of the rest of the dimensions, this function expects <code>m</code> and <code>n</code> to be equal.
The returned tensor <code>x</code> satisfies
<code>tensordot(A, x, dims=x$ndim) == B</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_tensorsolve(A, B, dims = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_tensorsolve_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor to solve for.</p>
</td></tr>
<tr><td><code id="linalg_tensorsolve_+3A_b">B</code></td>
<td>
<p>(Tensor): the solution</p>
</td></tr>
<tr><td><code id="linalg_tensorsolve_+3A_dims">dims</code></td>
<td>
<p>(Tuple<a href="rlang.html#topic+int">int</a>, optional): dimensions of <code>A</code> to be moved.
If <code>NULL</code>, no dimensions are moved. Default: <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>dims</code> is specified, <code>A</code> will be reshaped as
<code>A = movedim(A, dims, seq(len(dims) - A$ndim + 1, 0))</code>
</p>
<p>Supports inputs of float, double, cfloat and cdouble dtypes.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+linalg_tensorinv">linalg_tensorinv()</a></code> computes the multiplicative inverse of
<code><a href="#topic+torch_tensordot">torch_tensordot()</a></code>.
</p>
</li></ul>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_vector_norm">linalg_vector_norm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A &lt;- torch_eye(2 * 3 * 4)$reshape(c(2 * 3, 4, 2, 3, 4))
B &lt;- torch_randn(2 * 3, 4)
X &lt;- linalg_tensorsolve(A, B)
X$shape
torch_allclose(torch_tensordot(A, X, dims = X$ndim), B)

A &lt;- torch_randn(6, 4, 4, 3, 2)
B &lt;- torch_randn(4, 3, 2)
X &lt;- linalg_tensorsolve(A, B, dims = c(1, 3))
A &lt;- A$permute(c(2, 4, 5, 1, 3))
torch_allclose(torch_tensordot(A, X, dims = X$ndim), B, atol = 1e-6)
}
</code></pre>

<hr>
<h2 id='linalg_vector_norm'>Computes a vector norm.</h2><span id='topic+linalg_vector_norm'></span>

<h3>Description</h3>

<p>If <code>A</code> is complex valued, it computes the norm of <code>A$abs()</code>
Supports input of float, double, cfloat and cdouble dtypes.
This function does not necessarily treat multidimensonal <code>A</code> as a batch of
vectors, instead:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linalg_vector_norm(A, ord = 2, dim = NULL, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linalg_vector_norm_+3A_a">A</code></td>
<td>
<p>(Tensor): tensor, flattened by default, but this behavior can be
controlled using <code>dim</code>.</p>
</td></tr>
<tr><td><code id="linalg_vector_norm_+3A_ord">ord</code></td>
<td>
<p>(int, float, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: <code>2</code></p>
</td></tr>
<tr><td><code id="linalg_vector_norm_+3A_dim">dim</code></td>
<td>
<p>(int, Tuple<a href="rlang.html#topic+int">int</a>, optional): dimensions over which to compute
the vector or matrix norm. See above for the behavior when <code>dim=NULL</code>.
Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="linalg_vector_norm_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool, optional): If set to <code>TRUE</code>, the reduced dimensions are retained
in the result as dimensions with size one. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="linalg_vector_norm_+3A_dtype">dtype</code></td>
<td>
<p>dtype (<code>torch_dtype</code>, optional): If specified, the input tensor is cast to
<code>dtype</code> before performing the operation, and the returned tensor's type
will be <code>dtype</code>. Default: <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If <code>dim=NULL</code>, <code>A</code> will be flattened before the norm is computed.
</p>
</li>
<li><p> If <code>dim</code> is an <code>int</code> or a <code>tuple</code>, the norm will be computed over these dimensions
and the other dimensions will be treated as batch dimensions.
</p>
</li></ul>

<p>This behavior is for consistency with <code><a href="#topic+linalg_norm">linalg_norm()</a></code>.
</p>
<p><code>ord</code> defines the norm that is computed. The following norms are
supported:</p>

<table>
<tr>
 <td style="text-align: left;">
   <code>ord</code> </td><td style="text-align: left;"> norm for matrices </td><td style="text-align: left;"> norm for vectors </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>NULL</code> (default) </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;"> <code>2</code>-norm (see below) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"fro"</code> </td><td style="text-align: left;"> Frobenius norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"nuc"</code> </td><td style="text-align: left;"> nuclear norm </td><td style="text-align: left;">  not supported  </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>Inf</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>max(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-Inf</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=2))</code> </td><td style="text-align: left;"> <code>min(abs(x))</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>0</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(x != 0)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>1</code> </td><td style="text-align: left;"> <code>max(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-1</code> </td><td style="text-align: left;"> <code>min(sum(abs(x), dim=1))</code> </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>2</code> </td><td style="text-align: left;"> largest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>-2</code> </td><td style="text-align: left;"> smallest singular value </td><td style="text-align: left;"> as below </td>
</tr>
<tr>
 <td style="text-align: left;">
   other <code>int</code> or <code>float</code> </td><td style="text-align: left;">  not supported  </td><td style="text-align: left;"> <code>sum(abs(x)^{ord})^{(1 / ord)}</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>See Also</h3>

<p>Other linalg: 
<code><a href="#topic+linalg_cholesky_ex">linalg_cholesky_ex</a>()</code>,
<code><a href="#topic+linalg_cholesky">linalg_cholesky</a>()</code>,
<code><a href="#topic+linalg_det">linalg_det</a>()</code>,
<code><a href="#topic+linalg_eigh">linalg_eigh</a>()</code>,
<code><a href="#topic+linalg_eigvalsh">linalg_eigvalsh</a>()</code>,
<code><a href="#topic+linalg_eigvals">linalg_eigvals</a>()</code>,
<code><a href="#topic+linalg_eig">linalg_eig</a>()</code>,
<code><a href="#topic+linalg_householder_product">linalg_householder_product</a>()</code>,
<code><a href="#topic+linalg_inv_ex">linalg_inv_ex</a>()</code>,
<code><a href="#topic+linalg_inv">linalg_inv</a>()</code>,
<code><a href="#topic+linalg_lstsq">linalg_lstsq</a>()</code>,
<code><a href="#topic+linalg_matrix_norm">linalg_matrix_norm</a>()</code>,
<code><a href="#topic+linalg_matrix_power">linalg_matrix_power</a>()</code>,
<code><a href="#topic+linalg_matrix_rank">linalg_matrix_rank</a>()</code>,
<code><a href="#topic+linalg_multi_dot">linalg_multi_dot</a>()</code>,
<code><a href="#topic+linalg_norm">linalg_norm</a>()</code>,
<code><a href="#topic+linalg_pinv">linalg_pinv</a>()</code>,
<code><a href="#topic+linalg_qr">linalg_qr</a>()</code>,
<code><a href="#topic+linalg_slogdet">linalg_slogdet</a>()</code>,
<code><a href="#topic+linalg_solve">linalg_solve</a>()</code>,
<code><a href="#topic+linalg_svdvals">linalg_svdvals</a>()</code>,
<code><a href="#topic+linalg_svd">linalg_svd</a>()</code>,
<code><a href="#topic+linalg_tensorinv">linalg_tensorinv</a>()</code>,
<code><a href="#topic+linalg_tensorsolve">linalg_tensorsolve</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_arange(0, 8, dtype = torch_float()) - 4
a
b &lt;- a$reshape(c(3, 3))
b

linalg_vector_norm(a, ord = 3.5)
linalg_vector_norm(b, ord = 3.5)
}
</code></pre>

<hr>
<h2 id='load_state_dict'>Load a state dict file</h2><span id='topic+load_state_dict'></span>

<h3>Description</h3>

<p>This function should only be used to load models saved in python.
For it to work correctly you need to use <code>torch.save</code> with the flag:
<code style="white-space: pre;">&#8288;_use_new_zipfile_serialization=True&#8288;</code> and also remove all <code>nn.Parameter</code>
classes from the tensors in the dict.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_state_dict(path, ..., legacy_stream = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_state_dict_+3A_path">path</code></td>
<td>
<p>to the state dict file</p>
</td></tr>
<tr><td><code id="load_state_dict_+3A_...">...</code></td>
<td>
<p>additional arguments that are currently not used.</p>
</td></tr>
<tr><td><code id="load_state_dict_+3A_legacy_stream">legacy_stream</code></td>
<td>
<p>if <code>TRUE</code> then the state dict is loaded using a
a legacy way of handling streams.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The above might change with development of <a href="https://github.com/pytorch/pytorch/issues/37213">this</a>
in pytorch's C++ api.
</p>


<h3>Value</h3>

<p>a named list of tensors.
</p>

<hr>
<h2 id='local_autocast'>Autocast context manager</h2><span id='topic+local_autocast'></span><span id='topic+with_autocast'></span>

<h3>Description</h3>

<p>Allow regions of your code to run in mixed precision.
In these regions, ops run in an op-specific dtype chosen by autocast
to improve performance while maintaining accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_autocast(
  device_type,
  dtype = NULL,
  enabled = TRUE,
  cache_enabled = NULL,
  ...,
  .env = parent.frame()
)

with_autocast(
  code,
  ...,
  device_type,
  dtype = NULL,
  enabled = TRUE,
  cache_enabled = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_autocast_+3A_device_type">device_type</code></td>
<td>
<p>a character string indicating whether to use 'cuda' or 'cpu' device</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_dtype">dtype</code></td>
<td>
<p>a torch data type indicating whether to use <code>torch_float16()</code> or <code>torch_bfloat16()</code>.</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_enabled">enabled</code></td>
<td>
<p>a logical value indicating whether autocasting should be enabled in the region. Default: TRUE</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_cache_enabled">cache_enabled</code></td>
<td>
<p>a logical value indicating whether the weight cache inside autocast should be enabled.</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_.env">.env</code></td>
<td>
<p>The environment to use for scoping.</p>
</td></tr>
<tr><td><code id="local_autocast_+3A_code">code</code></td>
<td>
<p>code to be executed with no gradient recording.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When entering an autocast-enabled region, Tensors may be any type.
You should not call <code>half()</code> or <code>bfloat16()</code> on your model(s) or inputs
when using autocasting.
</p>
<p><code>autocast</code> should only be enabled during the forward pass(es) of your network,
including the loss computation(s).  Backward passes under autocast are not
recommended. Backward ops run in the same type that autocast used for
corresponding forward ops.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>with_autocast()</code>: A with context for automatic mixed precision.
</p>
</li></ul>


<h3>See Also</h3>

<p><code><a href="#topic+cuda_amp_grad_scaler">cuda_amp_grad_scaler()</a></code> to perform dynamic gradient scaling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_randn(5, 5, dtype = torch_float32())
y &lt;- torch_randn(5, 5, dtype = torch_float32())

foo &lt;- function(x, y) {
  local_autocast(device = "cpu")
  z &lt;- torch_mm(x, y)
  w &lt;- torch_mm(z, x)
  w
}

out &lt;- foo(x, y)
}
</code></pre>

<hr>
<h2 id='local_device'>Device contexts</h2><span id='topic+local_device'></span><span id='topic+with_device'></span>

<h3>Description</h3>

<p>Device contexts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_device(device, ..., .env = parent.frame())

with_device(code, ..., device)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_device_+3A_device">device</code></td>
<td>
<p>A torch device to be used by default when creating new tensors.</p>
</td></tr>
<tr><td><code id="local_device_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
<tr><td><code id="local_device_+3A_.env">.env</code></td>
<td>
<p>The environment to use for scoping.</p>
</td></tr>
<tr><td><code id="local_device_+3A_code">code</code></td>
<td>
<p>The code to be evaluated in the modified environment.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>with_device()</code>: Modifies the default device for the selected context.
</p>
</li></ul>

<hr>
<h2 id='lr_cosine_annealing'>Set the learning rate of each parameter group using a cosine annealing schedule</h2><span id='topic+lr_cosine_annealing'></span>

<h3>Description</h3>

<p>Set the learning rate of each parameter group using a cosine annealing schedule
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_cosine_annealing(
  optimizer,
  T_max,
  eta_min = 0,
  last_epoch = -1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_cosine_annealing_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_cosine_annealing_+3A_t_max">T_max</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="lr_cosine_annealing_+3A_eta_min">eta_min</code></td>
<td>
<p>Minimum learning rate. Default: 0.</p>
</td></tr>
<tr><td><code id="lr_cosine_annealing_+3A_last_epoch">last_epoch</code></td>
<td>
<p>The index of the last epoch</p>
</td></tr>
<tr><td><code id="lr_cosine_annealing_+3A_verbose">verbose</code></td>
<td>
<p>(bool): If <code>TRUE</code>, prints a message to stdout for
each update. Default: <code>FALSE</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='lr_lambda'>Sets the learning rate of each parameter group to the initial lr
times a given function. When last_epoch=-1, sets initial lr as lr.</h2><span id='topic+lr_lambda'></span>

<h3>Description</h3>

<p>Sets the learning rate of each parameter group to the initial lr
times a given function. When last_epoch=-1, sets initial lr as lr.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_lambda(optimizer, lr_lambda, last_epoch = -1, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_lambda_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_lambda_+3A_lr_lambda">lr_lambda</code></td>
<td>
<p>(function or list): A function which computes a multiplicative
factor given an integer parameter epoch, or a list of such
functions, one for each group in optimizer.param_groups.</p>
</td></tr>
<tr><td><code id="lr_lambda_+3A_last_epoch">last_epoch</code></td>
<td>
<p>(int): The index of last epoch. Default: -1.</p>
</td></tr>
<tr><td><code id="lr_lambda_+3A_verbose">verbose</code></td>
<td>
<p>(bool): If <code>TRUE</code>, prints a message to stdout for
each update. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# Assuming optimizer has two groups.
lambda1 &lt;- function(epoch) epoch %/% 30
lambda2 &lt;- function(epoch) 0.95^epoch
## Not run: 
scheduler &lt;- lr_lambda(optimizer, lr_lambda = list(lambda1, lambda2))
for (epoch in 1:100) {
  train(...)
  validate(...)
  scheduler$step()
}

## End(Not run)

}
</code></pre>

<hr>
<h2 id='lr_multiplicative'>Multiply the learning rate of each parameter group by the factor given
in the specified function. When last_epoch=-1, sets initial lr as lr.</h2><span id='topic+lr_multiplicative'></span>

<h3>Description</h3>

<p>Multiply the learning rate of each parameter group by the factor given
in the specified function. When last_epoch=-1, sets initial lr as lr.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_multiplicative(optimizer, lr_lambda, last_epoch = -1, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_multiplicative_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_multiplicative_+3A_lr_lambda">lr_lambda</code></td>
<td>
<p>(function or list): A function which computes a multiplicative
factor given an integer parameter epoch, or a list of such
functions, one for each group in optimizer.param_groups.</p>
</td></tr>
<tr><td><code id="lr_multiplicative_+3A_last_epoch">last_epoch</code></td>
<td>
<p>(int): The index of last epoch. Default: -1.</p>
</td></tr>
<tr><td><code id="lr_multiplicative_+3A_verbose">verbose</code></td>
<td>
<p>(bool): If <code>TRUE</code>, prints a message to stdout for
each update. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
lmbda &lt;- function(epoch) 0.95
scheduler &lt;- lr_multiplicative(optimizer, lr_lambda = lmbda)
for (epoch in 1:100) {
  train(...)
  validate(...)
  scheduler$step()
}

## End(Not run)

}
</code></pre>

<hr>
<h2 id='lr_one_cycle'>Once cycle learning rate</h2><span id='topic+lr_one_cycle'></span>

<h3>Description</h3>

<p>Sets the learning rate of each parameter group according to the
1cycle learning rate policy. The 1cycle policy anneals the learning
rate from an initial learning rate to some maximum learning rate and then
from that maximum learning rate to some minimum learning rate much lower
than the initial learning rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_one_cycle(
  optimizer,
  max_lr,
  total_steps = NULL,
  epochs = NULL,
  steps_per_epoch = NULL,
  pct_start = 0.3,
  anneal_strategy = "cos",
  cycle_momentum = TRUE,
  base_momentum = 0.85,
  max_momentum = 0.95,
  div_factor = 25,
  final_div_factor = 10000,
  last_epoch = -1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_one_cycle_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_max_lr">max_lr</code></td>
<td>
<p>(float or list): Upper learning rate boundaries in the cycle
for each parameter group.</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_total_steps">total_steps</code></td>
<td>
<p>(int): The total number of steps in the cycle. Note that
if a value is not provided here, then it must be inferred by providing
a value for epochs and steps_per_epoch.
Default: NULL</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_epochs">epochs</code></td>
<td>
<p>(int): The number of epochs to train for. This is used along
with steps_per_epoch in order to infer the total number of steps in the cycle
if a value for total_steps is not provided.
Default: NULL</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_steps_per_epoch">steps_per_epoch</code></td>
<td>
<p>(int): The number of steps per epoch to train for. This is
used along with epochs in order to infer the total number of steps in the
cycle if a value for total_steps is not provided.
Default: NULL</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_pct_start">pct_start</code></td>
<td>
<p>(float): The percentage of the cycle (in number of steps) spent
increasing the learning rate.
Default: 0.3</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_anneal_strategy">anneal_strategy</code></td>
<td>
<p>(str): {'cos', 'linear'}
Specifies the annealing strategy: &quot;cos&quot; for cosine annealing, &quot;linear&quot; for
linear annealing.
Default: 'cos'</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_cycle_momentum">cycle_momentum</code></td>
<td>
<p>(bool): If <code>TRUE</code>, momentum is cycled inversely
to learning rate between 'base_momentum' and 'max_momentum'.
Default: TRUE</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_base_momentum">base_momentum</code></td>
<td>
<p>(float or list): Lower momentum boundaries in the cycle
for each parameter group. Note that momentum is cycled inversely
to learning rate; at the peak of a cycle, momentum is
'base_momentum' and learning rate is 'max_lr'.
Default: 0.85</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_max_momentum">max_momentum</code></td>
<td>
<p>(float or list): Upper momentum boundaries in the cycle
for each parameter group. Functionally,
it defines the cycle amplitude (max_momentum - base_momentum).
Note that momentum is cycled inversely
to learning rate; at the start of a cycle, momentum is 'max_momentum'
and learning rate is 'base_lr'
Default: 0.95</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_div_factor">div_factor</code></td>
<td>
<p>(float): Determines the initial learning rate via
initial_lr = max_lr/div_factor
Default: 25</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_final_div_factor">final_div_factor</code></td>
<td>
<p>(float): Determines the minimum learning rate via
min_lr = initial_lr/final_div_factor
Default: 1e4</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_last_epoch">last_epoch</code></td>
<td>
<p>(int): The index of the last batch. This parameter is used when
resuming a training job. Since <code>step()</code> should be invoked after each
batch instead of after each epoch, this number represents the total
number of <em>batches</em> computed, not the total number of epochs computed.
When last_epoch=-1, the schedule is started from the beginning.
Default: -1</p>
</td></tr>
<tr><td><code id="lr_one_cycle_+3A_verbose">verbose</code></td>
<td>
<p>(bool): If <code>TRUE</code>, prints a message to stdout for
each update. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This policy was initially described in the paper
<a href="https://arxiv.org/abs/1708.07120">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</a>.
</p>
<p>The 1cycle learning rate policy changes the learning rate after every batch.
<code>step</code> should be called after a batch has been used for training.
This scheduler is not chainable.
</p>
<p>Note also that the total number of steps in the cycle can be determined in one
of two ways (listed in order of precedence):
</p>

<ul>
<li><p> A value for total_steps is explicitly provided.
</p>
</li>
<li><p> A number of epochs (epochs) and a number of steps per epoch
(steps_per_epoch) are provided.
</p>
</li></ul>

<p>In this case, the number of total steps is inferred by
total_steps = epochs * steps_per_epoch
</p>
<p>You must either provide a value for total_steps or provide a value for both
epochs and steps_per_epoch.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
data_loader &lt;- dataloader(...)
optimizer &lt;- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9)
scheduler &lt;- lr_one_cycle(optimizer,
  max_lr = 0.01, steps_per_epoch = length(data_loader),
  epochs = 10
)

for (i in 1:epochs) {
  coro::loop(for (batch in data_loader) {
    train_batch(...)
    scheduler$step()
  })
}

## End(Not run)

}
</code></pre>

<hr>
<h2 id='lr_reduce_on_plateau'>Reduce learning rate on plateau</h2><span id='topic+lr_reduce_on_plateau'></span>

<h3>Description</h3>

<p>Reduce learning rate when a metric has stopped improving.
Models often benefit from reducing the learning rate by a factor
of 2-10 once learning stagnates. This scheduler reads a metrics
quantity and if no improvement is seen for a 'patience' number
of epochs, the learning rate is reduced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_reduce_on_plateau(
  optimizer,
  mode = "min",
  factor = 0.1,
  patience = 10,
  threshold = 1e-04,
  threshold_mode = "rel",
  cooldown = 0,
  min_lr = 0,
  eps = 1e-08,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_reduce_on_plateau_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_mode">mode</code></td>
<td>
<p>(str): One of <code>min</code>, <code>max</code>. In <code>min</code> mode, lr will be reduced
when the quantity monitored has stopped decreasing; in <code>max</code> mode it will be
reduced when the quantity monitored has stopped increasing. Default: 'min'.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_factor">factor</code></td>
<td>
<p>(float): Factor by which the learning rate will be reduced.
new_lr &lt;- lr * factor. Default: 0.1.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_patience">patience</code></td>
<td>
<p>(int): Number of epochs with no improvement after which
learning rate will be reduced. For example, if <code>patience = 2</code>, then we will
ignore the first 2 epochs with no improvement, and will only decrease the LR
after the 3rd epoch if the loss still hasn't improved then. Default: 10.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_threshold">threshold</code></td>
<td>
<p>(float):Threshold for measuring the new optimum, to only
focus on significant changes. Default: 1e-4.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_threshold_mode">threshold_mode</code></td>
<td>
<p>(str): One of <code>rel</code>, <code>abs</code>. In <code>rel</code> mode,
dynamic_threshold &lt;- best * ( 1 + threshold ) in 'max' mode
or best * ( 1 - threshold ) in <code>min</code> mode. In <code>abs</code> mode,
dynamic_threshold &lt;- best + threshold in <code>max</code> mode or
best - threshold in <code>min</code> mode. Default: 'rel'.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_cooldown">cooldown</code></td>
<td>
<p>(int): Number of epochs to wait before resuming normal
operation after lr has been reduced. Default: 0.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_min_lr">min_lr</code></td>
<td>
<p>(float or list): A scalar or a list of scalars. A lower bound
on the learning rate of all param groups or each group respectively. Default: 0.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_eps">eps</code></td>
<td>
<p>(float): Minimal decay applied to lr. If the difference between
new and old lr is smaller than eps, the update is ignored. Default: 1e-8.</p>
</td></tr>
<tr><td><code id="lr_reduce_on_plateau_+3A_verbose">verbose</code></td>
<td>
<p>(bool): If <code>TRUE</code>, prints a message to stdout for
each update. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run:  
optimizer &lt;- optim_sgd(model$parameters(), lr=0.1, momentum=0.9)
scheduler &lt;- lr_reduce_on_plateau(optimizer, 'min')
for (epoch in 1:10) {
 train(...)
 val_loss &lt;- validate(...)
 # note that step should be called after validate
 scheduler$step(val_loss)
}

## End(Not run)
}
</code></pre>

<hr>
<h2 id='lr_scheduler'>Creates learning rate schedulers</h2><span id='topic+lr_scheduler'></span>

<h3>Description</h3>

<p>Creates learning rate schedulers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_scheduler(
  classname = NULL,
  inherit = LRScheduler,
  ...,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_scheduler_+3A_classname">classname</code></td>
<td>
<p>optional name for the learning rate scheduler</p>
</td></tr>
<tr><td><code id="lr_scheduler_+3A_inherit">inherit</code></td>
<td>
<p>an optional learning rate scheduler to inherit from</p>
</td></tr>
<tr><td><code id="lr_scheduler_+3A_...">...</code></td>
<td>
<p>named list of methods. You must implement the <code>get_lr()</code>
method that doesn't take any argument and returns learning rates
for each <code>param_group</code> in the optimizer.</p>
</td></tr>
<tr><td><code id="lr_scheduler_+3A_parent_env">parent_env</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='lr_step'>Step learning rate decay</h2><span id='topic+lr_step'></span>

<h3>Description</h3>

<p>Decays the learning rate of each parameter group by gamma every
step_size epochs. Notice that such decay can happen simultaneously with
other changes to the learning rate from outside this scheduler. When
last_epoch=-1, sets initial lr as lr.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr_step(optimizer, step_size, gamma = 0.1, last_epoch = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr_step_+3A_optimizer">optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td></tr>
<tr><td><code id="lr_step_+3A_step_size">step_size</code></td>
<td>
<p>(int): Period of learning rate decay.</p>
</td></tr>
<tr><td><code id="lr_step_+3A_gamma">gamma</code></td>
<td>
<p>(float): Multiplicative factor of learning rate decay.
Default: 0.1.</p>
</td></tr>
<tr><td><code id="lr_step_+3A_last_epoch">last_epoch</code></td>
<td>
<p>(int): The index of last epoch. Default: -1.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
# Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch &lt; 30
# lr = 0.005    if 30 &lt;= epoch &lt; 60
# lr = 0.0005   if 60 &lt;= epoch &lt; 90
# ...
scheduler &lt;- lr_step(optimizer, step_size = 30, gamma = 0.1)
for (epoch in 1:100) {
  train(...)
  validate(...)
  scheduler$step()
}

## End(Not run)

}
</code></pre>

<hr>
<h2 id='nn_adaptive_avg_pool1d'>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_avg_pool1d'></span>

<h3>Description</h3>

<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_avg_pool1d(output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_avg_pool1d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size H</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5
m &lt;- nn_adaptive_avg_pool1d(5)
input &lt;- torch_randn(1, 64, 8)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_adaptive_avg_pool2d'>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_avg_pool2d'></span>

<h3>Description</h3>

<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_avg_pool2d(output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_avg_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code>int</code>, or <code>NULL</code> which means the size will
be the same as that of the input.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5x7
m &lt;- nn_adaptive_avg_pool2d(c(5, 7))
input &lt;- torch_randn(1, 64, 8, 9)
output &lt;- m(input)
# target output size of 7x7 (square)
m &lt;- nn_adaptive_avg_pool2d(7)
input &lt;- torch_randn(1, 64, 10, 9)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_adaptive_avg_pool3d'>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_avg_pool3d'></span>

<h3>Description</h3>

<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_avg_pool3d(output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_avg_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the form D x H x W.
Can be a tuple (D, H, W) or a single number D for a cube D x D x D.
D, H and W can be either a <code>int</code>, or <code>None</code> which means the size will
be the same as that of the input.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5x7x9
m &lt;- nn_adaptive_avg_pool3d(c(5, 7, 9))
input &lt;- torch_randn(1, 64, 8, 9, 10)
output &lt;- m(input)
# target output size of 7x7x7 (cube)
m &lt;- nn_adaptive_avg_pool3d(7)
input &lt;- torch_randn(1, 64, 10, 9, 8)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_adaptive_log_softmax_with_loss'>AdaptiveLogSoftmaxWithLoss module</h2><span id='topic+nn_adaptive_log_softmax_with_loss'></span>

<h3>Description</h3>

<p>Efficient softmax approximation as described in
<a href="https://arxiv.org/abs/1609.04309">Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss, David Grangier, and Herv Jgou</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_log_softmax_with_loss(
  in_features,
  n_classes,
  cutoffs,
  div_value = 4,
  head_bias = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_log_softmax_with_loss_+3A_in_features">in_features</code></td>
<td>
<p>(int): Number of features in the input tensor</p>
</td></tr>
<tr><td><code id="nn_adaptive_log_softmax_with_loss_+3A_n_classes">n_classes</code></td>
<td>
<p>(int): Number of classes in the dataset</p>
</td></tr>
<tr><td><code id="nn_adaptive_log_softmax_with_loss_+3A_cutoffs">cutoffs</code></td>
<td>
<p>(Sequence): Cutoffs used to assign targets to their buckets</p>
</td></tr>
<tr><td><code id="nn_adaptive_log_softmax_with_loss_+3A_div_value">div_value</code></td>
<td>
<p>(float, optional): value used as an exponent to compute sizes
of the clusters. Default: 4.0</p>
</td></tr>
<tr><td><code id="nn_adaptive_log_softmax_with_loss_+3A_head_bias">head_bias</code></td>
<td>
<p>(bool, optional): If <code>True</code>, adds a bias term to the 'head' of the
adaptive softmax. Default: <code>False</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adaptive softmax is an approximate strategy for training models with large
output spaces. It is most effective when the label distribution is highly
imbalanced, for example in natural language modelling, where the word
frequency distribution approximately follows the Zipf's law.
</p>
<p>Adaptive softmax partitions the labels into several clusters, according to
their frequency. These clusters may contain different number of targets
each.
</p>
<p>Additionally, clusters containing less frequent labels assign lower
dimensional embeddings to those labels, which speeds up the computation.
For each minibatch, only clusters for which at least one target is
present are evaluated.
</p>
<p>The idea is that the clusters which are accessed frequently
(like the first one, containing most frequent labels), should also be cheap
to compute &ndash; that is, contain a small number of assigned labels.
We highly recommend taking a look at the original paper for more details.
</p>

<ul>
<li> <p><code>cutoffs</code> should be an ordered Sequence of integers sorted
in the increasing order.
It controls number of clusters and the partitioning of targets into
clusters. For example setting <code>cutoffs = c(10, 100, 1000)</code>
means that first <code>10</code> targets will be assigned
to the 'head' of the adaptive softmax, targets <code style="white-space: pre;">&#8288;11, 12, ..., 100&#8288;</code> will be
assigned to the first cluster, and targets <code style="white-space: pre;">&#8288;101, 102, ..., 1000&#8288;</code> will be
assigned to the second cluster, while targets
<code style="white-space: pre;">&#8288;1001, 1002, ..., n_classes - 1&#8288;</code> will be assigned
to the last, third cluster.
</p>
</li>
<li> <p><code>div_value</code> is used to compute the size of each additional cluster,
which is given as
<code class="reqn">\left\lfloor\frac{\mbox{in\_features}}{\mbox{div\_value}^{idx}}\right\rfloor</code>,
where <code class="reqn">idx</code> is the cluster index (with clusters
for less frequent words having larger indices,
and indices starting from <code class="reqn">1</code>).
</p>
</li>
<li> <p><code>head_bias</code> if set to True, adds a bias term to the 'head' of the
adaptive softmax. See paper for details. Set to False in the official
implementation.
</p>
</li></ul>



<h3>Value</h3>

<p><code>NamedTuple</code> with <code>output</code> and <code>loss</code> fields:
</p>

<ul>
<li> <p><strong>output</strong> is a Tensor of size <code>N</code> containing computed target
log probabilities for each example
</p>
</li>
<li> <p><strong>loss</strong> is a Scalar representing the computed negative
log likelihood loss
</p>
</li></ul>



<h3>Warning</h3>

<p>Labels passed as inputs to this module should be sorted according to
their frequency. This means that the most frequent label should be
represented by the index <code>0</code>, and the least frequent
label should be represented by the index <code>n_classes - 1</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> input: <code class="reqn">(N, \mbox{in\_features})</code>
</p>
</li>
<li><p> target: <code class="reqn">(N)</code> where each value satisfies <code class="reqn">0 &lt;= \mbox{target[i]} &lt;= \mbox{n\_classes}</code>
</p>
</li>
<li><p> output1: <code class="reqn">(N)</code>
</p>
</li>
<li><p> output2: <code>Scalar</code>
</p>
</li></ul>



<h3>Note</h3>

<p>This module returns a <code>NamedTuple</code> with <code>output</code>
and <code>loss</code> fields. See further documentation for details.
</p>
<p>To compute log-probabilities for all classes, the <code>log_prob</code>
method can be used.
</p>

<hr>
<h2 id='nn_adaptive_max_pool1d'>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_max_pool1d'></span>

<h3>Description</h3>

<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_max_pool1d(output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_max_pool1d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size H</p>
</td></tr>
<tr><td><code id="nn_adaptive_max_pool1d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the indices along with the outputs.
Useful to pass to <code><a href="#topic+nn_max_unpool1d">nn_max_unpool1d()</a></code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5
m &lt;- nn_adaptive_max_pool1d(5)
input &lt;- torch_randn(1, 64, 8)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_adaptive_max_pool2d'>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_max_pool2d'></span>

<h3>Description</h3>

<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_max_pool2d(output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_max_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form H x W.
Can be a tuple <code style="white-space: pre;">&#8288;(H, W)&#8288;</code> or a single H for a square image H x H.
H and W can be either a <code>int</code>, or <code>None</code> which means the size will
be the same as that of the input.</p>
</td></tr>
<tr><td><code id="nn_adaptive_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the indices along with the outputs.
Useful to pass to <code><a href="#topic+nn_max_unpool2d">nn_max_unpool2d()</a></code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5x7
m &lt;- nn_adaptive_max_pool2d(c(5, 7))
input &lt;- torch_randn(1, 64, 8, 9)
output &lt;- m(input)
# target output size of 7x7 (square)
m &lt;- nn_adaptive_max_pool2d(7)
input &lt;- torch_randn(1, 64, 10, 9)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_adaptive_max_pool3d'>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</h2><span id='topic+nn_adaptive_max_pool3d'></span>

<h3>Description</h3>

<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_adaptive_max_pool3d(output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_adaptive_max_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form D x H x W.
Can be a tuple (D, H, W) or a single D for a cube D x D x D.
D, H and W can be either a <code>int</code>, or <code>None</code> which means the size will
be the same as that of the input.</p>
</td></tr>
<tr><td><code id="nn_adaptive_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the indices along with the outputs.
Useful to pass to <code><a href="#topic+nn_max_unpool3d">nn_max_unpool3d()</a></code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# target output size of 5x7x9
m &lt;- nn_adaptive_max_pool3d(c(5, 7, 9))
input &lt;- torch_randn(1, 64, 8, 9, 10)
output &lt;- m(input)
# target output size of 7x7x7 (cube)
m &lt;- nn_adaptive_max_pool3d(7)
input &lt;- torch_randn(1, 64, 10, 9, 8)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_avg_pool1d'>Applies a 1D average pooling over an input signal composed of several
input planes.</h2><span id='topic+nn_avg_pool1d'></span>

<h3>Description</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, L)</code>,
output <code class="reqn">(N, C, L_{out})</code> and <code>kernel_size</code> <code class="reqn">k</code>
can be precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_avg_pool1d(
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_avg_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window</p>
</td></tr>
<tr><td><code id="nn_avg_pool1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_avg_pool1d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides</p>
</td></tr>
<tr><td><code id="nn_avg_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
<tr><td><code id="nn_avg_pool1d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when TRUE, will include the zero-padding in the averaging calculation</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1}
\mbox{input}(N_i, C_j, \mbox{stride} \times l + m)
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points.
</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code> can each be
an <code>int</code> or a one-element tuple.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, L_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, L_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  L_{out} = \left\lfloor \frac{L_{in} +
      2 \times \mbox{padding} - \mbox{kernel\_size}}{\mbox{stride}} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# pool with window of size=3, stride=2
m &lt;- nn_avg_pool1d(3, stride = 2)
m(torch_randn(1, 1, 8))
}
</code></pre>

<hr>
<h2 id='nn_avg_pool2d'>Applies a 2D average pooling over an input signal composed of several input
planes.</h2><span id='topic+nn_avg_pool2d'></span>

<h3>Description</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, H, W)</code>,
output <code class="reqn">(N, C, H_{out}, W_{out})</code> and <code>kernel_size</code> <code class="reqn">(kH, kW)</code>
can be precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_avg_pool2d(
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE,
  divisor_override = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_avg_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window</p>
</td></tr>
<tr><td><code id="nn_avg_pool2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_avg_pool2d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides</p>
</td></tr>
<tr><td><code id="nn_avg_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
<tr><td><code id="nn_avg_pool2d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when TRUE, will include the zero-padding in the averaging calculation</p>
</td></tr>
<tr><td><code id="nn_avg_pool2d_+3A_divisor_override">divisor_override</code></td>
<td>
<p>if specified, it will be used as divisor, otherwise <code>kernel_size</code> will be used</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points.
</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H_{out}, W_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \mbox{padding}[0] -
      \mbox{kernel\_size}[0]}{\mbox{stride}[0]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \mbox{padding}[1] -
      \mbox{kernel\_size}[1]}{\mbox{stride}[1]} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# pool of square window of size=3, stride=2
m &lt;- nn_avg_pool2d(3, stride = 2)
# pool of non-square window
m &lt;- nn_avg_pool2d(c(3, 2), stride = c(2, 1))
input &lt;- torch_randn(20, 16, 50, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_avg_pool3d'>Applies a 3D average pooling over an input signal composed of several input
planes.</h2><span id='topic+nn_avg_pool3d'></span>

<h3>Description</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, D, H, W)</code>,
output <code class="reqn">(N, C, D_{out}, H_{out}, W_{out})</code> and <code>kernel_size</code> <code class="reqn">(kD, kH, kW)</code>
can be precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_avg_pool3d(
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE,
  divisor_override = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_avg_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window</p>
</td></tr>
<tr><td><code id="nn_avg_pool3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_avg_pool3d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on all three sides</p>
</td></tr>
<tr><td><code id="nn_avg_pool3d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
<tr><td><code id="nn_avg_pool3d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when TRUE, will include the zero-padding in the averaging calculation</p>
</td></tr>
<tr><td><code id="nn_avg_pool3d_+3A_divisor_override">divisor_override</code></td>
<td>
<p>if specified, it will be used as divisor, otherwise <code>kernel_size</code> will be used</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\begin{array}{ll}
\mbox{out}(N_i, C_j, d, h, w) = &amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\
&amp; \frac{\mbox{input}(N_i, C_j, \mbox{stride}[0] \times d + k, \mbox{stride}[1] \times h + m, \mbox{stride}[2] \times w + n)}{kD \times kH \times kW}
\end{array}
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on all three sides
for <code>padding</code> number of points.
</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the depth, height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of three ints &ndash; in which case, the first <code>int</code> is used for the depth dimension,
the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, D_{out}, H_{out}, W_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  D_{out} = \left\lfloor\frac{D_{in} + 2 \times \mbox{padding}[0] -
      \mbox{kernel\_size}[0]}{\mbox{stride}[0]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in} + 2 \times \mbox{padding}[1] -
      \mbox{kernel\_size}[1]}{\mbox{stride}[1]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in} + 2 \times \mbox{padding}[2] -
      \mbox{kernel\_size}[2]}{\mbox{stride}[2]} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# pool of square window of size=3, stride=2
m &lt;- nn_avg_pool3d(3, stride = 2)
# pool of non-square window
m &lt;- nn_avg_pool3d(c(3, 2, 2), stride = c(2, 1, 2))
input &lt;- torch_randn(20, 16, 50, 44, 31)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_batch_norm1d'>BatchNorm1D module</h2><span id='topic+nn_batch_norm1d'></span>

<h3>Description</h3>

<p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_batch_norm1d(
  num_features,
  eps = 1e-05,
  momentum = 0.1,
  affine = TRUE,
  track_running_stats = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_batch_norm1d_+3A_num_features">num_features</code></td>
<td>
<p><code class="reqn">C</code> from an expected input of size
<code class="reqn">(N, C, L)</code> or <code class="reqn">L</code> from input of size <code class="reqn">(N, L)</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm1d_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability.
Default: 1e-5</p>
</td></tr>
<tr><td><code id="nn_batch_norm1d_+3A_momentum">momentum</code></td>
<td>
<p>the value used for the running_mean and running_var
computation. Can be set to <code>NULL</code> for cumulative moving average
(i.e. simple average). Default: 0.1</p>
</td></tr>
<tr><td><code id="nn_batch_norm1d_+3A_affine">affine</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this module has
learnable affine parameters. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm1d_+3A_track_running_stats">track_running_stats</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this
module tracks the running mean and variance, and when set to <code>FALSE</code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
</code>
</p>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <code class="reqn">\gamma</code> and <code class="reqn">\beta</code> are learnable parameter vectors
of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <code class="reqn">\gamma</code>
are set to 1 and the elements of <code class="reqn">\beta</code> are set to 0.
</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default :attr:<code>momentum</code>
of 0.1.
If <code>track_running_stats</code> is set to <code>FALSE</code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.
</p>


<h3>Note</h3>

<p>This <code>momentum</code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<code class="reqn">\hat{x}_{\mbox{new}} = (1 - \mbox{momentum}) \times \hat{x} + \mbox{momentum} \times x_t</code>,
where <code class="reqn">\hat{x}</code> is the estimated statistic and <code class="reqn">x_t</code> is the
new observed value.
</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics
on <code style="white-space: pre;">&#8288;(N, L)&#8288;</code> slices, it's common terminology to call this Temporal Batch Normalization.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C)</code> or <code class="reqn">(N, C, L)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C)</code> or <code class="reqn">(N, C, L)</code> (same shape as input)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# With Learnable Parameters
m &lt;- nn_batch_norm1d(100)
# Without Learnable Parameters
m &lt;- nn_batch_norm1d(100, affine = FALSE)
input &lt;- torch_randn(20, 100)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_batch_norm2d'>BatchNorm2D</h2><span id='topic+nn_batch_norm2d'></span>

<h3>Description</h3>

<p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
additional channel dimension) as described in the paper
<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_batch_norm2d(
  num_features,
  eps = 1e-05,
  momentum = 0.1,
  affine = TRUE,
  track_running_stats = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_batch_norm2d_+3A_num_features">num_features</code></td>
<td>
<p><code class="reqn">C</code> from an expected input of size
<code class="reqn">(N, C, H, W)</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm2d_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability.
Default: 1e-5</p>
</td></tr>
<tr><td><code id="nn_batch_norm2d_+3A_momentum">momentum</code></td>
<td>
<p>the value used for the running_mean and running_var
computation. Can be set to <code>None</code> for cumulative moving average
(i.e. simple average). Default: 0.1</p>
</td></tr>
<tr><td><code id="nn_batch_norm2d_+3A_affine">affine</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this module has
learnable affine parameters. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm2d_+3A_track_running_stats">track_running_stats</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this
module tracks the running mean and variance, and when set to <code>FALSE</code>,
this module does not track such statistics and uses batch statistics instead
in both training and eval modes if the running mean and variance are <code>None</code>.
Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
</code>
</p>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <code class="reqn">\gamma</code> and <code class="reqn">\beta</code> are learnable parameter vectors
of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <code class="reqn">\gamma</code> are set
to 1 and the elements of <code class="reqn">\beta</code> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <code>torch_var(input, unbiased=FALSE)</code>.
Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code>momentum</code>
of 0.1.
</p>
<p>If <code>track_running_stats</code> is set to <code>FALSE</code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H, W)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H, W)</code> (same shape as input)
</p>
</li></ul>



<h3>Note</h3>

<p>This <code>momentum</code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<code class="reqn">\hat{x}_{\mbox{new}} = (1 - \mbox{momentum}) \times \hat{x} + \mbox{momentum} \times x_t</code>,
where <code class="reqn">\hat{x}</code> is the estimated statistic and <code class="reqn">x_t</code> is the
new observed value.
Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics
on <code style="white-space: pre;">&#8288;(N, H, W)&#8288;</code> slices, it's common terminology to call this Spatial Batch Normalization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# With Learnable Parameters
m &lt;- nn_batch_norm2d(100)
# Without Learnable Parameters
m &lt;- nn_batch_norm2d(100, affine = FALSE)
input &lt;- torch_randn(20, 100, 35, 45)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_batch_norm3d'>BatchNorm3D</h2><span id='topic+nn_batch_norm3d'></span>

<h3>Description</h3>

<p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_batch_norm3d(
  num_features,
  eps = 1e-05,
  momentum = 0.1,
  affine = TRUE,
  track_running_stats = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_batch_norm3d_+3A_num_features">num_features</code></td>
<td>
<p><code class="reqn">C</code> from an expected input of size
<code class="reqn">(N, C, D, H, W)</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm3d_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability.
Default: 1e-5</p>
</td></tr>
<tr><td><code id="nn_batch_norm3d_+3A_momentum">momentum</code></td>
<td>
<p>the value used for the running_mean and running_var
computation. Can be set to <code>None</code> for cumulative moving average
(i.e. simple average). Default: 0.1</p>
</td></tr>
<tr><td><code id="nn_batch_norm3d_+3A_affine">affine</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this module has
learnable affine parameters. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_batch_norm3d_+3A_track_running_stats">track_running_stats</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this
module tracks the running mean and variance, and when set to <code>FALSE</code>,
this module does not track such statistics and uses batch statistics instead
in both training and eval modes if the running mean and variance are <code>None</code>.
Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
</code>
</p>

<p>The mean and standard-deviation are calculated per-dimension over the
mini-batches and <code class="reqn">\gamma</code> and <code class="reqn">\beta</code> are learnable parameter
vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements
of <code class="reqn">\gamma</code> are set to 1 and the elements of <code class="reqn">\beta</code> are set to
0. The standard-deviation is calculated via the biased estimator,
equivalent to <code>torch_var(input, unbiased = FALSE)</code>.
</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code>momentum</code>
of 0.1.
</p>
<p>If <code>track_running_stats</code> is set to <code>FALSE</code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, D, H, W)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, D, H, W)</code> (same shape as input)
</p>
</li></ul>



<h3>Note</h3>

<p>This <code>momentum</code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is:
<code class="reqn">\hat{x}_{\mbox{new}} = (1 - \mbox{momentum}) \times \hat{x} + \mbox{momentum} \times x_t</code>,
where <code class="reqn">\hat{x}</code> is the estimated statistic and <code class="reqn">x_t</code> is the
new observed value.
</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing
statistics on <code style="white-space: pre;">&#8288;(N, D, H, W)&#8288;</code> slices, it's common terminology to call this
Volumetric Batch Normalization or Spatio-temporal Batch Normalization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# With Learnable Parameters
m &lt;- nn_batch_norm3d(100)
# Without Learnable Parameters
m &lt;- nn_batch_norm3d(100, affine = FALSE)
input &lt;- torch_randn(20, 100, 35, 45, 55)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_bce_loss'>Binary cross entropy loss</h2><span id='topic+nn_bce_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_bce_loss(weight = NULL, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_bce_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size <code>nbatch</code>.</p>
</td></tr>
<tr><td><code id="nn_bce_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \left\{ \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
\right.
</code>
</p>

<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <code class="reqn">y</code> should be numbers
between 0 and 1.
</p>
<p>Notice that if <code class="reqn">x_n</code> is either 0 or 1, one of the log terms would be
mathematically undefined in the above loss equation. PyTorch chooses to set
<code class="reqn">\log (0) = -\infty</code>, since <code class="reqn">\lim_{x\to 0} \log (x) = -\infty</code>.
</p>
<p>However, an infinite term in the loss equation is not desirable for several reasons.
For one, if either <code class="reqn">y_n = 0</code> or <code class="reqn">(1 - y_n) = 0</code>, then we would be
multiplying 0 with infinity. Secondly, if we have an infinite loss value, then
we would also have an infinite term in our gradient, since
<code class="reqn">\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</code>.
</p>
<p>This would make BCELoss's backward method nonlinear with respect to <code class="reqn">x_n</code>,
and using it for things like linear regression would not be straight-forward.
Our solution is that BCELoss clamps its log function outputs to be greater than
or equal to -100. This way, we can always have a finite loss value and a linear
backward method.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>, same
shape as input.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_sigmoid()
loss &lt;- nn_bce_loss()
input &lt;- torch_randn(3, requires_grad = TRUE)
target &lt;- torch_rand(3)
output &lt;- loss(m(input), target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_bce_with_logits_loss'>BCE with logits loss</h2><span id='topic+nn_bce_with_logits_loss'></span>

<h3>Description</h3>

<p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single
class. This version is more numerically stable than using a plain <code>Sigmoid</code>
followed by a <code>BCELoss</code> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_bce_with_logits_loss(weight = NULL, reduction = "mean", pos_weight = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_bce_with_logits_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size <code>nbatch</code>.</p>
</td></tr>
<tr><td><code id="nn_bce_with_logits_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
<tr><td><code id="nn_bce_with_logits_loss_+3A_pos_weight">pos_weight</code></td>
<td>
<p>(Tensor, optional): a weight of positive examples.
Must be a vector with length equal to the number of classes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
                   + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <code>t[i]</code> should be numbers
between 0 and 1.
It's possible to trade off recall and precision by adding weights to positive examples.
In the case of multi-label classification the loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad
l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})
+ (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],
</code>
</p>

<p>where <code class="reqn">c</code> is the class number (<code class="reqn">c &gt; 1</code> for multi-label binary
classification,
</p>
<p><code class="reqn">c = 1</code> for single-label binary classification),
<code class="reqn">n</code> is the number of the sample in the batch and
<code class="reqn">p_c</code> is the weight of the positive answer for the class <code class="reqn">c</code>.
<code class="reqn">p_c &gt; 1</code> increases the recall, <code class="reqn">p_c &lt; 1</code> increases the precision.
For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <code>pos_weight</code> for the class should be equal to <code class="reqn">\frac{300}{100}=3</code>.
The loss would act as if the dataset contains <code class="reqn">3\times 100=300</code> positive examples.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>, same
shape as input.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_bce_with_logits_loss()
input &lt;- torch_randn(3, requires_grad = TRUE)
target &lt;- torch_empty(3)$random_(1, 2)
output &lt;- loss(input, target)
output$backward()

target &lt;- torch_ones(10, 64, dtype = torch_float32()) # 64 classes, batch size = 10
output &lt;- torch_full(c(10, 64), 1.5) # A prediction (logit)
pos_weight &lt;- torch_ones(64) # All weights are equal to 1
criterion &lt;- nn_bce_with_logits_loss(pos_weight = pos_weight)
criterion(output, target) # -log(sigmoid(1.5))
}
</code></pre>

<hr>
<h2 id='nn_bilinear'>Bilinear module</h2><span id='topic+nn_bilinear'></span>

<h3>Description</h3>

<p>Applies a bilinear transformation to the incoming data
<code class="reqn">y = x_1^T A x_2 + b</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_bilinear(in1_features, in2_features, out_features, bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_bilinear_+3A_in1_features">in1_features</code></td>
<td>
<p>size of each first input sample</p>
</td></tr>
<tr><td><code id="nn_bilinear_+3A_in2_features">in2_features</code></td>
<td>
<p>size of each second input sample</p>
</td></tr>
<tr><td><code id="nn_bilinear_+3A_out_features">out_features</code></td>
<td>
<p>size of each output sample</p>
</td></tr>
<tr><td><code id="nn_bilinear_+3A_bias">bias</code></td>
<td>
<p>If set to <code>FALSE</code>, the layer will not learn an additive bias.
Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Input1: <code class="reqn">(N, *, H_{in1})</code> <code class="reqn">H_{in1}=\mbox{in1\_features}</code> and
<code class="reqn">*</code> means any number of additional dimensions. All but the last
dimension of the inputs should be the same.
</p>
</li>
<li><p> Input2: <code class="reqn">(N, *, H_{in2})</code> where <code class="reqn">H_{in2}=\mbox{in2\_features}</code>.
</p>
</li>
<li><p> Output: <code class="reqn">(N, *, H_{out})</code> where <code class="reqn">H_{out}=\mbox{out\_features}</code>
and all but the last dimension are the same shape as the input.
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight: the learnable weights of the module of shape
<code class="reqn">(\mbox{out\_features}, \mbox{in1\_features}, \mbox{in2\_features})</code>.
The values are initialized from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>, where
<code class="reqn">k = \frac{1}{\mbox{in1\_features}}</code>
</p>
</li>
<li><p> bias: the learnable bias of the module of shape <code class="reqn">(\mbox{out\_features})</code>.
If <code>bias</code> is <code>TRUE</code>, the values are initialized from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>, where
<code class="reqn">k = \frac{1}{\mbox{in1\_features}}</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_bilinear(20, 30, 50)
input1 &lt;- torch_randn(128, 20)
input2 &lt;- torch_randn(128, 30)
output &lt;- m(input1, input2)
print(output$size())
}
</code></pre>

<hr>
<h2 id='nn_buffer'>Creates a nn_buffer</h2><span id='topic+nn_buffer'></span>

<h3>Description</h3>

<p>Indicates that a tensor is a buffer in a nn_module
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_buffer(x, persistent = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_buffer_+3A_x">x</code></td>
<td>
<p>the tensor that will be converted to nn_buffer</p>
</td></tr>
<tr><td><code id="nn_buffer_+3A_persistent">persistent</code></td>
<td>
<p>whether the buffer should be persistent or not.</p>
</td></tr>
</table>

<hr>
<h2 id='nn_celu'>CELU module</h2><span id='topic+nn_celu'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_celu(alpha = 1, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_celu_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code> value for the CELU formulation. Default: 1.0</p>
</td></tr>
<tr><td><code id="nn_celu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))
</code>
</p>

<p>More details can be found in the paper
<a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_celu()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_contrib_sparsemax'>Sparsemax activation</h2><span id='topic+nn_contrib_sparsemax'></span>

<h3>Description</h3>

<p>Sparsemax activation module.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_contrib_sparsemax(dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_contrib_sparsemax_+3A_dim">dim</code></td>
<td>
<p>The dimension over which to apply the sparsemax function. (-1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SparseMax activation is described in
<a href="https://arxiv.org/abs/1602.02068">'From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification'</a>
The implementation is based on <a href="https://github.com/aced125/sparsemax/tree/master/sparsemax">aced125/sparsemax</a>
</p>

<hr>
<h2 id='nn_conv_transpose1d'>ConvTranspose1D</h2><span id='topic+nn_conv_transpose1d'></span>

<h3>Description</h3>

<p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv_transpose1d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  bias = TRUE,
  dilation = 1,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv_transpose1d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple, optional): <code>dilation * (kernel_size - 1) - padding</code> zero-padding
will be added to both sides of the input. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_output_padding">output_padding</code></td>
<td>
<p>(int or tuple, optional): Additional size added to one side
of the output shape. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>True</code>, adds a learnable bias to the output. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose1d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>,
<code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note
below for details.
</p>
</li>
<li> <p><code>output_padding</code> controls the additional size added to one side
of the output shape. See note below for details.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also known as the
 trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic">link</a>
has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters (of size
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>).
</p>
</li></ul>

</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, L_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, L_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  L_{out} = (L_{in} - 1) \times \mbox{stride} - 2 \times \mbox{padding} + \mbox{dilation}
\times (\mbox{kernel\_size} - 1) + \mbox{output\_padding} + 1
</code>
</p>

</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{in\_channels}, \frac{\mbox{out\_channels}}{\mbox{groups}},</code>
<code class="reqn">\mbox{kernel\_size})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \mbox{kernel\_size}}</code>
</p>
</li>
<li><p> bias (Tensor):   the learnable bias of the module of shape (out_channels).
If <code>bias</code> is <code>TRUE</code>, then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \mbox{kernel\_size}}</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <code>cross-correlation</code><em>,
and not a full <code>cross-correlation</code></em>.
It is up to the user to add proper padding.
</p>
<p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code>
amount of zero padding to both sizes of the input. This is set so that
when a <code>~torch.nn.Conv1d</code> and a <code>~torch.nn.ConvTranspose1d</code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code>stride &gt; 1</code>,
<code>~torch.nn.Conv1d</code> maps multiple input shapes to the same output
shape. <code>output_padding</code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code>output_padding</code> is only used to find output shape, but does
not actually add zero-padding to output.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_conv_transpose1d(32, 16, 2)
input &lt;- torch_randn(10, 32, 2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_conv_transpose2d'>ConvTranpose2D module</h2><span id='topic+nn_conv_transpose2d'></span>

<h3>Description</h3>

<p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv_transpose2d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  bias = TRUE,
  dilation = 1,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv_transpose2d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple, optional): <code>dilation * (kernel_size - 1) - padding</code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_output_padding">output_padding</code></td>
<td>
<p>(int or tuple, optional): Additional size added to one side
of each dimension in the output shape. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose2d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>,
<code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note
below for details.
</p>
</li>
<li> <p><code>output_padding</code> controls the additional size added to one side
of the output shape. See note below for details.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also known as the  trous algorithm.
It is harder to describe, but this <code>link</code>_ has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters (of size
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>).
</p>
</li></ul>

</li></ul>

<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code>
can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the height and width dimensions
</p>
</li>
<li><p> a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, H_{out}, W_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride}[0] - 2 \times \mbox{padding}[0] + \mbox{dilation}[0]
\times (\mbox{kernel\_size}[0] - 1) + \mbox{output\_padding}[0] + 1
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = (W_{in} - 1) \times \mbox{stride}[1] - 2 \times \mbox{padding}[1] + \mbox{dilation}[1]
\times (\mbox{kernel\_size}[1] - 1) + \mbox{output\_padding}[1] + 1
</code>
</p>

</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{in\_channels}, \frac{\mbox{out\_channels}}{\mbox{groups}},</code>
<code class="reqn">\mbox{kernel\_size[0]}, \mbox{kernel\_size[1]})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{1}\mbox{kernel\_size}[i]}</code>
</p>
</li>
<li><p> bias (Tensor):   the learnable bias of the module of shape (out_channels)
If <code>bias</code> is <code>True</code>, then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{1}\mbox{kernel\_size}[i]}</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <code>cross-correlation</code>_,
and not a full <code>cross-correlation</code>. It is up to the user to add proper padding.
</p>
<p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code>
amount of zero padding to both sizes of the input. This is set so that
when a <a href="#topic+nn_conv2d">nn_conv2d</a> and a <a href="#topic+nn_conv_transpose2d">nn_conv_transpose2d</a> are initialized with same
parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code>stride &gt; 1</code>,
<a href="#topic+nn_conv2d">nn_conv2d</a> maps multiple input shapes to the same output
shape. <code>output_padding</code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code>output_padding</code> is only used to find output shape, but does
not actually add zero-padding to output.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# With square kernels and equal stride
m &lt;- nn_conv_transpose2d(16, 33, 3, stride = 2)
# non-square kernels and unequal stride and with padding
m &lt;- nn_conv_transpose2d(16, 33, c(3, 5), stride = c(2, 1), padding = c(4, 2))
input &lt;- torch_randn(20, 16, 50, 100)
output &lt;- m(input)
# exact output size can be also specified as an argument
input &lt;- torch_randn(1, 16, 12, 12)
downsample &lt;- nn_conv2d(16, 16, 3, stride = 2, padding = 1)
upsample &lt;- nn_conv_transpose2d(16, 16, 3, stride = 2, padding = 1)
h &lt;- downsample(input)
h$size()
output &lt;- upsample(h, output_size = input$size())
output$size()
}
</code></pre>

<hr>
<h2 id='nn_conv_transpose3d'>ConvTranpose3D module</h2><span id='topic+nn_conv_transpose3d'></span>

<h3>Description</h3>

<p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv_transpose3d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  bias = TRUE,
  dilation = 1,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv_transpose3d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple, optional): <code>dilation * (kernel_size - 1) - padding</code> zero-padding
will be added to both sides of each dimension in the input. Default: 0
output_padding (int or tuple, optional): Additional size added to one side
of each dimension in the output shape. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_output_padding">output_padding</code></td>
<td>
<p>(int or tuple, optional): Additional size added to one side
of each dimension in the output shape. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv_transpose3d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note
below for details.
</p>
</li>
<li> <p><code>output_padding</code> controls the additional size added to one side
of the output shape. See note below for details.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also known as the  trous algorithm.
It is harder to describe, but this <code>link</code>_ has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters (of size
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>).
</p>
</li></ul>

</li></ul>

<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code>
can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the depth, height and width dimensions
</p>
</li>
<li><p> a <code>tuple</code> of three ints &ndash; in which case, the first <code>int</code> is used for the depth dimension,
the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, D_{out}, H_{out}, W_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  D_{out} = (D_{in} - 1) \times \mbox{stride}[0] - 2 \times \mbox{padding}[0] + \mbox{dilation}[0]
\times (\mbox{kernel\_size}[0] - 1) + \mbox{output\_padding}[0] + 1
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride}[1] - 2 \times \mbox{padding}[1] + \mbox{dilation}[1]
\times (\mbox{kernel\_size}[1] - 1) + \mbox{output\_padding}[1] + 1
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = (W_{in} - 1) \times \mbox{stride}[2] - 2 \times \mbox{padding}[2] + \mbox{dilation}[2]
\times (\mbox{kernel\_size}[2] - 1) + \mbox{output\_padding}[2] + 1
</code>
</p>

</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{in\_channels}, \frac{\mbox{out\_channels}}{\mbox{groups}},</code>
<code class="reqn">\mbox{kernel\_size[0]}, \mbox{kernel\_size[1]}, \mbox{kernel\_size[2]})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li>
<li><p> bias (Tensor):   the learnable bias of the module of shape (out_channels)
If <code>bias</code> is <code>True</code>, then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <code>cross-correlation</code><em>,
and not a full <code>cross-correlation</code></em>.
It is up to the user to add proper padding.
</p>
<p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code>
amount of zero padding to both sizes of the input. This is set so that
when a <code>~torch.nn.Conv3d</code> and a <code>~torch.nn.ConvTranspose3d</code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code>stride &gt; 1</code>,
<code>~torch.nn.Conv3d</code> maps multiple input shapes to the same output
shape. <code>output_padding</code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code>output_padding</code> is only used to find output shape, but does
not actually add zero-padding to output.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
# With square kernels and equal stride
m &lt;- nn_conv_transpose3d(16, 33, 3, stride = 2)
# non-square kernels and unequal stride and with padding
m &lt;- nn_conv_transpose3d(16, 33, c(3, 5, 2), stride = c(2, 1, 1), padding = c(0, 4, 2))
input &lt;- torch_randn(20, 16, 10, 50, 100)
output &lt;- m(input)

## End(Not run)
}
</code></pre>

<hr>
<h2 id='nn_conv1d'>Conv1D module</h2><span id='topic+nn_conv1d'></span>

<h3>Description</h3>

<p>Applies a 1D convolution over an input signal composed of several input
planes.
In the simplest case, the output value of the layer with input size
<code class="reqn">(N, C_{\mbox{in}}, L)</code> and output <code class="reqn">(N, C_{\mbox{out}}, L_{\mbox{out}})</code> can be
precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv1d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1,
  bias = TRUE,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv1d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_padding">padding</code></td>
<td>
<p>(int, tuple or str, optional)  Padding added to both sides of
the input. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel
elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input
channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>TRUE</code>, adds a learnable bias to the
output. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_conv1d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>,
<code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\mbox{out}(N_i, C_{\mbox{out}_j}) = \mbox{bias}(C_{\mbox{out}_j}) +
  \sum_{k = 0}^{C_{in} - 1} \mbox{weight}(C_{\mbox{out}_j}, k)
\star \mbox{input}(N_i, k)
</code>
</p>

<p>where <code class="reqn">\star</code> is the valid
<a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<code class="reqn">N</code> is a batch size, <code class="reqn">C</code> denotes a number of channels,
<code class="reqn">L</code> is a length of signal sequence.
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation, a single
number or a one-element tuple.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both sides
for <code>padding</code> number of points.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also
known as the  trous algorithm. It is harder to describe, but this
<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters,
of size <code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>.
</p>
</li></ul>

</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
<code>cross-correlation</code><em>, and not a full <code>cross-correlation</code></em>.
It is up to the user to add proper padding.
</p>
<p>When <code>groups == in_channels</code> and <code>out_channels == K * in_channels</code>,
where <code>K</code> is a positive integer, this operation is also termed in
literature as depthwise convolution.
In other words, for an input of size <code class="reqn">(N, C_{in}, L_{in})</code>,
a depthwise convolution with a depthwise multiplier <code>K</code>, can be constructed by arguments
<code class="reqn">(C_{\mbox{in}}=C_{in}, C_{\mbox{out}}=C_{in} \times K, ..., \mbox{groups}=C_{in})</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, L_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, L_{out})</code> where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  L_{out} = \left\lfloor\frac{L_{in} + 2 \times \mbox{padding} - \mbox{dilation}
    \times (\mbox{kernel\_size} - 1) - 1}{\mbox{stride}} + 1\right\rfloor
</code>
</p>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{out\_channels}, \frac{\mbox{in\_channels}}{\mbox{groups}}, \mbox{kernel\_size})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \mbox{kernel\_size}}</code>
</p>
</li>
<li><p> bias (Tensor): the learnable bias of the module of shape
(out_channels). If <code>bias</code> is <code>TRUE</code>, then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \mbox{kernel\_size}}</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_conv1d(16, 33, 3, stride = 2)
input &lt;- torch_randn(20, 16, 50)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_conv2d'>Conv2D module</h2><span id='topic+nn_conv2d'></span>

<h3>Description</h3>

<p>Applies a 2D convolution over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv2d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1,
  bias = TRUE,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv2d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple or string, optional): Zero-padding added to both sides of
the input. controls the amount of padding applied to the input. It
can be either a string <code>'valid'</code>, <code>'same'</code> or a tuple of ints giving the
amount of implicit padding applied on both sides. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input
channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>TRUE</code>, adds a learnable bias to the
output. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_conv2d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>,
<code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the simplest case, the output value of the layer with input size
<code class="reqn">(N, C_{\mbox{in}}, H, W)</code> and output <code class="reqn">(N, C_{\mbox{out}}, H_{\mbox{out}}, W_{\mbox{out}})</code>
can be precisely described as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{out}(N_i, C_{\mbox{out}_j}) = \mbox{bias}(C_{\mbox{out}_j}) +
  \sum_{k = 0}^{C_{\mbox{in}} - 1} \mbox{weight}(C_{\mbox{out}_j}, k) \star \mbox{input}(N_i, k)
</code>
</p>

<p>where <code class="reqn">\star</code> is the valid 2D cross-correlation operator,
<code class="reqn">N</code> is a batch size, <code class="reqn">C</code> denotes a number of channels,
<code class="reqn">H</code> is a height of input planes in pixels, and <code class="reqn">W</code> is
width in pixels.
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation, a single
number or a tuple.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>padding</code> number of points for each dimension.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also
known as the  trous algorithm. It is harder to describe, but this <code>link</code>_
has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters, of size:
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>.
</p>
</li></ul>

</li></ul>

<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the height and
width dimension
</p>
</li>
<li><p> a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid cross-correlation,
and not a full cross-correlation.
It is up to the user to add proper padding.
</p>
<p>When <code>groups == in_channels</code> and <code>out_channels == K * in_channels</code>,
where <code>K</code> is a positive integer, this operation is also termed in
literature as depthwise convolution.
In other words, for an input of size :math:<code style="white-space: pre;">&#8288;(N, C_{in}, H_{in}, W_{in})&#8288;</code>,
a depthwise convolution with a depthwise multiplier <code>K</code>, can be constructed by arguments
<code class="reqn">(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})</code>.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>backends_cudnn_deterministic = TRUE</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, H_{out}, W_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \mbox{padding}[0] - \mbox{dilation}[0]
    \times (\mbox{kernel\_size}[0] - 1) - 1}{\mbox{stride}[0]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \mbox{padding}[1] - \mbox{dilation}[1]
    \times (\mbox{kernel\_size}[1] - 1) - 1}{\mbox{stride}[1]} + 1\right\rfloor
</code>
</p>

</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{out\_channels}, \frac{\mbox{in\_channels}}{\mbox{groups}}</code>,
<code class="reqn">\mbox{kernel\_size[0]}, \mbox{kernel\_size[1]})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \prod_{i=0}^{1}\mbox{kernel\_size}[i]}</code>
</p>
</li>
<li><p> bias (Tensor): the learnable bias of the module of shape
(out_channels). If <code>bias</code> is <code>TRUE</code>,
then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \prod_{i=0}^{1}\mbox{kernel\_size}[i]}</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# With square kernels and equal stride
m &lt;- nn_conv2d(16, 33, 3, stride = 2)
# non-square kernels and unequal stride and with padding
m &lt;- nn_conv2d(16, 33, c(3, 5), stride = c(2, 1), padding = c(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m &lt;- nn_conv2d(16, 33, c(3, 5), stride = c(2, 1), padding = c(4, 2), dilation = c(3, 1))
input &lt;- torch_randn(20, 16, 50, 100)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_conv3d'>Conv3D module</h2><span id='topic+nn_conv3d'></span>

<h3>Description</h3>

<p>Applies a 3D convolution over an input signal composed of several input
planes.
In the simplest case, the output value of the layer with input size <code class="reqn">(N, C_{in}, D, H, W)</code>
and output <code class="reqn">(N, C_{out}, D_{out}, H_{out}, W_{out})</code> can be precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_conv3d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1,
  bias = TRUE,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_conv3d_+3A_in_channels">in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_out_channels">out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_padding">padding</code></td>
<td>
<p>(int, tuple or str, optional): padding added to all six sides of the input. Default: 0</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_groups">groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input channels to output channels. Default: 1</p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_bias">bias</code></td>
<td>
<p>(bool, optional): If <code>TRUE</code>, adds a learnable bias to the output. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_conv3d_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  out(N_i, C_{out_j}) = bias(C_{out_j}) +
  \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)
</code>
</p>

<p>where <code class="reqn">\star</code> is the valid 3D <code>cross-correlation</code> operator
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>padding</code> number of points for each dimension.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also known as the  trous algorithm.
It is harder to describe, but this <code>link</code>_ has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>
</li>
<li><p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li><p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li><p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters, of size
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>.
</p>
</li></ul>

<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the depth, height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of three ints &ndash; in which case, the first <code>int</code> is used for the depth dimension,
the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C_{in}, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C_{out}, D_{out}, H_{out}, W_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  D_{out} = \left\lfloor\frac{D_{in} + 2 \times \mbox{padding}[0] - \mbox{dilation}[0]
    \times (\mbox{kernel\_size}[0] - 1) - 1}{\mbox{stride}[0]} + 1\right\rfloor
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in} + 2 \times \mbox{padding}[1] - \mbox{dilation}[1]
    \times (\mbox{kernel\_size}[1] - 1) - 1}{\mbox{stride}[1]} + 1\right\rfloor
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in} + 2 \times \mbox{padding}[2] - \mbox{dilation}[2]
    \times (\mbox{kernel\_size}[2] - 1) - 1}{\mbox{stride}[2]} + 1\right\rfloor
 </code>
</p>

</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{out\_channels}, \frac{\mbox{in\_channels}}{\mbox{groups}},</code>
<code class="reqn">\mbox{kernel\_size[0]}, \mbox{kernel\_size[1]}, \mbox{kernel\_size[2]})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li>
<li><p> bias (Tensor):   the learnable bias of the module of shape (out_channels). If <code>bias</code> is <code>True</code>,
then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{in}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <code>cross-correlation</code><em>,
and not a full <code>cross-correlation</code></em>.
It is up to the user to add proper padding.
</p>
<p>When <code>groups == in_channels</code> and <code>out_channels == K * in_channels</code>,
where <code>K</code> is a positive integer, this operation is also termed in
literature as depthwise convolution.
In other words, for an input of size <code class="reqn">(N, C_{in}, D_{in}, H_{in}, W_{in})</code>,
a depthwise convolution with a depthwise multiplier <code>K</code>, can be constructed by arguments
<code class="reqn">(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})</code>.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
Please see the notes on :doc:<code style="white-space: pre;">&#8288;/notes/randomness&#8288;</code> for background.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# With square kernels and equal stride
m &lt;- nn_conv3d(16, 33, 3, stride = 2)
# non-square kernels and unequal stride and with padding
m &lt;- nn_conv3d(16, 33, c(3, 5, 2), stride = c(2, 1, 1), padding = c(4, 2, 0))
input &lt;- torch_randn(20, 16, 10, 50, 100)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_cosine_embedding_loss'>Cosine embedding loss</h2><span id='topic+nn_cosine_embedding_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the loss given input tensors
<code class="reqn">x_1</code>, <code class="reqn">x_2</code> and a <code>Tensor</code> label <code class="reqn">y</code> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.
The loss function for each sample is:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_cosine_embedding_loss(margin = 0, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_cosine_embedding_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): Should be a number from <code class="reqn">-1</code> to <code class="reqn">1</code>,
<code class="reqn">0</code> to <code class="reqn">0.5</code> is suggested. If <code>margin</code> is missing, the
default value is <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="nn_cosine_embedding_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) =
  \begin{array}{ll}
1 - \cos(x_1, x_2), &amp; \mbox{if } y = 1 \\
\max(0, \cos(x_1, x_2) - \mbox{margin}), &amp; \mbox{if } y = -1
\end{array}
</code>
</p>


<hr>
<h2 id='nn_cross_entropy_loss'>CrossEntropyLoss module</h2><span id='topic+nn_cross_entropy_loss'></span>

<h3>Description</h3>

<p>This criterion combines <code><a href="#topic+nn_log_softmax">nn_log_softmax()</a></code> and <code>nn_nll_loss()</code> in one single class.
It is useful when training a classification problem with <code>C</code> classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_cross_entropy_loss(weight = NULL, ignore_index = -100, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_cross_entropy_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to each class.
If given, has to be a Tensor of size <code>C</code></p>
</td></tr>
<tr><td><code id="nn_cross_entropy_loss_+3A_ignore_index">ignore_index</code></td>
<td>
<p>(int, optional): Specifies a target value that is ignored
and does not contribute to the input gradient. When <code>size_average</code> is
<code>TRUE</code>, the loss is averaged over non-ignored targets.</p>
</td></tr>
<tr><td><code id="nn_cross_entropy_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If provided, the optional argument <code>weight</code> should be a 1D <code>Tensor</code>
assigning weight to each of the classes.
</p>
<p>This is particularly useful when you have an unbalanced training set.
The <code>input</code> is expected to contain raw, unnormalized scores for each class.
<code>input</code> has to be a Tensor of size either <code class="reqn">(minibatch, C)</code> or
<code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code>
with <code class="reqn">K \geq 1</code> for the <code>K</code>-dimensional case (described later).
</p>
<p>This criterion expects a class index in the range <code class="reqn">[0, C-1]</code> as the
<code>target</code> for each value of a 1D tensor of size <code>minibatch</code>; if <code>ignore_index</code>
is specified, this criterion also accepts this class index (this index may not
necessarily be in the class range).
</p>
<p>The loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
= -x[class] + \log\left(\sum_j \exp(x[j])\right)
</code>
</p>

<p>or in the case of the <code>weight</code> argument being specified:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)
</code>
</p>

<p>The losses are averaged across observations for each minibatch.
Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>,
where <code class="reqn">K</code> is the number of dimensions, and a target of appropriate shape
(see below).
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C)</code> where <code style="white-space: pre;">&#8288;C = number of classes&#8288;</code>, or
<code class="reqn">(N, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>
in the case of <code>K</code>-dimensional loss.
</p>
</li>
<li><p> Target: <code class="reqn">(N)</code> where each value is <code class="reqn">0 \leq \mbox{targets}[i] \leq C-1</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case of
K-dimensional loss.
</p>
</li>
<li><p> Output: scalar.
If <code>reduction</code> is <code>'none'</code>, then the same size as the target:
<code class="reqn">(N)</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case
of K-dimensional loss.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_cross_entropy_loss()
input &lt;- torch_randn(3, 5, requires_grad = TRUE)
target &lt;- torch_randint(low = 1, high = 5, size = 3, dtype = torch_long())
output &lt;- loss(input, target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_ctc_loss'>The Connectionist Temporal Classification loss.</h2><span id='topic+nn_ctc_loss'></span>

<h3>Description</h3>

<p>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the
probability of possible alignments of input to target, producing a loss value which is differentiable
with respect to each input node. The alignment of input to target is assumed to be &quot;many-to-one&quot;, which
limits the length of the target sequence such that it must be <code class="reqn">\leq</code> the input length.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_ctc_loss(blank = 0, reduction = "mean", zero_infinity = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_ctc_loss_+3A_blank">blank</code></td>
<td>
<p>(int, optional): blank label. Default <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="nn_ctc_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the output losses will be divided by the target lengths and
then the mean over the batch is taken. Default: <code>'mean'</code></p>
</td></tr>
<tr><td><code id="nn_ctc_loss_+3A_zero_infinity">zero_infinity</code></td>
<td>
<p>(bool, optional):
Whether to zero infinite losses and the associated gradients.
Default: <code>FALSE</code>
Infinite losses mainly occur when the inputs are too short
to be aligned to the targets.</p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Log_probs: Tensor of size <code class="reqn">(T, N, C)</code>,
where <code class="reqn">T = \mbox{input length}</code>,
<code class="reqn">N = \mbox{batch size}</code>, and
<code class="reqn">C = \mbox{number of classes (including blank)}</code>.
The logarithmized probabilities of the outputs (e.g. obtained with
[nnf)log_softmax()]).
</p>
</li>
<li><p> Targets: Tensor of size <code class="reqn">(N, S)</code> or
<code class="reqn">(\mbox{sum}(\mbox{target\_lengths}))</code>,
where <code class="reqn">N = \mbox{batch size}</code> and
<code class="reqn">S = \mbox{max target length, if shape is } (N, S)</code>.
It represent the target sequences. Each element in the target
sequence is a class index. And the target index cannot be blank (default=0).
In the <code class="reqn">(N, S)</code> form, targets are padded to the
length of the longest sequence, and stacked.
In the <code class="reqn">(\mbox{sum}(\mbox{target\_lengths}))</code> form,
the targets are assumed to be un-padded and
concatenated within 1 dimension.
</p>
</li>
<li><p> Input_lengths: Tuple or tensor of size <code class="reqn">(N)</code>,
where <code class="reqn">N = \mbox{batch size}</code>. It represent the lengths of the
inputs (must each be <code class="reqn">\leq T</code>). And the lengths are specified
for each sequence to achieve masking under the assumption that sequences
are padded to equal lengths.
</p>
</li>
<li><p> Target_lengths: Tuple or tensor of size <code class="reqn">(N)</code>,
where <code class="reqn">N = \mbox{batch size}</code>. It represent lengths of the targets.
Lengths are specified for each sequence to achieve masking under the
assumption that sequences are padded to equal lengths. If target shape is
<code class="reqn">(N,S)</code>, target_lengths are effectively the stop index
<code class="reqn">s_n</code> for each target sequence, such that <code>target_n = targets[n,0:s_n]</code> for
each target in a batch. Lengths must each be <code class="reqn">\leq S</code>
If the targets are given as a 1d tensor that is the concatenation of individual
targets, the target_lengths must add up to the total length of the tensor.
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then
<code class="reqn">(N)</code>, where <code class="reqn">N = \mbox{batch size}</code>.
</p>
</li></ul>

<p>[nnf)log_softmax()]: R:nnf)log_softmax()
[n,0:s_n]: R:n,0:s_n
</p>


<h3>Note</h3>

<p>In order to use CuDNN, the following must be satisfied: <code>targets</code> must be
in concatenated format, all <code>input_lengths</code> must be <code>T</code>.  <code class="reqn">blank=0</code>,
<code>target_lengths</code> <code class="reqn">\leq 256</code>, the integer arguments must be of
The regular implementation uses the (more common in PyTorch) <code>torch_long</code> dtype.
dtype <code>torch_int32</code>.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
</p>


<h3>References</h3>

<p>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
https://www.cs.toronto.edu/~graves/icml_2006.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# Target are to be padded
T &lt;- 50 # Input sequence length
C &lt;- 20 # Number of classes (including blank)
N &lt;- 16 # Batch size
S &lt;- 30 # Target sequence length of longest target in batch (padding length)
S_min &lt;- 10 # Minimum target length, for demonstration purposes

# Initialize random batch of input vectors, for *size = (T,N,C)
input &lt;- torch_randn(T, N, C)$log_softmax(2)$detach()$requires_grad_()

# Initialize random batch of targets (0 = blank, 1:C = classes)
target &lt;- torch_randint(low = 1, high = C, size = c(N, S), dtype = torch_long())

input_lengths &lt;- torch_full(size = c(N), fill_value = TRUE, dtype = torch_long())
target_lengths &lt;- torch_randint(low = S_min, high = S, size = c(N), dtype = torch_long())
ctc_loss &lt;- nn_ctc_loss()
loss &lt;- ctc_loss(input, target, input_lengths, target_lengths)
loss$backward()


# Target are to be un-padded
T &lt;- 50 # Input sequence length
C &lt;- 20 # Number of classes (including blank)
N &lt;- 16 # Batch size

# Initialize random batch of input vectors, for *size = (T,N,C)
input &lt;- torch_randn(T, N, C)$log_softmax(2)$detach()$requires_grad_()
input_lengths &lt;- torch_full(size = c(N), fill_value = TRUE, dtype = torch_long())

# Initialize random batch of targets (0 = blank, 1:C = classes)
target_lengths &lt;- torch_randint(low = 1, high = T, size = c(N), dtype = torch_long())
target &lt;- torch_randint(
  low = 1, high = C, size = as.integer(sum(target_lengths)),
  dtype = torch_long()
)
ctc_loss &lt;- nn_ctc_loss()
loss &lt;- ctc_loss(input, target, input_lengths, target_lengths)
loss$backward()
}
</code></pre>

<hr>
<h2 id='nn_dropout'>Dropout module</h2><span id='topic+nn_dropout'></span>

<h3>Description</h3>

<p>During training, randomly zeroes some of the elements of the input
tensor with probability <code>p</code> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_dropout(p = 0.5, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_dropout_+3A_p">p</code></td>
<td>
<p>probability of an element to be zeroed. Default: 0.5</p>
</td></tr>
<tr><td><code id="nn_dropout_+3A_inplace">inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a>.
</p>
<p>Furthermore, the outputs are scaled by a factor of :math:<code style="white-space: pre;">&#8288;\frac{1}{1-p}&#8288;</code> during
training. This means that during evaluation the module simply computes an
identity function.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code>. Input can be of any shape
</p>
</li>
<li><p> Output: <code class="reqn">(*)</code>. Output is of the same shape as input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_dropout(p = 0.2)
input &lt;- torch_randn(20, 16)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_dropout2d'>Dropout2D module</h2><span id='topic+nn_dropout2d'></span>

<h3>Description</h3>

<p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <code class="reqn">j</code>-th channel of the <code class="reqn">i</code>-th sample in the
batched input is a 2D tensor <code class="reqn">\mbox{input}[i, j]</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_dropout2d(p = 0.5, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_dropout2d_+3A_p">p</code></td>
<td>
<p>(float, optional): probability of an element to be zero-ed.</p>
</td></tr>
<tr><td><code id="nn_dropout2d_+3A_inplace">inplace</code></td>
<td>
<p>(bool, optional): If set to <code>TRUE</code>, will do this operation
in-place</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each channel will be zeroed out independently on every forward call with
probability <code>p</code> using samples from a Bernoulli distribution.
Usually the input comes from <a href="#topic+nn_conv2d">nn_conv2d</a> modules.
</p>
<p>As described in the paper
<a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.
In this case, <a href="#topic+nn_dropout2d">nn_dropout2d</a> will help promote independence between
feature maps and should be used instead.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H, W)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H, W)</code> (same shape as input)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_dropout2d(p = 0.2)
input &lt;- torch_randn(20, 16, 32, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_dropout3d'>Dropout3D module</h2><span id='topic+nn_dropout3d'></span>

<h3>Description</h3>

<p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <code class="reqn">j</code>-th channel of the <code class="reqn">i</code>-th sample in the
batched input is a 3D tensor <code class="reqn">\mbox{input}[i, j]</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_dropout3d(p = 0.5, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_dropout3d_+3A_p">p</code></td>
<td>
<p>(float, optional): probability of an element to be zeroed.</p>
</td></tr>
<tr><td><code id="nn_dropout3d_+3A_inplace">inplace</code></td>
<td>
<p>(bool, optional): If set to <code>TRUE</code>, will do this operation
in-place</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each channel will be zeroed out independently on every forward call with
probability <code>p</code> using samples from a Bernoulli distribution.
Usually the input comes from <a href="#topic+nn_conv2d">nn_conv2d</a> modules.
</p>
<p>As described in the paper
<a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.
</p>
<p>In this case, <a href="#topic+nn_dropout3d">nn_dropout3d</a> will help promote independence between
feature maps and should be used instead.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, D, H, W)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, D, H, W)</code> (same shape as input)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_dropout3d(p = 0.2)
input &lt;- torch_randn(20, 16, 4, 32, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_elu'>ELU module</h2><span id='topic+nn_elu'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_elu(alpha = 1, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_elu_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code> value for the ELU formulation. Default: 1.0</p>
</td></tr>
<tr><td><code id="nn_elu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_elu()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_embedding'>Embedding module</h2><span id='topic+nn_embedding'></span>

<h3>Description</h3>

<p>A simple lookup table that stores embeddings of a fixed dictionary and size.
This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_embedding(
  num_embeddings,
  embedding_dim,
  padding_idx = NULL,
  max_norm = NULL,
  norm_type = 2,
  scale_grad_by_freq = FALSE,
  sparse = FALSE,
  .weight = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_embedding_+3A_num_embeddings">num_embeddings</code></td>
<td>
<p>(int): size of the dictionary of embeddings</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_embedding_dim">embedding_dim</code></td>
<td>
<p>(int): the size of each embedding vector</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_padding_idx">padding_idx</code></td>
<td>
<p>(int, optional): If given, pads the output with the embedding vector at <code>padding_idx</code>
(initialized to zeros) whenever it encounters the index.</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_max_norm">max_norm</code></td>
<td>
<p>(float, optional): If given, each embedding vector with norm larger than <code>max_norm</code>
is renormalized to have norm <code>max_norm</code>.</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_norm_type">norm_type</code></td>
<td>
<p>(float, optional): The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_scale_grad_by_freq">scale_grad_by_freq</code></td>
<td>
<p>(boolean, optional): If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code>False</code>.</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_sparse">sparse</code></td>
<td>
<p>(bool, optional): If <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor.</p>
</td></tr>
<tr><td><code id="nn_embedding_+3A_.weight">.weight</code></td>
<td>
<p>(Tensor) embeddings weights (in case you want to set it manually)
</p>
<p>See Notes for more details regarding sparse gradients.</p>
</td></tr>
</table>


<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)
initialized from <code class="reqn">\mathcal{N}(0, 1)</code>
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code>, LongTensor of arbitrary shape containing the indices to extract
</p>
</li>
<li><p> Output: <code class="reqn">(*, H)</code>, where <code>*</code> is the input shape and <code class="reqn">H=\mbox{embedding\_dim}</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Keep in mind that only a limited number of optimizers support
sparse gradients: currently it's <code>optim.SGD</code> (<code>CUDA</code> and <code>CPU</code>),
<code>optim.SparseAdam</code> (<code>CUDA</code> and <code>CPU</code>) and <code>optim.Adagrad</code> (<code>CPU</code>)
</p>
<p>With <code>padding_idx</code> set, the embedding vector at
<code>padding_idx</code> is initialized to all zeros. However, note that this
vector can be modified afterwards, e.g., using a customized
initialization method, and thus changing the vector used to pad the
output. The gradient for this vector from <a href="#topic+nn_embedding">nn_embedding</a>
is always zero.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# an Embedding module containing 10 tensors of size 3
embedding &lt;- nn_embedding(10, 3)
# a batch of 2 samples of 4 indices each
input &lt;- torch_tensor(rbind(c(1, 2, 4, 5), c(4, 3, 2, 9)), dtype = torch_long())
embedding(input)
# example with padding_idx
embedding &lt;- nn_embedding(10, 3, padding_idx = 1)
input &lt;- torch_tensor(matrix(c(1, 3, 1, 6), nrow = 1), dtype = torch_long())
embedding(input)
}
</code></pre>

<hr>
<h2 id='nn_embedding_bag'>Embedding bag module</h2><span id='topic+nn_embedding_bag'></span>

<h3>Description</h3>

<p>Computes sums, means or maxes of <code>bags</code> of embeddings, without instantiating the
intermediate embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_embedding_bag(
  num_embeddings,
  embedding_dim,
  max_norm = NULL,
  norm_type = 2,
  scale_grad_by_freq = FALSE,
  mode = "mean",
  sparse = FALSE,
  include_last_offset = FALSE,
  padding_idx = NULL,
  .weight = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_embedding_bag_+3A_num_embeddings">num_embeddings</code></td>
<td>
<p>(int): size of the dictionary of embeddings</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_embedding_dim">embedding_dim</code></td>
<td>
<p>(int): the size of each embedding vector</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_max_norm">max_norm</code></td>
<td>
<p>(float, optional): If given, each embedding vector with norm larger than <code>max_norm</code>
is renormalized to have norm <code>max_norm</code>.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_norm_type">norm_type</code></td>
<td>
<p>(float, optional): The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code></p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_scale_grad_by_freq">scale_grad_by_freq</code></td>
<td>
<p>(boolean, optional): If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code>False</code>.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_mode">mode</code></td>
<td>
<p>(string, optional): <code>"sum"</code>, <code>"mean"</code> or <code>"max"</code>. Specifies the way to reduce the bag.
<code>"sum"</code> computes the weighted sum, taking <code>per_sample_weights</code>  into consideration. <code>"mean"</code> computes
the average of the values in the bag, <code>"max"</code> computes the max value over each bag.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_sparse">sparse</code></td>
<td>
<p>(bool, optional): If <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_include_last_offset">include_last_offset</code></td>
<td>
<p>(bool, optional): if <code>True</code>, <code>offsets</code> has one additional element, where the last element
is equivalent to the size of <code>indices</code>. This matches the CSR format.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_padding_idx">padding_idx</code></td>
<td>
<p>(int, optional):  If given, pads the output with the embedding vector at <code>padding_idx</code>
(initialized to zeros) whenever it encounters the index.</p>
</td></tr>
<tr><td><code id="nn_embedding_bag_+3A_.weight">.weight</code></td>
<td>
<p>(Tensor, optional) embeddings weights (in case you want to set it manually)</p>
</td></tr>
</table>


<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)
initialized from <code class="reqn">\mathcal{N}(0, 1)</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# an EmbeddingBag module containing 10 tensors of size 3
embedding_sum &lt;- nn_embedding_bag(10, 3, mode = 'sum')
# a batch of 2 samples of 4 indices each
input &lt;- torch_tensor(c(1, 2, 4, 5, 4, 3, 2, 9), dtype = torch_long())
offsets &lt;- torch_tensor(c(0, 4), dtype = torch_long())
embedding_sum(input, offsets)
# example with padding_idx
embedding_sum &lt;- nn_embedding_bag(10, 3, mode = 'sum', padding_idx = 1)
input &lt;- torch_tensor(c(2, 2, 2, 2, 4, 3, 2, 9), dtype = torch_long())
offsets &lt;- torch_tensor(c(0, 4), dtype = torch_long())
embedding_sum(input, offsets)
# An EmbeddingBag can be loaded from an Embedding like so
embedding &lt;- nn_embedding(10, 3, padding_idx = 2)
embedding_sum &lt;- nn_embedding_bag$from_pretrained(embedding$weight,
                                                 padding_idx = embedding$padding_idx,
                                                 mode='sum')
}
</code></pre>

<hr>
<h2 id='nn_flatten'>Flattens a contiguous range of dims into a tensor.</h2><span id='topic+nn_flatten'></span>

<h3>Description</h3>

<p>For use with <a href="#topic+nn_sequential">nn_sequential</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_flatten(start_dim = 2, end_dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_flatten_+3A_start_dim">start_dim</code></td>
<td>
<p>first dim to flatten (default = 2).</p>
</td></tr>
<tr><td><code id="nn_flatten_+3A_end_dim">end_dim</code></td>
<td>
<p>last dim to flatten (default = -1).</p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Input: <code style="white-space: pre;">&#8288;(*, S_start,..., S_i, ..., S_end, *)&#8288;</code>,
where <code>S_i</code> is the size at dimension <code>i</code> and <code>*</code> means any
number of dimensions including none.
</p>
</li>
<li><p> Output: <code style="white-space: pre;">&#8288;(*, S_start*...*S_i*...S_end, *)&#8288;</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+nn_unflatten">nn_unflatten</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
input &lt;- torch_randn(32, 1, 5, 5)
m &lt;- nn_flatten()
m(input)
}
</code></pre>

<hr>
<h2 id='nn_fractional_max_pool2d'>Applies a 2D fractional max pooling over an input signal composed of several input planes.</h2><span id='topic+nn_fractional_max_pool2d'></span>

<h3>Description</h3>

<p>Fractional MaxPooling is described in detail in the paper
<a href="https://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_fractional_max_pool2d(
  kernel_size,
  output_size = NULL,
  output_ratio = NULL,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_fractional_max_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple <code style="white-space: pre;">&#8288;(kh, kw)&#8288;</code></p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form <code style="white-space: pre;">&#8288;oH x oW&#8288;</code>.
Can be a tuple <code style="white-space: pre;">&#8288;(oH, oW)&#8288;</code> or a single number oH for a square image <code style="white-space: pre;">&#8288;oH x oH&#8288;</code></p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool2d_+3A_output_ratio">output_ratio</code></td>
<td>
<p>If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the indices along with the outputs.
Useful to pass to <code><a href="#topic+nn_max_unpool2d">nn_max_unpool2d()</a></code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The max-pooling operation is applied in <code class="reqn">kH \times kW</code> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# pool of square window of size=3, and target output size 13x12
m &lt;- nn_fractional_max_pool2d(3, output_size = c(13, 12))
# pool of square window and target output size being half of input image size
m &lt;- nn_fractional_max_pool2d(3, output_ratio = c(0.5, 0.5))
input &lt;- torch_randn(20, 16, 50, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_fractional_max_pool3d'>Applies a 3D fractional max pooling over an input signal composed of several input planes.</h2><span id='topic+nn_fractional_max_pool3d'></span>

<h3>Description</h3>

<p>Fractional MaxPooling is described in detail in the paper
<a href="https://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_fractional_max_pool3d(
  kernel_size,
  output_size = NULL,
  output_ratio = NULL,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_fractional_max_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k x k) or a tuple <code style="white-space: pre;">&#8288;(kt x kh x kw)&#8288;</code></p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form <code style="white-space: pre;">&#8288;oT x oH x oW&#8288;</code>.
Can be a tuple <code style="white-space: pre;">&#8288;(oT, oH, oW)&#8288;</code> or a single number oH for a square image <code style="white-space: pre;">&#8288;oH x oH x oH&#8288;</code></p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool3d_+3A_output_ratio">output_ratio</code></td>
<td>
<p>If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</p>
</td></tr>
<tr><td><code id="nn_fractional_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the indices along with the outputs.
Useful to pass to <code><a href="#topic+nn_max_unpool3d">nn_max_unpool3d()</a></code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The max-pooling operation is applied in <code class="reqn">kTxkHxkW</code> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# pool of cubic window of size=3, and target output size 13x12x11
m &lt;- nn_fractional_max_pool3d(3, output_size = c(13, 12, 11))
# pool of cubic window and target output size being half of input size
m &lt;- nn_fractional_max_pool3d(3, output_ratio = c(0.5, 0.5, 0.5))
input &lt;- torch_randn(20, 16, 50, 32, 16)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_gelu'>GELU module</h2><span id='topic+nn_gelu'></span>

<h3>Description</h3>

<p>Applies the Gaussian Error Linear Units function:
</p>
<p style="text-align: center;"><code class="reqn">\mbox{GELU}(x) = x * \Phi(x)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_gelu(approximate = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_gelu_+3A_approximate">approximate</code></td>
<td>
<p>the gelu approximation algorithm to use: <code>'none'</code> or <code>'tanh'</code>.
Default: <code>'none'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>where <code class="reqn">\Phi(x)</code> is the Cumulative Distribution Function for Gaussian Distribution.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_gelu()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_glu'>GLU module</h2><span id='topic+nn_glu'></span>

<h3>Description</h3>

<p>Applies the gated linear unit function
<code class="reqn">{GLU}(a, b)= a \otimes \sigma(b)</code> where <code class="reqn">a</code> is the first half
of the input matrices and <code class="reqn">b</code> is the second half.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_glu(dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_glu_+3A_dim">dim</code></td>
<td>
<p>(int): the dimension on which to split the input. Default: -1</p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(\ast_1, N, \ast_2)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(\ast_1, M, \ast_2)</code> where <code class="reqn">M=N/2</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_glu()
input &lt;- torch_randn(4, 2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_group_norm'>Group normalization</h2><span id='topic+nn_group_norm'></span>

<h3>Description</h3>

<p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a href="https://arxiv.org/abs/1803.08494">Group Normalization</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_group_norm(num_groups, num_channels, eps = 1e-05, affine = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_group_norm_+3A_num_groups">num_groups</code></td>
<td>
<p>(int): number of groups to separate the channels into</p>
</td></tr>
<tr><td><code id="nn_group_norm_+3A_num_channels">num_channels</code></td>
<td>
<p>(int): number of channels expected in input</p>
</td></tr>
<tr><td><code id="nn_group_norm_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability. Default: 1e-5</p>
</td></tr>
<tr><td><code id="nn_group_norm_+3A_affine">affine</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this module
has learnable per-channel affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
</code>
</p>

<p>The input channels are separated into <code>num_groups</code> groups, each containing
<code>num_channels / num_groups</code> channels. The mean and standard-deviation are calculated
separately over the each group. <code class="reqn">\gamma</code> and <code class="reqn">\beta</code> are learnable
per-channel affine transform parameter vectors of size <code>num_channels</code> if
<code>affine</code> is <code>TRUE</code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<code>torch_var(input, unbiased=FALSE)</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, *)</code> where <code class="reqn">C=\mbox{num\_channels}</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, *)</code>' (same shape as input)
</p>
</li></ul>



<h3>Note</h3>

<p>This layer uses statistics computed from input data in both training and
evaluation modes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input &lt;- torch_randn(20, 6, 10, 10)
# Separate 6 channels into 3 groups
m &lt;- nn_group_norm(3, 6)
# Separate 6 channels into 6 groups (equivalent with [nn_instance_morm])
m &lt;- nn_group_norm(6, 6)
# Put all 6 channels into a single group (equivalent with [nn_layer_norm])
m &lt;- nn_group_norm(1, 6)
# Activating the module
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_gru'>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</h2><span id='topic+nn_gru'></span>

<h3>Description</h3>

<p>For each element in the input sequence, each layer computes the following
function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_gru(
  input_size,
  hidden_size,
  num_layers = 1,
  bias = TRUE,
  batch_first = FALSE,
  dropout = 0,
  bidirectional = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_gru_+3A_input_size">input_size</code></td>
<td>
<p>The number of expected features in the input <code>x</code></p>
</td></tr>
<tr><td><code id="nn_gru_+3A_hidden_size">hidden_size</code></td>
<td>
<p>The number of features in the hidden state <code>h</code></p>
</td></tr>
<tr><td><code id="nn_gru_+3A_num_layers">num_layers</code></td>
<td>
<p>Number of recurrent layers. E.g., setting <code>num_layers=2</code>
would mean stacking two GRUs together to form a <code style="white-space: pre;">&#8288;stacked GRU&#8288;</code>,
with the second GRU taking in outputs of the first GRU and
computing the final results. Default: 1</p>
</td></tr>
<tr><td><code id="nn_gru_+3A_bias">bias</code></td>
<td>
<p>If <code>FALSE</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>.
Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_gru_+3A_batch_first">batch_first</code></td>
<td>
<p>If <code>TRUE</code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_gru_+3A_dropout">dropout</code></td>
<td>
<p>If non-zero, introduces a <code>Dropout</code> layer on the outputs of each
GRU layer except the last layer, with dropout probability equal to
<code>dropout</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nn_gru_+3A_bidirectional">bidirectional</code></td>
<td>
<p>If <code>TRUE</code>, becomes a bidirectional GRU. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_gru_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\begin{array}{ll}
r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\
h_t = (1 - z_t) n_t + z_t h_{(t-1)}
\end{array}
</code>
</p>

<p>where <code class="reqn">h_t</code> is the hidden state at time <code>t</code>, <code class="reqn">x_t</code> is the input
at time <code>t</code>, <code class="reqn">h_{(t-1)}</code> is the hidden state of the previous layer
at time <code>t-1</code> or the initial hidden state at time <code>0</code>, and <code class="reqn">r_t</code>,
<code class="reqn">z_t</code>, <code class="reqn">n_t</code> are the reset, update, and new gates, respectively.
<code class="reqn">\sigma</code> is the sigmoid function.
</p>


<h3>Inputs</h3>

<p>Inputs: input, h_0
</p>

<ul>
<li> <p><strong>input</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, input_size)&#8288;</code>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <code><a href="#topic+nn_utils_rnn_pack_padded_sequence">nn_utils_rnn_pack_padded_sequence()</a></code>
for details.
</p>
</li>
<li> <p><strong>h_0</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided.
</p>
</li></ul>



<h3>Outputs</h3>

<p>Outputs: output, h_n
</p>

<ul>
<li> <p><strong>output</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, num_directions * hidden_size)&#8288;</code>: tensor
containing the output features h_t from the last layer of the GRU,
for each t. If a <code>PackedSequence</code> has been
given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code>output$view(c(seq_len, batch, num_directions, hidden_size))</code>,
with forward and backward being direction <code>0</code> and <code>1</code> respectively.
Similarly, the directions can be separated in the packed case.
</p>
</li>
<li> <p><strong>h_n</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the hidden state for <code>t = seq_len</code>
Like <em>output</em>, the layers can be separated using
<code>h_n$view(num_layers, num_directions, batch, hidden_size)</code>.
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li> <p><code>weight_ih_l[k]</code> : the learnable input-hidden weights of the <code class="reqn">\mbox{k}^{th}</code> layer
(W_ir|W_iz|W_in), of shape <code style="white-space: pre;">&#8288;(3*hidden_size x input_size)&#8288;</code>
</p>
</li>
<li> <p><code>weight_hh_l[k]</code> : the learnable hidden-hidden weights of the <code class="reqn">\mbox{k}^{th}</code> layer
(W_hr|W_hz|W_hn), of shape <code style="white-space: pre;">&#8288;(3*hidden_size x hidden_size)&#8288;</code>
</p>
</li>
<li> <p><code>bias_ih_l[k]</code> : the learnable input-hidden bias of the <code class="reqn">\mbox{k}^{th}</code> layer
(b_ir|b_iz|b_in), of shape <code>(3*hidden_size)</code>
</p>
</li>
<li> <p><code>bias_hh_l[k]</code> : the learnable hidden-hidden bias of the <code class="reqn">\mbox{k}^{th}</code> layer
(b_hr|b_hz|b_hn), of shape <code>(3*hidden_size)</code>
</p>
</li></ul>



<h3>Note</h3>

<p>All the weights and biases are initialized from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>
where <code class="reqn">k = \frac{1}{\mbox{hidden\_size}}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

rnn &lt;- nn_gru(10, 20, 2)
input &lt;- torch_randn(5, 3, 10)
h0 &lt;- torch_randn(2, 3, 20)
output &lt;- rnn(input, h0)
}
</code></pre>

<hr>
<h2 id='nn_hardshrink'>Hardshwink module</h2><span id='topic+nn_hardshrink'></span>

<h3>Description</h3>

<p>Applies the hard shrinkage function element-wise:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_hardshrink(lambd = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_hardshrink_+3A_lambd">lambd</code></td>
<td>
<p>the <code class="reqn">\lambda</code> value for the Hardshrink formulation. Default: 0.5</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{HardShrink}(x) =
  \left\{ \begin{array}{ll}
x, &amp; \mbox{ if } x &gt; \lambda \\
x, &amp; \mbox{ if } x &lt; -\lambda \\
0, &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_hardshrink()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_hardsigmoid'>Hardsigmoid module</h2><span id='topic+nn_hardsigmoid'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_hardsigmoid()
</code></pre>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\mbox{Hardsigmoid}(x) = \left\{ \begin{array}{ll}
  0 &amp; \mbox{if~} x \le -3, \\
  1 &amp; \mbox{if~} x \ge +3, \\
  x / 6 + 1 / 2 &amp; \mbox{otherwise}
\end{array}
\right.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_hardsigmoid()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_hardswish'>Hardswish module</h2><span id='topic+nn_hardswish'></span>

<h3>Description</h3>

<p>Applies the hardswish function, element-wise, as described in the paper:
<a href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_hardswish()
</code></pre>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn"> \mbox{Hardswish}(x) = \left\{
  \begin{array}{ll}
  0 &amp; \mbox{if } x \le -3, \\
  x &amp; \mbox{if } x \ge +3, \\
  x \cdot (x + 3)/6 &amp; \mbox{otherwise}
  \end{array}
  \right. </code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
m &lt;- nn_hardswish()
input &lt;- torch_randn(2)
output &lt;- m(input)

## End(Not run)

}
</code></pre>

<hr>
<h2 id='nn_hardtanh'>Hardtanh module</h2><span id='topic+nn_hardtanh'></span>

<h3>Description</h3>

<p>Applies the HardTanh function element-wise
HardTanh is defined as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_hardtanh(min_val = -1, max_val = 1, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_hardtanh_+3A_min_val">min_val</code></td>
<td>
<p>minimum value of the linear region range. Default: -1</p>
</td></tr>
<tr><td><code id="nn_hardtanh_+3A_max_val">max_val</code></td>
<td>
<p>maximum value of the linear region range. Default: 1</p>
</td></tr>
<tr><td><code id="nn_hardtanh_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\mbox{HardTanh}(x) = \left\{ \begin{array}{ll}
  1 &amp; \mbox{ if } x &gt; 1 \\
  -1 &amp; \mbox{ if } x &lt; -1 \\
  x &amp; \mbox{ otherwise } \\
\end{array}
\right.
</code>
</p>

<p>The range of the linear region :math:<code style="white-space: pre;">&#8288;[-1, 1]&#8288;</code> can be adjusted using
<code>min_val</code> and <code>max_val</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_hardtanh(-2, 2)
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_hinge_embedding_loss'>Hinge embedding loss</h2><span id='topic+nn_hinge_embedding_loss'></span>

<h3>Description</h3>

<p>Measures the loss given an input tensor <code class="reqn">x</code> and a labels tensor <code class="reqn">y</code>
(containing 1 or -1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_hinge_embedding_loss(margin = 1, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_hinge_embedding_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): Has a default value of <code>1</code>.</p>
</td></tr>
<tr><td><code id="nn_hinge_embedding_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as <code class="reqn">x</code>, and is typically
used for learning nonlinear embeddings or semi-supervised learning.
The loss function for <code class="reqn">n</code>-th sample in the mini-batch is
</p>
<p style="text-align: center;"><code class="reqn">
  l_n = \begin{array}{ll}
x_n, &amp; \mbox{if}\; y_n = 1,\\
\max \{0, \Delta - x_n\}, &amp; \mbox{if}\; y_n = -1,
\end{array}
</code>
</p>

<p>and the total loss functions is
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>where <code class="reqn">L = \{l_1,\dots,l_N\}^\top</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code> where <code class="reqn">*</code> means, any number of dimensions. The sum operation
operates over all the elements.
</p>
</li>
<li><p> Target: <code class="reqn">(*)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then same shape as the input
</p>
</li></ul>


<hr>
<h2 id='nn_identity'>Identity module</h2><span id='topic+nn_identity'></span>

<h3>Description</h3>

<p>A placeholder identity operator that is argument-insensitive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_identity(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_identity_+3A_...">...</code></td>
<td>
<p>any arguments (unused)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_identity(54, unused_argument1 = 0.1, unused_argument2 = FALSE)
input &lt;- torch_randn(128, 20)
output &lt;- m(input)
print(output$size())
}
</code></pre>

<hr>
<h2 id='nn_init_calculate_gain'>Calculate gain</h2><span id='topic+nn_init_calculate_gain'></span>

<h3>Description</h3>

<p>Return the recommended gain value for the given nonlinearity function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_calculate_gain(nonlinearity, param = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_calculate_gain_+3A_nonlinearity">nonlinearity</code></td>
<td>
<p>the non-linear function</p>
</td></tr>
<tr><td><code id="nn_init_calculate_gain_+3A_param">param</code></td>
<td>
<p>optional parameter for the non-linear function</p>
</td></tr>
</table>

<hr>
<h2 id='nn_init_constant_'>Constant initialization</h2><span id='topic+nn_init_constant_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with the value <code>val</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_constant_(tensor, val)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_constant__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_constant__+3A_val">val</code></td>
<td>
<p>the value to fill the tensor with</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_constant_(w, 0.3)
}
</code></pre>

<hr>
<h2 id='nn_init_dirac_'>Dirac initialization</h2><span id='topic+nn_init_dirac_'></span>

<h3>Description</h3>

<p>Fills the {3, 4, 5}-dimensional input <code>Tensor</code> with the Dirac
delta function. Preserves the identity of the inputs in <code>Convolutional</code>
layers, where as many input channels are preserved as possible. In case
of groups&gt;1, each group of channels preserves identity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_dirac_(tensor, groups = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_dirac__+3A_tensor">tensor</code></td>
<td>
<p>a {3, 4, 5}-dimensional <code>torch.Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_dirac__+3A_groups">groups</code></td>
<td>
<p>(optional) number of groups in the conv layer (default: 1)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
w &lt;- torch_empty(3, 16, 5, 5)
nn_init_dirac_(w)

## End(Not run)

}
</code></pre>

<hr>
<h2 id='nn_init_eye_'>Eye initialization</h2><span id='topic+nn_init_eye_'></span>

<h3>Description</h3>

<p>Fills the 2-dimensional input <code>Tensor</code> with the identity matrix.
Preserves the identity of the inputs in <code>Linear</code> layers, where as
many inputs are preserved as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_eye_(tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_eye__+3A_tensor">tensor</code></td>
<td>
<p>a 2-dimensional torch tensor.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_eye_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_kaiming_normal_'>Kaiming normal initialization</h2><span id='topic+nn_init_kaiming_normal_'></span>

<h3>Description</h3>

<p>Fills the input <code>Tensor</code> with values according to the method
described in <code style="white-space: pre;">&#8288;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&#8288;</code> - He, K. et al. (2015), using a
normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_kaiming_normal_(
  tensor,
  a = 0,
  mode = "fan_in",
  nonlinearity = "leaky_relu"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_kaiming_normal__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>torch.Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_kaiming_normal__+3A_a">a</code></td>
<td>
<p>the negative slope of the rectifier used after this layer (only used
with <code>'leaky_relu'</code>)</p>
</td></tr>
<tr><td><code id="nn_init_kaiming_normal__+3A_mode">mode</code></td>
<td>
<p>either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves
the magnitude of the variance of the weights in the forward pass. Choosing
'fan_out' preserves the magnitudes in the backwards pass.</p>
</td></tr>
<tr><td><code id="nn_init_kaiming_normal__+3A_nonlinearity">nonlinearity</code></td>
<td>
<p>the non-linear function. recommended to use only with 'relu'
or 'leaky_relu' (default).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_kaiming_normal_(w, mode = "fan_in", nonlinearity = "leaky_relu")
}
</code></pre>

<hr>
<h2 id='nn_init_kaiming_uniform_'>Kaiming uniform initialization</h2><span id='topic+nn_init_kaiming_uniform_'></span>

<h3>Description</h3>

<p>Fills the input <code>Tensor</code> with values according to the method
described in <code style="white-space: pre;">&#8288;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&#8288;</code> - He, K. et al. (2015), using a
uniform distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_kaiming_uniform_(
  tensor,
  a = 0,
  mode = "fan_in",
  nonlinearity = "leaky_relu"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_kaiming_uniform__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>torch.Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_kaiming_uniform__+3A_a">a</code></td>
<td>
<p>the negative slope of the rectifier used after this layer (only used
with <code>'leaky_relu'</code>)</p>
</td></tr>
<tr><td><code id="nn_init_kaiming_uniform__+3A_mode">mode</code></td>
<td>
<p>either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves
the magnitude of the variance of the weights in the forward pass. Choosing
'fan_out' preserves the magnitudes in the backwards pass.</p>
</td></tr>
<tr><td><code id="nn_init_kaiming_uniform__+3A_nonlinearity">nonlinearity</code></td>
<td>
<p>the non-linear function. recommended to use only with 'relu'
or 'leaky_relu' (default).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_kaiming_uniform_(w, mode = "fan_in", nonlinearity = "leaky_relu")
}
</code></pre>

<hr>
<h2 id='nn_init_normal_'>Normal initialization</h2><span id='topic+nn_init_normal_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with values drawn from the normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_normal_(tensor, mean = 0, std = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_normal__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional Tensor</p>
</td></tr>
<tr><td><code id="nn_init_normal__+3A_mean">mean</code></td>
<td>
<p>the mean of the normal distribution</p>
</td></tr>
<tr><td><code id="nn_init_normal__+3A_std">std</code></td>
<td>
<p>the standard deviation of the normal distribution</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_normal_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_ones_'>Ones initialization</h2><span id='topic+nn_init_ones_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with the scalar value <code>1</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_ones_(tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_ones__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_ones_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_orthogonal_'>Orthogonal initialization</h2><span id='topic+nn_init_orthogonal_'></span>

<h3>Description</h3>

<p>Fills the input <code>Tensor</code> with a (semi) orthogonal matrix, as
described in <code style="white-space: pre;">&#8288;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&#8288;</code> - Saxe, A. et al. (2013). The input tensor must have
at least 2 dimensions, and for tensors with more than 2 dimensions the
trailing dimensions are flattened.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_orthogonal_(tensor, gain = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_orthogonal__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_orthogonal__+3A_gain">gain</code></td>
<td>
<p>optional scaling factor</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_orthogonal_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_sparse_'>Sparse initialization</h2><span id='topic+nn_init_sparse_'></span>

<h3>Description</h3>

<p>Fills the 2D input <code>Tensor</code> as a sparse matrix, where the
non-zero elements will be drawn from the normal distribution
as described in <code style="white-space: pre;">&#8288;Deep learning via Hessian-free optimization&#8288;</code> - Martens, J. (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_sparse_(tensor, sparsity, std = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_sparse__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_sparse__+3A_sparsity">sparsity</code></td>
<td>
<p>The fraction of elements in each column to be set to zero</p>
</td></tr>
<tr><td><code id="nn_init_sparse__+3A_std">std</code></td>
<td>
<p>the standard deviation of the normal distribution used to generate
the non-zero values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
w &lt;- torch_empty(3, 5)
nn_init_sparse_(w, sparsity = 0.1)

## End(Not run)
}
</code></pre>

<hr>
<h2 id='nn_init_trunc_normal_'>Truncated normal initialization</h2><span id='topic+nn_init_trunc_normal_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with values drawn from a truncated
normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_trunc_normal_(tensor, mean = 0, std = 1, a = -2, b = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_trunc_normal__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional Tensor</p>
</td></tr>
<tr><td><code id="nn_init_trunc_normal__+3A_mean">mean</code></td>
<td>
<p>the mean of the normal distribution</p>
</td></tr>
<tr><td><code id="nn_init_trunc_normal__+3A_std">std</code></td>
<td>
<p>the standard deviation of the normal distribution</p>
</td></tr>
<tr><td><code id="nn_init_trunc_normal__+3A_a">a</code></td>
<td>
<p>the minimum cutoff value</p>
</td></tr>
<tr><td><code id="nn_init_trunc_normal__+3A_b">b</code></td>
<td>
<p>the maximum cutoff value</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_trunc_normal_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_uniform_'>Uniform initialization</h2><span id='topic+nn_init_uniform_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with values drawn from the uniform distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_uniform_(tensor, a = 0, b = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_uniform__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional Tensor</p>
</td></tr>
<tr><td><code id="nn_init_uniform__+3A_a">a</code></td>
<td>
<p>the lower bound of the uniform distribution</p>
</td></tr>
<tr><td><code id="nn_init_uniform__+3A_b">b</code></td>
<td>
<p>the upper bound of the uniform distribution</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_uniform_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_xavier_normal_'>Xavier normal initialization</h2><span id='topic+nn_init_xavier_normal_'></span>

<h3>Description</h3>

<p>Fills the input <code>Tensor</code> with values according to the method
described in <code style="white-space: pre;">&#8288;Understanding the difficulty of training deep feedforward neural networks&#8288;</code> - Glorot, X. &amp; Bengio, Y. (2010), using a normal
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_xavier_normal_(tensor, gain = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_xavier_normal__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_xavier_normal__+3A_gain">gain</code></td>
<td>
<p>an optional scaling factor</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_xavier_normal_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_xavier_uniform_'>Xavier uniform initialization</h2><span id='topic+nn_init_xavier_uniform_'></span>

<h3>Description</h3>

<p>Fills the input <code>Tensor</code> with values according to the method
described in <code style="white-space: pre;">&#8288;Understanding the difficulty of training deep feedforward neural networks&#8288;</code> - Glorot, X. &amp; Bengio, Y. (2010), using a uniform
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_xavier_uniform_(tensor, gain = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_xavier_uniform__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional <code>Tensor</code></p>
</td></tr>
<tr><td><code id="nn_init_xavier_uniform__+3A_gain">gain</code></td>
<td>
<p>an optional scaling factor</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_xavier_uniform_(w)
}
</code></pre>

<hr>
<h2 id='nn_init_zeros_'>Zeros initialization</h2><span id='topic+nn_init_zeros_'></span>

<h3>Description</h3>

<p>Fills the input Tensor with the scalar value <code>0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_init_zeros_(tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_init_zeros__+3A_tensor">tensor</code></td>
<td>
<p>an n-dimensional tensor</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
w &lt;- torch_empty(3, 5)
nn_init_zeros_(w)
}
</code></pre>

<hr>
<h2 id='nn_kl_div_loss'>Kullback-Leibler divergence loss</h2><span id='topic+nn_kl_div_loss'></span>

<h3>Description</h3>

<p>The Kullback-Leibler divergence loss measure
<a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a>
is a useful distance measure for continuous distributions and is often
useful when performing direct regression over the space of (discretely sampled)
continuous output distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_kl_div_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_kl_div_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'batchmean'</code> | <code>'sum'</code> | <code>'mean'</code>.
<code>'none'</code>: no reduction will be applied.
<code>'batchmean'</code>: the sum of the output will be divided by batchsize.
<code>'sum'</code>: the output will be summed.
<code>'mean'</code>: the output will be divided by the number of elements in the output.
Default: <code>'mean'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>As with <code><a href="#topic+nn_nll_loss">nn_nll_loss()</a></code>, the <code>input</code> given is expected to contain
<em>log-probabilities</em> and is not restricted to a 2D Tensor.
</p>
<p>The targets are interpreted as <em>probabilities</em> by default, but could be considered
as <em>log-probabilities</em> with <code>log_target</code> set to <code>TRUE</code>.
</p>
<p>This criterion expects a <code>target</code> <code>Tensor</code> of the same size as the
<code>input</code> <code>Tensor</code>.
</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described
as:
</p>
<p style="text-align: center;"><code class="reqn">
  l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)
</code>
</p>

<p>where the index <code class="reqn">N</code> spans all dimensions of <code>input</code> and <code class="reqn">L</code> has the same
shape as <code>input</code>. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';} \\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>In default <code>reduction</code> mode <code>'mean'</code>, the losses are averaged for each minibatch
over observations <strong>as well as</strong> over dimensions. <code>'batchmean'</code> mode gives the
correct KL divergence where losses are averaged over batch dimension only.
<code>'mean'</code> mode's behavior will be changed to the same as <code>'batchmean'</code> in the next
major release.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar by default. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>,
the same shape as the input
</p>
</li></ul>



<h3>Note</h3>

<p><code>reduction</code> = <code>'mean'</code> doesn't return the true kl divergence value,
please use <code>reduction</code> = <code>'batchmean'</code> which aligns with KL math
definition.
In the next major release, <code>'mean'</code> will be changed to be the same as
<code>'batchmean'</code>.
</p>

<hr>
<h2 id='nn_l1_loss'>L1 loss</h2><span id='topic+nn_l1_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the mean absolute error (MAE) between each
element in the input <code class="reqn">x</code> and target <code class="reqn">y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_l1_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_l1_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described
as:
</p>
<p style="text-align: center;"><code class="reqn">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left| x_n - y_n \right|,
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then:
</p>
<p style="text-align: center;"><code class="reqn">
\ell(x, y) =
\begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p><code class="reqn">x</code> and <code class="reqn">y</code> are tensors of arbitrary shapes with a total
of <code class="reqn">n</code> elements each.
</p>
<p>The sum operation still operates over all the elements, and divides by <code class="reqn">n</code>.
The division by <code class="reqn">n</code> can be avoided if one sets <code>reduction = 'sum'</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then
<code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_l1_loss()
input &lt;- torch_randn(3, 5, requires_grad = TRUE)
target &lt;- torch_randn(3, 5)
output &lt;- loss(input, target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_layer_norm'>Layer normalization</h2><span id='topic+nn_layer_norm'></span>

<h3>Description</h3>

<p>Applies Layer Normalization over a mini-batch of inputs as described in
the paper <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_layer_norm(normalized_shape, eps = 1e-05, elementwise_affine = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_layer_norm_+3A_normalized_shape">normalized_shape</code></td>
<td>
<p>(int or list): input shape from an expected input
of size
<code class="reqn">[* \times \mbox{normalized\_shape}[0] \times \mbox{normalized\_shape}[1] \times \ldots \times \mbox{normalized\_shape}[-1]]</code>
If a single integer is used, it is treated as a singleton list, and this module will
normalize over the last dimension which is expected to be of that specific size.</p>
</td></tr>
<tr><td><code id="nn_layer_norm_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability. Default: 1e-5</p>
</td></tr>
<tr><td><code id="nn_layer_norm_+3A_elementwise_affine">elementwise_affine</code></td>
<td>
<p>a boolean value that when set to <code>TRUE</code>, this module
has learnable per-element affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
</code>
</p>

<p>The mean and standard-deviation are calculated separately over the last
certain number dimensions which have to be of the shape specified by
<code>normalized_shape</code>.
</p>
<p><code class="reqn">\gamma</code> and <code class="reqn">\beta</code> are learnable affine transform parameters of
<code>normalized_shape</code> if <code>elementwise_affine</code> is <code>TRUE</code>.
</p>
<p>The standard-deviation is calculated via the biased estimator, equivalent to
<code>torch_var(input, unbiased=FALSE)</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code> (same shape as input)
</p>
</li></ul>



<h3>Note</h3>

<p>Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code>affine</code> option, Layer Normalization applies per-element scale and
bias with <code>elementwise_affine</code>.
</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input &lt;- torch_randn(20, 5, 10, 10)
# With Learnable Parameters
m &lt;- nn_layer_norm(input$size()[-1])
# Without Learnable Parameters
m &lt;- nn_layer_norm(input$size()[-1], elementwise_affine = FALSE)
# Normalize over last two dimensions
m &lt;- nn_layer_norm(c(10, 10))
# Normalize over last dimension of size 10
m &lt;- nn_layer_norm(10)
# Activating the module
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_leaky_relu'>LeakyReLU module</h2><span id='topic+nn_leaky_relu'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_leaky_relu(negative_slope = 0.01, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_leaky_relu_+3A_negative_slope">negative_slope</code></td>
<td>
<p>Controls the angle of the negative slope. Default: 1e-2</p>
</td></tr>
<tr><td><code id="nn_leaky_relu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{LeakyReLU}(x) = \max(0, x) + \mbox{negative\_slope} * \min(0, x)
</code>
</p>

<p>or
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{LeakyRELU}(x) =
  \left\{ \begin{array}{ll}
x, &amp; \mbox{ if } x \geq 0 \\
\mbox{negative\_slope} \times x, &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_leaky_relu(0.1)
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_linear'>Linear module</h2><span id='topic+nn_linear'></span>

<h3>Description</h3>

<p>Applies a linear transformation to the incoming data: <code>y = xA^T + b</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_linear(in_features, out_features, bias = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_linear_+3A_in_features">in_features</code></td>
<td>
<p>size of each input sample</p>
</td></tr>
<tr><td><code id="nn_linear_+3A_out_features">out_features</code></td>
<td>
<p>size of each output sample</p>
</td></tr>
<tr><td><code id="nn_linear_+3A_bias">bias</code></td>
<td>
<p>If set to <code>FALSE</code>, the layer will not learn an additive bias.
Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Input: <code style="white-space: pre;">&#8288;(N, *, H_in)&#8288;</code> where <code>*</code> means any number of
additional dimensions and <code>H_in = in_features</code>.
</p>
</li>
<li><p> Output: <code style="white-space: pre;">&#8288;(N, *, H_out)&#8288;</code> where all but the last dimension
are the same shape as the input and :math:<code>H_out = out_features</code>.
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight: the learnable weights of the module of shape
<code style="white-space: pre;">&#8288;(out_features, in_features)&#8288;</code>. The values are
initialized from <code class="reqn">U(-\sqrt{k}, \sqrt{k})</code>s, where
<code class="reqn">k = \frac{1}{\mbox{in\_features}}</code>
</p>
</li>
<li><p> bias: the learnable bias of the module of shape <code class="reqn">(\mbox{out\_features})</code>.
If <code>bias</code> is <code>TRUE</code>, the values are initialized from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{1}{\mbox{in\_features}}</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_linear(20, 30)
input &lt;- torch_randn(128, 20)
output &lt;- m(input)
print(output$size())
}
</code></pre>

<hr>
<h2 id='nn_log_sigmoid'>LogSigmoid module</h2><span id='topic+nn_log_sigmoid'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)
 </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_log_sigmoid()
</code></pre>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_log_sigmoid()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_log_softmax'>LogSoftmax module</h2><span id='topic+nn_log_softmax'></span>

<h3>Description</h3>

<p>Applies the <code class="reqn">\log(\mbox{Softmax}(x))</code> function to an n-dimensional
input Tensor. The LogSoftmax formulation can be simplified as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_log_softmax(dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_log_softmax_+3A_dim">dim</code></td>
<td>
<p>(int): A dimension along which LogSoftmax will be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
</code>
</p>



<h3>Value</h3>

<p>a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(*)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_log_softmax(1)
input &lt;- torch_randn(2, 3)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_lp_pool1d'>Applies a 1D power-average pooling over an input signal composed of several input
planes.</h2><span id='topic+nn_lp_pool1d'></span>

<h3>Description</h3>

<p>On each window, the function computed is:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_lp_pool1d(norm_type, kernel_size, stride = NULL, ceil_mode = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_lp_pool1d_+3A_norm_type">norm_type</code></td>
<td>
<p>if inf than one gets max pooling if 0 you get sum pooling (
proportional to the avg pooling)</p>
</td></tr>
<tr><td><code id="nn_lp_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>a single int, the size of the window</p>
</td></tr>
<tr><td><code id="nn_lp_pool1d_+3A_stride">stride</code></td>
<td>
<p>a single int, the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_lp_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}
</code>
</p>


<ul>
<li><p> At p = <code class="reqn">\infty</code>, one gets Max Pooling
</p>
</li>
<li><p> At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, L_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, L_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  L_{out} = \left\lfloor\frac{L_{in} - \mbox{kernel\_size}}{\mbox{stride}} + 1\right\rfloor
</code>
</p>



<h3>Note</h3>

<p>If the sum to the power of <code>p</code> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# power-2 pool of window of length 3, with stride 2.
m &lt;- nn_lp_pool1d(2, 3, stride = 2)
input &lt;- torch_randn(20, 16, 50)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_lp_pool2d'>Applies a 2D power-average pooling over an input signal composed of several input
planes.</h2><span id='topic+nn_lp_pool2d'></span>

<h3>Description</h3>

<p>On each window, the function computed is:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_lp_pool2d(norm_type, kernel_size, stride = NULL, ceil_mode = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_lp_pool2d_+3A_norm_type">norm_type</code></td>
<td>
<p>if inf than one gets max pooling if 0 you get sum pooling (
proportional to the avg pooling)</p>
</td></tr>
<tr><td><code id="nn_lp_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window</p>
</td></tr>
<tr><td><code id="nn_lp_pool2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_lp_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}
</code>
</p>


<ul>
<li><p> At p = <code class="reqn">\infty</code>, one gets Max Pooling
</p>
</li>
<li><p> At p = 1, one gets Sum Pooling (which is proportional to average pooling)
</p>
</li></ul>

<p>The parameters <code>kernel_size</code>, <code>stride</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H_{out}, W_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in} - \mbox{kernel\_size}[0]}{\mbox{stride}[0]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in} - \mbox{kernel\_size}[1]}{\mbox{stride}[1]} + 1\right\rfloor
</code>
</p>



<h3>Note</h3>

<p>If the sum to the power of <code>p</code> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# power-2 pool of square window of size=3, stride=2
m &lt;- nn_lp_pool2d(2, 3, stride = 2)
# pool of non-square window of power 1.2
m &lt;- nn_lp_pool2d(1.2, c(3, 2), stride = c(2, 1))
input &lt;- torch_randn(20, 16, 50, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_lstm'>Applies a multi-layer long short-term memory (LSTM) RNN to an input
sequence.</h2><span id='topic+nn_lstm'></span>

<h3>Description</h3>

<p>For each element in the input sequence, each layer computes the following
function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_lstm(
  input_size,
  hidden_size,
  num_layers = 1,
  bias = TRUE,
  batch_first = FALSE,
  dropout = 0,
  bidirectional = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_lstm_+3A_input_size">input_size</code></td>
<td>
<p>The number of expected features in the input <code>x</code></p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_hidden_size">hidden_size</code></td>
<td>
<p>The number of features in the hidden state <code>h</code></p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_num_layers">num_layers</code></td>
<td>
<p>Number of recurrent layers. E.g., setting <code>num_layers=2</code>
would mean stacking two LSTMs together to form a <code style="white-space: pre;">&#8288;stacked LSTM&#8288;</code>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_bias">bias</code></td>
<td>
<p>If <code>FALSE</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>.
Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_batch_first">batch_first</code></td>
<td>
<p>If <code>TRUE</code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_dropout">dropout</code></td>
<td>
<p>If non-zero, introduces a <code>Dropout</code> layer on the outputs of each
LSTM layer except the last layer, with dropout probability equal to
<code>dropout</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_bidirectional">bidirectional</code></td>
<td>
<p>If <code>TRUE</code>, becomes a bidirectional LSTM. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_lstm_+3A_...">...</code></td>
<td>
<p>currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\begin{array}{ll} \\
i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\
o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t c_{(t-1)} + i_t g_t \\
h_t = o_t \tanh(c_t) \\
\end{array}
</code>
</p>

<p>where <code class="reqn">h_t</code> is the hidden state at time <code>t</code>, <code class="reqn">c_t</code> is the cell
state at time <code>t</code>, <code class="reqn">x_t</code> is the input at time <code>t</code>, <code class="reqn">h_{(t-1)}</code>
is the hidden state of the previous layer at time <code>t-1</code> or the initial hidden
state at time <code>0</code>, and <code class="reqn">i_t</code>, <code class="reqn">f_t</code>, <code class="reqn">g_t</code>,
<code class="reqn">o_t</code> are the input, forget, cell, and output gates, respectively.
<code class="reqn">\sigma</code> is the sigmoid function.
</p>


<h3>Inputs</h3>

<p>Inputs: input, (h_0, c_0)
</p>

<ul>
<li> <p><strong>input</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, input_size)&#8288;</code>: tensor containing the features
of the input sequence.
The input can also be a packed variable length sequence.
See <code><a href="#topic+nn_utils_rnn_pack_padded_sequence">nn_utils_rnn_pack_padded_sequence()</a></code> or
<code><a href="#topic+nn_utils_rnn_pack_sequence">nn_utils_rnn_pack_sequence()</a></code> for details.
</p>
</li>
<li> <p><strong>h_0</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the initial hidden state for each element in the batch.
</p>
</li>
<li> <p><strong>c_0</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the initial cell state for each element in the batch.
</p>
</li></ul>

<p>If <code style="white-space: pre;">&#8288;(h_0, c_0)&#8288;</code> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.
</p>


<h3>Outputs</h3>

<p>Outputs: output, (h_n, c_n)
</p>

<ul>
<li> <p><strong>output</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, num_directions * hidden_size)&#8288;</code>: tensor
containing the output features <code>(h_t)</code> from the last layer of the LSTM,
for each t. If a <code>torch_nn.utils.rnn.PackedSequence</code> has been
given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code>output$view(c(seq_len, batch, num_directions, hidden_size))</code>,
with forward and backward being direction <code>0</code> and <code>1</code> respectively.
Similarly, the directions can be separated in the packed case.
</p>
</li>
<li> <p><strong>h_n</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the hidden state for <code>t = seq_len</code>.
Like <em>output</em>, the layers can be separated using
<code>h_n$view(c(num_layers, num_directions, batch, hidden_size))</code> and similarly for <em>c_n</em>.
</p>
</li>
<li> <p><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the cell state for <code>t = seq_len</code>
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li> <p><code>weight_ih_l[k]</code> : the learnable input-hidden weights of the <code class="reqn">\mbox{k}^{th}</code> layer
<code>(W_ii|W_if|W_ig|W_io)</code>, of shape <code style="white-space: pre;">&#8288;(4*hidden_size x input_size)&#8288;</code>
</p>
</li>
<li> <p><code>weight_hh_l[k]</code> : the learnable hidden-hidden weights of the <code class="reqn">\mbox{k}^{th}</code> layer
<code>(W_hi|W_hf|W_hg|W_ho)</code>, of shape <code style="white-space: pre;">&#8288;(4*hidden_size x hidden_size)&#8288;</code>
</p>
</li>
<li> <p><code>bias_ih_l[k]</code> : the learnable input-hidden bias of the <code class="reqn">\mbox{k}^{th}</code> layer
<code>(b_ii|b_if|b_ig|b_io)</code>, of shape <code>(4*hidden_size)</code>
</p>
</li>
<li> <p><code>bias_hh_l[k]</code> : the learnable hidden-hidden bias of the <code class="reqn">\mbox{k}^{th}</code> layer
<code>(b_hi|b_hf|b_hg|b_ho)</code>, of shape <code>(4*hidden_size)</code>
</p>
</li></ul>



<h3>Note</h3>

<p>All the weights and biases are initialized from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>
where <code class="reqn">k = \frac{1}{\mbox{hidden\_size}}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
rnn &lt;- nn_lstm(10, 20, 2)
input &lt;- torch_randn(5, 3, 10)
h0 &lt;- torch_randn(2, 3, 20)
c0 &lt;- torch_randn(2, 3, 20)
output &lt;- rnn(input, list(h0, c0))
}
</code></pre>

<hr>
<h2 id='nn_margin_ranking_loss'>Margin ranking loss</h2><span id='topic+nn_margin_ranking_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the loss given
inputs <code class="reqn">x1</code>, <code class="reqn">x2</code>, two 1D mini-batch <code>Tensors</code>,
and a label 1D mini-batch tensor <code class="reqn">y</code> (containing 1 or -1).
If <code class="reqn">y = 1</code> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <code class="reqn">y = -1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_margin_ranking_loss(margin = 0, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_margin_ranking_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): Has a default value of <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="nn_margin_ranking_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The loss function for each pair of samples in the mini-batch is:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \mbox{margin})
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input1: <code class="reqn">(N)</code> where <code>N</code> is the batch size.
</p>
</li>
<li><p> Input2: <code class="reqn">(N)</code>, same shape as the Input1.
</p>
</li>
<li><p> Target: <code class="reqn">(N)</code>, same shape as the inputs.
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N)</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_margin_ranking_loss()
input1 &lt;- torch_randn(3, requires_grad = TRUE)
input2 &lt;- torch_randn(3, requires_grad = TRUE)
target &lt;- torch_randn(3)$sign()
output &lt;- loss(input1, input2, target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_max_pool1d'>MaxPool1D module</h2><span id='topic+nn_max_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D max pooling over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_pool1d(
  kernel_size,
  stride = NULL,
  padding = 0,
  dilation = 1,
  return_indices = FALSE,
  ceil_mode = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over</p>
</td></tr>
<tr><td><code id="nn_max_pool1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_max_pool1d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides</p>
</td></tr>
<tr><td><code id="nn_max_pool1d_+3A_dilation">dilation</code></td>
<td>
<p>a parameter that controls the stride of elements in the window</p>
</td></tr>
<tr><td><code id="nn_max_pool1d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the max indices along with the outputs.
Useful for  <code>nn_max_unpool1d()</code> later.</p>
</td></tr>
<tr><td><code id="nn_max_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when <code>TRUE</code>, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, L)</code>
and output <code class="reqn">(N, C, L_{out})</code> can be precisely described as:
</p>
<p style="text-align: center;"><code class="reqn">
  out(N_i, C_j, k) = \max_{m=0, \ldots, \mbox{kernel\_size} - 1}
input(N_i, C_j, stride \times k + m)
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points.
It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code>dilation</code> does.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, L_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, L_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  L_{out} = \left\lfloor \frac{L_{in} + 2 \times \mbox{padding} - \mbox{dilation}
    \times (\mbox{kernel\_size} - 1) - 1}{\mbox{stride}} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# pool of size=3, stride=2
m &lt;- nn_max_pool1d(3, stride = 2)
input &lt;- torch_randn(20, 16, 50)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_max_pool2d'>MaxPool2D module</h2><span id='topic+nn_max_pool2d'></span>

<h3>Description</h3>

<p>Applies a 2D max pooling over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_pool2d(
  kernel_size,
  stride = NULL,
  padding = 0,
  dilation = 1,
  return_indices = FALSE,
  ceil_mode = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over</p>
</td></tr>
<tr><td><code id="nn_max_pool2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_max_pool2d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides</p>
</td></tr>
<tr><td><code id="nn_max_pool2d_+3A_dilation">dilation</code></td>
<td>
<p>a parameter that controls the stride of elements in the window</p>
</td></tr>
<tr><td><code id="nn_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the max indices along with the outputs.
Useful for <code>nn_max_unpool2d()</code> later.</p>
</td></tr>
<tr><td><code id="nn_max_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when <code>TRUE</code>, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, H, W)</code>,
output <code class="reqn">(N, C, H_{out}, W_{out})</code> and <code>kernel_size</code> <code class="reqn">(kH, kW)</code>
can be precisely described as:
</p>
<p style="text-align: center;"><code class="reqn">
\begin{array}{ll}
out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
&amp; \mbox{input}(N_i, C_j, \mbox{stride[0]} \times h + m,
               \mbox{stride[1]} \times w + n)
\end{array}
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points.
It is harder to describe, but this <code>link</code> has a nice visualization of what <code>dilation</code> does.
</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H_{out}, W_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in} + 2 * \mbox{padding[0]} - \mbox{dilation[0]}
    \times (\mbox{kernel\_size[0]} - 1) - 1}{\mbox{stride[0]}} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in} + 2 * \mbox{padding[1]} - \mbox{dilation[1]}
    \times (\mbox{kernel\_size[1]} - 1) - 1}{\mbox{stride[1]}} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# pool of square window of size=3, stride=2
m &lt;- nn_max_pool2d(3, stride = 2)
# pool of non-square window
m &lt;- nn_max_pool2d(c(3, 2), stride = c(2, 1))
input &lt;- torch_randn(20, 16, 50, 32)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_max_pool3d'>Applies a 3D max pooling over an input signal composed of several input
planes.</h2><span id='topic+nn_max_pool3d'></span>

<h3>Description</h3>

<p>In the simplest case, the output value of the layer with input size <code class="reqn">(N, C, D, H, W)</code>,
output <code class="reqn">(N, C, D_{out}, H_{out}, W_{out})</code> and <code>kernel_size</code> <code class="reqn">(kD, kH, kW)</code>
can be precisely described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_pool3d(
  kernel_size,
  stride = NULL,
  padding = 0,
  dilation = 1,
  return_indices = FALSE,
  ceil_mode = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over</p>
</td></tr>
<tr><td><code id="nn_max_pool3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Default value is <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nn_max_pool3d_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on all three sides</p>
</td></tr>
<tr><td><code id="nn_max_pool3d_+3A_dilation">dilation</code></td>
<td>
<p>a parameter that controls the stride of elements in the window</p>
</td></tr>
<tr><td><code id="nn_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>TRUE</code>, will return the max indices along with the outputs.
Useful for <code>torch_nn.MaxUnpool3d</code> later</p>
</td></tr>
<tr><td><code id="nn_max_pool3d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when TRUE, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\begin{array}{ll}
\mbox{out}(N_i, C_j, d, h, w) = &amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
 &amp; \mbox{input}(N_i, C_j, \mbox{stride[0]} \times d + k, \mbox{stride[1]} \times h + m, \mbox{stride[2]} \times w + n)
\end{array}
</code>
</p>

<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points.
It is harder to describe, but this <code>link</code>_ has a nice visualization of what <code>dilation</code> does.
The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:
</p>

<ul>
<li><p> a single <code>int</code> &ndash; in which case the same value is used for the depth, height and width dimension
</p>
</li>
<li><p> a <code>tuple</code> of three ints &ndash; in which case, the first <code>int</code> is used for the depth dimension,
the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, D_{out}, H_{out}, W_{out})</code>, where
</p>
<p style="text-align: center;"><code class="reqn">
  D_{out} = \left\lfloor\frac{D_{in} + 2 \times \mbox{padding}[0] - \mbox{dilation}[0] \times
    (\mbox{kernel\_size}[0] - 1) - 1}{\mbox{stride}[0]} + 1\right\rfloor
</code>
</p>

</li></ul>

<p style="text-align: center;"><code class="reqn">
  H_{out} = \left\lfloor\frac{H_{in} + 2 \times \mbox{padding}[1] - \mbox{dilation}[1] \times
    (\mbox{kernel\_size}[1] - 1) - 1}{\mbox{stride}[1]} + 1\right\rfloor
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = \left\lfloor\frac{W_{in} + 2 \times \mbox{padding}[2] - \mbox{dilation}[2] \times
    (\mbox{kernel\_size}[2] - 1) - 1}{\mbox{stride}[2]} + 1\right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# pool of square window of size=3, stride=2
m &lt;- nn_max_pool3d(3, stride = 2)
# pool of non-square window
m &lt;- nn_max_pool3d(c(3, 2, 2), stride = c(2, 1, 2))
input &lt;- torch_randn(20, 16, 50, 44, 31)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_max_unpool1d'>Computes a partial inverse of <code>MaxPool1d</code>.</h2><span id='topic+nn_max_unpool1d'></span>

<h3>Description</h3>

<p><code>MaxPool1d</code> is not fully invertible, since the non-maximal values are lost.
<code>MaxUnpool1d</code> takes in as input the output of <code>MaxPool1d</code>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_unpool1d(kernel_size, stride = NULL, padding = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_unpool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nn_max_unpool1d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple): Stride of the max pooling window.
It is set to <code>kernel_size</code> by default.</p>
</td></tr>
<tr><td><code id="nn_max_unpool1d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple): Padding that was added to the input</p>
</td></tr>
</table>


<h3>Inputs</h3>


<ul>
<li> <p><code>input</code>: the input Tensor to invert
</p>
</li>
<li> <p><code>indices</code>: the indices given out by <code><a href="#topic+nn_max_pool1d">nn_max_pool1d()</a></code>
</p>
</li>
<li> <p><code>output_size</code> (optional): the targeted output size
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H_{out})</code>, where
</p>
<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride}[0] - 2 \times \mbox{padding}[0] + \mbox{kernel\_size}[0]
</code>
</p>

<p>or as given by <code>output_size</code> in the call operator
</p>
</li></ul>



<h3>Note</h3>

<p><code>MaxPool1d</code> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code>output_size</code> in the forward call.
See the Inputs and Example below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
pool &lt;- nn_max_pool1d(2, stride = 2, return_indices = TRUE)
unpool &lt;- nn_max_unpool1d(2, stride = 2)

input &lt;- torch_tensor(array(1:8 / 1, dim = c(1, 1, 8)))
out &lt;- pool(input)
unpool(out[[1]], out[[2]])

# Example showcasing the use of output_size
input &lt;- torch_tensor(array(1:8 / 1, dim = c(1, 1, 8)))
out &lt;- pool(input)
unpool(out[[1]], out[[2]], output_size = input$size())
unpool(out[[1]], out[[2]])
}
</code></pre>

<hr>
<h2 id='nn_max_unpool2d'>Computes a partial inverse of <code>MaxPool2d</code>.</h2><span id='topic+nn_max_unpool2d'></span>

<h3>Description</h3>

<p><code>MaxPool2d</code> is not fully invertible, since the non-maximal values are lost.
<code>MaxUnpool2d</code> takes in as input the output of <code>MaxPool2d</code>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_unpool2d(kernel_size, stride = NULL, padding = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_unpool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nn_max_unpool2d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple): Stride of the max pooling window.
It is set to <code>kernel_size</code> by default.</p>
</td></tr>
<tr><td><code id="nn_max_unpool2d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple): Padding that was added to the input</p>
</td></tr>
</table>


<h3>Inputs</h3>


<ul>
<li> <p><code>input</code>: the input Tensor to invert
</p>
</li>
<li> <p><code>indices</code>: the indices given out by <code><a href="#topic+nn_max_pool2d">nn_max_pool2d()</a></code>
</p>
</li>
<li> <p><code>output_size</code> (optional): the targeted output size
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H_{out}, W_{out})</code>, where
</p>
<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride[0]} - 2 \times \mbox{padding[0]} + \mbox{kernel\_size[0]}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = (W_{in} - 1) \times \mbox{stride[1]} - 2 \times \mbox{padding[1]} + \mbox{kernel\_size[1]}
</code>
</p>

<p>or as given by <code>output_size</code> in the call operator
</p>
</li></ul>



<h3>Note</h3>

<p><code>MaxPool2d</code> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code>output_size</code> in the forward call.
See the Inputs and Example below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

pool &lt;- nn_max_pool2d(2, stride = 2, return_indices = TRUE)
unpool &lt;- nn_max_unpool2d(2, stride = 2)
input &lt;- torch_randn(1, 1, 4, 4)
out &lt;- pool(input)
unpool(out[[1]], out[[2]])

# specify a different output size than input size
unpool(out[[1]], out[[2]], output_size = c(1, 1, 5, 5))
}
</code></pre>

<hr>
<h2 id='nn_max_unpool3d'>Computes a partial inverse of <code>MaxPool3d</code>.</h2><span id='topic+nn_max_unpool3d'></span>

<h3>Description</h3>

<p><code>MaxPool3d</code> is not fully invertible, since the non-maximal values are lost.
<code>MaxUnpool3d</code> takes in as input the output of <code>MaxPool3d</code>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_max_unpool3d(kernel_size, stride = NULL, padding = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_max_unpool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple): Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nn_max_unpool3d_+3A_stride">stride</code></td>
<td>
<p>(int or tuple): Stride of the max pooling window.
It is set to <code>kernel_size</code> by default.</p>
</td></tr>
<tr><td><code id="nn_max_unpool3d_+3A_padding">padding</code></td>
<td>
<p>(int or tuple): Padding that was added to the input</p>
</td></tr>
</table>


<h3>Inputs</h3>


<ul>
<li> <p><code>input</code>: the input Tensor to invert
</p>
</li>
<li> <p><code>indices</code>: the indices given out by <code><a href="#topic+nn_max_pool3d">nn_max_pool3d()</a></code>
</p>
</li>
<li> <p><code>output_size</code> (optional): the targeted output size
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, D_{out}, H_{out}, W_{out})</code>, where
</p>
</li></ul>

<p style="text-align: center;"><code class="reqn">
  D_{out} = (D_{in} - 1) \times \mbox{stride[0]} - 2 \times \mbox{padding[0]} + \mbox{kernel\_size[0]}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride[1]} - 2 \times \mbox{padding[1]} + \mbox{kernel\_size[1]}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = (W_{in} - 1) \times \mbox{stride[2]} - 2 \times \mbox{padding[2]} + \mbox{kernel\_size[2]}
</code>
</p>

<p>or as given by <code>output_size</code> in the call operator
</p>


<h3>Note</h3>

<p><code>MaxPool3d</code> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code>output_size</code> in the forward call.
See the Inputs section below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# pool of square window of size=3, stride=2
pool &lt;- nn_max_pool3d(3, stride = 2, return_indices = TRUE)
unpool &lt;- nn_max_unpool3d(3, stride = 2)
out &lt;- pool(torch_randn(20, 16, 51, 33, 15))
unpooled_output &lt;- unpool(out[[1]], out[[2]])
unpooled_output$size()
}
</code></pre>

<hr>
<h2 id='nn_module'>Base class for all neural network modules.</h2><span id='topic+nn_module'></span>

<h3>Description</h3>

<p>Your models should also subclass this class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_module(
  classname = NULL,
  inherit = nn_Module,
  ...,
  private = NULL,
  active = NULL,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_module_+3A_classname">classname</code></td>
<td>
<p>an optional name for the module</p>
</td></tr>
<tr><td><code id="nn_module_+3A_inherit">inherit</code></td>
<td>
<p>an optional module to inherit from</p>
</td></tr>
<tr><td><code id="nn_module_+3A_...">...</code></td>
<td>
<p>methods implementation</p>
</td></tr>
<tr><td><code id="nn_module_+3A_private">private</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="nn_module_+3A_active">active</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
<tr><td><code id="nn_module_+3A_parent_env">parent_env</code></td>
<td>
<p>passed to <code><a href="R6.html#topic+R6Class">R6::R6Class()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Modules can also contain other Modules, allowing to nest them in a tree
structure. You can assign the submodules as regular attributes.
</p>
<p>You are expected to implement the <code>initialize</code> and the <code>forward</code> to create a
new <code>nn_module</code>.
</p>


<h3>Initialize</h3>

<p>The initialize function will be called whenever a new instance of the <code>nn_module</code>
is created. We use the initialize functions to define submodules and parameters
of the module. For example:
</p>
<div class="sourceCode"><pre>initialize = function(input_size, output_size) {
   self$conv1 &lt;- nn_conv2d(input_size, output_size, 5)
   self$conv2 &lt;- nn_conv2d(output_size, output_size, 5)
}
</pre></div>
<p>The initialize function can have any number of parameters. All objects
assigned to <code style="white-space: pre;">&#8288;self$&#8288;</code> will be available for other methods that you implement.
Tensors wrapped with <code><a href="#topic+nn_parameter">nn_parameter()</a></code> or <code><a href="#topic+nn_buffer">nn_buffer()</a></code> and submodules are
automatically tracked when assigned to <code style="white-space: pre;">&#8288;self$&#8288;</code>.
</p>
<p>The initialize function is optional if the module you are defining doesn't
have weights, submodules or buffers.
</p>


<h3>Forward</h3>

<p>The forward method is called whenever an instance of <code>nn_module</code> is called.
This is usually used to implement the computation that the module does with
the weights ad submodules defined in the <code>initialize</code> function.
</p>
<p>For example:
</p>
<div class="sourceCode"><pre>forward = function(input) {
   input &lt;- self$conv1(input)
   input &lt;- nnf_relu(input)
   input &lt;- self$conv2(input)
   input &lt;- nnf_relu(input)
   input
 }
</pre></div>
<p>The <code>forward</code> function can use the <code>self$training</code> attribute to make different
computations depending wether the model is training or not, for example if you
were implementing the dropout module.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
model &lt;- nn_module(
  initialize = function() {
    self$conv1 &lt;- nn_conv2d(1, 20, 5)
    self$conv2 &lt;- nn_conv2d(20, 20, 5)
  },
  forward = function(input) {
    input &lt;- self$conv1(input)
    input &lt;- nnf_relu(input)
    input &lt;- self$conv2(input)
    input &lt;- nnf_relu(input)
    input
  }
)
}
</code></pre>

<hr>
<h2 id='nn_module_dict'>Container that allows named values</h2><span id='topic+nn_module_dict'></span>

<h3>Description</h3>

<p>Container that allows named values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_module_dict(dict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_module_dict_+3A_dict">dict</code></td>
<td>
<p>A named list of submodules that will be saved in that module.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+nn_module_list">nn_module_list()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
nn_module &lt;- nn_module(
  initialize = function() {
    self$dict &lt;- nn_module_dict(list(
      l1 = nn_linear(10, 20),
      l2 = nn_linear(20, 10)
    ))
  },
  forward = function(x) {
    x &lt;- self$dict$l1(x)
    self$dict$l2(x)
  }
)
}
</code></pre>

<hr>
<h2 id='nn_module_list'>Holds submodules in a list.</h2><span id='topic+nn_module_list'></span>

<h3>Description</h3>

<p><a href="#topic+nn_module_list">nn_module_list</a> can be indexed like a regular R list, but
modules it contains are properly registered, and will be visible by all
<code>nn_module</code> methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_module_list(modules = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_module_list_+3A_modules">modules</code></td>
<td>
<p>a list of modules to add</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+nn_module_dict">nn_module_dict()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

my_module &lt;- nn_module(
  initialize = function() {
    self$linears &lt;- nn_module_list(lapply(1:10, function(x) nn_linear(10, 10)))
  },
  forward = function(x) {
    for (i in 1:length(self$linears)) {
      x &lt;- self$linears[[i]](x)
    }
    x
  }
)
}
</code></pre>

<hr>
<h2 id='nn_mse_loss'>MSE loss</h2><span id='topic+nn_mse_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the mean squared error (squared L2 norm) between
each element in the input <code class="reqn">x</code> and target <code class="reqn">y</code>.
The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described
as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_mse_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_mse_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) =
  \begin{array}{ll}
\mbox{mean}(L), &amp;  \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp;  \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p><code class="reqn">x</code> and <code class="reqn">y</code> are tensors of arbitrary shapes with a total
of <code class="reqn">n</code> elements each.
</p>
<p>The mean operation still operates over all the elements, and divides by <code class="reqn">n</code>.
The division by <code class="reqn">n</code> can be avoided if one sets <code>reduction = 'sum'</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_mse_loss()
input &lt;- torch_randn(3, 5, requires_grad = TRUE)
target &lt;- torch_randn(3, 5)
output &lt;- loss(input, target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_multi_margin_loss'>Multi margin loss</h2><span id='topic+nn_multi_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-class classification hinge
loss (margin-based loss) between input <code class="reqn">x</code> (a 2D mini-batch <code>Tensor</code>) and
output <code class="reqn">y</code> (which is a 1D tensor of target class indices,
<code class="reqn">0 \leq y \leq \mbox{x.size}(1)-1</code>):
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_multi_margin_loss(p = 1, margin = 1, weight = NULL, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_multi_margin_loss_+3A_p">p</code></td>
<td>
<p>(int, optional): Has a default value of <code class="reqn">1</code>. <code class="reqn">1</code> and <code class="reqn">2</code>
are the only supported values.</p>
</td></tr>
<tr><td><code id="nn_multi_margin_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): Has a default value of <code class="reqn">1</code>.</p>
</td></tr>
<tr><td><code id="nn_multi_margin_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is
treated as if having all ones.</p>
</td></tr>
<tr><td><code id="nn_multi_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each mini-batch sample, the loss in terms of the 1D input <code class="reqn">x</code> and scalar
output <code class="reqn">y</code> is:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) = \frac{\sum_i \max(0, \mbox{margin} - x[y] + x[i]))^p}{\mbox{x.size}(0)}
</code>
</p>

<p>where <code class="reqn">x \in \left\{0, \; \cdots , \; \mbox{x.size}(0) - 1\right\}</code>
and <code class="reqn">i \neq y</code>.
</p>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <code>weight</code> tensor into the constructor.
The loss function then becomes:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\mbox{margin} - x[y] + x[i]))^p)}{\mbox{x.size}(0)}
</code>
</p>


<hr>
<h2 id='nn_multihead_attention'>MultiHead attention</h2><span id='topic+nn_multihead_attention'></span>

<h3>Description</h3>

<p>Allows the model to jointly attend to information from different
representation subspaces. See reference: Attention Is All You Need
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_multihead_attention(
  embed_dim,
  num_heads,
  dropout = 0,
  bias = TRUE,
  add_bias_kv = FALSE,
  add_zero_attn = FALSE,
  kdim = NULL,
  vdim = NULL,
  batch_first = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_multihead_attention_+3A_embed_dim">embed_dim</code></td>
<td>
<p>total dimension of the model.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_num_heads">num_heads</code></td>
<td>
<p>parallel attention heads. Note that <code>embed_dim</code> will be split
across <code>num_heads</code> (i.e. each head will have dimension <code>embed_dim %/% num_heads</code>).</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_dropout">dropout</code></td>
<td>
<p>a Dropout layer on attn_output_weights. Default: 0.0.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_bias">bias</code></td>
<td>
<p>add bias as module parameter. Default: True.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_add_bias_kv">add_bias_kv</code></td>
<td>
<p>add bias to the key and value sequences at dim=0.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_add_zero_attn">add_zero_attn</code></td>
<td>
<p>add a new batch of zeros to the key and value sequences
at dim=1.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_kdim">kdim</code></td>
<td>
<p>total number of features in key. Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_vdim">vdim</code></td>
<td>
<p>total number of features in value. Default: <code>NULL</code>. Note: if kdim
and vdim are <code>NULL</code>, they will be set to embed_dim such that query, key,
and value have the same number of features.</p>
</td></tr>
<tr><td><code id="nn_multihead_attention_+3A_batch_first">batch_first</code></td>
<td>
<p>if <code>TRUE</code> then the input and output tensors are <code class="reqn">(N,
  S, E)</code> instead of <code class="reqn">(S, N, E)</code>, where N is the batch size, S is the
sequence length, and E is the embedding dimension.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn"> \mbox{MultiHead}(Q, K, V) = \mbox{Concat}(head_1,\dots,head_h)W^O
\mbox{where} head_i = \mbox{Attention}(QW_i^Q, KW_i^K, VW_i^V) </code>
</p>



<h3>Shape</h3>

<p>Inputs:
</p>

<ul>
<li><p> query: <code class="reqn">(L, N, E)</code> where L is the target sequence length, N is the
batch size, E is the embedding dimension. (but see the <code>batch_first</code>
argument)
</p>
</li>
<li><p> key: <code class="reqn">(S, N, E)</code>, where S is the source sequence length, N is the
batch size, E is the embedding dimension. (but see the <code>batch_first</code>
argument)
</p>
</li>
<li><p> value: <code class="reqn">(S, N, E)</code> where S is the source sequence length,
N is the batch size, E is the embedding dimension. (but see the
<code>batch_first</code> argument)
</p>
</li>
<li><p> key_padding_mask: <code class="reqn">(N, S)</code> where N is the batch size, S is the source
sequence length. If a ByteTensor is provided, the non-zero positions will
be ignored while the position with the zero positions will be unchanged. If
a BoolTensor is provided, the positions with the value of <code>True</code> will be
ignored while the position with the value of <code>False</code> will be unchanged.
</p>
</li>
<li><p> attn_mask: 2D mask <code class="reqn">(L, S)</code> where L is the target sequence length, S
is the source sequence length. 3D mask <code class="reqn">(N*num_heads, L, S)</code> where N is
the batch size, L is the target sequence length, S is the source sequence
length. attn_mask ensure that position i is allowed to attend the unmasked
positions. If a ByteTensor is provided, the non-zero positions are not
allowed to attend while the zero positions will be unchanged. If a
BoolTensor is provided, positions with <code>True</code> are not allowed to attend
while <code>False</code> values will be unchanged. If a FloatTensor is provided, it
will be added to the attention weight.
</p>
</li></ul>

<p>Outputs:
</p>

<ul>
<li><p> attn_output: <code class="reqn">(L, N, E)</code> where L is the target sequence length, N is
the batch size, E is the embedding dimension. (but see the  <code>batch_first</code>
argument)
</p>
</li>
<li><p> attn_output_weights:
</p>

<ul>
<li><p> if <code>avg_weights</code> is <code>TRUE</code> (the default), the output attention
weights are averaged over the attention heads, giving a tensor of shape
<code class="reqn">(N, L, S)</code> where N is the batch size, L is the target sequence
length, S is the source sequence length.
</p>
</li>
<li><p> if <code>avg_weights</code> is <code>FALSE</code>, the attention weight tensor is output
as-is, with shape <code class="reqn">(N, H, L, S)</code>, where H is the number of attention
heads.
</p>
</li></ul>

</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
multihead_attn &lt;- nn_multihead_attention(embed_dim, num_heads)
out &lt;- multihead_attn(query, key, value)
attn_output &lt;- out[[1]]
attn_output_weights &lt;- out[[2]]

## End(Not run)

}
</code></pre>

<hr>
<h2 id='nn_multilabel_margin_loss'>Multilabel margin loss</h2><span id='topic+nn_multilabel_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <code class="reqn">x</code> (a 2D mini-batch <code>Tensor</code>)
and output <code class="reqn">y</code> (which is a 2D <code>Tensor</code> of target class indices).
For each sample in the mini-batch:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_multilabel_margin_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_multilabel_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\mbox{x.size}(0)}
</code>
</p>

<p>where <code class="reqn">x \in \left\{0, \; \cdots , \; \mbox{x.size}(0) - 1\right\}</code>, \
<code class="reqn">y \in \left\{0, \; \cdots , \; \mbox{y.size}(0) - 1\right\}</code>, \
<code class="reqn">0 \leq y[j] \leq \mbox{x.size}(0)-1</code>, \
and <code class="reqn">i \neq y[j]</code> for all <code class="reqn">i</code> and <code class="reqn">j</code>.
<code class="reqn">y</code> and <code class="reqn">x</code> must have the same size.
</p>
<p>The criterion only considers a contiguous block of non-negative targets that
starts at the front.
This allows for different samples to have variable amounts of target classes.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(C)</code> or <code class="reqn">(N, C)</code> where <code>N</code> is the batch size and <code>C</code>
is the number of classes.
</p>
</li>
<li><p> Target: <code class="reqn">(C)</code> or <code class="reqn">(N, C)</code>, label targets padded by -1 ensuring same shape as the input.
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N)</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_multilabel_margin_loss()
x &lt;- torch_tensor(c(0.1, 0.2, 0.4, 0.8))$view(c(1, 4))
# for target y, only consider labels 4 and 1, not after label -1
y &lt;- torch_tensor(c(4, 1, -1, 2), dtype = torch_long())$view(c(1, 4))
loss(x, y)
}
</code></pre>

<hr>
<h2 id='nn_multilabel_soft_margin_loss'>Multi label soft margin loss</h2><span id='topic+nn_multilabel_soft_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <code class="reqn">x</code> and target <code class="reqn">y</code> of size
<code class="reqn">(N, C)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_multilabel_soft_margin_loss(weight = NULL, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_multilabel_soft_margin_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is
treated as if having all ones.</p>
</td></tr>
<tr><td><code id="nn_multilabel_soft_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each sample in the minibatch:
</p>
<p style="text-align: center;"><code class="reqn">
  loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
+ (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)
</code>
</p>

<p>where <code class="reqn">i \in \left\{0, \; \cdots , \; \mbox{x.nElement}() - 1\right\}</code>,
<code class="reqn">y[i] \in \left\{0, \; 1\right\}</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C)</code> where <code>N</code> is the batch size and <code>C</code> is the number of classes.
</p>
</li>
<li><p> Target: <code class="reqn">(N, C)</code>, label targets padded by -1 ensuring same shape as the input.
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N)</code>.
</p>
</li></ul>


<hr>
<h2 id='nn_nll_loss'>Nll loss</h2><span id='topic+nn_nll_loss'></span>

<h3>Description</h3>

<p>The negative log likelihood loss. It is useful to train a classification
problem with <code>C</code> classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_nll_loss(weight = NULL, ignore_index = -100, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_nll_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is
treated as if having all ones.</p>
</td></tr>
<tr><td><code id="nn_nll_loss_+3A_ignore_index">ignore_index</code></td>
<td>
<p>(int, optional): Specifies a target value that is ignored
and does not contribute to the input gradient.</p>
</td></tr>
<tr><td><code id="nn_nll_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will
be applied, <code>'mean'</code>: the weighted mean of the output is taken,
<code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If provided, the optional argument <code>weight</code> should be a 1D Tensor assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.
</p>
<p>The <code>input</code> given through a forward call is expected to contain
log-probabilities of each class. <code>input</code> has to be a Tensor of size either
<code class="reqn">(minibatch, C)</code> or <code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code>
with <code class="reqn">K \geq 1</code> for the <code>K</code>-dimensional case (described later).
</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <code>LogSoftmax</code>  layer in the last layer of your network.
</p>
<p>You may use <code>CrossEntropyLoss</code> instead, if you prefer not to add an extra
layer.
</p>
<p>The <code>target</code> that this loss expects should be a class index in the range <code class="reqn">[0, C-1]</code>
where <code style="white-space: pre;">&#8288;C = number of classes&#8288;</code>; if <code>ignore_index</code> is specified, this loss also accepts
this class index (this index may not necessarily be in the class range).
</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \mbox{weight}[c] \cdot \mbox{1}\{c \not= \mbox{ignore\_index}\},
</code>
</p>

<p>where <code class="reqn">x</code> is the input, <code class="reqn">y</code> is the target, <code class="reqn">w</code> is the weight, and
<code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then
</p>
<p style="text-align: center;"><code class="reqn">
\ell(x, y) = \begin{array}{ll}
\sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;
  \mbox{if reduction} = \mbox{'mean';}\\
\sum_{n=1}^N l_n,  &amp;
  \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>,
where <code class="reqn">K</code> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C)</code> where <code style="white-space: pre;">&#8288;C = number of classes&#8288;</code>, or
<code class="reqn">(N, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>
in the case of <code>K</code>-dimensional loss.
</p>
</li>
<li><p> Target: <code class="reqn">(N)</code> where each value is <code class="reqn">0 \leq \mbox{targets}[i] \leq C-1</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case of
K-dimensional loss.
</p>
</li>
<li><p> Output: scalar.
</p>
</li></ul>

<p>If <code>reduction</code> is <code>'none'</code>, then the same size as the target: <code class="reqn">(N)</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case
of K-dimensional loss.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_log_softmax(dim = 2)
loss &lt;- nn_nll_loss()
# input is of size N x C = 3 x 5
input &lt;- torch_randn(3, 5, requires_grad = TRUE)
# each element in target has to have 0 &lt;= value &lt; C
target &lt;- torch_tensor(c(2, 1, 5), dtype = torch_long())
output &lt;- loss(m(input), target)
output$backward()

# 2D loss example (used, for example, with image inputs)
N &lt;- 5
C &lt;- 4
loss &lt;- nn_nll_loss()
# input is of size N x C x height x width
data &lt;- torch_randn(N, 16, 10, 10)
conv &lt;- nn_conv2d(16, C, c(3, 3))
m &lt;- nn_log_softmax(dim = 1)
# each element in target has to have 0 &lt;= value &lt; C
target &lt;- torch_empty(N, 8, 8, dtype = torch_long())$random_(1, C)
output &lt;- loss(m(conv(data)), target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_pairwise_distance'>Pairwise distance</h2><span id='topic+nn_pairwise_distance'></span>

<h3>Description</h3>

<p>Computes the batchwise pairwise distance between vectors <code class="reqn">v_1</code>, <code class="reqn">v_2</code>
using the p-norm:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_pairwise_distance(p = 2, eps = 1e-06, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_pairwise_distance_+3A_p">p</code></td>
<td>
<p>(real): the norm degree. Default: 2</p>
</td></tr>
<tr><td><code id="nn_pairwise_distance_+3A_eps">eps</code></td>
<td>
<p>(float, optional): Small value to avoid division by zero.
Default: 1e-6</p>
</td></tr>
<tr><td><code id="nn_pairwise_distance_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool, optional): Determines whether or not to keep the vector dimension.
Default: FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
 \Vert x \Vert _p = \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input1: <code class="reqn">(N, D)</code> where <code style="white-space: pre;">&#8288;D = vector dimension&#8288;</code>
</p>
</li>
<li><p> Input2: <code class="reqn">(N, D)</code>, same shape as the Input1
</p>
</li>
<li><p> Output: <code class="reqn">(N)</code>. If <code>keepdim</code> is <code>TRUE</code>, then <code class="reqn">(N, 1)</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
pdist &lt;- nn_pairwise_distance(p = 2)
input1 &lt;- torch_randn(100, 128)
input2 &lt;- torch_randn(100, 128)
output &lt;- pdist(input1, input2)
}
</code></pre>

<hr>
<h2 id='nn_parameter'>Creates an <code>nn_parameter</code></h2><span id='topic+nn_parameter'></span>

<h3>Description</h3>

<p>Indicates to nn_module that <code>x</code> is a parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_parameter(x, requires_grad = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_parameter_+3A_x">x</code></td>
<td>
<p>the tensor that you want to indicate as parameter</p>
</td></tr>
<tr><td><code id="nn_parameter_+3A_requires_grad">requires_grad</code></td>
<td>
<p>whether this parameter should  have
<code>requires_grad = TRUE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nn_poisson_nll_loss'>Poisson NLL loss</h2><span id='topic+nn_poisson_nll_loss'></span>

<h3>Description</h3>

<p>Negative log likelihood loss with Poisson distribution of target.
The loss can be described as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_poisson_nll_loss(
  log_input = TRUE,
  full = FALSE,
  eps = 1e-08,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_poisson_nll_loss_+3A_log_input">log_input</code></td>
<td>
<p>(bool, optional): if <code>TRUE</code> the loss is computed as
<code class="reqn">\exp(\mbox{input}) - \mbox{target}*\mbox{input}</code>, if <code>FALSE</code> the loss is
<code class="reqn">\mbox{input} - \mbox{target}*\log(\mbox{input}+\mbox{eps})</code>.</p>
</td></tr>
<tr><td><code id="nn_poisson_nll_loss_+3A_full">full</code></td>
<td>
<p>(bool, optional): whether to compute full loss, i. e. to add the
Stirling approximation term
<code class="reqn">\mbox{target}*\log(\mbox{target}) - \mbox{target} + 0.5 * \log(2\pi\mbox{target})</code>.</p>
</td></tr>
<tr><td><code id="nn_poisson_nll_loss_+3A_eps">eps</code></td>
<td>
<p>(float, optional): Small value to avoid evaluation of <code class="reqn">\log(0)</code> when
<code>log_input = FALSE</code>. Default: 1e-8</p>
</td></tr>
<tr><td><code id="nn_poisson_nll_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
\mbox{target} \sim \mathrm{Poisson}(\mbox{input})
\mbox{loss}(\mbox{input}, \mbox{target}) = \mbox{input} - \mbox{target} * \log(\mbox{input})
+ \log(\mbox{target!})
</code>
</p>

<p>The last term can be omitted or approximated with Stirling formula. The
approximation is used for target values more than 1. For targets less or
equal to 1 zeros are added to the loss.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar by default. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>,
the same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
loss &lt;- nn_poisson_nll_loss()
log_input &lt;- torch_randn(5, 2, requires_grad = TRUE)
target &lt;- torch_randn(5, 2)
output &lt;- loss(log_input, target)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_prelu'>PReLU module</h2><span id='topic+nn_prelu'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{PReLU}(x) = \max(0,x) + a * \min(0,x)
</code>
</p>

<p>or
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{PReLU}(x) =
  \left\{ \begin{array}{ll}
x, &amp; \mbox{ if } x \geq 0 \\
ax, &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_prelu(num_parameters = 1, init = 0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_prelu_+3A_num_parameters">num_parameters</code></td>
<td>
<p>(int): number of <code class="reqn">a</code> to learn.
Although it takes an int as input, there is only two values are legitimate:
1, or the number of channels at input. Default: 1</p>
</td></tr>
<tr><td><code id="nn_prelu_+3A_init">init</code></td>
<td>
<p>(float): the initial value of <code class="reqn">a</code>. Default: 0.25</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here <code class="reqn">a</code> is a learnable parameter. When called without arguments, <code>nn.prelu()</code> uses a single
parameter <code class="reqn">a</code> across all input channels. If called with <code>nn_prelu(nChannels)</code>,
a separate <code class="reqn">a</code> is used for each input channel.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li><p> weight (Tensor): the learnable weights of shape (<code>num_parameters</code>).
</p>
</li></ul>



<h3>Note</h3>

<p>weight decay should not be used when learning <code class="reqn">a</code> for good performance.
</p>
<p>Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is
no channel dim and the number of channels = 1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_prelu()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_prune_head'>Prune top layer(s) of a network</h2><span id='topic+nn_prune_head'></span>

<h3>Description</h3>

<p>Prune <code>head_size</code> last layers of a nn_module in order to
replace them by your own head, or in order to use the pruned module
as a sequential embedding module.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_prune_head(x, head_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_prune_head_+3A_x">x</code></td>
<td>
<p>nn_network to prune</p>
</td></tr>
<tr><td><code id="nn_prune_head_+3A_head_size">head_size</code></td>
<td>
<p>number of nn_layers to prune</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nn_sequential network with the top nn_layer removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
if (torch_is_installed()) {
x &lt;- nn_sequential(
  nn_relu(),
  nn_tanh(),
  nn_relu6(),
  nn_relu(),
  nn_linear(2,10),
  nn_batch_norm1d(10),
  nn_tanh(),
  nn_linear(10,3)
)  
prune &lt;- nn_prune_head(x, 3)
prune
}
}
</code></pre>

<hr>
<h2 id='nn_relu'>ReLU module</h2><span id='topic+nn_relu'></span>

<h3>Description</h3>

<p>Applies the rectified linear unit function element-wise
</p>
<p style="text-align: center;"><code class="reqn">\mbox{ReLU}(x) = (x)^+ = \max(0, x)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_relu(inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_relu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_relu()
input &lt;- torch_randn(2)
m(input)
}
</code></pre>

<hr>
<h2 id='nn_relu6'>ReLu6 module</h2><span id='topic+nn_relu6'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_relu6(inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_relu6_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{ReLU6}(x) = \min(\max(0,x), 6)
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_relu6()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_rnn'>RNN module</h2><span id='topic+nn_rnn'></span>

<h3>Description</h3>

<p>Applies a multi-layer Elman RNN with <code class="reqn">\tanh</code> or <code class="reqn">\mbox{ReLU}</code> non-linearity
to an input sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_rnn(
  input_size,
  hidden_size,
  num_layers = 1,
  nonlinearity = NULL,
  bias = TRUE,
  batch_first = FALSE,
  dropout = 0,
  bidirectional = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_rnn_+3A_input_size">input_size</code></td>
<td>
<p>The number of expected features in the input <code>x</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_hidden_size">hidden_size</code></td>
<td>
<p>The number of features in the hidden state <code>h</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_num_layers">num_layers</code></td>
<td>
<p>Number of recurrent layers. E.g., setting <code>num_layers=2</code>
would mean stacking two RNNs together to form a <code style="white-space: pre;">&#8288;stacked RNN&#8288;</code>,
with the second RNN taking in outputs of the first RNN and
computing the final results. Default: 1</p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_nonlinearity">nonlinearity</code></td>
<td>
<p>The non-linearity to use. Can be either <code>'tanh'</code> or
<code>'relu'</code>. Default: <code>'tanh'</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_bias">bias</code></td>
<td>
<p>If <code>FALSE</code>, then the layer does not use bias weights <code>b_ih</code> and
<code>b_hh</code>. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_batch_first">batch_first</code></td>
<td>
<p>If <code>TRUE</code>, then the input and output tensors are provided
as <code style="white-space: pre;">&#8288;(batch, seq, feature)&#8288;</code>. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_dropout">dropout</code></td>
<td>
<p>If non-zero, introduces a <code>Dropout</code> layer on the outputs of each
RNN layer except the last layer, with dropout probability equal to
<code>dropout</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_bidirectional">bidirectional</code></td>
<td>
<p>If <code>TRUE</code>, becomes a bidirectional RNN. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nn_rnn_+3A_...">...</code></td>
<td>
<p>other arguments that can be passed to the super class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each element in the input sequence, each layer computes the following
function:
</p>
<p style="text-align: center;"><code class="reqn">
h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})
</code>
</p>

<p>where <code class="reqn">h_t</code> is the hidden state at time <code>t</code>, <code class="reqn">x_t</code> is
the input at time <code>t</code>, and <code class="reqn">h_{(t-1)}</code> is the hidden state of the
previous layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>.
If <code>nonlinearity</code> is <code>'relu'</code>, then <code class="reqn">\mbox{ReLU}</code> is used instead of
<code class="reqn">\tanh</code>.
</p>


<h3>Inputs</h3>


<ul>
<li> <p><strong>input</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, input_size)&#8288;</code>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence.
</p>
</li>
<li> <p><strong>h_0</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.
</p>
</li></ul>



<h3>Outputs</h3>


<ul>
<li> <p><strong>output</strong> of shape <code style="white-space: pre;">&#8288;(seq_len, batch, num_directions * hidden_size)&#8288;</code>: tensor
containing the output features (<code>h_t</code>) from the last layer of the RNN,
for each <code>t</code>.  If a :class:<code>nn_packed_sequence</code> has
been given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code>output$view(seq_len, batch, num_directions, hidden_size)</code>,
with forward and backward being direction <code>0</code> and <code>1</code> respectively.
Similarly, the directions can be separated in the packed case.
</p>
</li>
<li> <p><strong>h_n</strong> of shape <code style="white-space: pre;">&#8288;(num_layers * num_directions, batch, hidden_size)&#8288;</code>: tensor
containing the hidden state for <code>t = seq_len</code>.
Like <em>output</em>, the layers can be separated using
<code>h_n$view(num_layers, num_directions, batch, hidden_size)</code>.
</p>
</li></ul>



<h3>Shape</h3>


<ul>
<li><p> Input1: <code class="reqn">(L, N, H_{in})</code> tensor containing input features where
<code class="reqn">H_{in}=\mbox{input\_size}</code> and <code>L</code> represents a sequence length.
</p>
</li>
<li><p> Input2: <code class="reqn">(S, N, H_{out})</code> tensor
containing the initial hidden state for each element in the batch.
<code class="reqn">H_{out}=\mbox{hidden\_size}</code>
Defaults to zero if not provided. where <code class="reqn">S=\mbox{num\_layers} * \mbox{num\_directions}</code>
If the RNN is bidirectional, num_directions should be 2, else it should be 1.
</p>
</li>
<li><p> Output1: <code class="reqn">(L, N, H_{all})</code> where <code class="reqn">H_{all}=\mbox{num\_directions} * \mbox{hidden\_size}</code>
</p>
</li>
<li><p> Output2: <code class="reqn">(S, N, H_{out})</code> tensor containing the next hidden state
for each element in the batch
</p>
</li></ul>



<h3>Attributes</h3>


<ul>
<li> <p><code>weight_ih_l[k]</code>: the learnable input-hidden weights of the k-th layer,
of shape <code style="white-space: pre;">&#8288;(hidden_size, input_size)&#8288;</code> for <code>k = 0</code>. Otherwise, the shape is
<code style="white-space: pre;">&#8288;(hidden_size, num_directions * hidden_size)&#8288;</code>
</p>
</li>
<li> <p><code>weight_hh_l[k]</code>: the learnable hidden-hidden weights of the k-th layer,
of shape <code style="white-space: pre;">&#8288;(hidden_size, hidden_size)&#8288;</code>
</p>
</li>
<li> <p><code>bias_ih_l[k]</code>: the learnable input-hidden bias of the k-th layer,
of shape <code>(hidden_size)</code>
</p>
</li>
<li> <p><code>bias_hh_l[k]</code>: the learnable hidden-hidden bias of the k-th layer,
of shape <code>(hidden_size)</code>
</p>
</li></ul>



<h3>Note</h3>

<p>All the weights and biases are initialized from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>
where <code class="reqn">k = \frac{1}{\mbox{hidden\_size}}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
rnn &lt;- nn_rnn(10, 20, 2)
input &lt;- torch_randn(5, 3, 10)
h0 &lt;- torch_randn(2, 3, 20)
rnn(input, h0)
}
</code></pre>

<hr>
<h2 id='nn_rrelu'>RReLU module</h2><span id='topic+nn_rrelu'></span>

<h3>Description</h3>

<p>Applies the randomized leaky rectified liner unit function, element-wise,
as described in the paper:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_rrelu(lower = 1/8, upper = 1/3, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_rrelu_+3A_lower">lower</code></td>
<td>
<p>lower bound of the uniform distribution. Default: <code class="reqn">\frac{1}{8}</code></p>
</td></tr>
<tr><td><code id="nn_rrelu_+3A_upper">upper</code></td>
<td>
<p>upper bound of the uniform distribution. Default: <code class="reqn">\frac{1}{3}</code></p>
</td></tr>
<tr><td><code id="nn_rrelu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code style="white-space: pre;">&#8288;Empirical Evaluation of Rectified Activations in Convolutional Network&#8288;</code>.
</p>
<p>The function is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{RReLU}(x) =
\left\{ \begin{array}{ll}
x &amp; \mbox{if } x \geq 0 \\
ax &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>

<p>where <code class="reqn">a</code> is randomly sampled from uniform distribution
<code class="reqn">\mathcal{U}(\mbox{lower}, \mbox{upper})</code>.
See: https://arxiv.org/pdf/1505.00853.pdf
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_rrelu(0.1, 0.3)
input &lt;- torch_randn(2)
m(input)
}
</code></pre>

<hr>
<h2 id='nn_selu'>SELU module</h2><span id='topic+nn_selu'></span>

<h3>Description</h3>

<p>Applied element-wise, as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_selu(inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_selu_+3A_inplace">inplace</code></td>
<td>
<p>(bool, optional): can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{SELU}(x) = \mbox{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))
</code>
</p>

<p>with <code class="reqn">\alpha = 1.6732632423543772848170429916717</code> and
<code class="reqn">\mbox{scale} = 1.0507009873554804934193349852946</code>.
</p>
<p>More details can be found in the paper
<a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_selu()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_sequential'>A sequential container</h2><span id='topic+nn_sequential'></span>

<h3>Description</h3>

<p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
See examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_sequential(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_sequential_+3A_...">...</code></td>
<td>
<p>sequence of modules to be added</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

model &lt;- nn_sequential(
  nn_conv2d(1, 20, 5),
  nn_relu(),
  nn_conv2d(20, 64, 5),
  nn_relu()
)
input &lt;- torch_randn(32, 1, 28, 28)
output &lt;- model(input)
}
</code></pre>

<hr>
<h2 id='nn_sigmoid'>Sigmoid module</h2><span id='topic+nn_sigmoid'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_sigmoid()
</code></pre>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_sigmoid()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_silu'>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
The SiLU function is also known as the swish function.</h2><span id='topic+nn_silu'></span>

<h3>Description</h3>

<p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
The SiLU function is also known as the swish function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_silu(inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_silu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>
where the SiLU (Sigmoid Linear Unit) was originally coined, and see
<a href="https://arxiv.org/abs/1702.03118">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a>
and <a href="https://arxiv.org/abs/1710.05941v1">Swish: a Self-Gated Activation Function</a>
where the SiLU was experimented with later.
</p>

<hr>
<h2 id='nn_smooth_l1_loss'>Smooth L1 loss</h2><span id='topic+nn_smooth_l1_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <code>MSELoss</code> and in some cases
prevents exploding gradients (e.g. see <code style="white-space: pre;">&#8288;Fast R-CNN&#8288;</code> paper by Ross Girshick).
Also known as the Huber loss:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_smooth_l1_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_smooth_l1_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}
</code>
</p>

<p>where <code class="reqn">z_{i}</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">
  z_{i} =
  \begin{array}{ll}
0.5 (x_i - y_i)^2, &amp; \mbox{if } |x_i - y_i| &lt; 1 \\
|x_i - y_i| - 0.5, &amp; \mbox{otherwise }
\end{array}
</code>
</p>

<p><code class="reqn">x</code> and <code class="reqn">y</code> arbitrary shapes with a total of <code class="reqn">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="reqn">n</code>.
The division by <code class="reqn">n</code> can be avoided if sets <code>reduction = 'sum'</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then
<code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>


<hr>
<h2 id='nn_soft_margin_loss'>Soft margin loss</h2><span id='topic+nn_soft_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a two-class classification
logistic loss between input tensor <code class="reqn">x</code> and target tensor <code class="reqn">y</code>
(containing 1 or -1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_soft_margin_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_soft_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\mbox{x.nelement}()}
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Target: <code class="reqn">(*)</code>, same shape as the input
</p>
</li>
<li><p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then same shape as the input
</p>
</li></ul>


<hr>
<h2 id='nn_softmax'>Softmax module</h2><span id='topic+nn_softmax'></span>

<h3>Description</h3>

<p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <code style="white-space: pre;">&#8288;[0,1]&#8288;</code> and sum to 1.
Softmax is defined as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_softmax(dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_softmax_+3A_dim">dim</code></td>
<td>
<p>(int): A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
</code>
</p>

<p>When the input Tensor is a sparse tensor then the unspecifed
values are treated as <code>-Inf</code>.
</p>


<h3>Value</h3>

<p>:
a Tensor of the same dimension and shape as the input with
values in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(*)</code>, same shape as the input
</p>
</li></ul>



<h3>Note</h3>

<p>This module doesn't work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use <code>LogSoftmax</code> instead (it's faster and has better numerical properties).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softmax(1)
input &lt;- torch_randn(2, 3)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_softmax2d'>Softmax2d module</h2><span id='topic+nn_softmax2d'></span>

<h3>Description</h3>

<p>Applies SoftMax over features to each spatial location.
When given an image of <code style="white-space: pre;">&#8288;Channels x Height x Width&#8288;</code>, it will
apply <code>Softmax</code> to each location <code class="reqn">(Channels, h_i, w_j)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_softmax2d()
</code></pre>


<h3>Value</h3>

<p>a Tensor of the same dimension and shape as the input with
values in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, C, H, W)</code>
</p>
</li>
<li><p> Output: <code class="reqn">(N, C, H, W)</code> (same shape as input)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softmax2d()
input &lt;- torch_randn(2, 3, 12, 13)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_softmin'>Softmin</h2><span id='topic+nn_softmin'></span>

<h3>Description</h3>

<p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> and sum to 1.
Softmin is defined as:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_softmin(dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_softmin_+3A_dim">dim</code></td>
<td>
<p>(int): A dimension along which Softmin will be computed (so every slice
along dim will sum to 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}
</code>
</p>



<h3>Value</h3>

<p>a Tensor of the same dimension and shape as the input, with
values in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(*)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(*)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softmin(dim = 1)
input &lt;- torch_randn(2, 2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_softplus'>Softplus module</h2><span id='topic+nn_softplus'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_softplus(beta = 1, threshold = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_softplus_+3A_beta">beta</code></td>
<td>
<p>the <code class="reqn">\beta</code> value for the Softplus formulation. Default: 1</p>
</td></tr>
<tr><td><code id="nn_softplus_+3A_threshold">threshold</code></td>
<td>
<p>values above this revert to a linear function. Default: 20</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.
For numerical stability the implementation reverts to the linear function
when <code class="reqn">input \times \beta &gt; threshold</code>.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softplus()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_softshrink'>Softshrink module</h2><span id='topic+nn_softshrink'></span>

<h3>Description</h3>

<p>Applies the soft shrinkage function elementwise:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_softshrink(lambd = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_softshrink_+3A_lambd">lambd</code></td>
<td>
<p>the <code class="reqn">\lambda</code> (must be no less than zero) value for the Softshrink formulation. Default: 0.5</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{SoftShrinkage}(x) =
  \left\{ \begin{array}{ll}
x - \lambda, &amp; \mbox{ if } x &gt; \lambda \\
x + \lambda, &amp; \mbox{ if } x &lt; -\lambda \\
0, &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softshrink()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_softsign'>Softsign module</h2><span id='topic+nn_softsign'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{SoftSign}(x) = \frac{x}{ 1 + |x|}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nn_softsign()
</code></pre>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_softsign()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_tanh'>Tanh module</h2><span id='topic+nn_tanh'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_tanh()
</code></pre>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_tanh()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_tanhshrink'>Tanhshrink module</h2><span id='topic+nn_tanhshrink'></span>

<h3>Description</h3>

<p>Applies the element-wise function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_tanhshrink()
</code></pre>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Tanhshrink}(x) = x - \tanh(x)
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_tanhshrink()
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_threshold'>Threshold module</h2><span id='topic+nn_threshold'></span>

<h3>Description</h3>

<p>Thresholds each element of the input Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_threshold(threshold, value, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_threshold_+3A_threshold">threshold</code></td>
<td>
<p>The value to threshold at</p>
</td></tr>
<tr><td><code id="nn_threshold_+3A_value">value</code></td>
<td>
<p>The value to replace with</p>
</td></tr>
<tr><td><code id="nn_threshold_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Threshold is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
  y =
  \left\{ \begin{array}{ll}
  x, &amp;\mbox{ if } x &gt; \mbox{threshold} \\
  \mbox{value}, &amp;\mbox{ otherwise }
  \end{array}
  \right.
</code>
</p>



<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li><p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
m &lt;- nn_threshold(0.1, 20)
input &lt;- torch_randn(2)
output &lt;- m(input)
}
</code></pre>

<hr>
<h2 id='nn_triplet_margin_loss'>Triplet margin loss</h2><span id='topic+nn_triplet_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the triplet loss given an input
tensors <code class="reqn">x1</code>, <code class="reqn">x2</code>, <code class="reqn">x3</code> and a margin with a value greater than <code class="reqn">0</code>.
This is used for measuring a relative similarity between samples. A triplet
is composed by <code>a</code>, <code>p</code> and <code>n</code> (i.e., <code>anchor</code>, <code style="white-space: pre;">&#8288;positive examples&#8288;</code> and <code style="white-space: pre;">&#8288;negative examples&#8288;</code> respectively). The shapes of all input tensors should be
<code class="reqn">(N, D)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_triplet_margin_loss(
  margin = 1,
  p = 2,
  eps = 1e-06,
  swap = FALSE,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_triplet_margin_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): Default: <code class="reqn">1</code>.</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_loss_+3A_p">p</code></td>
<td>
<p>(int, optional): The norm degree for pairwise distance. Default: <code class="reqn">2</code>.</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_loss_+3A_eps">eps</code></td>
<td>
<p>constant to avoid NaN's</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_loss_+3A_swap">swap</code></td>
<td>
<p>(bool, optional): The distance swap is described in detail in the paper
<a href="https://paperswithcode.com/paper/learning-local-feature-descriptors-with">Learning shallow convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distance swap is described in detail in the paper
<a href="https://paperswithcode.com/paper/learning-local-feature-descriptors-with">Learning shallow convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.
</p>
<p>The loss function for each sample in the mini-batch is:
</p>
<p style="text-align: center;"><code class="reqn">
  L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}
</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">
  d(x_i, y_i) = | {\bf x}_i - {\bf y}_i |_p
</code>
</p>

<p>See also <code><a href="#topic+nn_triplet_margin_with_distance_loss">nn_triplet_margin_with_distance_loss()</a></code>, which computes the
triplet margin loss for input tensors using a custom distance function.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, D)</code> where <code class="reqn">D</code> is the vector dimension.
</p>
</li>
<li><p> Output: A Tensor of shape <code class="reqn">(N)</code> if <code>reduction</code> is <code>'none'</code>, or a scalar
otherwise.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
triplet_loss &lt;- nn_triplet_margin_loss(margin = 1, p = 2)
anchor &lt;- torch_randn(100, 128, requires_grad = TRUE)
positive &lt;- torch_randn(100, 128, requires_grad = TRUE)
negative &lt;- torch_randn(100, 128, requires_grad = TRUE)
output &lt;- triplet_loss(anchor, positive, negative)
output$backward()
}
</code></pre>

<hr>
<h2 id='nn_triplet_margin_with_distance_loss'>Triplet margin with distance loss</h2><span id='topic+nn_triplet_margin_with_distance_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the triplet loss given input
tensors <code class="reqn">a</code>, <code class="reqn">p</code>, and <code class="reqn">n</code> (representing anchor,
positive, and negative examples, respectively), and a nonnegative,
real-valued function (&quot;distance function&quot;) used to compute the relationship
between the anchor and positive example (&quot;positive distance&quot;) and the
anchor and negative example (&quot;negative distance&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_triplet_margin_with_distance_loss(
  distance_function = NULL,
  margin = 1,
  swap = FALSE,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_triplet_margin_with_distance_loss_+3A_distance_function">distance_function</code></td>
<td>
<p>(callable, optional): A nonnegative, real-valued function that
quantifies the closeness of two tensors. If not specified,
<code><a href="#topic+nn_pairwise_distance">nn_pairwise_distance()</a></code> will be used.  Default: <code>None</code></p>
</td></tr>
<tr><td><code id="nn_triplet_margin_with_distance_loss_+3A_margin">margin</code></td>
<td>
<p>(float, optional): A non-negative margin representing the minimum difference
between the positive and negative distances required for the loss to be 0. Larger
margins penalize cases where the negative examples are not distant enough from the
anchors, relative to the positives. Default: <code class="reqn">1</code>.</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_with_distance_loss_+3A_swap">swap</code></td>
<td>
<p>(bool, optional): Whether to use the distance swap described in the paper
<a href="https://paperswithcode.com/paper/learning-local-feature-descriptors-with">Learning shallow convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al. If TRUE, and if the positive example is closer to the
negative example than the anchor is, swaps the positive example and the anchor in
the loss computation. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nn_triplet_margin_with_distance_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional): Specifies the (optional) reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed. Default: <code>'mean'</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unreduced loss (i.e., with <code>reduction</code> set to <code>'none'</code>)
can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad
l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size; <code class="reqn">d</code> is a nonnegative, real-valued function
quantifying the closeness of two tensors, referred to as the <code>distance_function</code>;
and <code class="reqn">margin</code> is a non-negative margin representing the minimum difference
between the positive and negative distances that is required for the loss to
be 0.  The input tensors have <code class="reqn">N</code> elements each and can be of any shape
that the distance function can handle.
If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then:
</p>
<p style="text-align: center;"><code class="reqn">
\ell(x, y) =
\begin{array}{ll}
\mbox{mean}(L), &amp;  \mbox{if reduction} = \mbox{`mean';}\\
            \mbox{sum}(L),  &amp;  \mbox{if reduction} = \mbox{`sum'.}
\end{array}
</code>
</p>

<p>See also <code><a href="#topic+nn_triplet_margin_loss">nn_triplet_margin_loss()</a></code>, which computes the triplet
loss for input tensors using the <code class="reqn">l_p</code> distance as the distance function.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> represents any number of additional dimensions
as supported by the distance function.
</p>
</li>
<li><p> Output: A Tensor of shape <code class="reqn">(N)</code> if <code>reduction</code> is <code>'none'</code>, or a scalar
otherwise.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
# Initialize embeddings
embedding &lt;- nn_embedding(1000, 128)
anchor_ids &lt;- torch_randint(1, 1000, 1, dtype = torch_long())
positive_ids &lt;- torch_randint(1, 1000, 1, dtype = torch_long())
negative_ids &lt;- torch_randint(1, 1000, 1, dtype = torch_long())
anchor &lt;- embedding(anchor_ids)
positive &lt;- embedding(positive_ids)
negative &lt;- embedding(negative_ids)

# Built-in Distance Function
triplet_loss &lt;- nn_triplet_margin_with_distance_loss(
  distance_function = nn_pairwise_distance()
)
output &lt;- triplet_loss(anchor, positive, negative)

# Custom Distance Function
l_infinity &lt;- function(x1, x2) {
  torch_max(torch_abs(x1 - x2), dim = 1)[[1]]
}

triplet_loss &lt;- nn_triplet_margin_with_distance_loss(
  distance_function = l_infinity, margin = 1.5
)
output &lt;- triplet_loss(anchor, positive, negative)

# Custom Distance Function (Lambda)
triplet_loss &lt;- nn_triplet_margin_with_distance_loss(
  distance_function = function(x, y) {
    1 - nnf_cosine_similarity(x, y)
  }
)

output &lt;- triplet_loss(anchor, positive, negative)
}
</code></pre>

<hr>
<h2 id='nn_unflatten'>Unflattens a tensor dim expanding it to a desired shape.
For use with [<a href="#topic+nn_sequential">nn_sequential</a>.</h2><span id='topic+nn_unflatten'></span>

<h3>Description</h3>

<p>Unflattens a tensor dim expanding it to a desired shape.
For use with [<a href="#topic+nn_sequential">nn_sequential</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_unflatten(dim, unflattened_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_unflatten_+3A_dim">dim</code></td>
<td>
<p>Dimension to be unflattened</p>
</td></tr>
<tr><td><code id="nn_unflatten_+3A_unflattened_size">unflattened_size</code></td>
<td>
<p>New shape of the unflattened dimension</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
input &lt;- torch_randn(2, 50)

m &lt;- nn_sequential(
  nn_linear(50, 50),
  nn_unflatten(2, c(2, 5, 5))
)
output &lt;- m(input)
output$size()
}
</code></pre>

<hr>
<h2 id='nn_upsample'>Upsample module</h2><span id='topic+nn_upsample'></span>

<h3>Description</h3>

<p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.
The input data is assumed to be of the form minibatch x channels x optional depth x
optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for
volumetric inputs, we expect a 5D Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_upsample(
  size = NULL,
  scale_factor = NULL,
  mode = "nearest",
  align_corners = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_upsample_+3A_size">size</code></td>
<td>
<p>(int or <code>Tuple[int]</code> or <code>Tuple[int, int]</code> or <code>Tuple[int, int, int]</code>, optional):
output spatial sizes</p>
</td></tr>
<tr><td><code id="nn_upsample_+3A_scale_factor">scale_factor</code></td>
<td>
<p>(float or <code>Tuple[float]</code> or <code>Tuple[float, float]</code> or <code>Tuple[float, float, float]</code>, optional):
multiplier for spatial size. Has to match input size if it is a tuple.</p>
</td></tr>
<tr><td><code id="nn_upsample_+3A_mode">mode</code></td>
<td>
<p>(str, optional): the upsampling algorithm: one of <code>'nearest'</code>,
<code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code> and <code>'trilinear'</code>.
Default: <code>'nearest'</code></p>
</td></tr>
<tr><td><code id="nn_upsample_+3A_align_corners">align_corners</code></td>
<td>
<p>(bool, optional): if <code>TRUE</code>, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code>mode</code> is
<code>'linear'</code>, <code>'bilinear'</code>, or <code>'trilinear'</code>. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithms available for upsampling are nearest neighbor and linear, bilinear,
bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.
</p>
<p>One can either give a scale_factor or the target output size to calculate the
output size. (You cannot give both, as it is ambiguous)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
input &lt;- torch_arange(start = 1, end = 4, dtype = torch_float())$view(c(1, 1, 2, 2))
nn_upsample(scale_factor = c(2), mode = "nearest")(input)
nn_upsample(scale_factor = c(2, 2), mode = "nearest")(input)
}
</code></pre>

<hr>
<h2 id='nn_utils_clip_grad_norm_'>Clips gradient norm of an iterable of parameters.</h2><span id='topic+nn_utils_clip_grad_norm_'></span>

<h3>Description</h3>

<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_clip_grad_norm_(parameters, max_norm, norm_type = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_clip_grad_norm__+3A_parameters">parameters</code></td>
<td>
<p>(IterableTensor or Tensor): an iterable of Tensors or a
single Tensor that will have gradients normalized</p>
</td></tr>
<tr><td><code id="nn_utils_clip_grad_norm__+3A_max_norm">max_norm</code></td>
<td>
<p>(float or int): max norm of the gradients</p>
</td></tr>
<tr><td><code id="nn_utils_clip_grad_norm__+3A_norm_type">norm_type</code></td>
<td>
<p>(float or int): type of the used p-norm. Can be <code>Inf</code> for
infinity norm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Total norm of the parameters (viewed as a single vector).
</p>

<hr>
<h2 id='nn_utils_clip_grad_value_'>Clips gradient of an iterable of parameters at specified value.</h2><span id='topic+nn_utils_clip_grad_value_'></span>

<h3>Description</h3>

<p>Gradients are modified in-place.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_clip_grad_value_(parameters, clip_value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_clip_grad_value__+3A_parameters">parameters</code></td>
<td>
<p>(Iterable(Tensor) or Tensor): an iterable of Tensors or a
single Tensor that will have gradients normalized</p>
</td></tr>
<tr><td><code id="nn_utils_clip_grad_value__+3A_clip_value">clip_value</code></td>
<td>
<p>(float or int): maximum allowed value of the gradients.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gradients are clipped in the range
<code class="reqn">\left[\mbox{-clip\_value}, \mbox{clip\_value}\right]</code>
</p>

<hr>
<h2 id='nn_utils_rnn_pack_padded_sequence'>Packs a Tensor containing padded sequences of variable length.</h2><span id='topic+nn_utils_rnn_pack_padded_sequence'></span>

<h3>Description</h3>

<p><code>input</code> can be of size <code style="white-space: pre;">&#8288;T x B x *&#8288;</code> where <code>T</code> is the length of the
longest sequence (equal to <code>lengths[1]</code>), <code>B</code> is the batch size, and
<code>*</code> is any number of dimensions (including 0). If <code>batch_first</code> is
<code>TRUE</code>, <code style="white-space: pre;">&#8288;B x T x *&#8288;</code> <code>input</code> is expected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_rnn_pack_padded_sequence(
  input,
  lengths,
  batch_first = FALSE,
  enforce_sorted = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_rnn_pack_padded_sequence_+3A_input">input</code></td>
<td>
<p>(Tensor): padded batch of variable length sequences.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pack_padded_sequence_+3A_lengths">lengths</code></td>
<td>
<p>(Tensor): list of sequences lengths of each batch element.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pack_padded_sequence_+3A_batch_first">batch_first</code></td>
<td>
<p>(bool, optional): if <code>TRUE</code>, the input is expected in <code style="white-space: pre;">&#8288;B x T x *&#8288;</code>
format.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pack_padded_sequence_+3A_enforce_sorted">enforce_sorted</code></td>
<td>
<p>(bool, optional): if <code>TRUE</code>, the input is expected to
contain sequences sorted by length in a decreasing order. If
<code>FALSE</code>, the input will get sorted unconditionally. Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For unsorted sequences, use <code>enforce_sorted = FALSE</code>. If <code>enforce_sorted</code> is
<code>TRUE</code>, the sequences should be sorted by length in a decreasing order, i.e.
<code>input[,1]</code> should be the longest sequence, and <code>input[,B]</code> the shortest
one. <code>enforce_sorted = TRUE</code> is only necessary for ONNX export.
</p>


<h3>Value</h3>

<p>a <code>PackedSequence</code> object
</p>


<h3>Note</h3>

<p>This function accepts any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Tensor can be retrieved from
a <code>PackedSequence</code> object by accessing its <code>.data</code> attribute.
</p>

<hr>
<h2 id='nn_utils_rnn_pack_sequence'>Packs a list of variable length Tensors</h2><span id='topic+nn_utils_rnn_pack_sequence'></span>

<h3>Description</h3>

<p><code>sequences</code> should be a list of Tensors of size <code style="white-space: pre;">&#8288;L x *&#8288;</code>, where <code>L</code> is
the length of a sequence and <code>*</code> is any number of trailing dimensions,
including zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_rnn_pack_sequence(sequences, enforce_sorted = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_rnn_pack_sequence_+3A_sequences">sequences</code></td>
<td>
<p><code>(list[Tensor])</code>: A list of sequences of decreasing length.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pack_sequence_+3A_enforce_sorted">enforce_sorted</code></td>
<td>
<p>(bool, optional): if <code>TRUE</code>, checks that the input
contains sequences sorted by length in a decreasing order. If
<code>FALSE</code>, this condition is not checked. Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For unsorted sequences, use <code>enforce_sorted = FALSE</code>. If <code>enforce_sorted</code>
is <code>TRUE</code>, the sequences should be sorted in the order of decreasing length.
<code>enforce_sorted = TRUE</code> is only necessary for ONNX export.
</p>


<h3>Value</h3>

<p>a <code>PackedSequence</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_tensor(c(1, 2, 3), dtype = torch_long())
y &lt;- torch_tensor(c(4, 5), dtype = torch_long())
z &lt;- torch_tensor(c(6), dtype = torch_long())

p &lt;- nn_utils_rnn_pack_sequence(list(x, y, z))
}
</code></pre>

<hr>
<h2 id='nn_utils_rnn_pad_packed_sequence'>Pads a packed batch of variable length sequences.</h2><span id='topic+nn_utils_rnn_pad_packed_sequence'></span>

<h3>Description</h3>

<p>It is an inverse operation to <code><a href="#topic+nn_utils_rnn_pack_padded_sequence">nn_utils_rnn_pack_padded_sequence()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_rnn_pad_packed_sequence(
  sequence,
  batch_first = FALSE,
  padding_value = 0,
  total_length = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_rnn_pad_packed_sequence_+3A_sequence">sequence</code></td>
<td>
<p>(PackedSequence): batch to pad</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pad_packed_sequence_+3A_batch_first">batch_first</code></td>
<td>
<p>(bool, optional): if <code>True</code>, the output will be in &ldquo;B x T x *'
format.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pad_packed_sequence_+3A_padding_value">padding_value</code></td>
<td>
<p>(float, optional): values for padded elements.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pad_packed_sequence_+3A_total_length">total_length</code></td>
<td>
<p>(int, optional): if not <code>NULL</code>, the output will be padded to
have length <code>total_length</code>. This method will throw <code>ValueError</code>
if <code>total_length</code> is less than the max sequence length in
<code>sequence</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned Tensor's data will be of size <code style="white-space: pre;">&#8288;T x B x *&#8288;</code>, where <code>T</code> is the length
of the longest sequence and <code>B</code> is the batch size. If <code>batch_first</code> is <code>TRUE</code>,
the data will be transposed into <code style="white-space: pre;">&#8288;B x T x *&#8288;</code> format.
</p>


<h3>Value</h3>

<p>Tuple of Tensor containing the padded sequence, and a Tensor
containing the list of lengths of each sequence in the batch.
Batch elements will be re-ordered as they were ordered originally when
the batch was passed to <code><a href="#topic+nn_utils_rnn_pack_padded_sequence">nn_utils_rnn_pack_padded_sequence()</a></code> or
<code><a href="#topic+nn_utils_rnn_pack_sequence">nn_utils_rnn_pack_sequence()</a></code>.
</p>


<h3>Note</h3>

<p><code>total_length</code> is useful to implement the
<code style="white-space: pre;">&#8288;pack sequence -&gt; recurrent network -&gt; unpack sequence&#8288;</code> pattern in a
<code>nn_module</code> wrapped in <code>~torch.nn.DataParallel</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
seq &lt;- torch_tensor(rbind(c(1, 2, 0), c(3, 0, 0), c(4, 5, 6)))
lens &lt;- c(2, 1, 3)
packed &lt;- nn_utils_rnn_pack_padded_sequence(seq, lens,
  batch_first = TRUE,
  enforce_sorted = FALSE
)
packed
nn_utils_rnn_pad_packed_sequence(packed, batch_first = TRUE)
}
</code></pre>

<hr>
<h2 id='nn_utils_rnn_pad_sequence'>Pad a list of variable length Tensors with <code>padding_value</code></h2><span id='topic+nn_utils_rnn_pad_sequence'></span>

<h3>Description</h3>

<p><code>pad_sequence</code> stacks a list of Tensors along a new dimension,
and pads them to equal length. For example, if the input is list of
sequences with size <code style="white-space: pre;">&#8288;L x *&#8288;</code> and if batch_first is False, and <code style="white-space: pre;">&#8288;T x B x *&#8288;</code>
otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_utils_rnn_pad_sequence(sequences, batch_first = FALSE, padding_value = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_utils_rnn_pad_sequence_+3A_sequences">sequences</code></td>
<td>
<p><code>(list[Tensor])</code>: list of variable length sequences.</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pad_sequence_+3A_batch_first">batch_first</code></td>
<td>
<p>(bool, optional): output will be in <code style="white-space: pre;">&#8288;B x T x *&#8288;</code> if <code>TRUE</code>,
or in <code style="white-space: pre;">&#8288;T x B x *&#8288;</code> otherwise</p>
</td></tr>
<tr><td><code id="nn_utils_rnn_pad_sequence_+3A_padding_value">padding_value</code></td>
<td>
<p>(float, optional): value for padded elements. Default: 0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>B</code> is batch size. It is equal to the number of elements in <code>sequences</code>.
<code>T</code> is length of the longest sequence.
<code>L</code> is length of the sequence.
<code>*</code> is any number of trailing dimensions, including none.
</p>


<h3>Value</h3>

<p>Tensor of size <code style="white-space: pre;">&#8288;T x B x *&#8288;</code> if <code>batch_first</code> is <code>FALSE</code>.
Tensor of size <code style="white-space: pre;">&#8288;B x T x *&#8288;</code> otherwise
</p>


<h3>Note</h3>

<p>This function returns a Tensor of size <code style="white-space: pre;">&#8288;T x B x *&#8288;</code> or <code style="white-space: pre;">&#8288;B x T x *&#8288;</code>
where <code>T</code> is the length of the longest sequence. This function assumes
trailing dimensions and type of all the Tensors in sequences are same.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_ones(25, 300)
b &lt;- torch_ones(22, 300)
c &lt;- torch_ones(15, 300)
nn_utils_rnn_pad_sequence(list(a, b, c))$size()
}
</code></pre>

<hr>
<h2 id='nn_utils_weight_norm'>nn_utils_weight_norm</h2><span id='topic+nn_utils_weight_norm'></span>

<h3>Description</h3>

<p>Applies weight normalization to a parameter in the given module.
</p>


<h3>Details</h3>

<div class="sourceCode"><pre>    \eqn{\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}}
</pre></div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <code>name</code>  (e.g. <code>'weight'</code>) with two parameters: one specifying the
magnitude (e.g. <code>'weight_g'</code>) and one specifying the direction
(e.g. <code>'weight_v'</code>).
</p>


<h3>Value</h3>

<p>The original module with the weight_v and weight_g paramters.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-nn_utils_weight_norm-new"><code>nn_utils_weight_norm$new()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-compute_weight"><code>nn_utils_weight_norm$compute_weight()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-apply"><code>nn_utils_weight_norm$apply()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-call"><code>nn_utils_weight_norm$call()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-recompute"><code>nn_utils_weight_norm$recompute()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-remove"><code>nn_utils_weight_norm$remove()</code></a>
</p>
</li>
<li> <p><a href="#method-nn_utils_weight_norm-clone"><code>nn_utils_weight_norm$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-nn_utils_weight_norm-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$new(name, dim)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>name</code></dt><dd><p>(str, optional): name of weight parameter</p>
</dd>
<dt><code>dim</code></dt><dd><p>(int, optional): dimension over which to compute the norm</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-compute_weight"></a>



<h4>Method <code>compute_weight()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$compute_weight(module, name = NULL, dim = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>module</code></dt><dd><p>(Module): containing module</p>
</dd>
<dt><code>name</code></dt><dd><p>(str, optional): name of weight parameter</p>
</dd>
<dt><code>dim</code></dt><dd><p>(int, optional): dimension over which to compute the norm</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-apply"></a>



<h4>Method <code>apply()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$apply(module, name = NULL, dim = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>module</code></dt><dd><p>(Module): containing module</p>
</dd>
<dt><code>name</code></dt><dd><p>(str, optional): name of weight parameter</p>
</dd>
<dt><code>dim</code></dt><dd><p>(int, optional): dimension over which to compute the norm</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-call"></a>



<h4>Method <code>call()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$call(module)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>module</code></dt><dd><p>(Module): containing module</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-recompute"></a>



<h4>Method <code>recompute()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$recompute(module)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>module</code></dt><dd><p>(Module): containing module</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-remove"></a>



<h4>Method <code>remove()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$remove(module, name = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>module</code></dt><dd><p>(Module): containing module</p>
</dd>
<dt><code>name</code></dt><dd><p>(str, optional): name of weight parameter</p>
</dd>
</dl>

</div>


<hr>
<a id="method-nn_utils_weight_norm-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>nn_utils_weight_norm$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>The pytorch Weight normalization is implemented via a hook that recomputes
the weight tensor from the magnitude and direction before every <code>forward()</code>
call. Since torch for R still do not support hooks, the weight recomputation
need to be done explicitly inside the <code>forward()</code> definition trough a call of
the <code>recompute()</code> method. See examples.
</p>
<p>By default, with <code>dim = 0</code>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<code>dim = NULL</code>.
</p>
<p>@references https://arxiv.org/abs/1602.07868
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x = nn_linear(in_features = 20, out_features = 40)
weight_norm = nn_utils_weight_norm$new(name = 'weight', dim = 2)
weight_norm$apply(x)
x$weight_g$size()
x$weight_v$size()
x$weight

# the recompute() method recomputes the weight using g and v. It must be called
# explicitly inside `forward()`.
weight_norm$recompute(x)

}
</code></pre>

<hr>
<h2 id='nnf_adaptive_avg_pool1d'>Adaptive_avg_pool1d</h2><span id='topic+nnf_adaptive_avg_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_avg_pool1d(input, output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_avg_pool1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch , in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_avg_pool1d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer)</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_adaptive_avg_pool2d'>Adaptive_avg_pool2d</h2><span id='topic+nnf_adaptive_avg_pool2d'></span>

<h3>Description</h3>

<p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_avg_pool2d(input, output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_avg_pool2d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_avg_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer or double-integer tuple)</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_adaptive_avg_pool3d'>Adaptive_avg_pool3d</h2><span id='topic+nnf_adaptive_avg_pool3d'></span>

<h3>Description</h3>

<p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_avg_pool3d(input, output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_avg_pool3d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iT * iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_avg_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer or triple-integer tuple)</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_adaptive_max_pool1d'>Adaptive_max_pool1d</h2><span id='topic+nnf_adaptive_max_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_max_pool1d(input, output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_max_pool1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch , in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool1d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool1d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return pooling indices. Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_adaptive_max_pool2d'>Adaptive_max_pool2d</h2><span id='topic+nnf_adaptive_max_pool2d'></span>

<h3>Description</h3>

<p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_max_pool2d(input, output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_max_pool2d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer or double-integer tuple)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return pooling indices. Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_adaptive_max_pool3d'>Adaptive_max_pool3d</h2><span id='topic+nnf_adaptive_max_pool3d'></span>

<h3>Description</h3>

<p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_adaptive_max_pool3d(input, output_size, return_indices = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_adaptive_max_pool3d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iT * iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer or triple-integer tuple)</p>
</td></tr>
<tr><td><code id="nnf_adaptive_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return pooling indices. Default:<code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_affine_grid'>Affine_grid</h2><span id='topic+nnf_affine_grid'></span>

<h3>Description</h3>

<p>Generates a 2D or 3D flow field (sampling grid), given a batch of
affine matrices <code>theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_affine_grid(theta, size, align_corners = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_affine_grid_+3A_theta">theta</code></td>
<td>
<p>(Tensor) input batch of affine matrices with shape
(<code class="reqn">N \times 2 \times 3</code>) for 2D or  (<code class="reqn">N \times 3 \times 4</code>) for 3D</p>
</td></tr>
<tr><td><code id="nnf_affine_grid_+3A_size">size</code></td>
<td>
<p>(torch.Size) the target output image size. (<code class="reqn">N \times C \times H \times W</code>
for 2D or <code class="reqn">N \times C \times D \times H \times W</code> for 3D)
Example: torch.Size((32, 3, 24, 24))</p>
</td></tr>
<tr><td><code id="nnf_affine_grid_+3A_align_corners">align_corners</code></td>
<td>
<p>(bool, optional) if <code>True</code>, consider <code>-1</code> and <code>1</code>
to refer to the centers of the corner pixels rather than the image corners.
Refer to <code><a href="#topic+nnf_grid_sample">nnf_grid_sample()</a></code> for a more complete description. A grid generated by
<code><a href="#topic+nnf_affine_grid">nnf_affine_grid()</a></code> should be passed to <code><a href="#topic+nnf_grid_sample">nnf_grid_sample()</a></code>  with the same setting for
this option. Default: <code>False</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is often used in conjunction with <code><a href="#topic+nnf_grid_sample">nnf_grid_sample()</a></code>
to build <code style="white-space: pre;">&#8288;Spatial Transformer Networks&#8288;</code>_ .
</p>

<hr>
<h2 id='nnf_alpha_dropout'>Alpha_dropout</h2><span id='topic+nnf_alpha_dropout'></span>

<h3>Description</h3>

<p>Applies alpha dropout to the input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_alpha_dropout(input, p = 0.5, training = FALSE, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_alpha_dropout_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_alpha_dropout_+3A_p">p</code></td>
<td>
<p>probability of an element to be zeroed. Default: 0.5</p>
</td></tr>
<tr><td><code id="nnf_alpha_dropout_+3A_training">training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nnf_alpha_dropout_+3A_inplace">inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place.
Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_avg_pool1d'>Avg_pool1d</h2><span id='topic+nnf_avg_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D average pooling over an input signal composed of several
input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_avg_pool1d(
  input,
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_avg_pool1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch , in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_avg_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kW,)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="nnf_avg_pool1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Can be a single number or a tuple
<code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool1d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_avg_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> to compute the
output shape. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool1d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when True, will include the zero-padding in the
averaging calculation. Default: <code>TRUE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_avg_pool2d'>Avg_pool2d</h2><span id='topic+nnf_avg_pool2d'></span>

<h3>Description</h3>

<p>Applies 2D average-pooling operation in <code class="reqn">kH * kW</code> regions by step size
<code class="reqn">sH * sW</code> steps. The number of output features is equal to the number of
input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_avg_pool2d(
  input,
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE,
  divisor_override = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_avg_pool2d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>size of the pooling region. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_stride">stride</code></td>
<td>
<p>stride of the pooling operation. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> in the formula
to compute the output shape. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when True, will include the zero-padding in the
averaging calculation. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool2d_+3A_divisor_override">divisor_override</code></td>
<td>
<p>if specified, it will be used as divisor, otherwise
size of the pooling region will be used. Default: <code>NULL</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_avg_pool3d'>Avg_pool3d</h2><span id='topic+nnf_avg_pool3d'></span>

<h3>Description</h3>

<p>Applies 3D average-pooling operation in <code class="reqn">kT * kH * kW</code> regions by step
size <code class="reqn">sT * sH * sW</code> steps. The number of output features is equal to
<code class="reqn">\lfloor \frac{ \mbox{input planes} }{sT} \rfloor</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_avg_pool3d(
  input,
  kernel_size,
  stride = NULL,
  padding = 0,
  ceil_mode = FALSE,
  count_include_pad = TRUE,
  divisor_override = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_avg_pool3d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iT * iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>size of the pooling region. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kT, kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_stride">stride</code></td>
<td>
<p>stride of the pooling operation. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>, Default: 0</p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> in the formula
to compute the output shape</p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when True, will include the zero-padding in the
averaging calculation</p>
</td></tr>
<tr><td><code id="nnf_avg_pool3d_+3A_divisor_override">divisor_override</code></td>
<td>
<p>NA if specified, it will be used as divisor, otherwise
size of the pooling region will be used. Default: <code>NULL</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_batch_norm'>Batch_norm</h2><span id='topic+nnf_batch_norm'></span>

<h3>Description</h3>

<p>Applies Batch Normalization for each channel across a batch of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_batch_norm(
  input,
  running_mean,
  running_var,
  weight = NULL,
  bias = NULL,
  training = FALSE,
  momentum = 0.1,
  eps = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_batch_norm_+3A_input">input</code></td>
<td>
<p>input tensor</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_running_mean">running_mean</code></td>
<td>
<p>the running_mean tensor</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_running_var">running_var</code></td>
<td>
<p>the running_var tensor</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_weight">weight</code></td>
<td>
<p>the weight tensor</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_bias">bias</code></td>
<td>
<p>the bias tensor</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_training">training</code></td>
<td>
<p>bool wether it's training. Default: FALSE</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_momentum">momentum</code></td>
<td>
<p>the value used for the <code>running_mean</code> and <code>running_var</code> computation.
Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1</p>
</td></tr>
<tr><td><code id="nnf_batch_norm_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability. Default: 1e-5</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_bilinear'>Bilinear</h2><span id='topic+nnf_bilinear'></span>

<h3>Description</h3>

<p>Applies a bilinear transformation to the incoming data:
<code class="reqn">y = x_1 A x_2 + b</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_bilinear(input1, input2, weight, bias = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_bilinear_+3A_input1">input1</code></td>
<td>
<p><code class="reqn">(N, *, H_{in1})</code> where <code class="reqn">H_{in1}=\mbox{in1\_features}</code>
and <code class="reqn">*</code> means any number of additional dimensions.
All but the last dimension of the inputs should be the same.</p>
</td></tr>
<tr><td><code id="nnf_bilinear_+3A_input2">input2</code></td>
<td>
<p><code class="reqn">(N, *, H_{in2})</code> where <code class="reqn">H_{in2}=\mbox{in2\_features}</code></p>
</td></tr>
<tr><td><code id="nnf_bilinear_+3A_weight">weight</code></td>
<td>
<p><code class="reqn">(\mbox{out\_features}, \mbox{in1\_features},
\mbox{in2\_features})</code></p>
</td></tr>
<tr><td><code id="nnf_bilinear_+3A_bias">bias</code></td>
<td>
<p><code class="reqn">(\mbox{out\_features})</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>output <code class="reqn">(N, *, H_{out})</code> where <code class="reqn">H_{out}=\mbox{out\_features}</code>
and all but the last dimension are the same shape as the input.
</p>

<hr>
<h2 id='nnf_binary_cross_entropy'>Binary_cross_entropy</h2><span id='topic+nnf_binary_cross_entropy'></span>

<h3>Description</h3>

<p>Function that measures the Binary Cross Entropy
between the target and the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_binary_cross_entropy(
  input,
  target,
  weight = NULL,
  reduction = c("mean", "sum", "none")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_binary_cross_entropy_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_+3A_weight">weight</code></td>
<td>
<p>(tensor) weight for each value.</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_binary_cross_entropy_with_logits'>Binary_cross_entropy_with_logits</h2><span id='topic+nnf_binary_cross_entropy_with_logits'></span>

<h3>Description</h3>

<p>Function that measures Binary Cross Entropy between target and output
logits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_binary_cross_entropy_with_logits(
  input,
  target,
  weight = NULL,
  reduction = c("mean", "sum", "none"),
  pos_weight = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_binary_cross_entropy_with_logits_+3A_input">input</code></td>
<td>
<p>Tensor of arbitrary shape</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_with_logits_+3A_target">target</code></td>
<td>
<p>Tensor of the same shape as input</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_with_logits_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional) a manual rescaling weight if provided it's
repeated to match input tensor shape.</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_with_logits_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
<tr><td><code id="nnf_binary_cross_entropy_with_logits_+3A_pos_weight">pos_weight</code></td>
<td>
<p>(Tensor, optional) a weight of positive examples.
Must be a vector with length equal to the number of classes.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_celu'>Celu</h2><span id='topic+nnf_celu'></span><span id='topic+nnf_celu_'></span>

<h3>Description</h3>

<p>Applies element-wise, <code class="reqn">CELU(x) = max(0,x) + min(0, \alpha * (exp(x \alpha) - 1))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_celu(input, alpha = 1, inplace = FALSE)

nnf_celu_(input, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_celu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_celu_+3A_alpha">alpha</code></td>
<td>
<p>the alpha value for the CELU formulation. Default: 1.0</p>
</td></tr>
<tr><td><code id="nnf_celu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_contrib_sparsemax'>Sparsemax</h2><span id='topic+nnf_contrib_sparsemax'></span>

<h3>Description</h3>

<p>Applies the SparseMax activation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_contrib_sparsemax(input, dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_contrib_sparsemax_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_contrib_sparsemax_+3A_dim">dim</code></td>
<td>
<p>The dimension over which to apply the sparsemax function. (-1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SparseMax activation is described in
<a href="https://arxiv.org/abs/1602.02068">'From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification'</a>
The implementation is based on <a href="https://github.com/aced125/sparsemax/tree/master/sparsemax">aced125/sparsemax</a>
</p>

<hr>
<h2 id='nnf_conv_tbc'>Conv_tbc</h2><span id='topic+nnf_conv_tbc'></span>

<h3>Description</h3>

<p>Applies a 1-dimensional sequence convolution over an input sequence.
Input and output dimensions are (Time, Batch, Channels) - hence TBC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv_tbc(input, weight, bias, pad = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv_tbc_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{sequence length} \times
batch \times \mbox{in\_channels})</code></p>
</td></tr>
<tr><td><code id="nnf_conv_tbc_+3A_weight">weight</code></td>
<td>
<p>filter of shape (<code class="reqn">\mbox{kernel width} \times \mbox{in\_channels}
\times \mbox{out\_channels}</code>)</p>
</td></tr>
<tr><td><code id="nnf_conv_tbc_+3A_bias">bias</code></td>
<td>
<p>bias of shape (<code class="reqn">\mbox{out\_channels}</code>)</p>
</td></tr>
<tr><td><code id="nnf_conv_tbc_+3A_pad">pad</code></td>
<td>
<p>number of timesteps to pad. Default: 0</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv_transpose1d'>Conv_transpose1d</h2><span id='topic+nnf_conv_transpose1d'></span>

<h3>Description</h3>

<p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called &quot;deconvolution&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv_transpose1d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  dilation = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv_transpose1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels, in_channels/groups , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or
a one-element tuple <code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a one-element tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_output_padding">output_padding</code></td>
<td>
<p>padding applied to the output</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by
the number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose1d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a one-element tuple <code style="white-space: pre;">&#8288;(dW,)&#8288;</code>. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv_transpose2d'>Conv_transpose2d</h2><span id='topic+nnf_conv_transpose2d'></span>

<h3>Description</h3>

<p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called &quot;deconvolution&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv_transpose2d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  dilation = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv_transpose2d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels, iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels , in_channels/groups, kH , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_output_padding">output_padding</code></td>
<td>
<p>padding applied to the output</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by the
number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose2d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a tuple <code style="white-space: pre;">&#8288;(dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv_transpose3d'>Conv_transpose3d</h2><span id='topic+nnf_conv_transpose3d'></span>

<h3>Description</h3>

<p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called &quot;deconvolution&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv_transpose3d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  dilation = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv_transpose3d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels , iT , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels , in_channels/groups, kT , kH , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_output_padding">output_padding</code></td>
<td>
<p>padding applied to the output</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by
the number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv_transpose3d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a tuple <code style="white-space: pre;">&#8288;(dT, dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv1d'>Conv1d</h2><span id='topic+nnf_conv1d'></span>

<h3>Description</h3>

<p>Applies a 1D convolution over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv1d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels, in_channels/groups , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or
a one-element tuple <code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a one-element tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a one-element tuple <code style="white-space: pre;">&#8288;(dW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv1d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by
the number of groups. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv2d'>Conv2d</h2><span id='topic+nnf_conv2d'></span>

<h3>Description</h3>

<p>Applies a 2D convolution over an input image composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv2d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv2d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels, iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels , in_channels/groups, kH , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a tuple <code style="white-space: pre;">&#8288;(dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv2d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by the
number of groups. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_conv3d'>Conv3d</h2><span id='topic+nnf_conv3d'></span>

<h3>Description</h3>

<p>Applies a 3D convolution over an input image composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_conv3d(
  input,
  weight,
  bias = NULL,
  stride = 1,
  padding = 0,
  dilation = 1,
  groups = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_conv3d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch, in_channels , iT , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_weight">weight</code></td>
<td>
<p>filters of shape (out_channels , in_channels/groups, kT , kH , kW)</p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape (out_channels). Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or
a tuple <code style="white-space: pre;">&#8288;(dT, dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_conv3d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code>in_channels</code> should be divisible by
the number of groups. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_cosine_embedding_loss'>Cosine_embedding_loss</h2><span id='topic+nnf_cosine_embedding_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the loss given input tensors x_1, x_2 and a
Tensor label y with values 1 or -1. This is used for measuring whether two inputs
are similar or dissimilar, using the cosine distance, and is typically used
for learning nonlinear embeddings or semi-supervised learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_cosine_embedding_loss(
  input1,
  input2,
  target,
  margin = 0,
  reduction = c("mean", "sum", "none")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_cosine_embedding_loss_+3A_input1">input1</code></td>
<td>
<p>the input x_1 tensor</p>
</td></tr>
<tr><td><code id="nnf_cosine_embedding_loss_+3A_input2">input2</code></td>
<td>
<p>the input x_2 tensor</p>
</td></tr>
<tr><td><code id="nnf_cosine_embedding_loss_+3A_target">target</code></td>
<td>
<p>the target tensor</p>
</td></tr>
<tr><td><code id="nnf_cosine_embedding_loss_+3A_margin">margin</code></td>
<td>
<p>Should be a number from -1 to 1 , 0 to 0.5 is suggested. If margin
is missing, the default value is 0.</p>
</td></tr>
<tr><td><code id="nnf_cosine_embedding_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_cosine_similarity'>Cosine_similarity</h2><span id='topic+nnf_cosine_similarity'></span>

<h3>Description</h3>

<p>Returns cosine similarity between x1 and x2, computed along dim.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_cosine_similarity(x1, x2, dim = 2, eps = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_cosine_similarity_+3A_x1">x1</code></td>
<td>
<p>(Tensor) First input.</p>
</td></tr>
<tr><td><code id="nnf_cosine_similarity_+3A_x2">x2</code></td>
<td>
<p>(Tensor) Second input (of size matching x1).</p>
</td></tr>
<tr><td><code id="nnf_cosine_similarity_+3A_dim">dim</code></td>
<td>
<p>(int, optional) Dimension of vectors. Default: 2</p>
</td></tr>
<tr><td><code id="nnf_cosine_similarity_+3A_eps">eps</code></td>
<td>
<p>(float, optional) Small value to avoid division by zero.
Default: 1e-8</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
    \mbox{similarity} = \frac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}
</code>
</p>


<hr>
<h2 id='nnf_cross_entropy'>Cross_entropy</h2><span id='topic+nnf_cross_entropy'></span>

<h3>Description</h3>

<p>This criterion combines <code>log_softmax</code> and <code>nll_loss</code> in a single
function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_cross_entropy(
  input,
  target,
  weight = NULL,
  ignore_index = -100,
  reduction = c("mean", "sum", "none")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_cross_entropy_+3A_input">input</code></td>
<td>
<p>(Tensor) <code class="reqn">(N, C)</code> where <code style="white-space: pre;">&#8288;C = number of classes&#8288;</code> or <code class="reqn">(N, C, H, W)</code>
in case of 2D Loss, or <code class="reqn">(N, C, d_1, d_2, ..., d_K)</code> where <code class="reqn">K \geq 1</code>
in the case of K-dimensional loss.</p>
</td></tr>
<tr><td><code id="nnf_cross_entropy_+3A_target">target</code></td>
<td>
<p>(Tensor) <code class="reqn">(N)</code> where each value is <code class="reqn">0 \leq \mbox{targets}[i] \leq C-1</code>,
or <code class="reqn">(N, d_1, d_2, ..., d_K)</code> where <code class="reqn">K \geq 1</code> for K-dimensional loss.</p>
</td></tr>
<tr><td><code id="nnf_cross_entropy_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional) a manual rescaling weight given to each class. If
given, has to be a Tensor of size <code>C</code></p>
</td></tr>
<tr><td><code id="nnf_cross_entropy_+3A_ignore_index">ignore_index</code></td>
<td>
<p>(int, optional) Specifies a target value that is ignored
and does not contribute to the input gradient.</p>
</td></tr>
<tr><td><code id="nnf_cross_entropy_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_ctc_loss'>Ctc_loss</h2><span id='topic+nnf_ctc_loss'></span>

<h3>Description</h3>

<p>The Connectionist Temporal Classification loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_ctc_loss(
  log_probs,
  targets,
  input_lengths,
  target_lengths,
  blank = 0,
  reduction = c("mean", "sum", "none"),
  zero_infinity = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_ctc_loss_+3A_log_probs">log_probs</code></td>
<td>
<p><code class="reqn">(T, N, C)</code> where C = number of characters in alphabet including blank,
T = input length, and N = batch size. The logarithmized probabilities of
the outputs (e.g. obtained with <a href="#topic+nnf_log_softmax">nnf_log_softmax</a>).</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_targets">targets</code></td>
<td>
<p><code class="reqn">(N, S)</code> or <code>(sum(target_lengths))</code>. Targets cannot be blank.
In the second form, the targets are assumed to be concatenated.</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_input_lengths">input_lengths</code></td>
<td>
<p><code class="reqn">(N)</code>. Lengths of the inputs (must each be <code class="reqn">\leq T</code>)</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_target_lengths">target_lengths</code></td>
<td>
<p><code class="reqn">(N)</code>. Lengths of the targets</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_blank">blank</code></td>
<td>
<p>(int, optional) Blank label. Default <code class="reqn">0</code>.</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
<tr><td><code id="nnf_ctc_loss_+3A_zero_infinity">zero_infinity</code></td>
<td>
<p>(bool, optional) Whether to zero infinite losses and the
associated gradients. Default: <code>FALSE</code> Infinite losses mainly occur when the
inputs are too short to be aligned to the targets.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_dropout'>Dropout</h2><span id='topic+nnf_dropout'></span>

<h3>Description</h3>

<p>During training, randomly zeroes some of the elements of the input
tensor with probability <code>p</code> using samples from a Bernoulli
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_dropout(input, p = 0.5, training = TRUE, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_dropout_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_dropout_+3A_p">p</code></td>
<td>
<p>probability of an element to be zeroed. Default: 0.5</p>
</td></tr>
<tr><td><code id="nnf_dropout_+3A_training">training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>. Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nnf_dropout_+3A_inplace">inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place.
Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_dropout2d'>Dropout2d</h2><span id='topic+nnf_dropout2d'></span>

<h3>Description</h3>

<p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <code class="reqn">j</code>-th channel of the <code class="reqn">i</code>-th sample in the
batched input is a 2D tensor <code class="reqn">input[i, j]</code>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code>p</code> using samples from a Bernoulli distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_dropout2d(input, p = 0.5, training = TRUE, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_dropout2d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_dropout2d_+3A_p">p</code></td>
<td>
<p>probability of a channel to be zeroed. Default: 0.5</p>
</td></tr>
<tr><td><code id="nnf_dropout2d_+3A_training">training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>. Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nnf_dropout2d_+3A_inplace">inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place.
Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_dropout3d'>Dropout3d</h2><span id='topic+nnf_dropout3d'></span>

<h3>Description</h3>

<p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <code class="reqn">j</code>-th channel of the <code class="reqn">i</code>-th sample in the
batched input is a 3D tensor <code class="reqn">input[i, j]</code>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code>p</code> using samples from a Bernoulli distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_dropout3d(input, p = 0.5, training = TRUE, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_dropout3d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_dropout3d_+3A_p">p</code></td>
<td>
<p>probability of a channel to be zeroed. Default: 0.5</p>
</td></tr>
<tr><td><code id="nnf_dropout3d_+3A_training">training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>. Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nnf_dropout3d_+3A_inplace">inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place.
Default: <code>FALSE</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_elu'>Elu</h2><span id='topic+nnf_elu'></span><span id='topic+nnf_elu_'></span>

<h3>Description</h3>

<p>Applies element-wise,
</p>
<p style="text-align: center;"><code class="reqn">ELU(x) = max(0,x) + min(0, \alpha * (exp(x) - 1))</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_elu(input, alpha = 1, inplace = FALSE)

nnf_elu_(input, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_elu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_elu_+3A_alpha">alpha</code></td>
<td>
<p>the alpha value for the ELU formulation. Default: 1.0</p>
</td></tr>
<tr><td><code id="nnf_elu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_randn(2, 2)
y &lt;- nnf_elu(x, alpha = 1)
nnf_elu_(x, alpha = 1)
torch_equal(x, y)
}
</code></pre>

<hr>
<h2 id='nnf_embedding'>Embedding</h2><span id='topic+nnf_embedding'></span>

<h3>Description</h3>

<p>A simple lookup table that looks up embeddings in a fixed dictionary and size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_embedding(
  input,
  weight,
  padding_idx = NULL,
  max_norm = NULL,
  norm_type = 2,
  scale_grad_by_freq = FALSE,
  sparse = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_embedding_+3A_input">input</code></td>
<td>
<p>(LongTensor) Tensor containing indices into the embedding matrix</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_weight">weight</code></td>
<td>
<p>(Tensor) The embedding matrix with number of rows equal to the
maximum possible index + 1, and number of columns equal to the embedding size</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_padding_idx">padding_idx</code></td>
<td>
<p>(int, optional) If given, pads the output with the embedding
vector at <code>padding_idx</code> (initialized to zeros) whenever it encounters the index.</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_max_norm">max_norm</code></td>
<td>
<p>(float, optional) If given, each embedding vector with norm larger
than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>. Note: this will modify
<code>weight</code> in-place.</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_norm_type">norm_type</code></td>
<td>
<p>(float, optional) The p of the p-norm to compute for the <code>max_norm</code>
option. Default <code>2</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_scale_grad_by_freq">scale_grad_by_freq</code></td>
<td>
<p>(boolean, optional) If given, this will scale gradients
by the inverse of frequency of the words in the mini-batch. Default <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_+3A_sparse">sparse</code></td>
<td>
<p>(bool, optional) If <code>TRUE</code>, gradient w.r.t. <code>weight</code> will be a
sparse tensor. See Notes under <code>nn_embedding</code> for more details regarding
sparse gradients.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This module is often used to retrieve word embeddings using indices.
The input to the module is a list of indices, and the embedding matrix,
and the output is the corresponding word embeddings.
</p>

<hr>
<h2 id='nnf_embedding_bag'>Embedding_bag</h2><span id='topic+nnf_embedding_bag'></span>

<h3>Description</h3>

<p>Computes sums, means or maxes of <code>bags</code> of embeddings, without instantiating the
intermediate embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_embedding_bag(
  input,
  weight,
  offsets = NULL,
  max_norm = NULL,
  norm_type = 2,
  scale_grad_by_freq = FALSE,
  mode = "mean",
  sparse = FALSE,
  per_sample_weights = NULL,
  include_last_offset = FALSE,
  padding_idx = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_embedding_bag_+3A_input">input</code></td>
<td>
<p>(LongTensor) Tensor containing bags of indices into the embedding matrix</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_weight">weight</code></td>
<td>
<p>(Tensor) The embedding matrix with number of rows equal to the
maximum possible index + 1, and number of columns equal to the embedding size</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_offsets">offsets</code></td>
<td>
<p>(LongTensor, optional) Only used when <code>input</code> is 1D. <code>offsets</code>
determines the starting index position of each bag (sequence) in <code>input</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_max_norm">max_norm</code></td>
<td>
<p>(float, optional) If given, each embedding vector with norm
larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.
Note: this will modify <code>weight</code> in-place.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_norm_type">norm_type</code></td>
<td>
<p>(float, optional) The <code>p</code> in the <code>p</code>-norm to compute for the
<code>max_norm</code> option. Default <code>2</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_scale_grad_by_freq">scale_grad_by_freq</code></td>
<td>
<p>(boolean, optional) if given, this will scale gradients
by the inverse of frequency of the words in the mini-batch. Default <code>FALSE</code>.                                            Note: this option is not supported when <code>mode="max"</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_mode">mode</code></td>
<td>
<p>(string, optional) <code>"sum"</code>, <code>"mean"</code> or <code>"max"</code>. Specifies
the way to reduce the bag. Default: 'mean'</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_sparse">sparse</code></td>
<td>
<p>(bool, optional) if <code>TRUE</code>, gradient w.r.t. <code>weight</code> will be a
sparse tensor. See Notes under <code>nn_embedding</code> for more details regarding
sparse gradients. Note: this option is not supported when <code>mode="max"</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_per_sample_weights">per_sample_weights</code></td>
<td>
<p>(Tensor, optional) a tensor of float / double weights,
or NULL to indicate all weights should be taken to be 1. If specified,
<code>per_sample_weights</code> must have exactly the same shape as input and is treated
as having the same <code>offsets</code>, if those are not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_include_last_offset">include_last_offset</code></td>
<td>
<p>(bool, optional) if <code>TRUE</code>, the size of offsets is
equal to the number of bags + 1.</p>
</td></tr>
<tr><td><code id="nnf_embedding_bag_+3A_padding_idx">padding_idx</code></td>
<td>
<p>(int, optional) If given, pads the output with the embedding
vector at <code>padding_idx</code> (initialized to zeros) whenever it encounters the index.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_fold'>Fold</h2><span id='topic+nnf_fold'></span>

<h3>Description</h3>

<p>Combines an array of sliding local blocks into a large containing
tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_fold(
  input,
  output_size,
  kernel_size,
  dilation = 1,
  padding = 0,
  stride = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_fold_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_fold_+3A_output_size">output_size</code></td>
<td>
<p>the shape of the spatial dimensions of the output (i.e.,
<code>output$sizes()[-c(1,2)]</code>)</p>
</td></tr>
<tr><td><code id="nnf_fold_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the sliding blocks</p>
</td></tr>
<tr><td><code id="nnf_fold_+3A_dilation">dilation</code></td>
<td>
<p>a parameter that controls the stride of elements within the
neighborhood. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_fold_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides of input.
Default: 0</p>
</td></tr>
<tr><td><code id="nnf_fold_+3A_stride">stride</code></td>
<td>
<p>the stride of the sliding blocks in the input spatial dimensions.
Default: 1</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>Currently, only 4-D output tensors (batched image-like tensors) are
supported.
</p>

<hr>
<h2 id='nnf_fractional_max_pool2d'>Fractional_max_pool2d</h2><span id='topic+nnf_fractional_max_pool2d'></span>

<h3>Description</h3>

<p>Applies 2D fractional max pooling over an input signal composed of several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_fractional_max_pool2d(
  input,
  kernel_size,
  output_size = NULL,
  output_ratio = NULL,
  return_indices = FALSE,
  random_samples = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over. Can be a
single number <code class="reqn">k</code> (for a square kernel of <code class="reqn">k * k</code>) or
a tuple <code style="white-space: pre;">&#8288;(kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the image of the form <code class="reqn">oH * oW</code>.
Can be a tuple <code style="white-space: pre;">&#8288;(oH, oW)&#8288;</code> or a single number <code class="reqn">oH</code> for a square image <code class="reqn">oH * oH</code></p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_output_ratio">output_ratio</code></td>
<td>
<p>If one wants to have an output size as a ratio of the input size,
this option can be given. This has to be a number or tuple in the range (0, 1)</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>True</code>, will return the indices along with the outputs.</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool2d_+3A_random_samples">random_samples</code></td>
<td>
<p>optional random samples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fractional MaxPooling is described in detail in the paper <code style="white-space: pre;">&#8288;Fractional MaxPooling&#8288;</code>_ by Ben Graham
</p>
<p>The max-pooling operation is applied in <code class="reqn">kH * kW</code> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.
</p>

<hr>
<h2 id='nnf_fractional_max_pool3d'>Fractional_max_pool3d</h2><span id='topic+nnf_fractional_max_pool3d'></span>

<h3>Description</h3>

<p>Applies 3D fractional max pooling over an input signal composed of several input planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_fractional_max_pool3d(
  input,
  kernel_size,
  output_size = NULL,
  output_ratio = NULL,
  return_indices = FALSE,
  random_samples = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window to take a max over. Can be a single number <code class="reqn">k</code>
(for a square kernel of <code class="reqn">k * k * k</code>) or a tuple <code style="white-space: pre;">&#8288;(kT, kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size of the form <code class="reqn">oT * oH * oW</code>.
Can be a tuple <code style="white-space: pre;">&#8288;(oT, oH, oW)&#8288;</code> or a single number <code class="reqn">oH</code> for a cubic output
<code class="reqn">oH * oH * oH</code></p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_output_ratio">output_ratio</code></td>
<td>
<p>If one wants to have an output size as a ratio of the
input size, this option can be given. This has to be a number or tuple in the
range (0, 1)</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>if <code>True</code>, will return the indices along with the outputs.</p>
</td></tr>
<tr><td><code id="nnf_fractional_max_pool3d_+3A_random_samples">random_samples</code></td>
<td>
<p>undocumented argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fractional MaxPooling is described in detail in the paper <code style="white-space: pre;">&#8288;Fractional MaxPooling&#8288;</code>_ by Ben Graham
</p>
<p>The max-pooling operation is applied in <code class="reqn">kT * kH * kW</code> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.
</p>

<hr>
<h2 id='nnf_gelu'>Gelu</h2><span id='topic+nnf_gelu'></span>

<h3>Description</h3>

<p>Gelu
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_gelu(input, approximate = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_gelu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_gelu_+3A_approximate">approximate</code></td>
<td>
<p>By default it's none, and applies element-wise x*pnorm(x),
if 'tanh', then GELU is estimated. See <a href="https://arxiv.org/abs/1606.08415">GELU</a> for
more info.</p>
</td></tr>
</table>


<h3>gelu(input) -&gt; Tensor </h3>

<p>Applies element-wise the function
<code class="reqn">GELU(x) = x * \Phi(x)</code>
</p>
<p>where <code class="reqn">\Phi(x)</code> is the Cumulative Distribution Function for
Gaussian Distribution.
</p>
<p>See <a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>.
</p>

<hr>
<h2 id='nnf_glu'>Glu</h2><span id='topic+nnf_glu'></span>

<h3>Description</h3>

<p>The gated linear unit. Computes:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_glu(input, dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_glu_+3A_input">input</code></td>
<td>
<p>(Tensor) input tensor</p>
</td></tr>
<tr><td><code id="nnf_glu_+3A_dim">dim</code></td>
<td>
<p>(int) dimension on which to split the input. Default: -1</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">GLU(a, b) = a \otimes \sigma(b)</code>
</p>

<p>where <code>input</code> is split in half along <code>dim</code> to form <code>a</code> and <code>b</code>, <code class="reqn">\sigma</code>
is the sigmoid function and <code class="reqn">\otimes</code> is the element-wise product
between matrices.
</p>
<p>See <a href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.
</p>

<hr>
<h2 id='nnf_grid_sample'>Grid_sample</h2><span id='topic+nnf_grid_sample'></span>

<h3>Description</h3>

<p>Given an <code>input</code> and a flow-field <code>grid</code>, computes the
<code>output</code> using <code>input</code> values and pixel locations from <code>grid</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_grid_sample(
  input,
  grid,
  mode = c("bilinear", "nearest"),
  padding_mode = c("zeros", "border", "reflection"),
  align_corners = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_grid_sample_+3A_input">input</code></td>
<td>
<p>(Tensor) input of shape <code class="reqn">(N, C, H_{\mbox{in}}, W_{\mbox{in}})</code> (4-D case)                    or <code class="reqn">(N, C, D_{\mbox{in}}, H_{\mbox{in}}, W_{\mbox{in}})</code> (5-D case)</p>
</td></tr>
<tr><td><code id="nnf_grid_sample_+3A_grid">grid</code></td>
<td>
<p>(Tensor) flow-field of shape <code class="reqn">(N, H_{\mbox{out}}, W_{\mbox{out}}, 2)</code> (4-D case)                   or <code class="reqn">(N, D_{\mbox{out}}, H_{\mbox{out}}, W_{\mbox{out}}, 3)</code> (5-D case)</p>
</td></tr>
<tr><td><code id="nnf_grid_sample_+3A_mode">mode</code></td>
<td>
<p>(str) interpolation mode to calculate output values <code>'bilinear'</code> | <code>'nearest'</code>.
Default: <code>'bilinear'</code></p>
</td></tr>
<tr><td><code id="nnf_grid_sample_+3A_padding_mode">padding_mode</code></td>
<td>
<p>(str) padding mode for outside grid values <code>'zeros'</code> | <code>'border'</code>
| <code>'reflection'</code>. Default: <code>'zeros'</code></p>
</td></tr>
<tr><td><code id="nnf_grid_sample_+3A_align_corners">align_corners</code></td>
<td>
<p>(bool, optional) Geometrically, we consider the pixels of the
input  as squares rather than points. If set to <code>True</code>, the extrema (<code>-1</code> and
<code>1</code>) are considered as referring to the center points of the input's corner pixels.
If set to <code>False</code>, they are instead considered as referring to the corner
points of the input's corner pixels, making the sampling more resolution
agnostic. This option parallels the <code>align_corners</code> option in  <code><a href="#topic+nnf_interpolate">nnf_interpolate()</a></code>, and
so whichever option is used here should also be used there to resize the input
image before grid sampling. Default: <code>False</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, only spatial (4-D) and volumetric (5-D) <code>input</code> are
supported.
</p>
<p>In the spatial (4-D) case, for <code>input</code> with shape
<code class="reqn">(N, C, H_{\mbox{in}}, W_{\mbox{in}})</code> and <code>grid</code> with shape
<code class="reqn">(N, H_{\mbox{out}}, W_{\mbox{out}}, 2)</code>, the output will have shape
<code class="reqn">(N, C, H_{\mbox{out}}, W_{\mbox{out}})</code>.
</p>
<p>For each output location <code style="white-space: pre;">&#8288;output[n, :, h, w]&#8288;</code>, the size-2 vector
<code>grid[n, h, w]</code> specifies <code>input</code> pixel locations <code>x</code> and <code>y</code>,
which are used to interpolate the output value <code style="white-space: pre;">&#8288;output[n, :, h, w]&#8288;</code>.
In the case of 5D inputs, <code>grid[n, d, h, w]</code> specifies the
<code>x</code>, <code>y</code>, <code>z</code> pixel locations for interpolating
<code style="white-space: pre;">&#8288;output[n, :, d, h, w]&#8288;</code>. <code>mode</code> argument specifies <code>nearest</code> or
<code>bilinear</code> interpolation method to sample the input pixels.
</p>
<p><code>grid</code> specifies the sampling pixel locations normalized by the
<code>input</code> spatial dimensions. Therefore, it should have most values in
the range of <code style="white-space: pre;">&#8288;[-1, 1]&#8288;</code>. For example, values <code style="white-space: pre;">&#8288;x = -1, y = -1&#8288;</code> is the
left-top pixel of <code>input</code>, and values  <code style="white-space: pre;">&#8288;x = 1, y = 1&#8288;</code> is the
right-bottom pixel of <code>input</code>.
</p>
<p>If <code>grid</code> has values outside the range of <code style="white-space: pre;">&#8288;[-1, 1]&#8288;</code>, the corresponding
outputs are handled as defined by <code>padding_mode</code>. Options are
</p>

<ul>
<li> <p><code>padding_mode="zeros"</code>: use <code>0</code> for out-of-bound grid locations,
</p>
</li>
<li> <p><code>padding_mode="border"</code>: use border values for out-of-bound grid locations,
</p>
</li>
<li> <p><code>padding_mode="reflection"</code>: use values at locations reflected by
the border for out-of-bound grid locations. For location far away
from the border, it will keep being reflected until becoming in bound,
e.g., (normalized) pixel location <code>x = -3.5</code> reflects by border <code>-1</code>
and becomes <code style="white-space: pre;">&#8288;x' = 1.5&#8288;</code>, then reflects by border <code>1</code> and becomes
<code style="white-space: pre;">&#8288;x'' = -0.5&#8288;</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>This function is often used in conjunction with <code><a href="#topic+nnf_affine_grid">nnf_affine_grid()</a></code>
to build <code style="white-space: pre;">&#8288;Spatial Transformer Networks&#8288;</code>_ .
</p>

<hr>
<h2 id='nnf_group_norm'>Group_norm</h2><span id='topic+nnf_group_norm'></span>

<h3>Description</h3>

<p>Applies Group Normalization for last certain number of dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_group_norm(input, num_groups, weight = NULL, bias = NULL, eps = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_group_norm_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_group_norm_+3A_num_groups">num_groups</code></td>
<td>
<p>number of groups to separate the channels into</p>
</td></tr>
<tr><td><code id="nnf_group_norm_+3A_weight">weight</code></td>
<td>
<p>the weight tensor</p>
</td></tr>
<tr><td><code id="nnf_group_norm_+3A_bias">bias</code></td>
<td>
<p>the bias tensor</p>
</td></tr>
<tr><td><code id="nnf_group_norm_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability. Default: 1e-5</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_gumbel_softmax'>Gumbel_softmax</h2><span id='topic+nnf_gumbel_softmax'></span>

<h3>Description</h3>

<p>Samples from the Gumbel-Softmax distribution and
optionally discretizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_gumbel_softmax(logits, tau = 1, hard = FALSE, dim = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_gumbel_softmax_+3A_logits">logits</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[..., num_features]&#8288;</code> unnormalized log probabilities</p>
</td></tr>
<tr><td><code id="nnf_gumbel_softmax_+3A_tau">tau</code></td>
<td>
<p>non-negative scalar temperature</p>
</td></tr>
<tr><td><code id="nnf_gumbel_softmax_+3A_hard">hard</code></td>
<td>
<p>if <code>True</code>, the returned samples will be discretized as one-hot vectors,        but will be differentiated as if it is the soft sample in autograd</p>
</td></tr>
<tr><td><code id="nnf_gumbel_softmax_+3A_dim">dim</code></td>
<td>
<p>(int) A dimension along which softmax will be computed. Default: -1.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_hardshrink'>Hardshrink</h2><span id='topic+nnf_hardshrink'></span>

<h3>Description</h3>

<p>Applies the hard shrinkage function element-wise
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_hardshrink(input, lambd = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_hardshrink_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_hardshrink_+3A_lambd">lambd</code></td>
<td>
<p>the lambda value for the Hardshrink formulation. Default: 0.5</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_hardsigmoid'>Hardsigmoid</h2><span id='topic+nnf_hardsigmoid'></span>

<h3>Description</h3>

<p>Applies the element-wise function <code class="reqn">\mbox{Hardsigmoid}(x) = \frac{ReLU6(x + 3)}{6}</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_hardsigmoid(input, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_hardsigmoid_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_hardsigmoid_+3A_inplace">inplace</code></td>
<td>
<p>NA If set to <code>True</code>, will do this operation in-place. Default: <code>False</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_hardswish'>Hardswish</h2><span id='topic+nnf_hardswish'></span>

<h3>Description</h3>

<p>Applies the hardswish function, element-wise, as described in the paper:
Searching for MobileNetV3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_hardswish(input, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_hardswish_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_hardswish_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn"> \mbox{Hardswish}(x) = \left\{
  \begin{array}{ll}
  0 &amp; \mbox{if } x \le -3, \\
  x &amp; \mbox{if } x \ge +3, \\
  x \cdot (x + 3)/6 &amp; \mbox{otherwise}
  \end{array}
  \right. </code>
</p>


<hr>
<h2 id='nnf_hardtanh'>Hardtanh</h2><span id='topic+nnf_hardtanh'></span><span id='topic+nnf_hardtanh_'></span>

<h3>Description</h3>

<p>Applies the HardTanh function element-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_hardtanh(input, min_val = -1, max_val = 1, inplace = FALSE)

nnf_hardtanh_(input, min_val = -1, max_val = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_hardtanh_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_hardtanh_+3A_min_val">min_val</code></td>
<td>
<p>minimum value of the linear region range. Default: -1</p>
</td></tr>
<tr><td><code id="nnf_hardtanh_+3A_max_val">max_val</code></td>
<td>
<p>maximum value of the linear region range. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_hardtanh_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_hinge_embedding_loss'>Hinge_embedding_loss</h2><span id='topic+nnf_hinge_embedding_loss'></span>

<h3>Description</h3>

<p>Measures the loss given an input tensor xx and a labels tensor yy (containing 1 or -1).
This is usually used for measuring whether two inputs are similar or dissimilar, e.g.
using the L1 pairwise distance as xx , and is typically used for learning nonlinear
embeddings or semi-supervised learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_hinge_embedding_loss(input, target, margin = 1, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_hinge_embedding_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_hinge_embedding_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_hinge_embedding_loss_+3A_margin">margin</code></td>
<td>
<p>Has a default value of 1.</p>
</td></tr>
<tr><td><code id="nnf_hinge_embedding_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_instance_norm'>Instance_norm</h2><span id='topic+nnf_instance_norm'></span>

<h3>Description</h3>

<p>Applies Instance Normalization for each channel in each data sample in a
batch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_instance_norm(
  input,
  running_mean = NULL,
  running_var = NULL,
  weight = NULL,
  bias = NULL,
  use_input_stats = TRUE,
  momentum = 0.1,
  eps = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_instance_norm_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_running_mean">running_mean</code></td>
<td>
<p>the running_mean tensor</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_running_var">running_var</code></td>
<td>
<p>the running var tensor</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_weight">weight</code></td>
<td>
<p>the weight tensor</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_bias">bias</code></td>
<td>
<p>the bias tensor</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_use_input_stats">use_input_stats</code></td>
<td>
<p>whether to use input stats</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_momentum">momentum</code></td>
<td>
<p>a double for the momentum</p>
</td></tr>
<tr><td><code id="nnf_instance_norm_+3A_eps">eps</code></td>
<td>
<p>an eps double for numerical stability</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_interpolate'>Interpolate</h2><span id='topic+nnf_interpolate'></span>

<h3>Description</h3>

<p>Down/up samples the input to either the given <code>size</code> or the given
<code>scale_factor</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_interpolate(
  input,
  size = NULL,
  scale_factor = NULL,
  mode = "nearest",
  align_corners = FALSE,
  recompute_scale_factor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_interpolate_+3A_input">input</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="nnf_interpolate_+3A_size">size</code></td>
<td>
<p>(int or <code>Tuple[int]</code> or <code>Tuple[int, int]</code> or <code>Tuple[int, int, int]</code>)
output spatial size.</p>
</td></tr>
<tr><td><code id="nnf_interpolate_+3A_scale_factor">scale_factor</code></td>
<td>
<p>(float or <code>Tuple[float]</code>) multiplier for spatial size.
Has to match input size if it is a tuple.</p>
</td></tr>
<tr><td><code id="nnf_interpolate_+3A_mode">mode</code></td>
<td>
<p>(str) algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear'
| 'bicubic' | 'trilinear' | 'area' Default: 'nearest'</p>
</td></tr>
<tr><td><code id="nnf_interpolate_+3A_align_corners">align_corners</code></td>
<td>
<p>(bool, optional) Geometrically, we consider the pixels
of the input and output as squares rather than points. If set to TRUE,
the input and output tensors are aligned by the center points of their corner
pixels, preserving the values at the corner pixels. If set to False, the
input and output tensors are aligned by the corner points of their corner pixels,
and the interpolation uses edge value padding for out-of-boundary values,
making this operation <em>independent</em> of input size when <code>scale_factor</code> is kept
the same. This only has an effect when <code>mode</code>  is <code>'linear'</code>, <code>'bilinear'</code>,
<code>'bicubic'</code> or <code>'trilinear'</code>.  Default: <code>False</code></p>
</td></tr>
<tr><td><code id="nnf_interpolate_+3A_recompute_scale_factor">recompute_scale_factor</code></td>
<td>
<p>(bool, optional) recompute the scale_factor
for use in the interpolation calculation.  When <code>scale_factor</code> is passed
as a parameter, it is used to compute the <code>output_size</code>.  If <code>recompute_scale_factor</code>
is &ldquo;'True&ldquo; or not specified, a new <code>scale_factor</code> will be computed based on
the output and input sizes for use in the interpolation computation (i.e. the
computation will be identical to if the computed 'output_size' were passed-in
explicitly).  Otherwise, the passed-in 'scale_factor' will be used in the
interpolation computation.  Note that when 'scale_factor' is floating-point,
the recomputed scale_factor may differ from the one passed in due to rounding
and precision issues.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm used for interpolation is determined by <code>mode</code>.
</p>
<p>Currently temporal, spatial and volumetric sampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.
</p>
<p>The input dimensions are interpreted in the form:
<code style="white-space: pre;">&#8288;mini-batch x channels x [optional depth] x [optional height] x width&#8288;</code>.
</p>
<p>The modes available for resizing are: <code>nearest</code>, <code>linear</code> (3D-only),
<code>bilinear</code>, <code>bicubic</code> (4D-only), <code>trilinear</code> (5D-only), <code>area</code>
</p>

<hr>
<h2 id='nnf_kl_div'>Kl_div</h2><span id='topic+nnf_kl_div'></span>

<h3>Description</h3>

<p>The Kullback-Leibler divergence Loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_kl_div(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_kl_div_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_kl_div_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_kl_div_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_l1_loss'>L1_loss</h2><span id='topic+nnf_l1_loss'></span>

<h3>Description</h3>

<p>Function that takes the mean element-wise absolute value difference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_l1_loss(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_l1_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_l1_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_l1_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_layer_norm'>Layer_norm</h2><span id='topic+nnf_layer_norm'></span>

<h3>Description</h3>

<p>Applies Layer Normalization for last certain number of dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_layer_norm(
  input,
  normalized_shape,
  weight = NULL,
  bias = NULL,
  eps = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_layer_norm_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_layer_norm_+3A_normalized_shape">normalized_shape</code></td>
<td>
<p>input shape from an expected input of size. If a single
integer is used, it is treated as a singleton list, and this module will normalize
over the last dimension which is expected to be of that specific size.</p>
</td></tr>
<tr><td><code id="nnf_layer_norm_+3A_weight">weight</code></td>
<td>
<p>the weight tensor</p>
</td></tr>
<tr><td><code id="nnf_layer_norm_+3A_bias">bias</code></td>
<td>
<p>the bias tensor</p>
</td></tr>
<tr><td><code id="nnf_layer_norm_+3A_eps">eps</code></td>
<td>
<p>a value added to the denominator for numerical stability. Default: 1e-5</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_leaky_relu'>Leaky_relu</h2><span id='topic+nnf_leaky_relu'></span>

<h3>Description</h3>

<p>Applies element-wise,
<code class="reqn">LeakyReLU(x) = max(0, x) + negative_slope * min(0, x)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_leaky_relu(input, negative_slope = 0.01, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_leaky_relu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_leaky_relu_+3A_negative_slope">negative_slope</code></td>
<td>
<p>Controls the angle of the negative slope. Default: 1e-2</p>
</td></tr>
<tr><td><code id="nnf_leaky_relu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_linear'>Linear</h2><span id='topic+nnf_linear'></span>

<h3>Description</h3>

<p>Applies a linear transformation to the incoming data: <code class="reqn">y = xA^T + b</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_linear(input, weight, bias = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_linear_+3A_input">input</code></td>
<td>
<p><code class="reqn">(N, *, in\_features)</code> where <code>*</code> means any number of
additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_linear_+3A_weight">weight</code></td>
<td>
<p><code class="reqn">(out\_features, in\_features)</code> the weights tensor.</p>
</td></tr>
<tr><td><code id="nnf_linear_+3A_bias">bias</code></td>
<td>
<p>optional tensor <code class="reqn">(out\_features)</code></p>
</td></tr>
</table>

<hr>
<h2 id='nnf_local_response_norm'>Local_response_norm</h2><span id='topic+nnf_local_response_norm'></span>

<h3>Description</h3>

<p>Applies local response normalization over an input signal composed of
several input planes, where channels occupy the second dimension.
Applies normalization across channels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_local_response_norm(input, size, alpha = 1e-04, beta = 0.75, k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_local_response_norm_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_local_response_norm_+3A_size">size</code></td>
<td>
<p>amount of neighbouring channels used for normalization</p>
</td></tr>
<tr><td><code id="nnf_local_response_norm_+3A_alpha">alpha</code></td>
<td>
<p>multiplicative factor. Default: 0.0001</p>
</td></tr>
<tr><td><code id="nnf_local_response_norm_+3A_beta">beta</code></td>
<td>
<p>exponent. Default: 0.75</p>
</td></tr>
<tr><td><code id="nnf_local_response_norm_+3A_k">k</code></td>
<td>
<p>additive factor. Default: 1</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_log_softmax'>Log_softmax</h2><span id='topic+nnf_log_softmax'></span>

<h3>Description</h3>

<p>Applies a softmax followed by a logarithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_log_softmax(input, dim = NULL, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_log_softmax_+3A_input">input</code></td>
<td>
<p>(Tensor) input</p>
</td></tr>
<tr><td><code id="nnf_log_softmax_+3A_dim">dim</code></td>
<td>
<p>(int) A dimension along which log_softmax will be computed.</p>
</td></tr>
<tr><td><code id="nnf_log_softmax_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.
If specified, the input tensor is casted to <code>dtype</code> before the operation
is performed. This is useful for preventing data type overflows.
Default: <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.
</p>

<hr>
<h2 id='nnf_logsigmoid'>Logsigmoid</h2><span id='topic+nnf_logsigmoid'></span>

<h3>Description</h3>

<p>Applies element-wise <code class="reqn">LogSigmoid(x_i) = log(\frac{1}{1 + exp(-x_i)})</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_logsigmoid(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_logsigmoid_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_lp_pool1d'>Lp_pool1d</h2><span id='topic+nnf_lp_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <code>p</code> is
zero, the gradient is set to zero as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_lp_pool1d(input, norm_type, kernel_size, stride = NULL, ceil_mode = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_lp_pool1d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_lp_pool1d_+3A_norm_type">norm_type</code></td>
<td>
<p>if inf than one gets max pooling if 0 you get sum pooling (
proportional to the avg pooling)</p>
</td></tr>
<tr><td><code id="nnf_lp_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>a single int, the size of the window</p>
</td></tr>
<tr><td><code id="nnf_lp_pool1d_+3A_stride">stride</code></td>
<td>
<p>a single int, the stride of the window. Default value is kernel_size</p>
</td></tr>
<tr><td><code id="nnf_lp_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use ceil instead of floor to compute the output shape</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_lp_pool2d'>Lp_pool2d</h2><span id='topic+nnf_lp_pool2d'></span>

<h3>Description</h3>

<p>Applies a 2D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <code>p</code> is
zero, the gradient is set to zero as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_lp_pool2d(input, norm_type, kernel_size, stride = NULL, ceil_mode = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_lp_pool2d_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_lp_pool2d_+3A_norm_type">norm_type</code></td>
<td>
<p>if inf than one gets max pooling if 0 you get sum pooling (
proportional to the avg pooling)</p>
</td></tr>
<tr><td><code id="nnf_lp_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>a single int, the size of the window</p>
</td></tr>
<tr><td><code id="nnf_lp_pool2d_+3A_stride">stride</code></td>
<td>
<p>a single int, the stride of the window. Default value is kernel_size</p>
</td></tr>
<tr><td><code id="nnf_lp_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use ceil instead of floor to compute the output shape</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_margin_ranking_loss'>Margin_ranking_loss</h2><span id='topic+nnf_margin_ranking_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the loss given inputs x1 , x2 , two 1D
mini-batch Tensors, and a label 1D mini-batch tensor y (containing 1 or -1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_margin_ranking_loss(input1, input2, target, margin = 0, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_margin_ranking_loss_+3A_input1">input1</code></td>
<td>
<p>the first tensor</p>
</td></tr>
<tr><td><code id="nnf_margin_ranking_loss_+3A_input2">input2</code></td>
<td>
<p>the second input tensor</p>
</td></tr>
<tr><td><code id="nnf_margin_ranking_loss_+3A_target">target</code></td>
<td>
<p>the target tensor</p>
</td></tr>
<tr><td><code id="nnf_margin_ranking_loss_+3A_margin">margin</code></td>
<td>
<p>Has a default value of 00 .</p>
</td></tr>
<tr><td><code id="nnf_margin_ranking_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_pool1d'>Max_pool1d</h2><span id='topic+nnf_max_pool1d'></span>

<h3>Description</h3>

<p>Applies a 1D max pooling over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_pool1d(
  input,
  kernel_size,
  stride = NULL,
  padding = 0,
  dilation = 1,
  ceil_mode = FALSE,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_pool1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape (minibatch , in_channels , iW)</p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kW,)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Can be a single number or a tuple
<code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_dilation">dilation</code></td>
<td>
<p>controls the spacing between the kernel points; also known as
the  trous algorithm.</p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> to compute the
output shape. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool1d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return the indices where the max occurs.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_pool2d'>Max_pool2d</h2><span id='topic+nnf_max_pool2d'></span>

<h3>Description</h3>

<p>Applies a 2D max pooling over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_pool2d(
  input,
  kernel_size,
  stride = kernel_size,
  padding = 0,
  dilation = 1,
  ceil_mode = FALSE,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_pool2d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>size of the pooling region. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_stride">stride</code></td>
<td>
<p>stride of the pooling operation. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_dilation">dilation</code></td>
<td>
<p>controls the spacing between the kernel points; also known as
the  trous algorithm.</p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> in the formula
to compute the output shape. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool2d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return the indices where the max occurs.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_pool3d'>Max_pool3d</h2><span id='topic+nnf_max_pool3d'></span>

<h3>Description</h3>

<p>Applies a 3D max pooling over an input signal composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_pool3d(
  input,
  kernel_size,
  stride = NULL,
  padding = 0,
  dilation = 1,
  ceil_mode = FALSE,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_pool3d_+3A_input">input</code></td>
<td>
<p>input tensor (minibatch, in_channels , iT * iH , iW)</p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>size of the pooling region. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(kT, kH, kW)&#8288;</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_stride">stride</code></td>
<td>
<p>stride of the pooling operation. Can be a single number or a
tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a
single number or a tuple <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>, Default: 0</p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_dilation">dilation</code></td>
<td>
<p>controls the spacing between the kernel points; also known as
the  trous algorithm.</p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when True, will use <code>ceil</code> instead of <code>floor</code> in the formula
to compute the output shape</p>
</td></tr>
<tr><td><code id="nnf_max_pool3d_+3A_return_indices">return_indices</code></td>
<td>
<p>whether to return the indices where the max occurs.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_unpool1d'>Max_unpool1d</h2><span id='topic+nnf_max_unpool1d'></span>

<h3>Description</h3>

<p>Computes a partial inverse of <code>MaxPool1d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_unpool1d(
  input,
  indices,
  kernel_size,
  stride = NULL,
  padding = 0,
  output_size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_unpool1d_+3A_input">input</code></td>
<td>
<p>the input Tensor to invert</p>
</td></tr>
<tr><td><code id="nnf_max_unpool1d_+3A_indices">indices</code></td>
<td>
<p>the indices given out by max pool</p>
</td></tr>
<tr><td><code id="nnf_max_unpool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool1d_+3A_stride">stride</code></td>
<td>
<p>Stride of the max pooling window. It is set to kernel_size by default.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool1d_+3A_padding">padding</code></td>
<td>
<p>Padding that was added to the input</p>
</td></tr>
<tr><td><code id="nnf_max_unpool1d_+3A_output_size">output_size</code></td>
<td>
<p>the targeted output size</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_unpool2d'>Max_unpool2d</h2><span id='topic+nnf_max_unpool2d'></span>

<h3>Description</h3>

<p>Computes a partial inverse of <code>MaxPool2d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_unpool2d(
  input,
  indices,
  kernel_size,
  stride = NULL,
  padding = 0,
  output_size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_unpool2d_+3A_input">input</code></td>
<td>
<p>the input Tensor to invert</p>
</td></tr>
<tr><td><code id="nnf_max_unpool2d_+3A_indices">indices</code></td>
<td>
<p>the indices given out by max pool</p>
</td></tr>
<tr><td><code id="nnf_max_unpool2d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool2d_+3A_stride">stride</code></td>
<td>
<p>Stride of the max pooling window. It is set to kernel_size by default.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool2d_+3A_padding">padding</code></td>
<td>
<p>Padding that was added to the input</p>
</td></tr>
<tr><td><code id="nnf_max_unpool2d_+3A_output_size">output_size</code></td>
<td>
<p>the targeted output size</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_max_unpool3d'>Max_unpool3d</h2><span id='topic+nnf_max_unpool3d'></span>

<h3>Description</h3>

<p>Computes a partial inverse of <code>MaxPool3d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_max_unpool3d(
  input,
  indices,
  kernel_size,
  stride = NULL,
  padding = 0,
  output_size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_max_unpool3d_+3A_input">input</code></td>
<td>
<p>the input Tensor to invert</p>
</td></tr>
<tr><td><code id="nnf_max_unpool3d_+3A_indices">indices</code></td>
<td>
<p>the indices given out by max pool</p>
</td></tr>
<tr><td><code id="nnf_max_unpool3d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>Size of the max pooling window.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool3d_+3A_stride">stride</code></td>
<td>
<p>Stride of the max pooling window. It is set to kernel_size by default.</p>
</td></tr>
<tr><td><code id="nnf_max_unpool3d_+3A_padding">padding</code></td>
<td>
<p>Padding that was added to the input</p>
</td></tr>
<tr><td><code id="nnf_max_unpool3d_+3A_output_size">output_size</code></td>
<td>
<p>the targeted output size</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_mse_loss'>Mse_loss</h2><span id='topic+nnf_mse_loss'></span>

<h3>Description</h3>

<p>Measures the element-wise mean squared error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_mse_loss(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_mse_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_mse_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_mse_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_multi_head_attention_forward'>Multi head attention forward</h2><span id='topic+nnf_multi_head_attention_forward'></span>

<h3>Description</h3>

<p>Allows the model to jointly attend to information from different representation
subspaces. See reference: Attention Is All You Need
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_multi_head_attention_forward(
  query,
  key,
  value,
  embed_dim_to_check,
  num_heads,
  in_proj_weight,
  in_proj_bias,
  bias_k,
  bias_v,
  add_zero_attn,
  dropout_p,
  out_proj_weight,
  out_proj_bias,
  training = TRUE,
  key_padding_mask = NULL,
  need_weights = TRUE,
  attn_mask = NULL,
  avg_weights = TRUE,
  use_separate_proj_weight = FALSE,
  q_proj_weight = NULL,
  k_proj_weight = NULL,
  v_proj_weight = NULL,
  static_k = NULL,
  static_v = NULL,
  batch_first = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_query">query</code></td>
<td>
<p><code class="reqn">(L, N, E)</code> where L is the target sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_key">key</code></td>
<td>
<p><code class="reqn">(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_value">value</code></td>
<td>
<p><code class="reqn">(S, N, E)</code> where S is the source sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_embed_dim_to_check">embed_dim_to_check</code></td>
<td>
<p>total dimension of the model.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_num_heads">num_heads</code></td>
<td>
<p>parallel attention heads.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_in_proj_weight">in_proj_weight</code></td>
<td>
<p>input projection weight and bias.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_in_proj_bias">in_proj_bias</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_bias_k">bias_k</code></td>
<td>
<p>bias of the key and value sequences to be added at dim=0.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_bias_v">bias_v</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_add_zero_attn">add_zero_attn</code></td>
<td>
<p>add a new batch of zeros to the key and
value sequences at dim=1.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_dropout_p">dropout_p</code></td>
<td>
<p>probability of an element to be zeroed.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_out_proj_weight">out_proj_weight</code></td>
<td>
<p>the output projection weight and bias.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_out_proj_bias">out_proj_bias</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_training">training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_key_padding_mask">key_padding_mask</code></td>
<td>
<p><code class="reqn">(N, S)</code> where N is the batch size, S is the source sequence length.
If a ByteTensor is provided, the non-zero positions will be ignored while the position
with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_need_weights">need_weights</code></td>
<td>
<p>output attn_output_weights.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_attn_mask">attn_mask</code></td>
<td>
<p>2D mask <code class="reqn">(L, S)</code> where L is the target sequence length, S is the source sequence length.
3D mask <code class="reqn">(N*num_heads, L, S)</code> where N is the batch size, L is the target sequence length,
S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked
positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>
is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor
is provided, it will be added to the attention weight.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_avg_weights">avg_weights</code></td>
<td>
<p>Logical; whether to average attn_output_weights over the
attention heads before outputting them. This doesn't change the returned
value of attn_output; it only affects the returned attention weight matrix.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_use_separate_proj_weight">use_separate_proj_weight</code></td>
<td>
<p>the function accept the proj. weights for
query, key, and value in different forms. If false, in_proj_weight will be used,
which is a combination of q_proj_weight, k_proj_weight, v_proj_weight.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_q_proj_weight">q_proj_weight</code></td>
<td>
<p>input projection weight and bias.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_k_proj_weight">k_proj_weight</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_v_proj_weight">v_proj_weight</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_static_k">static_k</code></td>
<td>
<p>static key and value used for attention operators.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_static_v">static_v</code></td>
<td>
<p>currently undocumented.</p>
</td></tr>
<tr><td><code id="nnf_multi_head_attention_forward_+3A_batch_first">batch_first</code></td>
<td>
<p>Logical; whether to expect query, key, and value to have
batch as their first parameter, and to return output with batch first.</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_multi_margin_loss'>Multi_margin_loss</h2><span id='topic+nnf_multi_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-class classification hinge loss
(margin-based loss) between input x (a 2D mini-batch Tensor) and output y
(which is a 1D tensor of target class indices, <code style="white-space: pre;">&#8288;0 &lt;= y &lt;= x$size(2) - 1&#8288;</code> ).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_multi_margin_loss(
  input,
  target,
  p = 1,
  margin = 1,
  weight = NULL,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_multi_margin_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_multi_margin_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_multi_margin_loss_+3A_p">p</code></td>
<td>
<p>Has a default value of 1. 1 and 2 are the only supported values.</p>
</td></tr>
<tr><td><code id="nnf_multi_margin_loss_+3A_margin">margin</code></td>
<td>
<p>Has a default value of 1.</p>
</td></tr>
<tr><td><code id="nnf_multi_margin_loss_+3A_weight">weight</code></td>
<td>
<p>a manual rescaling weight given to each class. If given, it has to
be a Tensor of size C. Otherwise, it is treated as if having all ones.</p>
</td></tr>
<tr><td><code id="nnf_multi_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_multilabel_margin_loss'>Multilabel_margin_loss</h2><span id='topic+nnf_multilabel_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-class multi-classification hinge loss
(margin-based loss) between input x (a 2D mini-batch Tensor) and output y (which
is a 2D Tensor of target class indices).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_multilabel_margin_loss(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_multilabel_margin_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_multilabel_margin_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_multilabel_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_multilabel_soft_margin_loss'>Multilabel_soft_margin_loss</h2><span id='topic+nnf_multilabel_soft_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a multi-label one-versus-all loss based on
max-entropy, between input x and target y of size (N, C).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_multilabel_soft_margin_loss(
  input,
  target,
  weight = NULL,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_multilabel_soft_margin_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_multilabel_soft_margin_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_multilabel_soft_margin_loss_+3A_weight">weight</code></td>
<td>
<p>weight tensor to apply on the loss.</p>
</td></tr>
<tr><td><code id="nnf_multilabel_soft_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It takes a one hot encoded target vector as input.
</p>

<hr>
<h2 id='nnf_nll_loss'>Nll_loss</h2><span id='topic+nnf_nll_loss'></span>

<h3>Description</h3>

<p>The negative log likelihood loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_nll_loss(
  input,
  target,
  weight = NULL,
  ignore_index = -100,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_nll_loss_+3A_input">input</code></td>
<td>
<p><code class="reqn">(N, C)</code> where <code style="white-space: pre;">&#8288;C = number of classes&#8288;</code> or <code class="reqn">(N, C, H, W)</code> in
case of 2D Loss, or <code class="reqn">(N, C, d_1, d_2, ..., d_K)</code> where <code class="reqn">K \geq 1</code> in
the case of K-dimensional loss.</p>
</td></tr>
<tr><td><code id="nnf_nll_loss_+3A_target">target</code></td>
<td>
<p><code class="reqn">(N)</code> where each value is <code class="reqn">0 \leq \mbox{targets}[i] \leq C-1</code>,
or <code class="reqn">(N, d_1, d_2, ..., d_K)</code> where <code class="reqn">K \geq 1</code> for K-dimensional loss.</p>
</td></tr>
<tr><td><code id="nnf_nll_loss_+3A_weight">weight</code></td>
<td>
<p>(Tensor, optional) a manual rescaling weight given to each class.
If given, has to be a Tensor of size <code>C</code></p>
</td></tr>
<tr><td><code id="nnf_nll_loss_+3A_ignore_index">ignore_index</code></td>
<td>
<p>(int, optional) Specifies a target value that is ignored and
does not contribute to the input gradient.</p>
</td></tr>
<tr><td><code id="nnf_nll_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_normalize'>Normalize</h2><span id='topic+nnf_normalize'></span>

<h3>Description</h3>

<p>Performs <code class="reqn">L_p</code> normalization of inputs over specified dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_normalize(input, p = 2, dim = 2, eps = 1e-12, out = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_normalize_+3A_input">input</code></td>
<td>
<p>input tensor of any shape</p>
</td></tr>
<tr><td><code id="nnf_normalize_+3A_p">p</code></td>
<td>
<p>(float) the exponent value in the norm formulation. Default: 2</p>
</td></tr>
<tr><td><code id="nnf_normalize_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_normalize_+3A_eps">eps</code></td>
<td>
<p>(float) small value to avoid division by zero. Default: 1e-12</p>
</td></tr>
<tr><td><code id="nnf_normalize_+3A_out">out</code></td>
<td>
<p>(Tensor, optional) the output tensor. If <code>out</code> is used, this                            operation won't be differentiable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a tensor <code>input</code> of sizes <code class="reqn">(n_0, ..., n_{dim}, ..., n_k)</code>, each
<code class="reqn">n_{dim}</code> -element vector <code class="reqn">v</code> along dimension <code>dim</code> is transformed as
</p>
<p style="text-align: center;"><code class="reqn">
        v = \frac{v}{\max(\Vert v \Vert_p, \epsilon)}.
</code>
</p>

<p>With the default arguments it uses the Euclidean norm over vectors along
dimension <code class="reqn">1</code> for normalization.
</p>

<hr>
<h2 id='nnf_one_hot'>One_hot</h2><span id='topic+nnf_one_hot'></span>

<h3>Description</h3>

<p>Takes LongTensor with index values of shape <code style="white-space: pre;">&#8288;(*)&#8288;</code> and returns a tensor
of shape <code style="white-space: pre;">&#8288;(*, num_classes)&#8288;</code> that have zeros everywhere except where the
index of last dimension matches the corresponding value of the input tensor,
in which case it will be 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_one_hot(tensor, num_classes = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_one_hot_+3A_tensor">tensor</code></td>
<td>
<p>(LongTensor) class values of any shape.</p>
</td></tr>
<tr><td><code id="nnf_one_hot_+3A_num_classes">num_classes</code></td>
<td>
<p>(int) Total number of classes. If set to -1, the number
of classes will be inferred as one greater than the largest class value in
the input tensor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>One-hot on Wikipedia: https://en.wikipedia.org/wiki/One-hot
</p>

<hr>
<h2 id='nnf_pad'>Pad</h2><span id='topic+nnf_pad'></span>

<h3>Description</h3>

<p>Pads tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_pad(input, pad, mode = "constant", value = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_pad_+3A_input">input</code></td>
<td>
<p>(Tensor) N-dimensional tensor</p>
</td></tr>
<tr><td><code id="nnf_pad_+3A_pad">pad</code></td>
<td>
<p>(tuple) m-elements tuple, where <code class="reqn">\frac{m}{2} \leq</code> input dimensions
and <code class="reqn">m</code> is even.</p>
</td></tr>
<tr><td><code id="nnf_pad_+3A_mode">mode</code></td>
<td>
<p>'constant', 'reflect', 'replicate' or 'circular'. Default: 'constant'</p>
</td></tr>
<tr><td><code id="nnf_pad_+3A_value">value</code></td>
<td>
<p>fill value for 'constant' padding. Default: 0.</p>
</td></tr>
</table>


<h3>Padding size</h3>

<p>The padding size by which to pad some dimensions of <code>input</code>
are described starting from the last dimension and moving forward.
<code class="reqn">\left\lfloor\frac{\mbox{len(pad)}}{2}\right\rfloor</code> dimensions
of <code>input</code> will be padded.
For example, to pad only the last dimension of the input tensor, then
<code>pad</code> has the form
<code class="reqn">(\mbox{padding\_left}, \mbox{padding\_right})</code>;
to pad the last 2 dimensions of the input tensor, then use
<code class="reqn">(\mbox{padding\_left}, \mbox{padding\_right},</code>
<code class="reqn">\mbox{padding\_top}, \mbox{padding\_bottom})</code>;
to pad the last 3 dimensions, use
<code class="reqn">(\mbox{padding\_left}, \mbox{padding\_right},</code>
<code class="reqn">\mbox{padding\_top}, \mbox{padding\_bottom}</code>
<code class="reqn">\mbox{padding\_front}, \mbox{padding\_back})</code>.
</p>


<h3>Padding mode</h3>

<p>See <code>nn_constant_pad_2d</code>, <code>nn_reflection_pad_2d</code>, and
<code>nn_replication_pad_2d</code> for concrete examples on how each of the
padding modes works. Constant padding is implemented for arbitrary dimensions.
tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
3D input tensor. Reflect padding is only implemented for padding the last 2
dimensions of 4D input tensor, or the last dimension of 3D input tensor.
</p>

<hr>
<h2 id='nnf_pairwise_distance'>Pairwise_distance</h2><span id='topic+nnf_pairwise_distance'></span>

<h3>Description</h3>

<p>Computes the batchwise pairwise distance between vectors using the p-norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_pairwise_distance(x1, x2, p = 2, eps = 1e-06, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_pairwise_distance_+3A_x1">x1</code></td>
<td>
<p>(Tensor) First input.</p>
</td></tr>
<tr><td><code id="nnf_pairwise_distance_+3A_x2">x2</code></td>
<td>
<p>(Tensor) Second input (of size matching x1).</p>
</td></tr>
<tr><td><code id="nnf_pairwise_distance_+3A_p">p</code></td>
<td>
<p>the norm degree. Default: 2</p>
</td></tr>
<tr><td><code id="nnf_pairwise_distance_+3A_eps">eps</code></td>
<td>
<p>(float, optional) Small value to avoid division by zero.
Default: 1e-8</p>
</td></tr>
<tr><td><code id="nnf_pairwise_distance_+3A_keepdim">keepdim</code></td>
<td>
<p>Determines whether or not to keep the vector dimension. Default: False</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_pdist'>Pdist</h2><span id='topic+nnf_pdist'></span>

<h3>Description</h3>

<p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<code style="white-space: pre;">&#8288;torch_norm(input[:, None] - input, dim=2, p=p)&#8288;</code>. This function will be faster
if the rows are contiguous.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_pdist(input, p = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_pdist_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">N \times M</code>.</p>
</td></tr>
<tr><td><code id="nnf_pdist_+3A_p">p</code></td>
<td>
<p>p value for the p-norm distance to calculate between each vector pair
<code class="reqn">\in [0, \infty]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If input has shape <code class="reqn">N \times M</code> then the output will have shape
<code class="reqn">\frac{1}{2} N (N - 1)</code>.
</p>

<hr>
<h2 id='nnf_pixel_shuffle'>Pixel_shuffle</h2><span id='topic+nnf_pixel_shuffle'></span>

<h3>Description</h3>

<p>Rearranges elements in a tensor of shape <code class="reqn">(*, C \times r^2, H, W)</code> to a
tensor of shape <code class="reqn">(*, C, H \times r, W \times r)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_pixel_shuffle(input, upscale_factor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_pixel_shuffle_+3A_input">input</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="nnf_pixel_shuffle_+3A_upscale_factor">upscale_factor</code></td>
<td>
<p>(int) factor to increase spatial resolution by</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_poisson_nll_loss'>Poisson_nll_loss</h2><span id='topic+nnf_poisson_nll_loss'></span>

<h3>Description</h3>

<p>Poisson negative log likelihood loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_poisson_nll_loss(
  input,
  target,
  log_input = TRUE,
  full = FALSE,
  eps = 1e-08,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_poisson_nll_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_poisson_nll_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_poisson_nll_loss_+3A_log_input">log_input</code></td>
<td>
<p>if <code>TRUE</code> the loss is computed as <code class="reqn">\exp(\mbox{input}) - \mbox{target} * \mbox{input}</code>,
if <code>FALSE</code> then loss is <code class="reqn">\mbox{input} - \mbox{target} * \log(\mbox{input}+\mbox{eps})</code>.
Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nnf_poisson_nll_loss_+3A_full">full</code></td>
<td>
<p>whether to compute full loss, i. e. to add the Stirling approximation
term. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nnf_poisson_nll_loss_+3A_eps">eps</code></td>
<td>
<p>(float, optional) Small value to avoid evaluation of <code class="reqn">\log(0)</code> when
<code>log_input</code>=<code>FALSE</code>. Default: 1e-8</p>
</td></tr>
<tr><td><code id="nnf_poisson_nll_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_prelu'>Prelu</h2><span id='topic+nnf_prelu'></span>

<h3>Description</h3>

<p>Applies element-wise the function
<code class="reqn">PReLU(x) = max(0,x) + weight * min(0,x)</code>
where weight is a learnable parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_prelu(input, weight)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_prelu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_prelu_+3A_weight">weight</code></td>
<td>
<p>(Tensor) the learnable weights</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_relu'>Relu</h2><span id='topic+nnf_relu'></span><span id='topic+nnf_relu_'></span>

<h3>Description</h3>

<p>Applies the rectified linear unit function element-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_relu(input, inplace = FALSE)

nnf_relu_(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_relu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_relu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_relu6'>Relu6</h2><span id='topic+nnf_relu6'></span>

<h3>Description</h3>

<p>Applies the element-wise function <code class="reqn">ReLU6(x) = min(max(0,x), 6)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_relu6(input, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_relu6_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_relu6_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_rrelu'>Rrelu</h2><span id='topic+nnf_rrelu'></span><span id='topic+nnf_rrelu_'></span>

<h3>Description</h3>

<p>Randomized leaky ReLU.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_rrelu(input, lower = 1/8, upper = 1/3, training = FALSE, inplace = FALSE)

nnf_rrelu_(input, lower = 1/8, upper = 1/3, training = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_rrelu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_rrelu_+3A_lower">lower</code></td>
<td>
<p>lower bound of the uniform distribution. Default: 1/8</p>
</td></tr>
<tr><td><code id="nnf_rrelu_+3A_upper">upper</code></td>
<td>
<p>upper bound of the uniform distribution. Default: 1/3</p>
</td></tr>
<tr><td><code id="nnf_rrelu_+3A_training">training</code></td>
<td>
<p>bool wether it's a training pass. DEfault: FALSE</p>
</td></tr>
<tr><td><code id="nnf_rrelu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_selu'>Selu</h2><span id='topic+nnf_selu'></span><span id='topic+nnf_selu_'></span>

<h3>Description</h3>

<p>Applies element-wise,
</p>
<p style="text-align: center;"><code class="reqn">SELU(x) = scale * (max(0,x) + min(0, \alpha * (exp(x) - 1)))</code>
</p>
<p>,
with <code class="reqn">\alpha=1.6732632423543772848170429916717</code> and
<code class="reqn">scale=1.0507009873554804934193349852946</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_selu(input, inplace = FALSE)

nnf_selu_(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_selu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_selu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_randn(2, 2)
y &lt;- nnf_selu(x)
nnf_selu_(x)
torch_equal(x, y)
}
</code></pre>

<hr>
<h2 id='nnf_sigmoid'>Sigmoid</h2><span id='topic+nnf_sigmoid'></span>

<h3>Description</h3>

<p>Applies element-wise <code class="reqn">Sigmoid(x_i) = \frac{1}{1 + exp(-x_i)}</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_sigmoid(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_sigmoid_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_silu'>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
See <code><a href="#topic+nn_silu">nn_silu()</a></code> for more information.</h2><span id='topic+nnf_silu'></span>

<h3>Description</h3>

<p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
See <code><a href="#topic+nn_silu">nn_silu()</a></code> for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_silu(input, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_silu_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_silu_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+nn_silu">nn_silu()</a></code>.
</p>

<hr>
<h2 id='nnf_smooth_l1_loss'>Smooth_l1_loss</h2><span id='topic+nnf_smooth_l1_loss'></span>

<h3>Description</h3>

<p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_smooth_l1_loss(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_smooth_l1_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_smooth_l1_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_smooth_l1_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_soft_margin_loss'>Soft_margin_loss</h2><span id='topic+nnf_soft_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that optimizes a two-class classification logistic loss
between input tensor x and target tensor y (containing 1 or -1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_soft_margin_loss(input, target, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_soft_margin_loss_+3A_input">input</code></td>
<td>
<p>tensor (N,*) where ** means, any number of additional dimensions</p>
</td></tr>
<tr><td><code id="nnf_soft_margin_loss_+3A_target">target</code></td>
<td>
<p>tensor (N,*) , same shape as the input</p>
</td></tr>
<tr><td><code id="nnf_soft_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_softmax'>Softmax</h2><span id='topic+nnf_softmax'></span>

<h3>Description</h3>

<p>Applies a softmax function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_softmax(input, dim, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_softmax_+3A_input">input</code></td>
<td>
<p>(Tensor) input</p>
</td></tr>
<tr><td><code id="nnf_softmax_+3A_dim">dim</code></td>
<td>
<p>(int) A dimension along which softmax will be computed.</p>
</td></tr>
<tr><td><code id="nnf_softmax_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.      If specified, the input tensor is casted to <code>dtype</code> before the operation      is performed. This is useful for preventing data type overflows.
Default: NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Softmax is defined as:
</p>
<p style="text-align: center;"><code class="reqn">Softmax(x_{i}) = exp(x_i)/\sum_j exp(x_j)</code>
</p>

<p>It is applied to all slices along dim, and will re-scale them so that the elements
lie in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> and sum to 1.
</p>

<hr>
<h2 id='nnf_softmin'>Softmin</h2><span id='topic+nnf_softmin'></span>

<h3>Description</h3>

<p>Applies a softmin function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_softmin(input, dim, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_softmin_+3A_input">input</code></td>
<td>
<p>(Tensor) input</p>
</td></tr>
<tr><td><code id="nnf_softmin_+3A_dim">dim</code></td>
<td>
<p>(int) A dimension along which softmin will be computed
(so every slice        along dim will sum to 1).</p>
</td></tr>
<tr><td><code id="nnf_softmin_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.      If specified, the input tensor is casted to <code>dtype</code> before the operation      is performed.
This is useful for preventing data type overflows. Default: NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that
</p>
<p style="text-align: center;"><code class="reqn">Softmin(x) = Softmax(-x)</code>
</p>
<p>.
</p>
<p>See <a href="#topic+nnf_softmax">nnf_softmax</a> definition for mathematical formula.
</p>

<hr>
<h2 id='nnf_softplus'>Softplus</h2><span id='topic+nnf_softplus'></span>

<h3>Description</h3>

<p>Applies element-wise, the function <code class="reqn">Softplus(x) = 1/\beta * log(1 + exp(\beta * x))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_softplus(input, beta = 1, threshold = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_softplus_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_softplus_+3A_beta">beta</code></td>
<td>
<p>the beta value for the Softplus formulation. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_softplus_+3A_threshold">threshold</code></td>
<td>
<p>values above this revert to a linear function. Default: 20</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For numerical stability the implementation reverts to the linear function
when <code class="reqn">input * \beta &gt; threshold</code>.
</p>

<hr>
<h2 id='nnf_softshrink'>Softshrink</h2><span id='topic+nnf_softshrink'></span>

<h3>Description</h3>

<p>Applies the soft shrinkage function elementwise
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_softshrink(input, lambd = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_softshrink_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_softshrink_+3A_lambd">lambd</code></td>
<td>
<p>the lambda (must be no less than zero) value for the Softshrink
formulation. Default: 0.5</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_softsign'>Softsign</h2><span id='topic+nnf_softsign'></span>

<h3>Description</h3>

<p>Applies element-wise, the function <code class="reqn">SoftSign(x) = x/(1 + |x|</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_softsign(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_softsign_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_tanhshrink'>Tanhshrink</h2><span id='topic+nnf_tanhshrink'></span>

<h3>Description</h3>

<p>Applies element-wise, <code class="reqn">Tanhshrink(x) = x - Tanh(x)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_tanhshrink(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_tanhshrink_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_threshold'>Threshold</h2><span id='topic+nnf_threshold'></span><span id='topic+nnf_threshold_'></span>

<h3>Description</h3>

<p>Thresholds each element of the input Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_threshold(input, threshold, value, inplace = FALSE)

nnf_threshold_(input, threshold, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_threshold_+3A_input">input</code></td>
<td>
<p>(N,*) tensor, where * means, any number of additional
dimensions</p>
</td></tr>
<tr><td><code id="nnf_threshold_+3A_threshold">threshold</code></td>
<td>
<p>The value to threshold at</p>
</td></tr>
<tr><td><code id="nnf_threshold_+3A_value">value</code></td>
<td>
<p>The value to replace with</p>
</td></tr>
<tr><td><code id="nnf_threshold_+3A_inplace">inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: FALSE</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_triplet_margin_loss'>Triplet_margin_loss</h2><span id='topic+nnf_triplet_margin_loss'></span>

<h3>Description</h3>

<p>Creates a criterion that measures the triplet loss given an input tensors x1 ,
x2 , x3 and a margin with a value greater than 0 . This is used for measuring
a relative similarity between samples. A triplet is composed by a, p and n (i.e.,
anchor, positive examples and negative examples respectively). The shapes of all
input tensors should be (N, D).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_triplet_margin_loss(
  anchor,
  positive,
  negative,
  margin = 1,
  p = 2,
  eps = 1e-06,
  swap = FALSE,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_triplet_margin_loss_+3A_anchor">anchor</code></td>
<td>
<p>the anchor input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_positive">positive</code></td>
<td>
<p>the positive input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_negative">negative</code></td>
<td>
<p>the negative input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_margin">margin</code></td>
<td>
<p>Default: 1.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_p">p</code></td>
<td>
<p>The norm degree for pairwise distance. Default: 2.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_eps">eps</code></td>
<td>
<p>(float, optional) Small value to avoid division by zero.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_swap">swap</code></td>
<td>
<p>The distance swap is described in detail in the paper Learning shallow
convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al.
Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_triplet_margin_with_distance_loss'>Triplet margin with distance loss</h2><span id='topic+nnf_triplet_margin_with_distance_loss'></span>

<h3>Description</h3>

<p>See <code><a href="#topic+nn_triplet_margin_with_distance_loss">nn_triplet_margin_with_distance_loss()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_triplet_margin_with_distance_loss(
  anchor,
  positive,
  negative,
  distance_function = NULL,
  margin = 1,
  swap = FALSE,
  reduction = "mean"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_anchor">anchor</code></td>
<td>
<p>the anchor input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_positive">positive</code></td>
<td>
<p>the positive input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_negative">negative</code></td>
<td>
<p>the negative input tensor</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_distance_function">distance_function</code></td>
<td>
<p>(callable, optional): A nonnegative, real-valued function that
quantifies the closeness of two tensors. If not specified,
<code><a href="#topic+nn_pairwise_distance">nn_pairwise_distance()</a></code> will be used.  Default: <code>None</code></p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_margin">margin</code></td>
<td>
<p>Default: 1.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_swap">swap</code></td>
<td>
<p>The distance swap is described in detail in the paper Learning shallow
convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al.
Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nnf_triplet_margin_with_distance_loss_+3A_reduction">reduction</code></td>
<td>
<p>(string, optional)  Specifies the reduction to apply to the
output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean':
the sum of the output will be divided by the number of elements in the output,
'sum': the output will be summed. Default: 'mean'</p>
</td></tr>
</table>

<hr>
<h2 id='nnf_unfold'>Unfold</h2><span id='topic+nnf_unfold'></span>

<h3>Description</h3>

<p>Extracts sliding local blocks from an batched input tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnf_unfold(input, kernel_size, dilation = 1, padding = 0, stride = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnf_unfold_+3A_input">input</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="nnf_unfold_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the sliding blocks</p>
</td></tr>
<tr><td><code id="nnf_unfold_+3A_dilation">dilation</code></td>
<td>
<p>a parameter that controls the stride of elements within the
neighborhood. Default: 1</p>
</td></tr>
<tr><td><code id="nnf_unfold_+3A_padding">padding</code></td>
<td>
<p>implicit zero padding to be added on both sides of input.
Default: 0</p>
</td></tr>
<tr><td><code id="nnf_unfold_+3A_stride">stride</code></td>
<td>
<p>the stride of the sliding blocks in the input spatial dimensions.
Default: 1</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>More than one element of the unfolded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensor, please clone it first.
</p>

<hr>
<h2 id='optim_adadelta'>Adadelta optimizer</h2><span id='topic+optim_adadelta'></span>

<h3>Description</h3>

<p>It has been proposed in <a href="https://arxiv.org/pdf/1212.5701.pdf">ADADELTA: An Adaptive Learning Rate Method</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adadelta(params, lr = 1, rho = 0.9, eps = 1e-06, weight_decay = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adadelta_+3A_params">params</code></td>
<td>
<p>(iterable): list of parameters to optimize or list defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_adadelta_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adadelta_+3A_rho">rho</code></td>
<td>
<p>(float, optional): coefficient used for computing a running average
of squared gradients (default: 0.9)</p>
</td></tr>
<tr><td><code id="optim_adadelta_+3A_eps">eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-6)</p>
</td></tr>
<tr><td><code id="optim_adadelta_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Note</h3>

<p>According to the original paper, decaying average of the squared gradients
is computed as follows:
</p>
<p style="text-align: center;"><code class="reqn">
E[g^2]_{t} = \rho E[g^2]_{t- 1} + (1 - \rho){g_{t}}^2
</code>
</p>

<p>RMS of previous squared gradients up to time t:
</p>
<p style="text-align: center;"><code class="reqn">
RMS[g_{t}] = \sqrt{E[g^2]_{t} + \epsilon }
</code>
</p>

<p>Adadelta update rule:
</p>
<p style="text-align: center;"><code class="reqn">
 \begin{array}{ll}
 \Delta \theta_{t} = - \frac{RMS [\Delta \theta]_{t - 1} }{RMS[g]_{t}}
 \theta_{t+1} = \theta_{t} + \Delta \theta_{t}
\end{array}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
optimizer &lt;- optim_adadelta(model$parameters, lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()

## End(Not run)
}
</code></pre>

<hr>
<h2 id='optim_adagrad'>Adagrad optimizer</h2><span id='topic+optim_adagrad'></span>

<h3>Description</h3>

<p>Proposed in <a href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adagrad(
  params,
  lr = 0.01,
  lr_decay = 0,
  weight_decay = 0,
  initial_accumulator_value = 0,
  eps = 1e-10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adagrad_+3A_params">params</code></td>
<td>
<p>(iterable): list of parameters to optimize or list parameter groups</p>
</td></tr>
<tr><td><code id="optim_adagrad_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-2)</p>
</td></tr>
<tr><td><code id="optim_adagrad_+3A_lr_decay">lr_decay</code></td>
<td>
<p>(float, optional): learning rate decay (default: 0)</p>
</td></tr>
<tr><td><code id="optim_adagrad_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_adagrad_+3A_initial_accumulator_value">initial_accumulator_value</code></td>
<td>
<p>the initial value for the accumulator. (default: 0)
</p>
<p>Adagrad is an especially good optimizer for sparse data.
It individually modifies learning rate for every single parameter,
dividing the original learning rate value by sum of the squares of the gradients.
It causes that the rarely occurring features get greater learning rates.
The main downside of this method is the fact that learning rate may be
getting small too fast, so that at some point a model cannot learn anymore.</p>
</td></tr>
<tr><td><code id="optim_adagrad_+3A_eps">eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-10)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Note</h3>

<p>Update rule:
</p>
<p style="text-align: center;"><code class="reqn">
\theta_{t+1} = \theta_{t} - \frac{\eta }{\sqrt{G_{t} + \epsilon}} \odot g_{t}
</code>
</p>

<p>The equation above and some remarks quoted
after <a href="https://web.archive.org/web/20220810011734/https://ruder.io/optimizing-gradient-descent/index.html"><em>An overview of gradient descent optimization algorithms</em></a>
by Sebastian Ruder.
</p>

<hr>
<h2 id='optim_adam'>Implements Adam algorithm.</h2><span id='topic+optim_adam'></span>

<h3>Description</h3>

<p>It has been proposed in <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adam(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  amsgrad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adam_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or dicts defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_adam_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adam_+3A_betas">betas</code></td>
<td>
<p>(<code>Tuple[float, float]</code>, optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_adam_+3A_eps">eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_adam_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_adam_+3A_amsgrad">amsgrad</code></td>
<td>
<p>(boolean, optional): whether to use the AMSGrad variant of this
algorithm from the paper <a href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: FALSE)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
optimizer &lt;- optim_adam(model$parameters(), lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()

## End(Not run)

}
</code></pre>

<hr>
<h2 id='optim_adamw'>Implements AdamW algorithm</h2><span id='topic+optim_adamw'></span>

<h3>Description</h3>

<p>For further details regarding the algorithm we refer to
<a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adamw(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0.01,
  amsgrad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adamw_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or dicts defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_betas">betas</code></td>
<td>
<p>(<code>Tuple[float, float]</code>, optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_eps">eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_amsgrad">amsgrad</code></td>
<td>
<p>(boolean, optional): whether to use the AMSGrad variant of this
algorithm from the paper <a href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: FALSE)</p>
</td></tr>
</table>

<hr>
<h2 id='optim_asgd'>Averaged Stochastic Gradient Descent optimizer</h2><span id='topic+optim_asgd'></span>

<h3>Description</h3>

<p>Proposed in <a href="https://dl.acm.org/doi/10.1137/0330046">Acceleration of stochastic approximation by averaging</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_asgd(
  params,
  lr = 0.01,
  lambda = 1e-04,
  alpha = 0.75,
  t0 = 1e+06,
  weight_decay = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_asgd_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or lists defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_asgd_+3A_lr">lr</code></td>
<td>
<p>(float): learning rate</p>
</td></tr>
<tr><td><code id="optim_asgd_+3A_lambda">lambda</code></td>
<td>
<p>(float, optional): decay term (default: 1e-4)</p>
</td></tr>
<tr><td><code id="optim_asgd_+3A_alpha">alpha</code></td>
<td>
<p>(float, optional): power for eta update (default: 0.75)</p>
</td></tr>
<tr><td><code id="optim_asgd_+3A_t0">t0</code></td>
<td>
<p>(float, optional): point at which to start averaging (default: 1e6)</p>
</td></tr>
<tr><td><code id="optim_asgd_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
optimizer &lt;- optim_asgd(model$parameters(), lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()

## End(Not run)

}
</code></pre>

<hr>
<h2 id='optim_lbfgs'>LBFGS optimizer</h2><span id='topic+optim_lbfgs'></span>

<h3>Description</h3>

<p>Implements L-BFGS algorithm, heavily inspired by
<a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_lbfgs(
  params,
  lr = 1,
  max_iter = 20,
  max_eval = NULL,
  tolerance_grad = 1e-07,
  tolerance_change = 1e-09,
  history_size = 100,
  line_search_fn = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_lbfgs_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or dicts defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_lr">lr</code></td>
<td>
<p>(float): learning rate (default: 1)</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_max_iter">max_iter</code></td>
<td>
<p>(int): maximal number of iterations per optimization step
(default: 20)</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_max_eval">max_eval</code></td>
<td>
<p>(int): maximal number of function evaluations per optimization
step (default: max_iter * 1.25).</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_tolerance_grad">tolerance_grad</code></td>
<td>
<p>(float): termination tolerance on first order optimality
(default: 1e-5).</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_tolerance_change">tolerance_change</code></td>
<td>
<p>(float): termination tolerance on function
value/parameter changes (default: 1e-9).</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_history_size">history_size</code></td>
<td>
<p>(int): update history size (default: 100).</p>
</td></tr>
<tr><td><code id="optim_lbfgs_+3A_line_search_fn">line_search_fn</code></td>
<td>
<p>(str): either 'strong_wolfe' or None (default: None).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This optimizer is different from the others in that in <code>optimizer$step()</code>,
it needs to be passed a closure that (1) calculates the loss, (2) calls
<code>backward()</code> on it, and (3) returns it. See example below.
</p>


<h3>Warning</h3>

<p>This optimizer doesn't support per-parameter options and parameter
groups (there can be only one).
</p>
<p>Right now all parameters have to be on a single device. This will be
improved in the future.
</p>
<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Note</h3>

<p>This is a very memory intensive optimizer (it requires additional
<code>param_bytes * (history_size + 1)</code> bytes). If it doesn't fit in memory
try reducing the history size, or use a different algorithm.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- 1
b &lt;- 5
rosenbrock &lt;- function(x) {
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
 
x &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer &lt;- optim_lbfgs(x)
calc_loss &lt;- function() {
  optimizer$zero_grad()
  value &lt;- rosenbrock(x)
  value$backward()
  value
}
  
num_iterations &lt;- 2
for (i in 1:num_iterations) {
  optimizer$step(calc_loss)
}
    
rosenbrock(x)

}
</code></pre>

<hr>
<h2 id='optim_required'>Dummy value indicating a required value.</h2><span id='topic+optim_required'></span>

<h3>Description</h3>

<p>export
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_required()
</code></pre>

<hr>
<h2 id='optim_rmsprop'>RMSprop optimizer</h2><span id='topic+optim_rmsprop'></span>

<h3>Description</h3>

<p>Proposed by G. Hinton in his course.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_rmsprop(
  params,
  lr = 0.01,
  alpha = 0.99,
  eps = 1e-08,
  weight_decay = 0,
  momentum = 0,
  centered = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_rmsprop_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or list defining parameter groups</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-2)</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_alpha">alpha</code></td>
<td>
<p>(float, optional): smoothing constant (default: 0.99)</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_eps">eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_weight_decay">weight_decay</code></td>
<td>
<p>optional weight decay penalty. (default: 0)</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_momentum">momentum</code></td>
<td>
<p>(float, optional): momentum factor (default: 0)</p>
</td></tr>
<tr><td><code id="optim_rmsprop_+3A_centered">centered</code></td>
<td>
<p>(bool, optional) : if <code>TRUE</code>, compute the centered RMSProp,
the gradient is normalized by an estimation of its variance
weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Note</h3>

<p>The centered version first appears in
<a href="https://arxiv.org/pdf/1308.0850v5.pdf">Generating Sequences With Recurrent Neural Networks</a>.
The implementation here takes the square root of the gradient average before
adding epsilon (note that TensorFlow interchanges these two operations). The effective
learning rate is thus <code class="reqn">\alpha/(\sqrt{v} + \epsilon)</code> where <code class="reqn">\alpha</code>
is the scheduled learning rate and <code class="reqn">v</code> is the weighted moving average
of the squared gradient.
</p>
<p>Update rule:
</p>
<p style="text-align: center;"><code class="reqn">
\theta_{t+1} = \theta_{t} - \frac{\eta }{\sqrt{{E[g^2]}_{t} + \epsilon}} * g_{t}
</code>
</p>


<hr>
<h2 id='optim_rprop'>Implements the resilient backpropagation algorithm.</h2><span id='topic+optim_rprop'></span>

<h3>Description</h3>

<p>Proposed first in <a href="https://ieeexplore.ieee.org/document/298623">RPROP - A Fast Adaptive Learning Algorithm</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_rprop(params, lr = 0.01, etas = c(0.5, 1.2), step_sizes = c(1e-06, 50))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_rprop_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or lists defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_rprop_+3A_lr">lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-2)</p>
</td></tr>
<tr><td><code id="optim_rprop_+3A_etas">etas</code></td>
<td>
<p>(Tuple(float, float), optional): pair of (etaminus, etaplis), that
are multiplicative increase and decrease factors
(default: (0.5, 1.2))</p>
</td></tr>
<tr><td><code id="optim_rprop_+3A_step_sizes">step_sizes</code></td>
<td>
<p>(vector(float, float), optional): a pair of minimal and
maximal allowed step sizes (default: (1e-6, 50))</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
optimizer &lt;- optim_rprop(model$parameters(), lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()

## End(Not run)
}
</code></pre>

<hr>
<h2 id='optim_sgd'>SGD optimizer</h2><span id='topic+optim_sgd'></span>

<h3>Description</h3>

<p>Implements stochastic gradient descent (optionally with momentum).
Nesterov momentum is based on the formula from
On the importance of initialization and momentum in deep learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_sgd(
  params,
  lr = optim_required(),
  momentum = 0,
  dampening = 0,
  weight_decay = 0,
  nesterov = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_sgd_+3A_params">params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or dicts defining
parameter groups</p>
</td></tr>
<tr><td><code id="optim_sgd_+3A_lr">lr</code></td>
<td>
<p>(float): learning rate</p>
</td></tr>
<tr><td><code id="optim_sgd_+3A_momentum">momentum</code></td>
<td>
<p>(float, optional): momentum factor (default: 0)</p>
</td></tr>
<tr><td><code id="optim_sgd_+3A_dampening">dampening</code></td>
<td>
<p>(float, optional): dampening for momentum (default: 0)</p>
</td></tr>
<tr><td><code id="optim_sgd_+3A_weight_decay">weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_sgd_+3A_nesterov">nesterov</code></td>
<td>
<p>(bool, optional): enables Nesterov momentum (default: FALSE)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The implementation of SGD with Momentum-Nesterov subtly differs from
Sutskever et. al. and implementations in some other frameworks.
</p>
<p>Considering the specific case of Momentum, the update can be written as
</p>
<p style="text-align: center;"><code class="reqn">
  \begin{array}{ll}
v_{t+1} &amp; = \mu * v_{t} + g_{t+1}, \\
p_{t+1} &amp; = p_{t} - \mbox{lr} * v_{t+1},
\end{array}
</code>
</p>

<p>where <code class="reqn">p</code>, <code class="reqn">g</code>, <code class="reqn">v</code> and <code class="reqn">\mu</code> denote the
parameters, gradient, velocity, and momentum respectively.
</p>
<p>This is in contrast to Sutskever et. al. and
other frameworks which employ an update of the form
</p>
<p style="text-align: center;"><code class="reqn">
  \begin{array}{ll}
v_{t+1} &amp; = \mu * v_{t} + \mbox{lr} * g_{t+1}, \\
p_{t+1} &amp; = p_{t} - v_{t+1}.
\end{array}
</code>
</p>

<p>The Nesterov version is analogously modified.
</p>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
optimizer &lt;- optim_sgd(model$parameters(), lr = 0.1, momentum = 0.9)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()

## End(Not run)

}
</code></pre>

<hr>
<h2 id='optimizer'>Creates a custom optimizer</h2><span id='topic+optimizer'></span>

<h3>Description</h3>

<p>When implementing custom optimizers you will usually need to implement
the <code>initialize</code> and <code>step</code> methods. See the example section below
for a full example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizer(
  name = NULL,
  inherit = Optimizer,
  ...,
  private = NULL,
  active = NULL,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizer_+3A_name">name</code></td>
<td>
<p>(optional) name of the optimizer</p>
</td></tr>
<tr><td><code id="optimizer_+3A_inherit">inherit</code></td>
<td>
<p>(optional) you can inherit from other optimizers to re-use
some methods.</p>
</td></tr>
<tr><td><code id="optimizer_+3A_...">...</code></td>
<td>
<p>Pass any number of fields or methods. You should at least define
the <code>initialize</code> and <code>step</code> methods. See the examples section.</p>
</td></tr>
<tr><td><code id="optimizer_+3A_private">private</code></td>
<td>
<p>(optional) a list of private methods for the optimizer.</p>
</td></tr>
<tr><td><code id="optimizer_+3A_active">active</code></td>
<td>
<p>(optional) a list of active methods for the optimizer.</p>
</td></tr>
<tr><td><code id="optimizer_+3A_parent_env">parent_env</code></td>
<td>
<p>used to capture the right environment to define the class.
The default is fine for most situations.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">&#8288;$cuda()&#8288;</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# In this example we will create a custom optimizer
# that's just a simplified version of the `optim_sgd` function.

optim_sgd2 &lt;- optimizer(
  initialize = function(params, learning_rate) {
    defaults &lt;- list(
      learning_rate = learning_rate
    )
    super$initialize(params, defaults)
  },
  step = function() {
    with_no_grad({
      for (g in seq_along(self$param_groups)) {
        group &lt;- self$param_groups[[g]]
        for (p in seq_along(group$params)) {
          param &lt;- group$params[[p]]

          if (is.null(param$grad) || is_undefined_tensor(param$grad)) {
            next
          }

          param$add_(param$grad, alpha = -group$learning_rate)
        }
      }
    })
  }
)

x &lt;- torch_randn(1, requires_grad = TRUE)
opt &lt;- optim_sgd2(x, learning_rate = 0.1)
for (i in 1:100) {
  opt$zero_grad()
  y &lt;- x^2
  y$backward()
  opt$step()
}
all.equal(x$item(), 0, tolerance = 1e-9)
}
</code></pre>

<hr>
<h2 id='reexports'>Re-exporting the as_iterator function.</h2><span id='topic+reexports'></span><span id='topic+as_iterator'></span><span id='topic+loop'></span><span id='topic+yield'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>coro</dt><dd><p><code><a href="coro.html#topic+as_iterator">as_iterator</a></code>, <code><a href="coro.html#topic+collect">loop</a></code>, <code><a href="coro.html#topic+yield">yield</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sampler'>Creates a new Sampler</h2><span id='topic+sampler'></span>

<h3>Description</h3>

<p>Samplers can be used with <code><a href="#topic+dataloader">dataloader()</a></code> when creating batches from a torch
<code><a href="#topic+dataset">dataset()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampler(
  name = NULL,
  inherit = Sampler,
  ...,
  private = NULL,
  active = NULL,
  parent_env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler_+3A_name">name</code></td>
<td>
<p>(optional) name of the sampler</p>
</td></tr>
<tr><td><code id="sampler_+3A_inherit">inherit</code></td>
<td>
<p>(optional) you can inherit from other samplers to re-use
some methods.</p>
</td></tr>
<tr><td><code id="sampler_+3A_...">...</code></td>
<td>
<p>Pass any number of fields or methods. You should at least define
the <code>initialize</code> and <code>step</code> methods. See the examples section.</p>
</td></tr>
<tr><td><code id="sampler_+3A_private">private</code></td>
<td>
<p>(optional) a list of private methods for the sampler</p>
</td></tr>
<tr><td><code id="sampler_+3A_active">active</code></td>
<td>
<p>(optional) a list of active methods for the sampler.</p>
</td></tr>
<tr><td><code id="sampler_+3A_parent_env">parent_env</code></td>
<td>
<p>used to capture the right environment to define the class.
The default is fine for most situations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sampler must implement the <code>.iter</code> and <code>.length()</code> methods.
</p>

<ul>
<li> <p><code>initialize</code> takes in a <code>data_source</code>. In general this is a <code><a href="#topic+dataset">dataset()</a></code>.
</p>
</li>
<li> <p><code>.iter</code> returns a function that returns a dataset index everytime it's called.
</p>
</li>
<li> <p><code>.length</code> returns the maximum number of samples that can be retrieved from
that sampler.
</p>
</li></ul>


<hr>
<h2 id='slc'>Creates a slice</h2><span id='topic+slc'></span>

<h3>Description</h3>

<p>Creates a slice object that can be used when indexing torch tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slc(start, end, step = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slc_+3A_start">start</code></td>
<td>
<p>(integer) starting index.</p>
</td></tr>
<tr><td><code id="slc_+3A_end">end</code></td>
<td>
<p>(integer) the last selected index.</p>
</td></tr>
<tr><td><code id="slc_+3A_step">step</code></td>
<td>
<p>(integer) the step between indexes.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_randn(10)
x[slc(start = 1, end = 5, step = 2)]

}
</code></pre>

<hr>
<h2 id='tensor_dataset'>Dataset wrapping tensors.</h2><span id='topic+tensor_dataset'></span>

<h3>Description</h3>

<p>Each sample will be retrieved by indexing tensors along the first dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor_dataset(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensor_dataset_+3A_...">...</code></td>
<td>
<p>tensors that have the same size of the first dimension.</p>
</td></tr>
</table>

<hr>
<h2 id='threads'>Number of threads</h2><span id='topic+threads'></span><span id='topic+torch_set_num_threads'></span><span id='topic+torch_set_num_interop_threads'></span><span id='topic+torch_get_num_interop_threads'></span><span id='topic+torch_get_num_threads'></span>

<h3>Description</h3>

<p>Get and set the numbers used by torch computations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_set_num_threads(num_threads)

torch_set_num_interop_threads(num_threads)

torch_get_num_interop_threads()

torch_get_num_threads()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="threads_+3A_num_threads">num_threads</code></td>
<td>
<p>number of threads to set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details see the <a href="https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html?highlight=set_num_threads">CPU threading article</a>
in the PyTorch documentation.
</p>


<h3>Note</h3>

<p>torch_set_threads do not work on macOS system as it must be 1.
</p>

<hr>
<h2 id='torch_abs'>Abs</h2><span id='topic+torch_abs'></span>

<h3>Description</h3>

<p>Abs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_abs(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_abs_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>abs(input) -&gt; Tensor </h3>

<p>Computes the element-wise absolute value of the given <code>input</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = |\mbox{input}_{i}|
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_abs(torch_tensor(c(-1, -2, 3)))
}
</code></pre>

<hr>
<h2 id='torch_absolute'>Absolute</h2><span id='topic+torch_absolute'></span>

<h3>Description</h3>

<p>Absolute
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_absolute(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_absolute_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>absolute(input, *, out=None) -&gt; Tensor</h3>

<p>Alias for <code><a href="#topic+torch_abs">torch_abs()</a></code>
</p>

<hr>
<h2 id='torch_acos'>Acos</h2><span id='topic+torch_acos'></span>

<h3>Description</h3>

<p>Acos
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_acos(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_acos_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>acos(input) -&gt; Tensor </h3>

<p>Returns a new tensor with the arccosine  of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \cos^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_acos(a)
}
</code></pre>

<hr>
<h2 id='torch_acosh'>Acosh</h2><span id='topic+torch_acosh'></span>

<h3>Description</h3>

<p>Acosh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_acosh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_acosh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>acosh(input, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code>input</code>.
</p>


<h3>Note</h3>

<p>The domain of the inverse hyperbolic cosine is <code style="white-space: pre;">&#8288;[1, inf)&#8288;</code> and values outside this range
will be mapped to <code>NaN</code>, except for <code>+ INF</code> for which the output is mapped to <code>+ INF</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \cosh^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(4))$uniform_(1, 2)
a
torch_acosh(a)
}
</code></pre>

<hr>
<h2 id='torch_adaptive_avg_pool1d'>Adaptive_avg_pool1d</h2><span id='topic+torch_adaptive_avg_pool1d'></span>

<h3>Description</h3>

<p>Adaptive_avg_pool1d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_adaptive_avg_pool1d(self, output_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_adaptive_avg_pool1d_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_adaptive_avg_pool1d_+3A_output_size">output_size</code></td>
<td>
<p>the target output size (single integer)</p>
</td></tr>
</table>


<h3>adaptive_avg_pool1d(input, output_size) -&gt; Tensor </h3>

<p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.
</p>
<p>See <code><a href="#topic+nn_adaptive_avg_pool1d">nn_adaptive_avg_pool1d()</a></code> for details and output shape.
</p>

<hr>
<h2 id='torch_add'>Add</h2><span id='topic+torch_add'></span>

<h3>Description</h3>

<p>Add
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_add(self, other, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_add_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_add_+3A_other">other</code></td>
<td>
<p>(Tensor/Number) the second input tensor/number.</p>
</td></tr>
<tr><td><code id="torch_add_+3A_alpha">alpha</code></td>
<td>
<p>(Number) the scalar multiplier for <code>other</code></p>
</td></tr>
</table>


<h3>add(input, other, out=NULL) </h3>

<p>Adds the scalar <code>other</code> to each element of the input <code>input</code>
and returns a new resulting tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \mbox{input} + \mbox{other}
</code>
</p>

<p>If <code>input</code> is of type FloatTensor or DoubleTensor, <code>other</code> must be
a real number, otherwise it should be an integer.
</p>


<h3>add(input, other, *, alpha=1, out=NULL) </h3>

<p>Each element of the tensor <code>other</code> is multiplied by the scalar
<code>alpha</code> and added to each element of the tensor <code>input</code>.
The resulting tensor is returned.
</p>
<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \mbox{input} + \mbox{alpha} \times \mbox{other}
</code>
</p>

<p>If <code>other</code> is of type FloatTensor or DoubleTensor, <code>alpha</code> must be
a real number, otherwise it should be an integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_add(a, 20)


a = torch_randn(c(4))
a
b = torch_randn(c(4, 1))
b
torch_add(a, b)
}
</code></pre>

<hr>
<h2 id='torch_addbmm'>Addbmm</h2><span id='topic+torch_addbmm'></span>

<h3>Description</h3>

<p>Addbmm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addbmm(self, batch1, batch2, beta = 1L, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addbmm_+3A_self">self</code></td>
<td>
<p>(Tensor) matrix to be added</p>
</td></tr>
<tr><td><code id="torch_addbmm_+3A_batch1">batch1</code></td>
<td>
<p>(Tensor) the first batch of matrices to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addbmm_+3A_batch2">batch2</code></td>
<td>
<p>(Tensor) the second batch of matrices to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addbmm_+3A_beta">beta</code></td>
<td>
<p>(Number, optional) multiplier for <code>input</code> (<code class="reqn">\beta</code>)</p>
</td></tr>
<tr><td><code id="torch_addbmm_+3A_alpha">alpha</code></td>
<td>
<p>(Number, optional) multiplier for <code>batch1 @ batch2</code> (<code class="reqn">\alpha</code>)</p>
</td></tr>
</table>


<h3>addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=NULL) -&gt; Tensor </h3>

<p>Performs a batch matrix-matrix product of matrices stored
in <code>batch1</code> and <code>batch2</code>,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).
<code>input</code> is added to the final result.
</p>
<p><code>batch1</code> and <code>batch2</code> must be 3-D tensors each containing the
same number of matrices.
</p>
<p>If <code>batch1</code> is a <code class="reqn">(b \times n \times m)</code> tensor, <code>batch2</code> is a
<code class="reqn">(b \times m \times p)</code> tensor, <code>input</code> must be
broadcastable  with a <code class="reqn">(n \times p)</code> tensor
and <code>out</code> will be a <code class="reqn">(n \times p)</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    out = \beta\ \mbox{input} + \alpha\ (\sum_{i=0}^{b-1} \mbox{batch1}_i \mathbin{@} \mbox{batch2}_i)
</code>
</p>

<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and <code>alpha</code>
must be real numbers, otherwise they should be integers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

M = torch_randn(c(3, 5))
batch1 = torch_randn(c(10, 3, 4))
batch2 = torch_randn(c(10, 4, 5))
torch_addbmm(M, batch1, batch2)
}
</code></pre>

<hr>
<h2 id='torch_addcdiv'>Addcdiv</h2><span id='topic+torch_addcdiv'></span>

<h3>Description</h3>

<p>Addcdiv
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addcdiv(self, tensor1, tensor2, value = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addcdiv_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to be added</p>
</td></tr>
<tr><td><code id="torch_addcdiv_+3A_tensor1">tensor1</code></td>
<td>
<p>(Tensor) the numerator tensor</p>
</td></tr>
<tr><td><code id="torch_addcdiv_+3A_tensor2">tensor2</code></td>
<td>
<p>(Tensor) the denominator tensor</p>
</td></tr>
<tr><td><code id="torch_addcdiv_+3A_value">value</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">\mbox{tensor1} / \mbox{tensor2}</code></p>
</td></tr>
</table>


<h3>addcdiv(input, tensor1, tensor2, *, value=1, out=NULL) -&gt; Tensor </h3>

<p>Performs the element-wise division of <code>tensor1</code> by <code>tensor2</code>,
multiply the result by the scalar <code>value</code> and add it to <code>input</code>.
</p>


<h3>Warning</h3>

<p>Integer division with addcdiv is deprecated, and in a future release
addcdiv will perform a true division of <code>tensor1</code> and <code>tensor2</code>.
The current addcdiv behavior can be replicated using <code><a href="#topic+torch_floor_divide">torch_floor_divide()</a></code>
for integral inputs
(<code>input</code> + <code>value</code> * <code>tensor1</code> // <code>tensor2</code>)
and <code><a href="#topic+torch_div">torch_div()</a></code> for float inputs
(<code>input</code> + <code>value</code> * <code>tensor1</code> / <code>tensor2</code>).
The new addcdiv behavior can be implemented with <code><a href="#topic+torch_true_divide">torch_true_divide()</a></code>
(<code>input</code> + <code>value</code> * torch.true_divide(<code>tensor1</code>,
<code>tensor2</code>).
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{input}_i + \mbox{value} \times \frac{\mbox{tensor1}_i}{\mbox{tensor2}_i}
</code>
</p>

<p>The shapes of <code>input</code>, <code>tensor1</code>, and <code>tensor2</code> must be
broadcastable .
</p>
<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code> must be
a real number, otherwise an integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t = torch_randn(c(1, 3))
t1 = torch_randn(c(3, 1))
t2 = torch_randn(c(1, 3))
torch_addcdiv(t, t1, t2, 0.1)
}
</code></pre>

<hr>
<h2 id='torch_addcmul'>Addcmul</h2><span id='topic+torch_addcmul'></span>

<h3>Description</h3>

<p>Addcmul
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addcmul(self, tensor1, tensor2, value = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addcmul_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to be added</p>
</td></tr>
<tr><td><code id="torch_addcmul_+3A_tensor1">tensor1</code></td>
<td>
<p>(Tensor) the tensor to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addcmul_+3A_tensor2">tensor2</code></td>
<td>
<p>(Tensor) the tensor to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addcmul_+3A_value">value</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">tensor1 .* tensor2</code></p>
</td></tr>
</table>


<h3>addcmul(input, tensor1, tensor2, *, value=1, out=NULL) -&gt; Tensor </h3>

<p>Performs the element-wise multiplication of <code>tensor1</code>
by <code>tensor2</code>, multiply the result by the scalar <code>value</code>
and add it to <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{input}_i + \mbox{value} \times \mbox{tensor1}_i \times \mbox{tensor2}_i
</code>
</p>

<p>The shapes of <code>tensor</code>, <code>tensor1</code>, and <code>tensor2</code> must be
broadcastable .
</p>
<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code> must be
a real number, otherwise an integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t = torch_randn(c(1, 3))
t1 = torch_randn(c(3, 1))
t2 = torch_randn(c(1, 3))
torch_addcmul(t, t1, t2, 0.1)
}
</code></pre>

<hr>
<h2 id='torch_addmm'>Addmm</h2><span id='topic+torch_addmm'></span>

<h3>Description</h3>

<p>Addmm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addmm(self, mat1, mat2, beta = 1L, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addmm_+3A_self">self</code></td>
<td>
<p>(Tensor) matrix to be added</p>
</td></tr>
<tr><td><code id="torch_addmm_+3A_mat1">mat1</code></td>
<td>
<p>(Tensor) the first matrix to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addmm_+3A_mat2">mat2</code></td>
<td>
<p>(Tensor) the second matrix to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addmm_+3A_beta">beta</code></td>
<td>
<p>(Number, optional) multiplier for <code>input</code> (<code class="reqn">\beta</code>)</p>
</td></tr>
<tr><td><code id="torch_addmm_+3A_alpha">alpha</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">mat1 @ mat2</code> (<code class="reqn">\alpha</code>)</p>
</td></tr>
</table>


<h3>addmm(input, mat1, mat2, *, beta=1, alpha=1, out=NULL) -&gt; Tensor </h3>

<p>Performs a matrix multiplication of the matrices <code>mat1</code> and <code>mat2</code>.
The matrix <code>input</code> is added to the final result.
</p>
<p>If <code>mat1</code> is a <code class="reqn">(n \times m)</code> tensor, <code>mat2</code> is a
<code class="reqn">(m \times p)</code> tensor, then <code>input</code> must be
broadcastable  with a <code class="reqn">(n \times p)</code> tensor
and <code>out</code> will be a <code class="reqn">(n \times p)</code> tensor.
</p>
<p><code>alpha</code> and <code>beta</code> are scaling factors on matrix-vector product between
<code>mat1</code> and <code>mat2</code> and the added matrix <code>input</code> respectively.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \beta\ \mbox{input} + \alpha\ (\mbox{mat1}_i \mathbin{@} \mbox{mat2}_i)
</code>
</p>

<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and
<code>alpha</code> must be real numbers, otherwise they should be integers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

M = torch_randn(c(2, 3))
mat1 = torch_randn(c(2, 3))
mat2 = torch_randn(c(3, 3))
torch_addmm(M, mat1, mat2)
}
</code></pre>

<hr>
<h2 id='torch_addmv'>Addmv</h2><span id='topic+torch_addmv'></span>

<h3>Description</h3>

<p>Addmv
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addmv(self, mat, vec, beta = 1L, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addmv_+3A_self">self</code></td>
<td>
<p>(Tensor) vector to be added</p>
</td></tr>
<tr><td><code id="torch_addmv_+3A_mat">mat</code></td>
<td>
<p>(Tensor) matrix to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addmv_+3A_vec">vec</code></td>
<td>
<p>(Tensor) vector to be multiplied</p>
</td></tr>
<tr><td><code id="torch_addmv_+3A_beta">beta</code></td>
<td>
<p>(Number, optional) multiplier for <code>input</code> (<code class="reqn">\beta</code>)</p>
</td></tr>
<tr><td><code id="torch_addmv_+3A_alpha">alpha</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">mat @ vec</code> (<code class="reqn">\alpha</code>)</p>
</td></tr>
</table>


<h3>addmv(input, mat, vec, *, beta=1, alpha=1, out=NULL) -&gt; Tensor </h3>

<p>Performs a matrix-vector product of the matrix <code>mat</code> and
the vector <code>vec</code>.
The vector <code>input</code> is added to the final result.
</p>
<p>If <code>mat</code> is a <code class="reqn">(n \times m)</code> tensor, <code>vec</code> is a 1-D tensor of
size <code>m</code>, then <code>input</code> must be
broadcastable  with a 1-D tensor of size <code>n</code> and
<code>out</code> will be 1-D tensor of size <code>n</code>.
</p>
<p><code>alpha</code> and <code>beta</code> are scaling factors on matrix-vector product between
<code>mat</code> and <code>vec</code> and the added tensor <code>input</code> respectively.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \beta\ \mbox{input} + \alpha\ (\mbox{mat} \mathbin{@} \mbox{vec})
</code>
</p>

<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and
<code>alpha</code> must be real numbers, otherwise they should be integers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

M = torch_randn(c(2))
mat = torch_randn(c(2, 3))
vec = torch_randn(c(3))
torch_addmv(M, mat, vec)
}
</code></pre>

<hr>
<h2 id='torch_addr'>Addr</h2><span id='topic+torch_addr'></span>

<h3>Description</h3>

<p>Addr
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_addr(self, vec1, vec2, beta = 1L, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_addr_+3A_self">self</code></td>
<td>
<p>(Tensor) matrix to be added</p>
</td></tr>
<tr><td><code id="torch_addr_+3A_vec1">vec1</code></td>
<td>
<p>(Tensor) the first vector of the outer product</p>
</td></tr>
<tr><td><code id="torch_addr_+3A_vec2">vec2</code></td>
<td>
<p>(Tensor) the second vector of the outer product</p>
</td></tr>
<tr><td><code id="torch_addr_+3A_beta">beta</code></td>
<td>
<p>(Number, optional) multiplier for <code>input</code> (<code class="reqn">\beta</code>)</p>
</td></tr>
<tr><td><code id="torch_addr_+3A_alpha">alpha</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">\mbox{vec1} \otimes \mbox{vec2}</code> (<code class="reqn">\alpha</code>)</p>
</td></tr>
</table>


<h3>addr(input, vec1, vec2, *, beta=1, alpha=1, out=NULL) -&gt; Tensor </h3>

<p>Performs the outer-product of vectors <code>vec1</code> and <code>vec2</code>
and adds it to the matrix <code>input</code>.
</p>
<p>Optional values <code>beta</code> and <code>alpha</code> are scaling factors on the
outer product between <code>vec1</code> and <code>vec2</code> and the added matrix
<code>input</code> respectively.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \beta\ \mbox{input} + \alpha\ (\mbox{vec1} \otimes \mbox{vec2})
</code>
</p>

<p>If <code>vec1</code> is a vector of size <code>n</code> and <code>vec2</code> is a vector
of size <code>m</code>, then <code>input</code> must be
broadcastable  with a matrix of size
<code class="reqn">(n \times m)</code> and <code>out</code> will be a matrix of size
<code class="reqn">(n \times m)</code>.
</p>
<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and
<code>alpha</code> must be real numbers, otherwise they should be integers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

vec1 = torch_arange(1, 3)
vec2 = torch_arange(1, 2)
M = torch_zeros(c(3, 2))
torch_addr(M, vec1, vec2)
}
</code></pre>

<hr>
<h2 id='torch_allclose'>Allclose</h2><span id='topic+torch_allclose'></span>

<h3>Description</h3>

<p>Allclose
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_allclose(self, other, rtol = 1e-05, atol = 1e-08, equal_nan = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_allclose_+3A_self">self</code></td>
<td>
<p>(Tensor) first tensor to compare</p>
</td></tr>
<tr><td><code id="torch_allclose_+3A_other">other</code></td>
<td>
<p>(Tensor) second tensor to compare</p>
</td></tr>
<tr><td><code id="torch_allclose_+3A_rtol">rtol</code></td>
<td>
<p>(float, optional) relative tolerance. Default: 1e-05</p>
</td></tr>
<tr><td><code id="torch_allclose_+3A_atol">atol</code></td>
<td>
<p>(float, optional) absolute tolerance. Default: 1e-08</p>
</td></tr>
<tr><td><code id="torch_allclose_+3A_equal_nan">equal_nan</code></td>
<td>
<p>(bool, optional) if <code>TRUE</code>, then two <code>NaN</code> s will be compared as equal. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -&gt; bool </h3>

<p>This function checks if all <code>input</code> and <code>other</code> satisfy the condition:
</p>
<p style="text-align: center;"><code class="reqn">
    \vert \mbox{input} - \mbox{other} \vert \leq \mbox{atol} + \mbox{rtol} \times \vert \mbox{other} \vert
</code>
</p>

<p>elementwise, for all elements of <code>input</code> and <code>other</code>. The behaviour of this function is analogous to
<code style="white-space: pre;">&#8288;numpy.allclose &lt;https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html&gt;&#8288;</code>_
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_allclose(torch_tensor(c(10000., 1e-07)), torch_tensor(c(10000.1, 1e-08)))
torch_allclose(torch_tensor(c(10000., 1e-08)), torch_tensor(c(10000.1, 1e-09)))
torch_allclose(torch_tensor(c(1.0, NaN)), torch_tensor(c(1.0, NaN)))
torch_allclose(torch_tensor(c(1.0, NaN)), torch_tensor(c(1.0, NaN)), equal_nan=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_amax'>Amax</h2><span id='topic+torch_amax'></span>

<h3>Description</h3>

<p>Amax
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_amax(self, dim = list(), keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_amax_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_amax_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_amax_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>amax(input, dim, keepdim=FALSE, *, out=None) -&gt; Tensor </h3>

<p>Returns the maximum value of each slice of the <code>input</code> tensor in the given
dimension(s) <code>dim</code>.
</p>


<h3>Note</h3>

<p>The difference between <code>max</code>/<code>min</code> and <code>amax</code>/<code>amin</code> is:
</p>

<ul>
<li> <p><code>amax</code>/<code>amin</code> supports reducing on multiple dimensions,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> does not return indices,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> evenly distributes gradient between equal values,
while <code>max(dim)</code>/<code>min(dim)</code> propagates gradient only to a single
index in the source tensor.
</p>
</li></ul>

<p>If <code style="white-space: pre;">&#8288;keepdim is &#8288;</code>TRUE<code style="white-space: pre;">&#8288;, the output tensors are of the same size as &#8288;</code>input<code style="white-space: pre;">&#8288;except in the dimension(s)&#8288;</code>dim<code style="white-space: pre;">&#8288;where they are of size 1. Otherwise,&#8288;</code>dim<code style="white-space: pre;">&#8288;s are squeezed (see [torch_squeeze()]), resulting in the output tensors having fewer dimension than &#8288;</code>input'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(4, 4))
a
torch_amax(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_amin'>Amin</h2><span id='topic+torch_amin'></span>

<h3>Description</h3>

<p>Amin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_amin(self, dim = list(), keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_amin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_amin_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_amin_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>amin(input, dim, keepdim=FALSE, *, out=None) -&gt; Tensor </h3>

<p>Returns the minimum value of each slice of the <code>input</code> tensor in the given
dimension(s) <code>dim</code>.
</p>


<h3>Note</h3>

<p>The difference between <code>max</code>/<code>min</code> and <code>amax</code>/<code>amin</code> is:
</p>

<ul>
<li> <p><code>amax</code>/<code>amin</code> supports reducing on multiple dimensions,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> does not return indices,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> evenly distributes gradient between equal values,
while <code>max(dim)</code>/<code>min(dim)</code> propagates gradient only to a single
index in the source tensor.
</p>
</li></ul>

<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size as
<code>input</code> except in the dimension(s) <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code>s are squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze()</a></code>), resulting in
the output tensors having fewer dimensions than <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(4, 4))
a
torch_amin(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_angle'>Angle</h2><span id='topic+torch_angle'></span>

<h3>Description</h3>

<p>Angle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_angle(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_angle_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>angle(input) -&gt; Tensor </h3>

<p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = angle(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
torch_angle(torch_tensor(c(-1 + 1i, -2 + 2i, 3 - 3i)))*180/3.14159

## End(Not run)

}
</code></pre>

<hr>
<h2 id='torch_arange'>Arange</h2><span id='topic+torch_arange'></span>

<h3>Description</h3>

<p>Arange
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arange(
  start,
  end,
  step = 1,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arange_+3A_start">start</code></td>
<td>
<p>(Number) the starting value for the set of points. Default: <code>0</code>.</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_end">end</code></td>
<td>
<p>(Number) the ending value for the set of points</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_step">step</code></td>
<td>
<p>(Number) the gap between each pair of adjacent points. Default: <code>1</code>.</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). If <code>dtype</code> is not given, infer the data type from the other input        arguments. If any of <code>start</code>, <code>end</code>, or <code>stop</code> are floating-point, the        <code>dtype</code> is inferred to be the default dtype, see        <code>~torch.get_default_dtype</code>. Otherwise, the <code>dtype</code> is inferred to        be <code>torch.int64</code>.</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_arange_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>arange(start=0, end, step=1, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a 1-D tensor of size <code class="reqn">\left\lceil \frac{\mbox{end} - \mbox{start}}{\mbox{step}} \right\rceil</code>
with values from the interval <code style="white-space: pre;">&#8288;[start, end)&#8288;</code> taken with common difference
<code>step</code> beginning from <code>start</code>.
</p>
<p>Note that non-integer <code>step</code> is subject to floating point rounding errors when
comparing against <code>end</code>; to avoid inconsistency, we advise adding a small epsilon to <code>end</code>
in such cases.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{{i+1}} = \mbox{out}_{i} + \mbox{step}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_arange(start = 0, end = 5)
torch_arange(1, 4)
torch_arange(1, 2.5, 0.5)
}
</code></pre>

<hr>
<h2 id='torch_arccos'>Arccos</h2><span id='topic+torch_arccos'></span>

<h3>Description</h3>

<p>Arccos
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arccos(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arccos_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arccos(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_acos">torch_acos()</a></code>.
</p>

<hr>
<h2 id='torch_arccosh'>Arccosh</h2><span id='topic+torch_arccosh'></span>

<h3>Description</h3>

<p>Arccosh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arccosh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arccosh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arccosh(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_acosh">torch_acosh()</a></code>.
</p>

<hr>
<h2 id='torch_arcsin'>Arcsin</h2><span id='topic+torch_arcsin'></span>

<h3>Description</h3>

<p>Arcsin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arcsin(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arcsin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arcsin(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_asin">torch_asin()</a></code>.
</p>

<hr>
<h2 id='torch_arcsinh'>Arcsinh</h2><span id='topic+torch_arcsinh'></span>

<h3>Description</h3>

<p>Arcsinh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arcsinh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arcsinh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arcsinh(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_asinh">torch_asinh()</a></code>.
</p>

<hr>
<h2 id='torch_arctan'>Arctan</h2><span id='topic+torch_arctan'></span>

<h3>Description</h3>

<p>Arctan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arctan(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arctan_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arctan(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_atan">torch_atan()</a></code>.
</p>

<hr>
<h2 id='torch_arctanh'>Arctanh</h2><span id='topic+torch_arctanh'></span>

<h3>Description</h3>

<p>Arctanh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_arctanh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_arctanh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>arctanh(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_atanh">torch_atanh()</a></code>.
</p>

<hr>
<h2 id='torch_argmax'>Argmax</h2><span id='topic+torch_argmax'></span>

<h3>Description</h3>

<p>Argmax
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_argmax_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_argmax_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce. If <code>NULL</code>, the argmax of the flattened input is returned.</p>
</td></tr>
<tr><td><code id="torch_argmax_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not. Ignored if <code>dim=NULL</code>.</p>
</td></tr>
</table>


<h3>argmax(input) -&gt; LongTensor </h3>

<p>Returns the indices of the maximum value of all elements in the <code>input</code> tensor.
</p>
<p>This is the second value returned by <code>torch_max</code>. See its
documentation for the exact semantics of this method.
</p>


<h3>argmax(input, dim, keepdim=False) -&gt; LongTensor </h3>

<p>Returns the indices of the maximum values of a tensor across a dimension.
</p>
<p>This is the second value returned by <code>torch_max</code>. See its
documentation for the exact semantics of this method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

## Not run: 
a = torch_randn(c(4, 4))
a
torch_argmax(a)

## End(Not run)


a = torch_randn(c(4, 4))
a
torch_argmax(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_argmin'>Argmin</h2><span id='topic+torch_argmin'></span>

<h3>Description</h3>

<p>Argmin
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_argmin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_argmin_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce. If <code>NULL</code>, the argmin of the flattened input is returned.</p>
</td></tr>
<tr><td><code id="torch_argmin_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not. Ignored if <code>dim=NULL</code>.</p>
</td></tr>
</table>


<h3>argmin(input) -&gt; LongTensor </h3>

<p>Returns the indices of the minimum value of all elements in the <code>input</code> tensor.
</p>
<p>This is the second value returned by <code>torch_min</code>. See its
documentation for the exact semantics of this method.
</p>


<h3>argmin(input, dim, keepdim=False, out=NULL) -&gt; LongTensor </h3>

<p>Returns the indices of the minimum values of a tensor across a dimension.
</p>
<p>This is the second value returned by <code>torch_min</code>. See its
documentation for the exact semantics of this method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4, 4))
a
torch_argmin(a)


a = torch_randn(c(4, 4))
a
torch_argmin(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_argsort'>Argsort</h2><span id='topic+torch_argsort'></span>

<h3>Description</h3>

<p>Argsort
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_argsort(self, dim = -1L, descending = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_argsort_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_argsort_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension to sort along</p>
</td></tr>
<tr><td><code id="torch_argsort_+3A_descending">descending</code></td>
<td>
<p>(bool, optional) controls the sorting order (ascending or descending)</p>
</td></tr>
</table>


<h3>argsort(input, dim=-1, descending=False) -&gt; LongTensor </h3>

<p>Returns the indices that sort a tensor along a given dimension in ascending
order by value.
</p>
<p>This is the second value returned by <code>torch_sort</code>.  See its documentation
for the exact semantics of this method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4, 4))
a
torch_argsort(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_as_strided'>As_strided</h2><span id='topic+torch_as_strided'></span>

<h3>Description</h3>

<p>As_strided
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_as_strided(self, size, stride, storage_offset = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_as_strided_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_as_strided_+3A_size">size</code></td>
<td>
<p>(tuple or ints) the shape of the output tensor</p>
</td></tr>
<tr><td><code id="torch_as_strided_+3A_stride">stride</code></td>
<td>
<p>(tuple or ints) the stride of the output tensor</p>
</td></tr>
<tr><td><code id="torch_as_strided_+3A_storage_offset">storage_offset</code></td>
<td>
<p>(int, optional) the offset in the underlying storage of the output tensor</p>
</td></tr>
</table>


<h3>as_strided(input, size, stride, storage_offset=0) -&gt; Tensor </h3>

<p>Create a view of an existing <code>torch_Tensor</code> <code>input</code> with specified
<code>size</code>, <code>stride</code> and <code>storage_offset</code>.
</p>


<h3>Warning</h3>

<p>More than one element of a created tensor may refer to a single memory
location. As a result, in-place operations (especially ones that are
vectorized) may result in incorrect behavior. If you need to write to
the tensors, please clone them first.
</p>
<div class="sourceCode"><pre>Many PyTorch functions, which return a view of a tensor, are internally
implemented with this function. Those functions, like
`torch_Tensor.expand`, are easier to read and are therefore more
advisable to use.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(3, 3))
x
t = torch_as_strided(x, list(2, 2), list(1, 2))
t
t = torch_as_strided(x, list(2, 2), list(1, 2), 1)
t
}
</code></pre>

<hr>
<h2 id='torch_asin'>Asin</h2><span id='topic+torch_asin'></span>

<h3>Description</h3>

<p>Asin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_asin(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_asin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>asin(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the arcsine  of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \sin^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_asin(a)
}
</code></pre>

<hr>
<h2 id='torch_asinh'>Asinh</h2><span id='topic+torch_asinh'></span>

<h3>Description</h3>

<p>Asinh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_asinh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_asinh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>asinh(input, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \sinh^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(4))
a
torch_asinh(a)
}
</code></pre>

<hr>
<h2 id='torch_atan'>Atan</h2><span id='topic+torch_atan'></span>

<h3>Description</h3>

<p>Atan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atan(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atan_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>atan(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the arctangent  of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \tan^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_atan(a)
}
</code></pre>

<hr>
<h2 id='torch_atan2'>Atan2</h2><span id='topic+torch_atan2'></span>

<h3>Description</h3>

<p>Atan2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atan2(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atan2_+3A_self">self</code></td>
<td>
<p>(Tensor) the first input tensor</p>
</td></tr>
<tr><td><code id="torch_atan2_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>atan2(input, other, out=NULL) -&gt; Tensor </h3>

<p>Element-wise arctangent of <code class="reqn">\mbox{input}_{i} / \mbox{other}_{i}</code>
with consideration of the quadrant. Returns a new tensor with the signed angles
in radians between vector <code class="reqn">(\mbox{other}_{i}, \mbox{input}_{i})</code>
and vector <code class="reqn">(1, 0)</code>. (Note that <code class="reqn">\mbox{other}_{i}</code>, the second
parameter, is the x-coordinate, while <code class="reqn">\mbox{input}_{i}</code>, the first
parameter, is the y-coordinate.)
</p>
<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_atan2(a, torch_randn(c(4)))
}
</code></pre>

<hr>
<h2 id='torch_atanh'>Atanh</h2><span id='topic+torch_atanh'></span>

<h3>Description</h3>

<p>Atanh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atanh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atanh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>atanh(input, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code>input</code>.
</p>


<h3>Note</h3>

<p>The domain of the inverse hyperbolic tangent is <code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code> and values outside this range
will be mapped to <code>NaN</code>, except for the values <code>1</code> and <code>-1</code> for which the output is
mapped to <code style="white-space: pre;">&#8288;+/-INF&#8288;</code> respectively.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \tanh^{-1}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))$uniform_(-1, 1)
a
torch_atanh(a)
}
</code></pre>

<hr>
<h2 id='torch_atleast_1d'>Atleast_1d</h2><span id='topic+torch_atleast_1d'></span>

<h3>Description</h3>

<p>Returns a 1-dimensional view of each input tensor with zero dimensions.
Input tensors with one or more dimensions are returned as-is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atleast_1d(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atleast_1d_+3A_self">self</code></td>
<td>
<p>(Tensor or list of Tensors)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_randn(c(2))
x
torch_atleast_1d(x)
x &lt;- torch_tensor(1.)
x
torch_atleast_1d(x)
x &lt;- torch_tensor(0.5)
y &lt;- torch_tensor(1.)
torch_atleast_1d(list(x,y))
}
</code></pre>

<hr>
<h2 id='torch_atleast_2d'>Atleast_2d</h2><span id='topic+torch_atleast_2d'></span>

<h3>Description</h3>

<p>Returns a 2-dimensional view of each each input tensor with zero dimensions.
Input tensors with two or more dimensions are returned as-is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atleast_2d(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atleast_2d_+3A_self">self</code></td>
<td>
<p>(Tensor or list of Tensors)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_tensor(1.)
x
torch_atleast_2d(x)
x &lt;- torch_randn(c(2,2))
x
torch_atleast_2d(x)
x &lt;- torch_tensor(0.5)
y &lt;- torch_tensor(1.)
torch_atleast_2d(list(x,y))
}
</code></pre>

<hr>
<h2 id='torch_atleast_3d'>Atleast_3d</h2><span id='topic+torch_atleast_3d'></span>

<h3>Description</h3>

<p>Returns a 3-dimensional view of each each input tensor with zero dimensions.
Input tensors with three or more dimensions are returned as-is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_atleast_3d(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_atleast_3d_+3A_self">self</code></td>
<td>
<p>(Tensor or list of Tensors)</p>
</td></tr>
</table>

<hr>
<h2 id='torch_avg_pool1d'>Avg_pool1d</h2><span id='topic+torch_avg_pool1d'></span>

<h3>Description</h3>

<p>Avg_pool1d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_avg_pool1d(
  self,
  kernel_size,
  stride = list(),
  padding = 0L,
  ceil_mode = FALSE,
  count_include_pad = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_avg_pool1d_+3A_self">self</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iW)</code></p>
</td></tr>
<tr><td><code id="torch_avg_pool1d_+3A_kernel_size">kernel_size</code></td>
<td>
<p>the size of the window. Can be a single number or a tuple <code style="white-space: pre;">&#8288;(kW,)&#8288;</code></p>
</td></tr>
<tr><td><code id="torch_avg_pool1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the window. Can be a single number or a tuple <code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: <code>kernel_size</code></p>
</td></tr>
<tr><td><code id="torch_avg_pool1d_+3A_padding">padding</code></td>
<td>
<p>implicit zero paddings on both sides of the input. Can be a single number or a tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_avg_pool1d_+3A_ceil_mode">ceil_mode</code></td>
<td>
<p>when <code>TRUE</code>, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="torch_avg_pool1d_+3A_count_include_pad">count_include_pad</code></td>
<td>
<p>when <code>TRUE</code>, will include the zero-padding in the averaging calculation. Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>avg_pool1d(input, kernel_size, stride=NULL, padding=0, ceil_mode=FALSE, count_include_pad=TRUE) -&gt; Tensor </h3>

<p>Applies a 1D average pooling over an input signal composed of several
input planes.
</p>
<p>See <code><a href="#topic+nn_avg_pool1d">nn_avg_pool1d()</a></code> for details and output shape.
</p>

<hr>
<h2 id='torch_baddbmm'>Baddbmm</h2><span id='topic+torch_baddbmm'></span>

<h3>Description</h3>

<p>Baddbmm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_baddbmm(self, batch1, batch2, beta = 1L, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_baddbmm_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to be added</p>
</td></tr>
<tr><td><code id="torch_baddbmm_+3A_batch1">batch1</code></td>
<td>
<p>(Tensor) the first batch of matrices to be multiplied</p>
</td></tr>
<tr><td><code id="torch_baddbmm_+3A_batch2">batch2</code></td>
<td>
<p>(Tensor) the second batch of matrices to be multiplied</p>
</td></tr>
<tr><td><code id="torch_baddbmm_+3A_beta">beta</code></td>
<td>
<p>(Number, optional) multiplier for <code>input</code> (<code class="reqn">\beta</code>)</p>
</td></tr>
<tr><td><code id="torch_baddbmm_+3A_alpha">alpha</code></td>
<td>
<p>(Number, optional) multiplier for <code class="reqn">\mbox{batch1} \mathbin{@} \mbox{batch2}</code> (<code class="reqn">\alpha</code>)</p>
</td></tr>
</table>


<h3>baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=NULL) -&gt; Tensor </h3>

<p>Performs a batch matrix-matrix product of matrices in <code>batch1</code>
and <code>batch2</code>.
<code>input</code> is added to the final result.
</p>
<p><code>batch1</code> and <code>batch2</code> must be 3-D tensors each containing the same
number of matrices.
</p>
<p>If <code>batch1</code> is a <code class="reqn">(b \times n \times m)</code> tensor, <code>batch2</code> is a
<code class="reqn">(b \times m \times p)</code> tensor, then <code>input</code> must be
broadcastable  with a
<code class="reqn">(b \times n \times p)</code> tensor and <code>out</code> will be a
<code class="reqn">(b \times n \times p)</code> tensor. Both <code>alpha</code> and <code>beta</code> mean the
same as the scaling factors used in <code>torch_addbmm</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \beta\ \mbox{input}_i + \alpha\ (\mbox{batch1}_i \mathbin{@} \mbox{batch2}_i)
</code>
</p>

<p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and
<code>alpha</code> must be real numbers, otherwise they should be integers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

M = torch_randn(c(10, 3, 5))
batch1 = torch_randn(c(10, 3, 4))
batch2 = torch_randn(c(10, 4, 5))
torch_baddbmm(M, batch1, batch2)
}
</code></pre>

<hr>
<h2 id='torch_bartlett_window'>Bartlett_window</h2><span id='topic+torch_bartlett_window'></span>

<h3>Description</h3>

<p>Bartlett_window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bartlett_window(
  window_length,
  periodic = TRUE,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bartlett_window_+3A_window_length">window_length</code></td>
<td>
<p>(int) the size of returned window</p>
</td></tr>
<tr><td><code id="torch_bartlett_window_+3A_periodic">periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a window to be used as periodic        function. If False, return a symmetric window.</p>
</td></tr>
<tr><td><code id="torch_bartlett_window_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). Only floating point types are supported.</p>
</td></tr>
<tr><td><code id="torch_bartlett_window_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned window tensor. Only          <code>torch_strided</code> (dense layout) is supported.</p>
</td></tr>
<tr><td><code id="torch_bartlett_window_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_bartlett_window_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>bartlett_window(window_length, periodic=TRUE, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Bartlett window function.
</p>
<p style="text-align: center;"><code class="reqn">
    w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \left\{ \begin{array}{ll}
        \frac{2n}{N - 1} &amp; \mbox{if } 0 \leq n \leq \frac{N - 1}{2} \\
        2 - \frac{2n}{N - 1} &amp; \mbox{if } \frac{N - 1}{2} &lt; n &lt; N \\
    \end{array}
    \right. ,
</code>
</p>

<p>where <code class="reqn">N</code> is the full window size.
</p>
<p>The input <code>window_length</code> is a positive integer controlling the
returned window size. <code>periodic</code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<code>torch_stft</code>. Therefore, if <code>periodic</code> is true, the <code class="reqn">N</code> in
above formula is in fact <code class="reqn">\mbox{window\_length} + 1</code>. Also, we always have
<code>torch_bartlett_window(L, periodic=TRUE)</code> equal to
<code style="white-space: pre;">&#8288;torch_bartlett_window(L + 1, periodic=False)[:-1])&#8288;</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `window_length` \eqn{=1}, the returned window contains a single value 1.
</pre></div>

<hr>
<h2 id='torch_bernoulli'>Bernoulli</h2><span id='topic+torch_bernoulli'></span>

<h3>Description</h3>

<p>Bernoulli
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bernoulli(self, p, generator = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bernoulli_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of probability values for the Bernoulli
distribution</p>
</td></tr>
<tr><td><code id="torch_bernoulli_+3A_p">p</code></td>
<td>
<p>(Number) a probability value. If <code>p</code> is passed than it's used instead of
the values in <code>self</code> tensor.</p>
</td></tr>
<tr><td><code id="torch_bernoulli_+3A_generator">generator</code></td>
<td>
<p>(<code>torch.Generator</code>, optional) a pseudorandom number generator for sampling</p>
</td></tr>
</table>


<h3>bernoulli(input, *, generator=NULL, out=NULL) -&gt; Tensor </h3>

<p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.
</p>
<p>The <code>input</code> tensor should be a tensor containing probabilities
to be used for drawing the binary random number.
Hence, all values in <code>input</code> have to be in the range:
<code class="reqn">0 \leq \mbox{input}_i \leq 1</code>.
</p>
<p>The <code class="reqn">\mbox{i}^{th}</code> element of the output tensor will draw a
value <code class="reqn">1</code> according to the <code class="reqn">\mbox{i}^{th}</code> probability value given
in <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} \sim \mathrm{Bernoulli}(p = \mbox{input}_{i})
</code>
</p>

<p>The returned <code>out</code> tensor only has values 0 or 1 and is of the same
shape as <code>input</code>.
</p>
<p><code>out</code> can have integral <code>dtype</code>, but <code>input</code> must have floating
point <code>dtype</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_empty(c(3, 3))$uniform_(0, 1)  # generate a uniform random matrix with range c(0, 1)
a
torch_bernoulli(a)
a = torch_ones(c(3, 3)) # probability of drawing "1" is 1
torch_bernoulli(a)
a = torch_zeros(c(3, 3)) # probability of drawing "1" is 0
torch_bernoulli(a)
}
</code></pre>

<hr>
<h2 id='torch_bincount'>Bincount</h2><span id='topic+torch_bincount'></span>

<h3>Description</h3>

<p>Bincount
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bincount_+3A_self">self</code></td>
<td>
<p>(Tensor) 1-d int tensor</p>
</td></tr>
<tr><td><code id="torch_bincount_+3A_weights">weights</code></td>
<td>
<p>(Tensor) optional, weight for each value in the input tensor.        Should be of same size as input tensor.</p>
</td></tr>
<tr><td><code id="torch_bincount_+3A_minlength">minlength</code></td>
<td>
<p>(int) optional, minimum number of bins. Should be non-negative.</p>
</td></tr>
</table>


<h3>bincount(input, weights=NULL, minlength=0) -&gt; Tensor </h3>

<p>Count the frequency of each value in an array of non-negative ints.
</p>
<p>The number of bins (size 1) is one larger than the largest value in
<code>input</code> unless <code>input</code> is empty, in which case the result is a
tensor of size 0. If <code>minlength</code> is specified, the number of bins is at least
<code>minlength</code> and if <code>input</code> is empty, then the result is tensor of size
<code>minlength</code> filled with zeros. If <code>n</code> is the value at position <code>i</code>,
<code style="white-space: pre;">&#8288;out[n] += weights[i]&#8288;</code> if <code>weights</code> is specified else
<code style="white-space: pre;">&#8288;out[n] += 1&#8288;</code>.
</p>
<p>.. include:: cuda_deterministic.rst
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_randint(1, 8, list(5), dtype=torch_int64())
weights = torch_linspace(0, 1, steps=5)
input
weights
torch_bincount(input, weights)
input$bincount(weights)
}
</code></pre>

<hr>
<h2 id='torch_bitwise_and'>Bitwise_and</h2><span id='topic+torch_bitwise_and'></span>

<h3>Description</h3>

<p>Bitwise_and
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bitwise_and(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bitwise_and_+3A_self">self</code></td>
<td>
<p>NA the first input tensor</p>
</td></tr>
<tr><td><code id="torch_bitwise_and_+3A_other">other</code></td>
<td>
<p>NA the second input tensor</p>
</td></tr>
</table>


<h3>bitwise_and(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the bitwise AND of <code>input</code> and <code>other</code>. The input tensor must be of
integral or Boolean types. For bool tensors, it computes the logical AND.
</p>

<hr>
<h2 id='torch_bitwise_not'>Bitwise_not</h2><span id='topic+torch_bitwise_not'></span>

<h3>Description</h3>

<p>Bitwise_not
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bitwise_not(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bitwise_not_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>bitwise_not(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the bitwise NOT of the given input tensor. The input tensor must be of
integral or Boolean types. For bool tensors, it computes the logical NOT.
</p>

<hr>
<h2 id='torch_bitwise_or'>Bitwise_or</h2><span id='topic+torch_bitwise_or'></span>

<h3>Description</h3>

<p>Bitwise_or
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bitwise_or(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bitwise_or_+3A_self">self</code></td>
<td>
<p>NA the first input tensor</p>
</td></tr>
<tr><td><code id="torch_bitwise_or_+3A_other">other</code></td>
<td>
<p>NA the second input tensor</p>
</td></tr>
</table>


<h3>bitwise_or(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the bitwise OR of <code>input</code> and <code>other</code>. The input tensor must be of
integral or Boolean types. For bool tensors, it computes the logical OR.
</p>

<hr>
<h2 id='torch_bitwise_xor'>Bitwise_xor</h2><span id='topic+torch_bitwise_xor'></span>

<h3>Description</h3>

<p>Bitwise_xor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bitwise_xor(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bitwise_xor_+3A_self">self</code></td>
<td>
<p>NA the first input tensor</p>
</td></tr>
<tr><td><code id="torch_bitwise_xor_+3A_other">other</code></td>
<td>
<p>NA the second input tensor</p>
</td></tr>
</table>


<h3>bitwise_xor(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the bitwise XOR of <code>input</code> and <code>other</code>. The input tensor must be of
integral or Boolean types. For bool tensors, it computes the logical XOR.
</p>

<hr>
<h2 id='torch_blackman_window'>Blackman_window</h2><span id='topic+torch_blackman_window'></span>

<h3>Description</h3>

<p>Blackman_window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_blackman_window(
  window_length,
  periodic = TRUE,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_blackman_window_+3A_window_length">window_length</code></td>
<td>
<p>(int) the size of returned window</p>
</td></tr>
<tr><td><code id="torch_blackman_window_+3A_periodic">periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a window to be used as periodic        function. If False, return a symmetric window.</p>
</td></tr>
<tr><td><code id="torch_blackman_window_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). Only floating point types are supported.</p>
</td></tr>
<tr><td><code id="torch_blackman_window_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned window tensor. Only          <code>torch_strided</code> (dense layout) is supported.</p>
</td></tr>
<tr><td><code id="torch_blackman_window_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_blackman_window_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>blackman_window(window_length, periodic=TRUE, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Blackman window function.
</p>
<p style="text-align: center;"><code class="reqn">
    w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right)
</code>
</p>

<p>where <code class="reqn">N</code> is the full window size.
</p>
<p>The input <code>window_length</code> is a positive integer controlling the
returned window size. <code>periodic</code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<code>torch_stft</code>. Therefore, if <code>periodic</code> is true, the <code class="reqn">N</code> in
above formula is in fact <code class="reqn">\mbox{window\_length} + 1</code>. Also, we always have
<code>torch_blackman_window(L, periodic=TRUE)</code> equal to
<code style="white-space: pre;">&#8288;torch_blackman_window(L + 1, periodic=False)[:-1])&#8288;</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `window_length` \eqn{=1}, the returned window contains a single value 1.
</pre></div>

<hr>
<h2 id='torch_block_diag'>Block_diag</h2><span id='topic+torch_block_diag'></span>

<h3>Description</h3>

<p>Create a block diagonal matrix from provided tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_block_diag(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_block_diag_+3A_tensors">tensors</code></td>
<td>
<p>(list of tensors) One or more tensors with 0, 1, or 2
dimensions.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A &lt;- torch_tensor(rbind(c(0, 1), c(1, 0)))
B &lt;- torch_tensor(rbind(c(3, 4, 5), c(6, 7, 8)))
C &lt;- torch_tensor(7)
D &lt;- torch_tensor(c(1, 2, 3))
E &lt;- torch_tensor(rbind(4, 5, 6))
torch_block_diag(list(A, B, C, D, E))
}
</code></pre>

<hr>
<h2 id='torch_bmm'>Bmm</h2><span id='topic+torch_bmm'></span>

<h3>Description</h3>

<p>Bmm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bmm(self, mat2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bmm_+3A_self">self</code></td>
<td>
<p>(Tensor) the first batch of matrices to be multiplied</p>
</td></tr>
<tr><td><code id="torch_bmm_+3A_mat2">mat2</code></td>
<td>
<p>(Tensor) the second batch of matrices to be multiplied</p>
</td></tr>
</table>


<h3>bmm(input, mat2, out=NULL) -&gt; Tensor </h3>

<p>Performs a batch matrix-matrix product of matrices stored in <code>input</code>
and <code>mat2</code>.
</p>
<p><code>input</code> and <code>mat2</code> must be 3-D tensors each containing
the same number of matrices.
</p>
<p>If <code>input</code> is a <code class="reqn">(b \times n \times m)</code> tensor, <code>mat2</code> is a
<code class="reqn">(b \times m \times p)</code> tensor, <code>out</code> will be a
<code class="reqn">(b \times n \times p)</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{input}_i \mathbin{@} \mbox{mat2}_i
</code>
</p>



<h3>Note</h3>

<p>This function does not broadcast .
For broadcasting matrix products, see <code><a href="#topic+torch_matmul">torch_matmul</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_randn(c(10, 3, 4))
mat2 = torch_randn(c(10, 4, 5))
res = torch_bmm(input, mat2)
res
}
</code></pre>

<hr>
<h2 id='torch_broadcast_tensors'>Broadcast_tensors</h2><span id='topic+torch_broadcast_tensors'></span>

<h3>Description</h3>

<p>Broadcast_tensors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_broadcast_tensors(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_broadcast_tensors_+3A_tensors">tensors</code></td>
<td>
<p>a list containing any number of tensors of the same type</p>
</td></tr>
</table>


<h3>broadcast_tensors(tensors) -&gt; List of Tensors </h3>

<p>Broadcasts the given tensors according to broadcasting-semantics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_arange(0, 3)$view(c(1, 4))
y = torch_arange(0, 2)$view(c(3, 1))
out = torch_broadcast_tensors(list(x, y))
out[[1]]
}
</code></pre>

<hr>
<h2 id='torch_bucketize'>Bucketize</h2><span id='topic+torch_bucketize'></span>

<h3>Description</h3>

<p>Bucketize
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_bucketize(self, boundaries, out_int32 = FALSE, right = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_bucketize_+3A_self">self</code></td>
<td>
<p>(Tensor or Scalar) N-D tensor or a Scalar containing the search value(s).</p>
</td></tr>
<tr><td><code id="torch_bucketize_+3A_boundaries">boundaries</code></td>
<td>
<p>(Tensor) 1-D tensor, must contain a monotonically increasing sequence.</p>
</td></tr>
<tr><td><code id="torch_bucketize_+3A_out_int32">out_int32</code></td>
<td>
<p>(bool, optional)  indicate the output data type. <code><a href="#topic+torch_int32">torch_int32()</a></code>
if True, <code><a href="#topic+torch_int64">torch_int64()</a></code> otherwise. Default value is FALSE, i.e. default output
data type is <code><a href="#topic+torch_int64">torch_int64()</a></code>.</p>
</td></tr>
<tr><td><code id="torch_bucketize_+3A_right">right</code></td>
<td>
<p>(bool, optional)  if False, return the first suitable location
that is found. If True, return the last such index. If no suitable index found,
return 0 for non-numerical value (eg. nan, inf) or the size of boundaries
(one pass the last index). In other words, if False, gets the lower bound index
for each value in input from boundaries. If True, gets the upper bound index
instead. Default value is False.</p>
</td></tr>
</table>


<h3>bucketize(input, boundaries, *, out_int32=FALSE, right=FALSE, out=None) -&gt; Tensor </h3>

<p>Returns the indices of the buckets to which each value in the <code>input</code> belongs, where the
boundaries of the buckets are set by <code>boundaries</code>. Return a new tensor with the same size
as <code>input</code>. If <code>right</code> is FALSE (default), then the left boundary is closed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

boundaries &lt;- torch_tensor(c(1, 3, 5, 7, 9))
boundaries
v &lt;- torch_tensor(rbind(c(3, 6, 9), c(3, 6, 9)))
v
torch_bucketize(v, boundaries)
torch_bucketize(v, boundaries, right=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_can_cast'>Can_cast</h2><span id='topic+torch_can_cast'></span>

<h3>Description</h3>

<p>Can_cast
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_can_cast(from, to)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_can_cast_+3A_from">from</code></td>
<td>
<p>(dtype) The original <code>torch_dtype</code>.</p>
</td></tr>
<tr><td><code id="torch_can_cast_+3A_to">to</code></td>
<td>
<p>(dtype) The target <code>torch_dtype</code>.</p>
</td></tr>
</table>


<h3>can_cast(from, to) -&gt; bool </h3>

<p>Determines if a type conversion is allowed under PyTorch casting rules
described in the type promotion documentation .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_can_cast(torch_double(), torch_float())
torch_can_cast(torch_float(), torch_int())
}
</code></pre>

<hr>
<h2 id='torch_cartesian_prod'>Cartesian_prod</h2><span id='topic+torch_cartesian_prod'></span>

<h3>Description</h3>

<p>Do cartesian product of the given sequence of tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cartesian_prod(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cartesian_prod_+3A_tensors">tensors</code></td>
<td>
<p>a list containing any number of 1 dimensional tensors.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = c(1, 2, 3)
b = c(4, 5)
tensor_a = torch_tensor(a)
tensor_b = torch_tensor(b)
torch_cartesian_prod(list(tensor_a, tensor_b))
}
</code></pre>

<hr>
<h2 id='torch_cat'>Cat</h2><span id='topic+torch_cat'></span>

<h3>Description</h3>

<p>Cat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cat(tensors, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cat_+3A_tensors">tensors</code></td>
<td>
<p>(sequence of Tensors) any python sequence of tensors of the same type.        Non-empty tensors provided must have the same shape, except in the        cat dimension.</p>
</td></tr>
<tr><td><code id="torch_cat_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension over which the tensors are concatenated</p>
</td></tr>
</table>


<h3>cat(tensors, dim=0, out=NULL) -&gt; Tensor </h3>

<p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension.
All tensors must either have the same shape (except in the concatenating
dimension) or be empty.
</p>
<p><code><a href="#topic+torch_cat">torch_cat</a></code> can be seen as an inverse operation for <code><a href="#topic+torch_split">torch_split()</a></code>
and <code><a href="#topic+torch_chunk">torch_chunk</a></code>.
</p>
<p><code><a href="#topic+torch_cat">torch_cat</a></code> can be best understood via examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(2, 3))
x
torch_cat(list(x, x, x), 1)
torch_cat(list(x, x, x), 2)
}
</code></pre>

<hr>
<h2 id='torch_cdist'>Cdist</h2><span id='topic+torch_cdist'></span>

<h3>Description</h3>

<p>Cdist
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cdist(x1, x2, p = 2L, compute_mode = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cdist_+3A_x1">x1</code></td>
<td>
<p>(Tensor) input tensor of shape <code class="reqn">B \times P \times M</code>.</p>
</td></tr>
<tr><td><code id="torch_cdist_+3A_x2">x2</code></td>
<td>
<p>(Tensor) input tensor of shape <code class="reqn">B \times R \times M</code>.</p>
</td></tr>
<tr><td><code id="torch_cdist_+3A_p">p</code></td>
<td>
<p>NA p value for the p-norm distance to calculate between each vector pair        <code class="reqn">\in [0, \infty]</code>.</p>
</td></tr>
<tr><td><code id="torch_cdist_+3A_compute_mode">compute_mode</code></td>
<td>
<p>NA 'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate        euclidean distance (p = 2) if P &gt; 25 or R &gt; 25        'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate        euclidean distance (p = 2)        'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate        euclidean distance (p = 2)        Default: use_mm_for_euclid_dist_if_necessary.</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Computes batched the p-norm distance between each pair of the two collections of row vectors.
</p>

<hr>
<h2 id='torch_ceil'>Ceil</h2><span id='topic+torch_ceil'></span>

<h3>Description</h3>

<p>Ceil
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ceil(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ceil_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>ceil(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the ceil of the elements of <code>input</code>,
the smallest integer greater than or equal to each element.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \left\lceil \mbox{input}_{i} \right\rceil = \left\lfloor \mbox{input}_{i} \right\rfloor + 1
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_ceil(a)
}
</code></pre>

<hr>
<h2 id='torch_celu'>Celu</h2><span id='topic+torch_celu'></span>

<h3>Description</h3>

<p>Celu
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_celu(self, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_celu_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_celu_+3A_alpha">alpha</code></td>
<td>
<p>the alpha value for the CELU formulation. Default: 1.0</p>
</td></tr>
</table>


<h3>celu(input, alpha=1.) -&gt; Tensor </h3>

<p>See <code><a href="#topic+nnf_celu">nnf_celu()</a></code> for more info.
</p>

<hr>
<h2 id='torch_celu_'>Celu_</h2><span id='topic+torch_celu_'></span>

<h3>Description</h3>

<p>Celu_
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_celu_(self, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_celu__+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_celu__+3A_alpha">alpha</code></td>
<td>
<p>the alpha value for the CELU formulation. Default: 1.0</p>
</td></tr>
</table>


<h3>celu_(input, alpha=1.) -&gt; Tensor </h3>

<p>In-place version of <code><a href="#topic+torch_celu">torch_celu()</a></code>.
</p>

<hr>
<h2 id='torch_chain_matmul'>Chain_matmul</h2><span id='topic+torch_chain_matmul'></span>

<h3>Description</h3>

<p>Chain_matmul
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_chain_matmul(matrices)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_chain_matmul_+3A_matrices">matrices</code></td>
<td>
<p>(Tensors...) a sequence of 2 or more 2-D tensors whose product is to be determined.</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Returns the matrix product of the <code class="reqn">N</code> 2-D tensors. This product is efficiently computed
using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms
of arithmetic operations (<code style="white-space: pre;">&#8288;[CLRS]&#8288;</code>_). Note that since this is a function to compute the product, <code class="reqn">N</code>
needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.
If <code class="reqn">N</code> is 1, then this is a no-op - the original matrix is returned as is.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 4))
b = torch_randn(c(4, 5))
c = torch_randn(c(5, 6))
d = torch_randn(c(6, 7))
torch_chain_matmul(list(a, b, c, d))
}
</code></pre>

<hr>
<h2 id='torch_channel_shuffle'>Channel_shuffle</h2><span id='topic+torch_channel_shuffle'></span>

<h3>Description</h3>

<p>Channel_shuffle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_channel_shuffle(self, groups)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_channel_shuffle_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_channel_shuffle_+3A_groups">groups</code></td>
<td>
<p>(int) number of groups to divide channels in and rearrange.</p>
</td></tr>
</table>


<h3>Divide the channels in a tensor of shape </h3>

<p>math:<code style="white-space: pre;">&#8288;(*, C , H, W)&#8288;</code> :
</p>
<p>Divide the channels in a tensor of shape <code class="reqn">(*, C , H, W)</code>
into g groups and rearrange them as <code class="reqn">(*, C \frac g, g, H, W)</code>,
while keeping the original tensor shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input &lt;- torch_randn(c(1, 4, 2, 2))
print(input)
output &lt;- torch_channel_shuffle(input, 2)
print(output)
}
</code></pre>

<hr>
<h2 id='torch_cholesky'>Cholesky</h2><span id='topic+torch_cholesky'></span>

<h3>Description</h3>

<p>Cholesky
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cholesky(self, upper = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cholesky_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor <code class="reqn">A</code> of size <code class="reqn">(*, n, n)</code> where <code>*</code> is zero or more
batch dimensions consisting of symmetric positive-definite matrices.</p>
</td></tr>
<tr><td><code id="torch_cholesky_+3A_upper">upper</code></td>
<td>
<p>(bool, optional) flag that indicates whether to return a
upper or lower triangular matrix. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>cholesky(input, upper=False, out=NULL) -&gt; Tensor </h3>

<p>Computes the Cholesky decomposition of a symmetric positive-definite
matrix <code class="reqn">A</code> or for batches of symmetric positive-definite matrices.
</p>
<p>If <code>upper</code> is <code>TRUE</code>, the returned matrix <code>U</code> is upper-triangular, and
the decomposition has the form:
</p>
<p style="text-align: center;"><code class="reqn">
  A = U^TU
</code>
</p>

<p>If <code>upper</code> is <code>FALSE</code>, the returned matrix <code>L</code> is lower-triangular, and
the decomposition has the form:
</p>
<p style="text-align: center;"><code class="reqn">
    A = LL^T
</code>
</p>

<p>If <code>upper</code> is <code>TRUE</code>, and <code class="reqn">A</code> is a batch of symmetric positive-definite
matrices, then the returned tensor will be composed of upper-triangular Cholesky factors
of each of the individual matrices. Similarly, when <code>upper</code> is <code>FALSE</code>, the returned
tensor will be composed of lower-triangular Cholesky factors of each of the individual
matrices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
a = torch_mm(a, a$t()) # make symmetric positive-definite
l = torch_cholesky(a)
a
l
torch_mm(l, l$t())
a = torch_randn(c(3, 2, 2))
## Not run: 
a = torch_matmul(a, a$transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
l = torch_cholesky(a)
z = torch_matmul(l, l$transpose(-1, -2))
torch_max(torch_abs(z - a)) # Max non-zero

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_cholesky_inverse'>Cholesky_inverse</h2><span id='topic+torch_cholesky_inverse'></span>

<h3>Description</h3>

<p>Cholesky_inverse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cholesky_inverse(self, upper = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cholesky_inverse_+3A_self">self</code></td>
<td>
<p>(Tensor) the input 2-D tensor <code class="reqn">u</code>, a upper or lower triangular           Cholesky factor</p>
</td></tr>
<tr><td><code id="torch_cholesky_inverse_+3A_upper">upper</code></td>
<td>
<p>(bool, optional) whether to return a lower (default) or upper triangular matrix</p>
</td></tr>
</table>


<h3>cholesky_inverse(input, upper=False, out=NULL) -&gt; Tensor </h3>

<p>Computes the inverse of a symmetric positive-definite matrix <code class="reqn">A</code> using its
Cholesky factor <code class="reqn">u</code>: returns matrix <code>inv</code>. The inverse is computed using
LAPACK routines <code>dpotri</code> and <code>spotri</code> (and the corresponding MAGMA routines).
</p>
<p>If <code>upper</code> is <code>FALSE</code>, <code class="reqn">u</code> is lower triangular
such that the returned tensor is
</p>
<p style="text-align: center;"><code class="reqn">
    inv = (uu^{{T}})^{{-1}}
</code>
</p>

<p>If <code>upper</code> is <code>TRUE</code> or not provided, <code class="reqn">u</code> is upper
triangular such that the returned tensor is
</p>
<p style="text-align: center;"><code class="reqn">
    inv = (u^T u)^{{-1}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

## Not run: 
a = torch_randn(c(3, 3))
a = torch_mm(a, a$t()) + 1e-05 * torch_eye(3) # make symmetric positive definite
u = torch_cholesky(a)
a
torch_cholesky_inverse(u)
a$inverse()

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_cholesky_solve'>Cholesky_solve</h2><span id='topic+torch_cholesky_solve'></span>

<h3>Description</h3>

<p>Cholesky_solve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cholesky_solve(self, input2, upper = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cholesky_solve_+3A_self">self</code></td>
<td>
<p>(Tensor) input matrix <code class="reqn">b</code> of size <code class="reqn">(*, m, k)</code>,                where <code class="reqn">*</code> is zero or more batch dimensions</p>
</td></tr>
<tr><td><code id="torch_cholesky_solve_+3A_input2">input2</code></td>
<td>
<p>(Tensor) input matrix <code class="reqn">u</code> of size <code class="reqn">(*, m, m)</code>,                where <code class="reqn">*</code> is zero of more batch dimensions composed of                upper or lower triangular Cholesky factor</p>
</td></tr>
<tr><td><code id="torch_cholesky_solve_+3A_upper">upper</code></td>
<td>
<p>(bool, optional) whether to consider the Cholesky factor as a                            lower or upper triangular matrix. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>cholesky_solve(input, input2, upper=False, out=NULL) -&gt; Tensor </h3>

<p>Solves a linear system of equations with a positive semidefinite
matrix to be inverted given its Cholesky factor matrix <code class="reqn">u</code>.
</p>
<p>If <code>upper</code> is <code>FALSE</code>, <code class="reqn">u</code> is and lower triangular and <code>c</code> is
returned such that:
</p>
<p style="text-align: center;"><code class="reqn">
    c = (u u^T)^{{-1}} b
</code>
</p>

<p>If <code>upper</code> is <code>TRUE</code> or not provided, <code class="reqn">u</code> is upper triangular
and <code>c</code> is returned such that:
</p>
<p style="text-align: center;"><code class="reqn">
    c = (u^T u)^{{-1}} b
</code>
</p>

<p><code>torch_cholesky_solve(b, u)</code> can take in 2D inputs <code style="white-space: pre;">&#8288;b, u&#8288;</code> or inputs that are
batches of 2D matrices. If the inputs are batches, then returns
batched outputs <code>c</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
a = torch_mm(a, a$t()) # make symmetric positive definite
u = torch_cholesky(a)
a
b = torch_randn(c(3, 2))
b
torch_cholesky_solve(b, u)
torch_mm(a$inverse(), b)
}
</code></pre>

<hr>
<h2 id='torch_chunk'>Chunk</h2><span id='topic+torch_chunk'></span>

<h3>Description</h3>

<p>Chunk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_chunk(self, chunks, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_chunk_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to split</p>
</td></tr>
<tr><td><code id="torch_chunk_+3A_chunks">chunks</code></td>
<td>
<p>(int) number of chunks to return</p>
</td></tr>
<tr><td><code id="torch_chunk_+3A_dim">dim</code></td>
<td>
<p>(int) dimension along which to split the tensor</p>
</td></tr>
</table>


<h3>chunk(input, chunks, dim=0) -&gt; List of Tensors </h3>

<p>Splits a tensor into a specific number of chunks. Each chunk is a view of
the input tensor.
</p>
<p>Last chunk will be smaller if the tensor size along the given dimension
<code>dim</code> is not divisible by <code>chunks</code>.
</p>

<hr>
<h2 id='torch_clamp'>Clamp</h2><span id='topic+torch_clamp'></span>

<h3>Description</h3>

<p>Clamp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_clamp(self, min = NULL, max = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_clamp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_clamp_+3A_min">min</code></td>
<td>
<p>(Number) lower-bound of the range to be clamped to</p>
</td></tr>
<tr><td><code id="torch_clamp_+3A_max">max</code></td>
<td>
<p>(Number) upper-bound of the range to be clamped to</p>
</td></tr>
</table>


<h3>clamp(input, min, max, out=NULL) -&gt; Tensor </h3>

<p>Clamp all elements in <code>input</code> into the range <code>[</code> <code>min</code>, <code>max</code> <code style="white-space: pre;">&#8288;]&#8288;</code> and return
a resulting tensor:
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = \left\{ \begin{array}{ll}
        \mbox{min} &amp; \mbox{if } x_i &lt; \mbox{min} \\
        x_i &amp; \mbox{if } \mbox{min} \leq x_i \leq \mbox{max} \\
        \mbox{max} &amp; \mbox{if } x_i &gt; \mbox{max}
    \end{array}
    \right.
</code>
</p>

<p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, args <code>min</code>
and <code>max</code> must be real numbers, otherwise they should be integers.
</p>


<h3>clamp(input, *, min, out=NULL) -&gt; Tensor </h3>

<p>Clamps all elements in <code>input</code> to be larger or equal <code>min</code>.
</p>
<p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code>
should be a real number, otherwise it should be an integer.
</p>


<h3>clamp(input, *, max, out=NULL) -&gt; Tensor </h3>

<p>Clamps all elements in <code>input</code> to be smaller or equal <code>max</code>.
</p>
<p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code>
should be a real number, otherwise it should be an integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_clamp(a, min=-0.5, max=0.5)


a = torch_randn(c(4))
a
torch_clamp(a, min=0.5)


a = torch_randn(c(4))
a
torch_clamp(a, max=0.5)
}
</code></pre>

<hr>
<h2 id='torch_clip'>Clip</h2><span id='topic+torch_clip'></span>

<h3>Description</h3>

<p>Clip
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_clip(self, min = NULL, max = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_clip_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_clip_+3A_min">min</code></td>
<td>
<p>(Number) lower-bound of the range to be clamped to</p>
</td></tr>
<tr><td><code id="torch_clip_+3A_max">max</code></td>
<td>
<p>(Number) upper-bound of the range to be clamped to</p>
</td></tr>
</table>


<h3>clip(input, min, max, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_clamp">torch_clamp()</a></code>.
</p>

<hr>
<h2 id='torch_clone'>Clone</h2><span id='topic+torch_clone'></span>

<h3>Description</h3>

<p>Clone
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_clone(self, memory_format = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_clone_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_clone_+3A_memory_format">memory_format</code></td>
<td>
<p>a torch memory format. see <code><a href="#topic+torch_preserve_format">torch_preserve_format()</a></code>.</p>
</td></tr>
</table>


<h3>clone(input, *, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns a copy of <code>input</code>.
</p>


<h3>Note</h3>

<p>This function is differentiable, so gradients will flow back from the
result of this operation to <code>input</code>. To create a tensor without an
autograd relationship to <code>input</code> see <code>Tensor$detach</code>.
</p>

<hr>
<h2 id='torch_combinations'>Combinations</h2><span id='topic+torch_combinations'></span>

<h3>Description</h3>

<p>Combinations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_combinations(self, r = 2L, with_replacement = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_combinations_+3A_self">self</code></td>
<td>
<p>(Tensor) 1D vector.</p>
</td></tr>
<tr><td><code id="torch_combinations_+3A_r">r</code></td>
<td>
<p>(int, optional) number of elements to combine</p>
</td></tr>
<tr><td><code id="torch_combinations_+3A_with_replacement">with_replacement</code></td>
<td>
<p>(boolean, optional) whether to allow duplication in combination</p>
</td></tr>
</table>


<h3>combinations(input, r=2, with_replacement=False) -&gt; seq </h3>

<p>Compute combinations of length <code class="reqn">r</code> of the given tensor. The behavior is similar to
python's <code>itertools.combinations</code> when <code>with_replacement</code> is set to <code>False</code>, and
<code>itertools.combinations_with_replacement</code> when <code>with_replacement</code> is set to <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = c(1, 2, 3)
tensor_a = torch_tensor(a)
torch_combinations(tensor_a)
torch_combinations(tensor_a, r=3)
torch_combinations(tensor_a, with_replacement=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_complex'>Complex</h2><span id='topic+torch_complex'></span>

<h3>Description</h3>

<p>Complex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_complex(real, imag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_complex_+3A_real">real</code></td>
<td>
<p>(Tensor) The real part of the complex tensor. Must be float or double.</p>
</td></tr>
<tr><td><code id="torch_complex_+3A_imag">imag</code></td>
<td>
<p>(Tensor) The imaginary part of the complex tensor. Must be same dtype
as <code>real</code>.</p>
</td></tr>
</table>


<h3>complex(real, imag, *, out=None) -&gt; Tensor </h3>

<p>Constructs a complex tensor with its real part equal to <code>real</code> and its
imaginary part equal to <code>imag</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

real &lt;- torch_tensor(c(1, 2), dtype=torch_float32())
imag &lt;- torch_tensor(c(3, 4), dtype=torch_float32())
z &lt;- torch_complex(real, imag)
z
z$dtype
}
</code></pre>

<hr>
<h2 id='torch_conj'>Conj</h2><span id='topic+torch_conj'></span>

<h3>Description</h3>

<p>Conj
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conj(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conj_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>conj(input) -&gt; Tensor </h3>

<p>Computes the element-wise conjugate of the given <code>input</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = conj(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
torch_conj(torch_tensor(c(-1 + 1i, -2 + 2i, 3 - 3i)))

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_conv_tbc'>Conv_tbc</h2><span id='topic+torch_conv_tbc'></span>

<h3>Description</h3>

<p>Conv_tbc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv_tbc(self, weight, bias, pad = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv_tbc_+3A_self">self</code></td>
<td>
<p>NA input tensor of shape <code class="reqn">(\mbox{sequence length} \times batch \times \mbox{in\_channels})</code></p>
</td></tr>
<tr><td><code id="torch_conv_tbc_+3A_weight">weight</code></td>
<td>
<p>NA filter of shape (<code class="reqn">\mbox{kernel width} \times \mbox{in\_channels} \times \mbox{out\_channels}</code>)</p>
</td></tr>
<tr><td><code id="torch_conv_tbc_+3A_bias">bias</code></td>
<td>
<p>NA bias of shape (<code class="reqn">\mbox{out\_channels}</code>)</p>
</td></tr>
<tr><td><code id="torch_conv_tbc_+3A_pad">pad</code></td>
<td>
<p>NA number of timesteps to pad. Default: 0</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Applies a 1-dimensional sequence convolution over an input sequence.
Input and output dimensions are (Time, Batch, Channels) - hence TBC.
</p>

<hr>
<h2 id='torch_conv_transpose1d'>Conv_transpose1d</h2><span id='topic+torch_conv_transpose1d'></span>

<h3>Description</h3>

<p>Conv_transpose1d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv_transpose1d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  output_padding = 0L,
  groups = 1L,
  dilation = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv_transpose1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{in\_channels} , \frac{\mbox{out\_channels}}{\mbox{groups}} , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: NULL</p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a      tuple <code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_padding">padding</code></td>
<td>
<p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both      sides of each dimension in the input. Can be a single number or a tuple      <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_output_padding">output_padding</code></td>
<td>
<p>additional size added to one side of each dimension in the      output shape. Can be a single number or a tuple <code>(out_padW)</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by the      number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose1d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a tuple <code style="white-space: pre;">&#8288;(dW,)&#8288;</code>. Default: 1</p>
</td></tr>
</table>


<h3>conv_transpose1d(input, weight, bias=NULL, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor </h3>

<p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called &quot;deconvolution&quot;.
</p>
<p>See <code><a href="#topic+nn_conv_transpose1d">nn_conv_transpose1d()</a></code>  for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

inputs = torch_randn(c(20, 16, 50))
weights = torch_randn(c(16, 33, 5))
nnf_conv_transpose1d(inputs, weights)
}
</code></pre>

<hr>
<h2 id='torch_conv_transpose2d'>Conv_transpose2d</h2><span id='topic+torch_conv_transpose2d'></span>

<h3>Description</h3>

<p>Conv_transpose2d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv_transpose2d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  output_padding = 0L,
  groups = 1L,
  dilation = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv_transpose2d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iH , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{in\_channels} , \frac{\mbox{out\_channels}}{\mbox{groups}} , kH , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: NULL</p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a      tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_padding">padding</code></td>
<td>
<p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both      sides of each dimension in the input. Can be a single number or a tuple      <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_output_padding">output_padding</code></td>
<td>
<p>additional size added to one side of each dimension in the      output shape. Can be a single number or a tuple <code style="white-space: pre;">&#8288;(out_padH, out_padW)&#8288;</code>.      Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by the      number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose2d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a tuple <code style="white-space: pre;">&#8288;(dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
</table>


<h3>conv_transpose2d(input, weight, bias=NULL, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor </h3>

<p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called &quot;deconvolution&quot;.
</p>
<p>See <code><a href="#topic+nn_conv_transpose2d">nn_conv_transpose2d()</a></code> for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# With square kernels and equal stride
inputs = torch_randn(c(1, 4, 5, 5))
weights = torch_randn(c(4, 8, 3, 3))
nnf_conv_transpose2d(inputs, weights, padding=1)
}
</code></pre>

<hr>
<h2 id='torch_conv_transpose3d'>Conv_transpose3d</h2><span id='topic+torch_conv_transpose3d'></span>

<h3>Description</h3>

<p>Conv_transpose3d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv_transpose3d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  output_padding = 0L,
  groups = 1L,
  dilation = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv_transpose3d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iT , iH , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{in\_channels} , \frac{\mbox{out\_channels}}{\mbox{groups}} , kT , kH , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: NULL</p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a      tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_padding">padding</code></td>
<td>
<p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both      sides of each dimension in the input. Can be a single number or a tuple      <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_output_padding">output_padding</code></td>
<td>
<p>additional size added to one side of each dimension in the      output shape. Can be a single number or a tuple      <code style="white-space: pre;">&#8288;(out_padT, out_padH, out_padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by the      number of groups. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv_transpose3d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a tuple <code style="white-space: pre;">&#8288;(dT, dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
</table>


<h3>conv_transpose3d(input, weight, bias=NULL, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor </h3>

<p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called &quot;deconvolution&quot;
</p>
<p>See <code><a href="#topic+nn_conv_transpose3d">nn_conv_transpose3d()</a></code> for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
inputs = torch_randn(c(20, 16, 50, 10, 20))
weights = torch_randn(c(16, 33, 3, 3, 3))
nnf_conv_transpose3d(inputs, weights)

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_conv1d'>Conv1d</h2><span id='topic+torch_conv1d'></span>

<h3>Description</h3>

<p>Conv1d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv1d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  dilation = 1L,
  groups = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv1d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{out\_channels} , \frac{\mbox{in\_channels}}{\mbox{groups}} , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_bias">bias</code></td>
<td>
<p>optional bias of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or      a one-element tuple <code style="white-space: pre;">&#8288;(sW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a      single number or a one-element tuple <code style="white-space: pre;">&#8288;(padW,)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a one-element tuple <code style="white-space: pre;">&#8288;(dW,)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv1d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by      the number of groups. Default: 1</p>
</td></tr>
</table>


<h3>conv1d(input, weight, bias=NULL, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor </h3>

<p>Applies a 1D convolution over an input signal composed of several input
planes.
</p>
<p>See <code><a href="#topic+nn_conv1d">nn_conv1d()</a></code> for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

filters = torch_randn(c(33, 16, 3))
inputs = torch_randn(c(20, 16, 50))
nnf_conv1d(inputs, filters)
}
</code></pre>

<hr>
<h2 id='torch_conv2d'>Conv2d</h2><span id='topic+torch_conv2d'></span>

<h3>Description</h3>

<p>Conv2d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv2d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  dilation = 1L,
  groups = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv2d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iH , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{out\_channels} , \frac{\mbox{in\_channels}}{\mbox{groups}} , kH , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a      tuple <code style="white-space: pre;">&#8288;(sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a      single number or a tuple <code style="white-space: pre;">&#8288;(padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a tuple <code style="white-space: pre;">&#8288;(dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv2d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by the      number of groups. Default: 1</p>
</td></tr>
</table>


<h3>conv2d(input, weight, bias=NULL, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor </h3>

<p>Applies a 2D convolution over an input image composed of several input
planes.
</p>
<p>See <code><a href="#topic+nn_conv2d">nn_conv2d()</a></code> for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# With square kernels and equal stride
filters = torch_randn(c(8,4,3,3))
inputs = torch_randn(c(1,4,5,5))
nnf_conv2d(inputs, filters, padding=1)
}
</code></pre>

<hr>
<h2 id='torch_conv3d'>Conv3d</h2><span id='topic+torch_conv3d'></span>

<h3>Description</h3>

<p>Conv3d
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_conv3d(
  input,
  weight,
  bias = list(),
  stride = 1L,
  padding = 0L,
  dilation = 1L,
  groups = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_conv3d_+3A_input">input</code></td>
<td>
<p>input tensor of shape <code class="reqn">(\mbox{minibatch} , \mbox{in\_channels} , iT , iH , iW)</code></p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_weight">weight</code></td>
<td>
<p>filters of shape <code class="reqn">(\mbox{out\_channels} , \frac{\mbox{in\_channels}}{\mbox{groups}} , kT , kH , kW)</code></p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_bias">bias</code></td>
<td>
<p>optional bias tensor of shape <code class="reqn">(\mbox{out\_channels})</code>. Default: NULL</p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_stride">stride</code></td>
<td>
<p>the stride of the convolving kernel. Can be a single number or a      tuple <code style="white-space: pre;">&#8288;(sT, sH, sW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_padding">padding</code></td>
<td>
<p>implicit paddings on both sides of the input. Can be a      single number or a tuple <code style="white-space: pre;">&#8288;(padT, padH, padW)&#8288;</code>. Default: 0</p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_dilation">dilation</code></td>
<td>
<p>the spacing between kernel elements. Can be a single number or      a tuple <code style="white-space: pre;">&#8288;(dT, dH, dW)&#8288;</code>. Default: 1</p>
</td></tr>
<tr><td><code id="torch_conv3d_+3A_groups">groups</code></td>
<td>
<p>split input into groups, <code class="reqn">\mbox{in\_channels}</code> should be divisible by      the number of groups. Default: 1</p>
</td></tr>
</table>


<h3>conv3d(input, weight, bias=NULL, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor </h3>

<p>Applies a 3D convolution over an input image composed of several input
planes.
</p>
<p>See <code><a href="#topic+nn_conv3d">nn_conv3d()</a></code> for details and output shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# filters = torch_randn(c(33, 16, 3, 3, 3))
# inputs = torch_randn(c(20, 16, 50, 10, 20))
# nnf_conv3d(inputs, filters)
}
</code></pre>

<hr>
<h2 id='torch_cos'>Cos</h2><span id='topic+torch_cos'></span>

<h3>Description</h3>

<p>Cos
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cos(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cos_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>cos(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the cosine  of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \cos(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_cos(a)
}
</code></pre>

<hr>
<h2 id='torch_cosh'>Cosh</h2><span id='topic+torch_cosh'></span>

<h3>Description</h3>

<p>Cosh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cosh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cosh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>cosh(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the hyperbolic cosine  of the elements of
<code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \cosh(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_cosh(a)
}
</code></pre>

<hr>
<h2 id='torch_cosine_similarity'>Cosine_similarity</h2><span id='topic+torch_cosine_similarity'></span>

<h3>Description</h3>

<p>Cosine_similarity
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cosine_similarity(x1, x2, dim = 2L, eps = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cosine_similarity_+3A_x1">x1</code></td>
<td>
<p>(Tensor) First input.</p>
</td></tr>
<tr><td><code id="torch_cosine_similarity_+3A_x2">x2</code></td>
<td>
<p>(Tensor) Second input (of size matching x1).</p>
</td></tr>
<tr><td><code id="torch_cosine_similarity_+3A_dim">dim</code></td>
<td>
<p>(int, optional) Dimension of vectors. Default: 1</p>
</td></tr>
<tr><td><code id="torch_cosine_similarity_+3A_eps">eps</code></td>
<td>
<p>(float, optional) Small value to avoid division by zero.        Default: 1e-8</p>
</td></tr>
</table>


<h3>cosine_similarity(x1, x2, dim=1, eps=1e-8) -&gt; Tensor </h3>

<p>Returns cosine similarity between x1 and x2, computed along dim.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{similarity} = \frac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input1 = torch_randn(c(100, 128))
input2 = torch_randn(c(100, 128))
output = torch_cosine_similarity(input1, input2)
output
}
</code></pre>

<hr>
<h2 id='torch_count_nonzero'>Count_nonzero</h2><span id='topic+torch_count_nonzero'></span>

<h3>Description</h3>

<p>Count_nonzero
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_count_nonzero(self, dim = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_count_nonzero_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_count_nonzero_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints, optional) Dim or tuple of dims along which
to count non-zeros.</p>
</td></tr>
</table>


<h3>count_nonzero(input, dim=None) -&gt; Tensor </h3>

<p>Counts the number of non-zero values in the tensor <code>input</code> along the given <code>dim</code>.
If no dim is specified then all non-zeros in the tensor are counted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_zeros(3,3)
x[torch_randn(3,3) &gt; 0.5] = 1
x
torch_count_nonzero(x)
torch_count_nonzero(x, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_cross'>Cross</h2><span id='topic+torch_cross'></span>

<h3>Description</h3>

<p>Cross
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cross(self, other, dim = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cross_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_cross_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
<tr><td><code id="torch_cross_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension to take the cross-product in.</p>
</td></tr>
</table>


<h3>cross(input, other, dim=-1, out=NULL) -&gt; Tensor </h3>

<p>Returns the cross product of vectors in dimension <code>dim</code> of <code>input</code>
and <code>other</code>.
</p>
<p><code>input</code> and <code>other</code> must have the same size, and the size of their
<code>dim</code> dimension should be 3.
</p>
<p>If <code>dim</code> is not given, it defaults to the first dimension found with the
size 3.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4, 3))
a
b = torch_randn(c(4, 3))
b
torch_cross(a, b, dim=2)
torch_cross(a, b)
}
</code></pre>

<hr>
<h2 id='torch_cummax'>Cummax</h2><span id='topic+torch_cummax'></span>

<h3>Description</h3>

<p>Cummax
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cummax(self, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cummax_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_cummax_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to do the operation over</p>
</td></tr>
</table>


<h3>cummax(input, dim) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the cumulative maximum of
elements of <code>input</code> in the dimension <code>dim</code>. And <code>indices</code> is the index
location of each maximum value found in the dimension <code>dim</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = max(x_1, x_2, x_3, \dots, x_i)
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(10))
a
torch_cummax(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_cummin'>Cummin</h2><span id='topic+torch_cummin'></span>

<h3>Description</h3>

<p>Cummin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cummin(self, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cummin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_cummin_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to do the operation over</p>
</td></tr>
</table>


<h3>cummin(input, dim) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the cumulative minimum of
elements of <code>input</code> in the dimension <code>dim</code>. And <code>indices</code> is the index
location of each maximum value found in the dimension <code>dim</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = min(x_1, x_2, x_3, \dots, x_i)
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(10))
a
torch_cummin(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_cumprod'>Cumprod</h2><span id='topic+torch_cumprod'></span>

<h3>Description</h3>

<p>Cumprod
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cumprod(self, dim, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cumprod_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_cumprod_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to do the operation over</p>
</td></tr>
<tr><td><code id="torch_cumprod_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        If specified, the input tensor is casted to <code>dtype</code> before the operation        is performed. This is useful for preventing data type overflows. Default: NULL.</p>
</td></tr>
</table>


<h3>cumprod(input, dim, out=NULL, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the cumulative product of elements of <code>input</code> in the dimension
<code>dim</code>.
</p>
<p>For example, if <code>input</code> is a vector of size N, the result will also be
a vector of size N, with elements.
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = x_1 \times x_2\times x_3\times \dots \times x_i
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(10))
a
torch_cumprod(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_cumsum'>Cumsum</h2><span id='topic+torch_cumsum'></span>

<h3>Description</h3>

<p>Cumsum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_cumsum(self, dim, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_cumsum_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_cumsum_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to do the operation over</p>
</td></tr>
<tr><td><code id="torch_cumsum_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        If specified, the input tensor is casted to <code>dtype</code> before the operation        is performed. This is useful for preventing data type overflows. Default: NULL.</p>
</td></tr>
</table>


<h3>cumsum(input, dim, out=NULL, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the cumulative sum of elements of <code>input</code> in the dimension
<code>dim</code>.
</p>
<p>For example, if <code>input</code> is a vector of size N, the result will also be
a vector of size N, with elements.
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = x_1 + x_2 + x_3 + \dots + x_i
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(10))
a
torch_cumsum(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_deg2rad'>Deg2rad</h2><span id='topic+torch_deg2rad'></span>

<h3>Description</h3>

<p>Deg2rad
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_deg2rad(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_deg2rad_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>deg2rad(input, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with each of the elements of <code>input</code>
converted from angles in degrees to radians.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(rbind(c(180.0, -180.0), c(360.0, -360.0), c(90.0, -90.0)))
torch_deg2rad(a)
}
</code></pre>

<hr>
<h2 id='torch_dequantize'>Dequantize</h2><span id='topic+torch_dequantize'></span>

<h3>Description</h3>

<p>Dequantize
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_dequantize(tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_dequantize_+3A_tensor">tensor</code></td>
<td>
<p>(Tensor) A quantized Tensor or a list oof quantized tensors</p>
</td></tr>
</table>


<h3>dequantize(tensor) -&gt; Tensor </h3>

<p>Returns an fp32 Tensor by dequantizing a quantized Tensor
</p>


<h3>dequantize(tensors) -&gt; sequence of Tensors </h3>

<p>Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors
</p>

<hr>
<h2 id='torch_det'>Det</h2><span id='topic+torch_det'></span>

<h3>Description</h3>

<p>Det
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_det(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_det_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more                batch dimensions.</p>
</td></tr>
</table>


<h3>det(input) -&gt; Tensor </h3>

<p>Calculates determinant of a square matrix or batches of square matrices.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>Backward through `det` internally uses SVD results when `input` is
not invertible. In this case, double backward through `det` will be
unstable in when `input` doesn't have distinct singular values. See
`~torch.svd` for details.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A = torch_randn(c(3, 3))
torch_det(A)
A = torch_randn(c(3, 2, 2))
A
A$det()
}
</code></pre>

<hr>
<h2 id='torch_device'>Create a Device object</h2><span id='topic+torch_device'></span>

<h3>Description</h3>

<p>A <code>torch_device</code>  is an object representing the device on which a <code>torch_tensor</code>
is or will be allocated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_device(type, index = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_device_+3A_type">type</code></td>
<td>
<p>(character) a device type <code>"cuda"</code> or <code>"cpu"</code></p>
</td></tr>
<tr><td><code id="torch_device_+3A_index">index</code></td>
<td>
<p>(integer) optional device ordinal for the device type.  If the device ordinal
is not present, this object will always represent the current device for the device
type, even after <code>torch_cuda_set_device()</code> is called; e.g., a <code>torch_tensor</code> constructed
with device <code>'cuda'</code> is equivalent to <code>'cuda:X'</code> where X is the result of
<code>torch_cuda_current_device()</code>.
</p>
<p>A <code>torch_device</code> can be constructed via a string or via a string and device ordinal</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# Via string
torch_device("cuda:1")
torch_device("cpu")
torch_device("cuda") # current cuda device

# Via string and device ordinal
torch_device("cuda", 0)
torch_device("cpu", 0)
}
</code></pre>

<hr>
<h2 id='torch_diag'>Diag</h2><span id='topic+torch_diag'></span>

<h3>Description</h3>

<p>Diag
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_diag(self, diagonal = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_diag_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_diag_+3A_diagonal">diagonal</code></td>
<td>
<p>(int, optional) the diagonal to consider</p>
</td></tr>
</table>


<h3>diag(input, diagonal=0, out=NULL) -&gt; Tensor </h3>


<ul>
<li><p> If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code>input</code> as the diagonal.
</p>
</li>
<li><p> If <code>input</code> is a matrix (2-D tensor), then returns a 1-D tensor with
the diagonal elements of <code>input</code>.
</p>
</li></ul>

<p>The argument <code>diagonal</code> controls which diagonal to consider:
</p>

<ul>
<li><p> If <code>diagonal</code> = 0, it is the main diagonal.
</p>
</li>
<li><p> If <code>diagonal</code> &gt; 0, it is above the main diagonal.
</p>
</li>
<li><p> If <code>diagonal</code> &lt; 0, it is below the main diagonal.
</p>
</li></ul>


<hr>
<h2 id='torch_diag_embed'>Diag_embed</h2><span id='topic+torch_diag_embed'></span>

<h3>Description</h3>

<p>Diag_embed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_diag_embed(self, offset = 0L, dim1 = -2L, dim2 = -1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_diag_embed_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor. Must be at least 1-dimensional.</p>
</td></tr>
<tr><td><code id="torch_diag_embed_+3A_offset">offset</code></td>
<td>
<p>(int, optional) which diagonal to consider. Default: 0        (main diagonal).</p>
</td></tr>
<tr><td><code id="torch_diag_embed_+3A_dim1">dim1</code></td>
<td>
<p>(int, optional) first dimension with respect to which to        take diagonal. Default: -2.</p>
</td></tr>
<tr><td><code id="torch_diag_embed_+3A_dim2">dim2</code></td>
<td>
<p>(int, optional) second dimension with respect to which to        take diagonal. Default: -1.</p>
</td></tr>
</table>


<h3>diag_embed(input, offset=0, dim1=-2, dim2=-1) -&gt; Tensor </h3>

<p>Creates a tensor whose diagonals of certain 2D planes (specified by
<code>dim1</code> and <code>dim2</code>) are filled by <code>input</code>.
To facilitate creating batched diagonal matrices, the 2D planes formed by
the last two dimensions of the returned tensor are chosen by default.
</p>
<p>The argument <code>offset</code> controls which diagonal to consider:
</p>

<ul>
<li><p> If <code>offset</code> = 0, it is the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &gt; 0, it is above the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &lt; 0, it is below the main diagonal.
</p>
</li></ul>

<p>The size of the new matrix will be calculated to make the specified diagonal
of the size of the last input dimension.
Note that for <code>offset</code> other than <code class="reqn">0</code>, the order of <code>dim1</code>
and <code>dim2</code> matters. Exchanging them is equivalent to changing the
sign of <code>offset</code>.
</p>
<p>Applying <code>torch_diagonal</code> to the output of this function with
the same arguments yields a matrix identical to input. However,
<code>torch_diagonal</code> has different default dimensions, so those
need to be explicitly specified.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(2, 3))
torch_diag_embed(a)
torch_diag_embed(a, offset=1, dim1=1, dim2=3)
}
</code></pre>

<hr>
<h2 id='torch_diagflat'>Diagflat</h2><span id='topic+torch_diagflat'></span>

<h3>Description</h3>

<p>Diagflat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_diagflat(self, offset = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_diagflat_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_diagflat_+3A_offset">offset</code></td>
<td>
<p>(int, optional) the diagonal to consider. Default: 0 (main        diagonal).</p>
</td></tr>
</table>


<h3>diagflat(input, offset=0) -&gt; Tensor </h3>


<ul>
<li><p> If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code>input</code> as the diagonal.
</p>
</li>
<li><p> If <code>input</code> is a tensor with more than one dimension, then returns a
2-D tensor with diagonal elements equal to a flattened <code>input</code>.
</p>
</li></ul>

<p>The argument <code>offset</code> controls which diagonal to consider:
</p>

<ul>
<li><p> If <code>offset</code> = 0, it is the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &gt; 0, it is above the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &lt; 0, it is below the main diagonal.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3))
a
torch_diagflat(a)
torch_diagflat(a, 1)
a = torch_randn(c(2, 2))
a
torch_diagflat(a)
}
</code></pre>

<hr>
<h2 id='torch_diagonal'>Diagonal</h2><span id='topic+torch_diagonal'></span>

<h3>Description</h3>

<p>Diagonal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_diagonal(self, outdim, dim1 = 1L, dim2 = 2L, offset = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_diagonal_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor. Must be at least 2-dimensional.</p>
</td></tr>
<tr><td><code id="torch_diagonal_+3A_outdim">outdim</code></td>
<td>
<p>dimension name if <code>self</code> is a named tensor.</p>
</td></tr>
<tr><td><code id="torch_diagonal_+3A_dim1">dim1</code></td>
<td>
<p>(int, optional) first dimension with respect to which to        take diagonal. Default: 0.</p>
</td></tr>
<tr><td><code id="torch_diagonal_+3A_dim2">dim2</code></td>
<td>
<p>(int, optional) second dimension with respect to which to        take diagonal. Default: 1.</p>
</td></tr>
<tr><td><code id="torch_diagonal_+3A_offset">offset</code></td>
<td>
<p>(int, optional) which diagonal to consider. Default: 0        (main diagonal).</p>
</td></tr>
</table>


<h3>diagonal(input, offset=0, dim1=0, dim2=1) -&gt; Tensor </h3>

<p>Returns a partial view of <code>input</code> with the its diagonal elements
with respect to <code>dim1</code> and <code>dim2</code> appended as a dimension
at the end of the shape.
</p>
<p>The argument <code>offset</code> controls which diagonal to consider:
</p>

<ul>
<li><p> If <code>offset</code> = 0, it is the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &gt; 0, it is above the main diagonal.
</p>
</li>
<li><p> If <code>offset</code> &lt; 0, it is below the main diagonal.
</p>
</li></ul>

<p>Applying <code>torch_diag_embed</code> to the output of this function with
the same arguments yields a diagonal matrix with the diagonal entries
of the input. However, <code>torch_diag_embed</code> has different default
dimensions, so those need to be explicitly specified.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
a
torch_diagonal(a, offset = 0)
torch_diagonal(a, offset = 1)
x = torch_randn(c(2, 5, 4, 2))
torch_diagonal(x, offset=-1, dim1=1, dim2=2)
}
</code></pre>

<hr>
<h2 id='torch_diff'>Computes the n-th forward difference along the given dimension.</h2><span id='topic+torch_diff'></span>

<h3>Description</h3>

<p>The first-order differences are given by <code>out[i] = input[i + 1] - input[i]</code>.
Higher-order differences are calculated by using <code><a href="#topic+torch_diff">torch_diff()</a></code> recursively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_diff(self, n = 1L, dim = -1L, prepend = list(), append = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_diff_+3A_self">self</code></td>
<td>
<p>the tensor to compute the differences on</p>
</td></tr>
<tr><td><code id="torch_diff_+3A_n">n</code></td>
<td>
<p>the number of times to recursively compute the difference</p>
</td></tr>
<tr><td><code id="torch_diff_+3A_dim">dim</code></td>
<td>
<p>the dimension to compute the difference along. Default is the last dimension.</p>
</td></tr>
<tr><td><code id="torch_diff_+3A_prepend">prepend</code></td>
<td>
<p>values to prepend to input along dim before computing the
difference. Their dimensions must be equivalent to that of input, and their
shapes must match inputs shape except on dim.</p>
</td></tr>
<tr><td><code id="torch_diff_+3A_append">append</code></td>
<td>
<p>values to append to input along dim before computing the
difference. Their dimensions must be equivalent to that of input, and their
shapes must match inputs shape except on dim.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Only n = 1 is currently supported
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
a &lt;- torch_tensor(c(1,2,3))
torch_diff(a)

b &lt;- torch_tensor(c(4, 5))
torch_diff(a, append = b)

c &lt;- torch_tensor(rbind(c(1,2,3), c(3,4,5)))
torch_diff(c, dim = 1)
torch_diff(c, dim = 2) 

}
</code></pre>

<hr>
<h2 id='torch_digamma'>Digamma</h2><span id='topic+torch_digamma'></span>

<h3>Description</h3>

<p>Digamma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_digamma(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_digamma_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compute the digamma function on</p>
</td></tr>
</table>


<h3>digamma(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the logarithmic derivative of the gamma function on <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_tensor(c(1, 0.5))
torch_digamma(a)
}
</code></pre>

<hr>
<h2 id='torch_dist'>Dist</h2><span id='topic+torch_dist'></span>

<h3>Description</h3>

<p>Dist
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_dist(self, other, p = 2L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_dist_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_dist_+3A_other">other</code></td>
<td>
<p>(Tensor) the Right-hand-side input tensor</p>
</td></tr>
<tr><td><code id="torch_dist_+3A_p">p</code></td>
<td>
<p>(float, optional) the norm to be computed</p>
</td></tr>
</table>


<h3>dist(input, other, p=2) -&gt; Tensor </h3>

<p>Returns the p-norm of (<code>input</code> - <code>other</code>)
</p>
<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(4))
x
y = torch_randn(c(4))
y
torch_dist(x, y, 3.5)
torch_dist(x, y, 3)
torch_dist(x, y, 0)
torch_dist(x, y, 1)
}
</code></pre>

<hr>
<h2 id='torch_div'>Div</h2><span id='topic+torch_div'></span>

<h3>Description</h3>

<p>Div
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_div(self, other, rounding_mode)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_div_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_div_+3A_other">other</code></td>
<td>
<p>(Number) the number to be divided to each element of <code>input</code></p>
</td></tr>
<tr><td><code id="torch_div_+3A_rounding_mode">rounding_mode</code></td>
<td>
<p>(str, optional)  Type of rounding applied to the result:
</p>

<ul>
<li> <p><code>NULL</code> - default behavior. Performs no rounding and, if both input and
other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the / operator) and NumPys
<code>np.true_divide</code>.
</p>
</li>
<li><p> &quot;trunc&quot; - rounds the results of the division towards zero. Equivalent to
C-style integer division.
</p>
</li>
<li><p> &quot;floor&quot; - rounds the results of the division down. Equivalent to floor
division in Python (the // operator) and NumPys <code>np.floor_divide</code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>div(input, other, out=NULL) -&gt; Tensor </h3>

<p>Divides each element of the input <code>input</code> with the scalar <code>other</code> and
returns a new resulting tensor.
</p>
<p>Each element of the tensor <code>input</code> is divided by each element of the tensor
<code>other</code>. The resulting tensor is returned.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \frac{\mbox{input}_i}{\mbox{other}_i}
</code>
</p>

<p>The shapes of <code>input</code> and <code>other</code> must be broadcastable
. If the <code>torch_dtype</code> of <code>input</code> and
<code>other</code> differ, the <code>torch_dtype</code> of the result tensor is determined
following rules described in the type promotion documentation
. If <code>out</code> is specified, the result must be
castable  to the <code>torch_dtype</code> of the
specified output tensor. Integral division by zero leads to undefined behavior.
</p>


<h3>Warning</h3>

<p>Integer division using div is deprecated, and in a future release div will
perform true division like <code><a href="#topic+torch_true_divide">torch_true_divide()</a></code>.
Use <code><a href="#topic+torch_floor_divide">torch_floor_divide()</a></code> to perform integer division,
instead.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \frac{\mbox{input}_i}{\mbox{other}}
</code>
</p>

<p>If the <code>torch_dtype</code> of <code>input</code> and <code>other</code> differ, the
<code>torch_dtype</code> of the result tensor is determined following rules
described in the type promotion documentation . If
<code>out</code> is specified, the result must be castable
to the <code>torch_dtype</code> of the specified output tensor. Integral division
by zero leads to undefined behavior.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(5))
a
torch_div(a, 0.5)


a = torch_randn(c(4, 4))
a
b = torch_randn(c(4))
b
torch_div(a, b)
}
</code></pre>

<hr>
<h2 id='torch_divide'>Divide</h2><span id='topic+torch_divide'></span>

<h3>Description</h3>

<p>Divide
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_divide(self, other, rounding_mode)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_divide_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_divide_+3A_other">other</code></td>
<td>
<p>(Number) the number to be divided to each element of <code>input</code></p>
</td></tr>
<tr><td><code id="torch_divide_+3A_rounding_mode">rounding_mode</code></td>
<td>
<p>(str, optional)  Type of rounding applied to the result:
</p>

<ul>
<li> <p><code>NULL</code> - default behavior. Performs no rounding and, if both input and
other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the / operator) and NumPys
<code>np.true_divide</code>.
</p>
</li>
<li><p> &quot;trunc&quot; - rounds the results of the division towards zero. Equivalent to
C-style integer division.
</p>
</li>
<li><p> &quot;floor&quot; - rounds the results of the division down. Equivalent to floor
division in Python (the // operator) and NumPys <code>np.floor_divide</code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>divide(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_div">torch_div()</a></code>.
</p>

<hr>
<h2 id='torch_dot'>Dot</h2><span id='topic+torch_dot'></span>

<h3>Description</h3>

<p>Dot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_dot(self, tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_dot_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_dot_+3A_tensor">tensor</code></td>
<td>
<p>the other input tensor</p>
</td></tr>
</table>


<h3>dot(input, tensor) -&gt; Tensor </h3>

<p>Computes the dot product (inner product) of two tensors.
</p>


<h3>Note</h3>

<p>This function does not broadcast .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_dot(torch_tensor(c(2, 3)), torch_tensor(c(2, 1)))
}
</code></pre>

<hr>
<h2 id='torch_dstack'>Dstack</h2><span id='topic+torch_dstack'></span>

<h3>Description</h3>

<p>Dstack
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_dstack(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_dstack_+3A_tensors">tensors</code></td>
<td>
<p>(sequence of Tensors) sequence of tensors to concatenate</p>
</td></tr>
</table>


<h3>dstack(tensors, *, out=None) -&gt; Tensor </h3>

<p>Stack tensors in sequence depthwise (along third axis).
</p>
<p>This is equivalent to concatenation along the third axis after 1-D and 2-D
tensors have been reshaped by <code><a href="#topic+torch_atleast_3d">torch_atleast_3d()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2, 3))
b &lt;- torch_tensor(c(4, 5, 6))
torch_dstack(list(a,b))
a &lt;- torch_tensor(rbind(1,2,3))
b &lt;- torch_tensor(rbind(4,5,6))
torch_dstack(list(a,b))
}
</code></pre>

<hr>
<h2 id='torch_dtype'>Torch data types</h2><span id='topic+torch_dtype'></span><span id='topic+torch_float32'></span><span id='topic+torch_float'></span><span id='topic+torch_float64'></span><span id='topic+torch_double'></span><span id='topic+torch_cfloat32'></span><span id='topic+torch_chalf'></span><span id='topic+torch_cfloat'></span><span id='topic+torch_cfloat64'></span><span id='topic+torch_cdouble'></span><span id='topic+torch_cfloat128'></span><span id='topic+torch_float16'></span><span id='topic+torch_half'></span><span id='topic+torch_uint8'></span><span id='topic+torch_int8'></span><span id='topic+torch_int16'></span><span id='topic+torch_short'></span><span id='topic+torch_int32'></span><span id='topic+torch_int'></span><span id='topic+torch_int64'></span><span id='topic+torch_long'></span><span id='topic+torch_bool'></span><span id='topic+torch_quint8'></span><span id='topic+torch_qint8'></span><span id='topic+torch_qint32'></span>

<h3>Description</h3>

<p>Returns the correspondent data type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_float32()

torch_float()

torch_float64()

torch_double()

torch_cfloat32()

torch_chalf()

torch_cfloat()

torch_cfloat64()

torch_cdouble()

torch_cfloat128()

torch_float16()

torch_half()

torch_uint8()

torch_int8()

torch_int16()

torch_short()

torch_int32()

torch_int()

torch_int64()

torch_long()

torch_bool()

torch_quint8()

torch_qint8()

torch_qint32()
</code></pre>

<hr>
<h2 id='torch_eig'>Eig</h2><span id='topic+torch_eig'></span>

<h3>Description</h3>

<p>Eig
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_eig_+3A_self">self</code></td>
<td>
<p>(Tensor) the square matrix of shape <code class="reqn">(n \times n)</code> for which the eigenvalues and eigenvectors        will be computed</p>
</td></tr>
<tr><td><code id="torch_eig_+3A_eigenvectors">eigenvectors</code></td>
<td>
<p>(bool) <code>TRUE</code> to compute both eigenvalues and eigenvectors;        otherwise, only eigenvalues will be computed</p>
</td></tr>
</table>


<h3>eig(input, eigenvectors=False, out=NULL) -&gt; (Tensor, Tensor) </h3>

<p>Computes the eigenvalues and eigenvectors of a real square matrix.
</p>

<hr>
<h2 id='torch_einsum'>Einsum</h2><span id='topic+torch_einsum'></span>

<h3>Description</h3>

<p>Einsum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_einsum(equation, tensors, path = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_einsum_+3A_equation">equation</code></td>
<td>
<p>(string) The equation is given in terms of lower case letters (indices) to be associated           with each dimension of the operands and result. The left hand side lists the operands           dimensions, separated by commas. There should be one index letter per tensor dimension.           The right hand side follows after <code style="white-space: pre;">&#8288;-&gt;&#8288;</code> and gives the indices for the output.           If the <code style="white-space: pre;">&#8288;-&gt;&#8288;</code> and right hand side are omitted, it implicitly defined as the alphabetically           sorted list of all indices appearing exactly once in the left hand side.           The indices not apprearing in the output are summed over after multiplying the operands           entries.           If an index appears several times for the same operand, a diagonal is taken.           Ellipses <code>...</code> represent a fixed number of dimensions. If the right hand side is inferred,           the ellipsis dimensions are at the beginning of the output.</p>
</td></tr>
<tr><td><code id="torch_einsum_+3A_tensors">tensors</code></td>
<td>
<p>(Tensor) The operands to compute the Einstein sum of.</p>
</td></tr>
<tr><td><code id="torch_einsum_+3A_path">path</code></td>
<td>
<p>(int) This function uses <a href="https://optimized-einsum.readthedocs.io/en/stable/">opt_einsum</a> to
speed up computation or to consume less memory by optimizing contraction order. This optimization
occurs when there are at least three inputs, since the order does not matter otherwise.
Note that finding <em>the</em> optimal path is an NP-hard problem, thus, <code>opt_einsum</code> relies
on different heuristics to achieve near-optimal results. If <code>opt_einsum</code> is not available,
the default order is to contract from left to right.
The path argument is used to changed that default, but it should only be set by advanced users.</p>
</td></tr>
</table>


<h3>einsum(equation, *operands) -&gt; Tensor </h3>

<p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(5))
y = torch_randn(c(4))
torch_einsum('i,j-&gt;ij', list(x, y))  # outer product
A = torch_randn(c(3,5,4))
l = torch_randn(c(2,5))
r = torch_randn(c(2,4))
torch_einsum('bn,anm,bm-&gt;ba', list(l, A, r)) # compare torch_nn$functional$bilinear
As = torch_randn(c(3,2,5))
Bs = torch_randn(c(3,5,4))
torch_einsum('bij,bjk-&gt;bik', list(As, Bs)) # batch matrix multiplication
A = torch_randn(c(3, 3))
torch_einsum('ii-&gt;i', list(A)) # diagonal
A = torch_randn(c(4, 3, 3))
torch_einsum('...ii-&gt;...i', list(A)) # batch diagonal
A = torch_randn(c(2, 3, 4, 5))
torch_einsum('...ij-&gt;...ji', list(A))$shape # batch permute

}
</code></pre>

<hr>
<h2 id='torch_empty'>Empty</h2><span id='topic+torch_empty'></span>

<h3>Description</h3>

<p>Empty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_empty(
  ...,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_empty_+3A_...">...</code></td>
<td>
<p>a sequence of integers defining the shape of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_empty_+3A_names">names</code></td>
<td>
<p>optional character vector naming each dimension.</p>
</td></tr>
<tr><td><code id="torch_empty_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_empty_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_empty_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>empty(*size, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False, pin_memory=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with uninitialized data. The shape of the tensor is
defined by the variable argument <code>size</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_empty(c(2, 3))
}
</code></pre>

<hr>
<h2 id='torch_empty_like'>Empty_like</h2><span id='topic+torch_empty_like'></span>

<h3>Description</h3>

<p>Empty_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_empty_like(
  input,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_empty_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_empty_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>empty_like(input, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns an uninitialized tensor with the same size as <code>input</code>.
<code>torch_empty_like(input)</code> is equivalent to
<code>torch_empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_empty(list(2,3), dtype = torch_int64())
}
</code></pre>

<hr>
<h2 id='torch_empty_strided'>Empty_strided</h2><span id='topic+torch_empty_strided'></span>

<h3>Description</h3>

<p>Empty_strided
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_empty_strided(
  size,
  stride,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  pin_memory = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_empty_strided_+3A_size">size</code></td>
<td>
<p>(tuple of ints) the shape of the output tensor</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_stride">stride</code></td>
<td>
<p>(tuple of ints) the strides of the output tensor</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_empty_strided_+3A_pin_memory">pin_memory</code></td>
<td>
<p>(bool, optional) If set, returned tensor would be allocated in        the pinned memory. Works only for CPU tensors. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>empty_strided(size, stride, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, pin_memory=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with uninitialized data. The shape and strides of the tensor is
defined by the variable argument <code>size</code> and <code>stride</code> respectively.
<code>torch_empty_strided(size, stride)</code> is equivalent to
<code style="white-space: pre;">&#8288;torch_empty(size).as_strided(size, stride)&#8288;</code>.
</p>


<h3>Warning</h3>

<p>More than one element of the created tensor may refer to a single memory
location. As a result, in-place operations (especially ones that are
vectorized) may result in incorrect behavior. If you need to write to
the tensors, please clone them first.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_empty_strided(list(2, 3), list(1, 2))
a
a$stride(1)
a$size(1)
}
</code></pre>

<hr>
<h2 id='torch_eq'>Eq</h2><span id='topic+torch_eq'></span>

<h3>Description</h3>

<p>Eq
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_eq(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_eq_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_eq_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare
Must be a <code>ByteTensor</code></p>
</td></tr>
</table>


<h3>eq(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes element-wise equality
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_eq(torch_tensor(c(1,2,3,4)), torch_tensor(c(1, 3, 2, 4)))
}
</code></pre>

<hr>
<h2 id='torch_equal'>Equal</h2><span id='topic+torch_equal'></span>

<h3>Description</h3>

<p>Equal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_equal(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_equal_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_equal_+3A_other">other</code></td>
<td>
<p>the other input tensor</p>
</td></tr>
</table>


<h3>equal(input, other) -&gt; bool </h3>

<p><code>TRUE</code> if two tensors have the same size and elements, <code>FALSE</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_equal(torch_tensor(c(1, 2)), torch_tensor(c(1, 2)))
}
</code></pre>

<hr>
<h2 id='torch_erf'>Erf</h2><span id='topic+torch_erf'></span>

<h3>Description</h3>

<p>Erf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_erf(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_erf_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>erf(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the error function of each element. The error function is defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_erf(torch_tensor(c(0, -1., 10.)))
}
</code></pre>

<hr>
<h2 id='torch_erfc'>Erfc</h2><span id='topic+torch_erfc'></span>

<h3>Description</h3>

<p>Erfc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_erfc(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_erfc_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>erfc(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the complementary error function of each element of <code>input</code>.
The complementary error function is defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_erfc(torch_tensor(c(0, -1., 10.)))
}
</code></pre>

<hr>
<h2 id='torch_erfinv'>Erfinv</h2><span id='topic+torch_erfinv'></span>

<h3>Description</h3>

<p>Erfinv
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_erfinv(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_erfinv_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>erfinv(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the inverse error function of each element of <code>input</code>.
The inverse error function is defined in the range <code class="reqn">(-1, 1)</code> as:
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{erfinv}(\mathrm{erf}(x)) = x
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_erfinv(torch_tensor(c(0, 0.5, -1.)))
}
</code></pre>

<hr>
<h2 id='torch_exp'>Exp</h2><span id='topic+torch_exp'></span>

<h3>Description</h3>

<p>Exp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_exp(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_exp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>exp(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the exponential of the elements
of the input tensor <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = e^{x_{i}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_exp(torch_tensor(c(0, log(2))))
}
</code></pre>

<hr>
<h2 id='torch_exp2'>Exp2</h2><span id='topic+torch_exp2'></span>

<h3>Description</h3>

<p>Exp2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_exp2(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_exp2_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>exp2(input, *, out=None) -&gt; Tensor </h3>

<p>Computes the base two exponential function of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = 2^{x_{i}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_exp2(torch_tensor(c(0, log2(2.), 3, 4)))
}
</code></pre>

<hr>
<h2 id='torch_expm1'>Expm1</h2><span id='topic+torch_expm1'></span>

<h3>Description</h3>

<p>Expm1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_expm1(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_expm1_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>expm1(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the exponential of the elements minus 1
of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = e^{x_{i}} - 1
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_expm1(torch_tensor(c(0, log(2))))
}
</code></pre>

<hr>
<h2 id='torch_eye'>Eye</h2><span id='topic+torch_eye'></span>

<h3>Description</h3>

<p>Eye
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_eye(
  n,
  m = n,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_eye_+3A_n">n</code></td>
<td>
<p>(int) the number of rows</p>
</td></tr>
<tr><td><code id="torch_eye_+3A_m">m</code></td>
<td>
<p>(int, optional) the number of columns with default being <code>n</code></p>
</td></tr>
<tr><td><code id="torch_eye_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_eye_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_eye_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_eye_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>eye(n, m=NULL, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_eye(3)
}
</code></pre>

<hr>
<h2 id='torch_fft_fft'>Fft</h2><span id='topic+torch_fft_fft'></span>

<h3>Description</h3>

<p>Computes the one dimensional discrete Fourier transform of input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fft_fft(self, n = NULL, dim = -1L, norm = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fft_fft_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_fft_fft_+3A_n">n</code></td>
<td>
<p>(int) Signal length. If given, the input will either be zero-padded
or trimmed to this length before computing the FFT.</p>
</td></tr>
<tr><td><code id="torch_fft_fft_+3A_dim">dim</code></td>
<td>
<p>(int, optional) The dimension along which to take the one dimensional FFT.</p>
</td></tr>
<tr><td><code id="torch_fft_fft_+3A_norm">norm</code></td>
<td>
<p>(str, optional) Normalization mode. For the forward transform, these
correspond to:
</p>

<ul>
<li><p> &quot;forward&quot; - normalize by 1/n
</p>
</li>
<li><p> &quot;backward&quot; - no normalization
</p>
</li>
<li><p> &quot;ortho&quot; - normalize by 1/sqrt(n) (making the FFT orthonormal)
Calling the backward transform (ifft()) with the same normalization mode will
apply an overall normalization of 1/n between the two transforms. This is
required to make IFFT the exact inverse.
Default is &quot;backward&quot; (no normalization).
</p>
</li></ul>
</td></tr>
</table>


<h3>Note</h3>

<p>The Fourier domain representation of any real signal satisfies the Hermitian
property: <code style="white-space: pre;">&#8288;X[i] = conj(X[-i]).&#8288;</code> This function always returns both the positive
and negative frequency terms even though, for real inputs, the negative
frequencies are redundant. rfft() returns the more compact one-sided representation
where only the positive frequencies are returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
t &lt;- torch_arange(start = 0, end = 3)
t
torch_fft_fft(t, norm = "backward")

}
</code></pre>

<hr>
<h2 id='torch_fft_fftfreq'>fftfreq</h2><span id='topic+torch_fft_fftfreq'></span>

<h3>Description</h3>

<p>Computes the discrete Fourier Transform sample frequencies for a signal of size <code>n</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fft_fftfreq(
  n,
  d = 1,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fft_fftfreq_+3A_n">n</code></td>
<td>
<p>(integer)  the FFT length</p>
</td></tr>
<tr><td><code id="torch_fft_fftfreq_+3A_d">d</code></td>
<td>
<p>(float, optional)  the sampling length scale. The spacing between individual
samples of the FFT input. The default assumes unit spacing, dividing that result by the
actual spacing gives the result in physical frequency units.</p>
</td></tr>
<tr><td><code id="torch_fft_fftfreq_+3A_dtype">dtype</code></td>
<td>
<p>(default: <code><a href="#topic+torch_get_default_dtype">torch_get_default_dtype()</a></code>) the desired data type of returned tensor.</p>
</td></tr>
<tr><td><code id="torch_fft_fftfreq_+3A_layout">layout</code></td>
<td>
<p>(default: <code><a href="#topic+torch_strided">torch_strided()</a></code>) the desired layout of returned tensor.</p>
</td></tr>
<tr><td><code id="torch_fft_fftfreq_+3A_device">device</code></td>
<td>
<p>(default: <code>NULL</code>) the desired device of returned tensor.  Default:
If <code>NULL</code>, uses the current device for the default tensor type.</p>
</td></tr>
<tr><td><code id="torch_fft_fftfreq_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(default: <code>FALSE</code>)  If autograd should record operations on the returned tensor.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>By convention, <code><a href="#topic+torch_fft_fft">torch_fft_fft()</a></code> returns positive frequency terms first, followed by the negative
frequencies in reverse order, so that <code>f[-i]</code> for all <code style="white-space: pre;">&#8288;0 &lt; i &lt;= n/2&#8288;</code>
gives the negative frequency terms. For an FFT of length <code>n</code> and with inputs spaced
in length unit <code>d</code>, the frequencies are:
<code style="white-space: pre;">&#8288;f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)&#8288;</code>
</p>
<p>For even lengths, the Nyquist frequency at <code>f[n/2]</code> can be thought of as either negative
or positive. <code>fftfreq()</code> follows NumPys convention of taking it to be negative.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
torch_fft_fftfreq(5) # Nyquist frequency at f[3] is positive
torch_fft_fftfreq(4) # Nyquist frequency at f[3] is given as negative

}
</code></pre>

<hr>
<h2 id='torch_fft_ifft'>Ifft</h2><span id='topic+torch_fft_ifft'></span>

<h3>Description</h3>

<p>Computes the one dimensional inverse discrete Fourier transform of input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fft_ifft(self, n = NULL, dim = -1L, norm = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fft_ifft_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_fft_ifft_+3A_n">n</code></td>
<td>
<p>(int, optional)  Signal length. If given, the input will either be
zero-padded or trimmed to this length before computing the IFFT.</p>
</td></tr>
<tr><td><code id="torch_fft_ifft_+3A_dim">dim</code></td>
<td>
<p>(int, optional)  The dimension along which to take the one
dimensional IFFT.</p>
</td></tr>
<tr><td><code id="torch_fft_ifft_+3A_norm">norm</code></td>
<td>
<p>(str, optional)  Normalization mode. For the backward transform,
these correspond to:
</p>

<ul>
<li><p> &quot;forward&quot; - no normalization
</p>
</li>
<li><p> &quot;backward&quot; - normalize by 1/n
</p>
</li>
<li><p> &quot;ortho&quot; - normalize by 1/sqrt(n) (making the IFFT orthonormal)
Calling the forward transform with the same normalization mode will apply an
overall normalization of 1/n between the two transforms. This is required to
make ifft() the exact inverse.
Default is &quot;backward&quot; (normalize by 1/n).
</p>
</li></ul>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
t &lt;- torch_arange(start = 0, end = 3)
t
x &lt;- torch_fft_fft(t, norm = "backward")
torch_fft_ifft(x)


}
</code></pre>

<hr>
<h2 id='torch_fft_irfft'>Irfft</h2><span id='topic+torch_fft_irfft'></span>

<h3>Description</h3>

<p>Computes the inverse of <code><a href="#topic+torch_fft_rfft">torch_fft_rfft()</a></code>.
Input is interpreted as a one-sided Hermitian signal in the Fourier domain,
as produced by <code><a href="#topic+torch_fft_rfft">torch_fft_rfft()</a></code>. By the Hermitian property, the output will
be real-valued.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fft_irfft(self, n = NULL, dim = -1L, norm = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fft_irfft_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor representing a half-Hermitian signal</p>
</td></tr>
<tr><td><code id="torch_fft_irfft_+3A_n">n</code></td>
<td>
<p>(int) Output signal length. This determines the length of the output
signal. If given, the input will either be zero-padded or trimmed to this
length before computing the real IFFT. Defaults to even output: <code>n=2*(input.size(dim) - 1)</code>.</p>
</td></tr>
<tr><td><code id="torch_fft_irfft_+3A_dim">dim</code></td>
<td>
<p>(int, optional)  The dimension along which to take the one
dimensional real IFFT.</p>
</td></tr>
<tr><td><code id="torch_fft_irfft_+3A_norm">norm</code></td>
<td>
<p>(str, optional)  Normalization mode. For the backward transform,
these correspond to:
</p>

<ul>
<li><p> &quot;forward&quot; - no normalization
</p>
</li>
<li><p> &quot;backward&quot; - normalize by 1/n
</p>
</li>
<li><p> &quot;ortho&quot; - normalize by 1/sqrt(n) (making the real IFFT orthonormal)
Calling the forward transform (<code><a href="#topic+torch_fft_rfft">torch_fft_rfft()</a></code>) with the same normalization
mode will apply an overall normalization of 1/n between the two transforms.
This is required to make irfft() the exact inverse.
Default is &quot;backward&quot; (normalize by 1/n).
</p>
</li></ul>
</td></tr>
</table>


<h3>Note</h3>

<p>Some input frequencies must be real-valued to satisfy the Hermitian property.
In these cases the imaginary component will be ignored. For example, any
imaginary component in the zero-frequency term cannot be represented in a real
output and so will always be ignored.
</p>
<p>The correct interpretation of the Hermitian input depends on the length of the
original data, as given by n. This is because each input shape could correspond
to either an odd or even length signal. By default, the signal is assumed to be
even length and odd signals will not round-trip properly. So, it is recommended
to always pass the signal length n.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
t &lt;- torch_arange(start = 0, end = 4)
x &lt;- torch_fft_rfft(t)
torch_fft_irfft(x)
torch_fft_irfft(x, n = t$numel())

}
</code></pre>

<hr>
<h2 id='torch_fft_rfft'>Rfft</h2><span id='topic+torch_fft_rfft'></span>

<h3>Description</h3>

<p>Computes the one dimensional Fourier transform of real-valued input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fft_rfft(self, n = NULL, dim = -1L, norm = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fft_rfft_+3A_self">self</code></td>
<td>
<p>(Tensor)  the real input tensor</p>
</td></tr>
<tr><td><code id="torch_fft_rfft_+3A_n">n</code></td>
<td>
<p>(int) Signal length. If given, the input will either be zero-padded
or trimmed to this length before computing the real FFT.</p>
</td></tr>
<tr><td><code id="torch_fft_rfft_+3A_dim">dim</code></td>
<td>
<p>(int, optional)  The dimension along which to take the one
dimensional real FFT.</p>
</td></tr>
<tr><td><code id="torch_fft_rfft_+3A_norm">norm</code></td>
<td>
<p>norm (str, optional)  Normalization mode. For the forward
transform, these correspond to:
</p>

<ul>
<li><p> &quot;forward&quot; - normalize by 1/n
</p>
</li>
<li><p> &quot;backward&quot; - no normalization
</p>
</li>
<li><p> &quot;ortho&quot; - normalize by 1/sqrt(n) (making the FFT orthonormal)
Calling the backward transform (<code><a href="#topic+torch_fft_irfft">torch_fft_irfft()</a></code>) with the same
normalization mode will apply an overall normalization of 1/n between the
two transforms. This is required to make irfft() the exact inverse.
Default is &quot;backward&quot; (no normalization).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The FFT of a real signal is Hermitian-symmetric, <code>X[i] = conj(X[-i])</code> so the
output contains only the positive frequencies below the Nyquist frequency.
To compute the full output, use <code><a href="#topic+torch_fft_fft">torch_fft_fft()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
t &lt;- torch_arange(start = 0, end = 3)
torch_fft_rfft(t)

}
</code></pre>

<hr>
<h2 id='torch_finfo'>Floating point type info</h2><span id='topic+torch_finfo'></span>

<h3>Description</h3>

<p>A list that represents the numerical properties of a
floating point torch.dtype
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_finfo(dtype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_finfo_+3A_dtype">dtype</code></td>
<td>
<p>dtype to check information</p>
</td></tr>
</table>

<hr>
<h2 id='torch_fix'>Fix</h2><span id='topic+torch_fix'></span>

<h3>Description</h3>

<p>Fix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fix(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fix_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>fix(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_trunc">torch_trunc()</a></code>
</p>

<hr>
<h2 id='torch_flatten'>Flatten</h2><span id='topic+torch_flatten'></span>

<h3>Description</h3>

<p>Flatten
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_flatten(self, dims, start_dim = 1L, end_dim = -1L, out_dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_flatten_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_flatten_+3A_dims">dims</code></td>
<td>
<p>if tensor is named you can pass the name of the dimensions to
flatten</p>
</td></tr>
<tr><td><code id="torch_flatten_+3A_start_dim">start_dim</code></td>
<td>
<p>(int) the first dim to flatten</p>
</td></tr>
<tr><td><code id="torch_flatten_+3A_end_dim">end_dim</code></td>
<td>
<p>(int) the last dim to flatten</p>
</td></tr>
<tr><td><code id="torch_flatten_+3A_out_dim">out_dim</code></td>
<td>
<p>the name of the resulting dimension if a named tensor.</p>
</td></tr>
</table>


<h3>flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor </h3>

<p>Flattens a contiguous range of dims in a tensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t = torch_tensor(matrix(c(1, 2), ncol = 2))
torch_flatten(t)
torch_flatten(t, start_dim=2)
}
</code></pre>

<hr>
<h2 id='torch_flip'>Flip</h2><span id='topic+torch_flip'></span>

<h3>Description</h3>

<p>Flip
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_flip(self, dims)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_flip_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_flip_+3A_dims">dims</code></td>
<td>
<p>(a list or tuple) axis to flip on</p>
</td></tr>
</table>


<h3>flip(input, dims) -&gt; Tensor </h3>

<p>Reverse the order of a n-D tensor along given axis in dims.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(1, 8)$view(c(2, 2, 2))
x
torch_flip(x, c(1, 2))
}
</code></pre>

<hr>
<h2 id='torch_fliplr'>Fliplr</h2><span id='topic+torch_fliplr'></span>

<h3>Description</h3>

<p>Fliplr
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fliplr(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fliplr_+3A_self">self</code></td>
<td>
<p>(Tensor) Must be at least 2-dimensional.</p>
</td></tr>
</table>


<h3>fliplr(input) -&gt; Tensor </h3>

<p>Flip array in the left/right direction, returning a new tensor.
</p>
<p>Flip the entries in each row in the left/right direction.
Columns are preserved, but appear in a different order than before.
</p>


<h3>Note</h3>

<p>Equivalent to <code>input[,-1]</code>. Requires the array to be at least 2-D.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(start = 1, end = 4)$view(c(2, 2))
x
torch_fliplr(x)
}
</code></pre>

<hr>
<h2 id='torch_flipud'>Flipud</h2><span id='topic+torch_flipud'></span>

<h3>Description</h3>

<p>Flipud
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_flipud(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_flipud_+3A_self">self</code></td>
<td>
<p>(Tensor) Must be at least 1-dimensional.</p>
</td></tr>
</table>


<h3>flipud(input) -&gt; Tensor </h3>

<p>Flip array in the up/down direction, returning a new tensor.
</p>
<p>Flip the entries in each column in the up/down direction.
Rows are preserved, but appear in a different order than before.
</p>


<h3>Note</h3>

<p>Equivalent to <code>input[-1,]</code>. Requires the array to be at least 1-D.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(start = 1, end = 4)$view(c(2, 2))
x
torch_flipud(x)
}
</code></pre>

<hr>
<h2 id='torch_floor'>Floor</h2><span id='topic+torch_floor'></span>

<h3>Description</h3>

<p>Floor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_floor(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_floor_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>floor(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the floor of the elements of <code>input</code>,
the largest integer less than or equal to each element.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \left\lfloor \mbox{input}_{i} \right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_floor(a)
}
</code></pre>

<hr>
<h2 id='torch_floor_divide'>Floor_divide</h2><span id='topic+torch_floor_divide'></span>

<h3>Description</h3>

<p>Floor_divide
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_floor_divide(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_floor_divide_+3A_self">self</code></td>
<td>
<p>(Tensor) the numerator tensor</p>
</td></tr>
<tr><td><code id="torch_floor_divide_+3A_other">other</code></td>
<td>
<p>(Tensor or Scalar) the denominator</p>
</td></tr>
</table>


<h3>floor_divide(input, other, out=NULL) -&gt; Tensor </h3>

<p>Return the division of the inputs rounded down to the nearest integer. See <code><a href="#topic+torch_div">torch_div</a></code>
for type promotion and broadcasting rules.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{{out}}_i = \left\lfloor \frac{{\mbox{{input}}_i}}{{\mbox{{other}}_i}} \right\rfloor
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_tensor(c(4.0, 3.0))
b = torch_tensor(c(2.0, 2.0))
torch_floor_divide(a, b)
torch_floor_divide(a, 1.4)
}
</code></pre>

<hr>
<h2 id='torch_fmod'>Fmod</h2><span id='topic+torch_fmod'></span>

<h3>Description</h3>

<p>Fmod
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_fmod(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_fmod_+3A_self">self</code></td>
<td>
<p>(Tensor) the dividend</p>
</td></tr>
<tr><td><code id="torch_fmod_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the divisor, which may be either a number or a tensor of the same shape as the dividend</p>
</td></tr>
</table>


<h3>fmod(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise remainder of division.
</p>
<p>The dividend and divisor may contain both for integer and floating point
numbers. The remainder has the same sign as the dividend <code>input</code>.
</p>
<p>When <code>other</code> is a tensor, the shapes of <code>input</code> and
<code>other</code> must be broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_fmod(torch_tensor(c(-3., -2, -1, 1, 2, 3)), 2)
torch_fmod(torch_tensor(c(1., 2, 3, 4, 5)), 1.5)
}
</code></pre>

<hr>
<h2 id='torch_frac'>Frac</h2><span id='topic+torch_frac'></span>

<h3>Description</h3>

<p>Frac
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_frac(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_frac_+3A_self">self</code></td>
<td>
<p>the input tensor.</p>
</td></tr>
</table>


<h3>frac(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the fractional portion of each element in <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \mbox{input}_{i} - \left\lfloor |\mbox{input}_{i}| \right\rfloor * \mbox{sgn}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_frac(torch_tensor(c(1, 2.5, -3.2)))
}
</code></pre>

<hr>
<h2 id='torch_full'>Full</h2><span id='topic+torch_full'></span>

<h3>Description</h3>

<p>Full
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_full(
  size,
  fill_value,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_full_+3A_size">size</code></td>
<td>
<p>(int...) a list, tuple, or <code>torch_Size</code> of integers defining the        shape of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_full_+3A_fill_value">fill_value</code></td>
<td>
<p>NA the number to fill the output tensor with.</p>
</td></tr>
<tr><td><code id="torch_full_+3A_names">names</code></td>
<td>
<p>optional names of the dimensions</p>
</td></tr>
<tr><td><code id="torch_full_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_full_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_full_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_full_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>full(size, fill_value, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a tensor of size <code>size</code> filled with <code>fill_value</code>.
</p>


<h3>Warning</h3>

<p>In PyTorch 1.5 a bool or integral <code>fill_value</code> will produce a warning if
<code>dtype</code> or <code>out</code> are not set.
In a future PyTorch release, when <code>dtype</code> and <code>out</code> are not set
a bool <code>fill_value</code> will return a tensor of torch.bool dtype,
and an integral <code>fill_value</code> will return a tensor of torch.long dtype.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_full(list(2, 3), 3.141592)
}
</code></pre>

<hr>
<h2 id='torch_full_like'>Full_like</h2><span id='topic+torch_full_like'></span>

<h3>Description</h3>

<p>Full_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_full_like(
  input,
  fill_value,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_full_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_fill_value">fill_value</code></td>
<td>
<p>the number to fill the output tensor with.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_full_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>full_like(input, fill_value, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False, </h3>

<p>memory_format=torch.preserve_format) -&gt; Tensor
</p>
<p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>.
<code>torch_full_like(input, fill_value)</code> is equivalent to
<code>torch_full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>

<hr>
<h2 id='torch_gather'>Gather</h2><span id='topic+torch_gather'></span>

<h3>Description</h3>

<p>Gather
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_gather(self, dim, index, sparse_grad = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_gather_+3A_self">self</code></td>
<td>
<p>(Tensor) the source tensor</p>
</td></tr>
<tr><td><code id="torch_gather_+3A_dim">dim</code></td>
<td>
<p>(int) the axis along which to index</p>
</td></tr>
<tr><td><code id="torch_gather_+3A_index">index</code></td>
<td>
<p>(LongTensor) the indices of elements to gather</p>
</td></tr>
<tr><td><code id="torch_gather_+3A_sparse_grad">sparse_grad</code></td>
<td>
<p>(bool,optional) If <code>TRUE</code>, gradient w.r.t. <code>input</code> will be a sparse tensor.</p>
</td></tr>
</table>


<h3>gather(input, dim, index, sparse_grad=FALSE) -&gt; Tensor </h3>

<p>Gathers values along an axis specified by <code>dim</code>.
</p>
<p>For a 3-D tensor the output is specified by::
</p>
<div class="sourceCode"><pre>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
</pre></div>
<p>If <code>input</code> is an n-dimensional tensor with size
<code class="reqn">(x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})</code>
and <code>dim = i</code>, then <code>index</code> must be an <code class="reqn">n</code>-dimensional tensor with
size <code class="reqn">(x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})</code> where <code class="reqn">y \geq 1</code>
and <code>out</code> will have the same size as <code>index</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t = torch_tensor(matrix(c(1,2,3,4), ncol = 2, byrow = TRUE))
torch_gather(t, 2, torch_tensor(matrix(c(1,1,2,1), ncol = 2, byrow=TRUE), dtype = torch_int64()))
}
</code></pre>

<hr>
<h2 id='torch_gcd'>Gcd</h2><span id='topic+torch_gcd'></span>

<h3>Description</h3>

<p>Gcd
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_gcd(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_gcd_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_gcd_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>gcd(input, other, *, out=None) -&gt; Tensor </h3>

<p>Computes the element-wise greatest common divisor (GCD) of <code>input</code> and <code>other</code>.
</p>
<p>Both <code>input</code> and <code>other</code> must have integer types.
</p>


<h3>Note</h3>

<p>This defines <code class="reqn">gcd(0, 0) = 0</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

if (torch::cuda_is_available()) {
a &lt;- torch_tensor(c(5, 10, 15), dtype = torch_long(), device = "cuda")
b &lt;- torch_tensor(c(3, 4, 5), dtype = torch_long(), device = "cuda")
torch_gcd(a, b)
c &lt;- torch_tensor(c(3L), device = "cuda")
torch_gcd(a, c)
}
}
</code></pre>

<hr>
<h2 id='torch_ge'>Ge</h2><span id='topic+torch_ge'></span>

<h3>Description</h3>

<p>Ge
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ge(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ge_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_ge_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>ge(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes <code class="reqn">\mbox{input} \geq \mbox{other}</code> element-wise.
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_ge(torch_tensor(matrix(1:4, ncol = 2, byrow=TRUE)), 
         torch_tensor(matrix(c(1,1,4,4), ncol = 2, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_generator'>Create a Generator object</h2><span id='topic+torch_generator'></span>

<h3>Description</h3>

<p>A <code>torch_generator</code>  is an object which manages the state of the algorithm
that produces pseudo random numbers. Used as a keyword argument in many
In-place random sampling functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_generator()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# Via string
generator &lt;- torch_generator()
generator$current_seed()
generator$set_current_seed(1234567L)
generator$current_seed()

}
</code></pre>

<hr>
<h2 id='torch_geqrf'>Geqrf</h2><span id='topic+torch_geqrf'></span>

<h3>Description</h3>

<p>Geqrf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_geqrf(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_geqrf_+3A_self">self</code></td>
<td>
<p>(Tensor) the input matrix</p>
</td></tr>
</table>


<h3>geqrf(input, out=NULL) -&gt; (Tensor, Tensor) </h3>

<p>This is a low-level function for calling LAPACK directly. This function
returns a namedtuple (a, tau) as defined in <code style="white-space: pre;">&#8288;LAPACK documentation for geqrf&#8288;</code>_ .
</p>
<p>You'll generally want to use <code><a href="#topic+torch_qr">torch_qr</a></code> instead.
</p>
<p>Computes a QR decomposition of <code>input</code>, but without constructing
<code class="reqn">Q</code> and <code class="reqn">R</code> as explicit separate matrices.
</p>
<p>Rather, this directly calls the underlying LAPACK function <code>?geqrf</code>
which produces a sequence of 'elementary reflectors'.
</p>
<p>See <code style="white-space: pre;">&#8288;LAPACK documentation for geqrf&#8288;</code>_ for further details.
</p>

<hr>
<h2 id='torch_ger'>Ger</h2><span id='topic+torch_ger'></span>

<h3>Description</h3>

<p>Ger
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ger(self, vec2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ger_+3A_self">self</code></td>
<td>
<p>(Tensor) 1-D input vector</p>
</td></tr>
<tr><td><code id="torch_ger_+3A_vec2">vec2</code></td>
<td>
<p>(Tensor) 1-D input vector</p>
</td></tr>
</table>


<h3>ger(input, vec2, out=NULL) -&gt; Tensor </h3>

<p>Outer product of <code>input</code> and <code>vec2</code>.
If <code>input</code> is a vector of size <code class="reqn">n</code> and <code>vec2</code> is a vector of
size <code class="reqn">m</code>, then <code>out</code> must be a matrix of size <code class="reqn">(n \times m)</code>.
</p>


<h3>Note</h3>

<p>This function does not broadcast .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

v1 = torch_arange(1., 5.)
v2 = torch_arange(1., 4.)
torch_ger(v1, v2)
}
</code></pre>

<hr>
<h2 id='torch_get_rng_state'>RNG state management</h2><span id='topic+torch_get_rng_state'></span><span id='topic+torch_set_rng_state'></span><span id='topic+cuda_get_rng_state'></span><span id='topic+cuda_set_rng_state'></span>

<h3>Description</h3>

<p>Low level functionality to set and change the RNG state.
It's recommended to use <code><a href="#topic+torch_manual_seed">torch_manual_seed()</a></code> for most cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_get_rng_state()

torch_set_rng_state(state)

cuda_get_rng_state(device = NULL)

cuda_set_rng_state(state, device = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_get_rng_state_+3A_state">state</code></td>
<td>
<p>A tensor with the current state or a list containing the state
for each device - (for CUDA).</p>
</td></tr>
<tr><td><code id="torch_get_rng_state_+3A_device">device</code></td>
<td>
<p>The cuda device index to get or set the state. If <code>NULL</code> gets the state
for all available devices.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>torch_set_rng_state()</code>: Sets the RNG state for the CPU
</p>
</li>
<li> <p><code>cuda_get_rng_state()</code>: Gets the RNG state for CUDA.
</p>
</li>
<li> <p><code>cuda_set_rng_state()</code>: Sets the RNG state for CUDA.
</p>
</li></ul>

<hr>
<h2 id='torch_greater'>Greater</h2><span id='topic+torch_greater'></span>

<h3>Description</h3>

<p>Greater
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_greater(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_greater_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_greater_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>greater(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_gt">torch_gt()</a></code>.
</p>

<hr>
<h2 id='torch_greater_equal'>Greater_equal</h2><span id='topic+torch_greater_equal'></span>

<h3>Description</h3>

<p>Greater_equal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_greater_equal(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_greater_equal_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_greater_equal_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>greater_equal(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_ge">torch_ge()</a></code>.
</p>

<hr>
<h2 id='torch_gt'>Gt</h2><span id='topic+torch_gt'></span>

<h3>Description</h3>

<p>Gt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_gt(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_gt_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_gt_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>gt(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes <code class="reqn">\mbox{input} &gt; \mbox{other}</code> element-wise.
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_gt(torch_tensor(matrix(1:4, ncol = 2, byrow=TRUE)), 
         torch_tensor(matrix(c(1,1,4,4), ncol = 2, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_hamming_window'>Hamming_window</h2><span id='topic+torch_hamming_window'></span>

<h3>Description</h3>

<p>Hamming_window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_hamming_window(
  window_length,
  periodic = TRUE,
  alpha = 0.54,
  beta = 0.46,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_hamming_window_+3A_window_length">window_length</code></td>
<td>
<p>(int) the size of returned window</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_periodic">periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a window to be used as periodic        function. If False, return a symmetric window.</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_alpha">alpha</code></td>
<td>
<p>(float, optional) The coefficient <code class="reqn">\alpha</code> in the equation above</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_beta">beta</code></td>
<td>
<p>(float, optional) The coefficient <code class="reqn">\beta</code> in the equation above</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). Only floating point types are supported.</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned window tensor. Only          <code>torch_strided</code> (dense layout) is supported.</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_hamming_window_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>hamming_window(window_length, periodic=TRUE, alpha=0.54, beta=0.46, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Hamming window function.
</p>
<p style="text-align: center;"><code class="reqn">
    w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),
</code>
</p>

<p>where <code class="reqn">N</code> is the full window size.
</p>
<p>The input <code>window_length</code> is a positive integer controlling the
returned window size. <code>periodic</code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<code>torch_stft</code>. Therefore, if <code>periodic</code> is true, the <code class="reqn">N</code> in
above formula is in fact <code class="reqn">\mbox{window\_length} + 1</code>. Also, we always have
<code>torch_hamming_window(L, periodic=TRUE)</code> equal to
<code style="white-space: pre;">&#8288;torch_hamming_window(L + 1, periodic=False)[:-1])&#8288;</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `window_length` \eqn{=1}, the returned window contains a single value 1.
</pre></div>
<div class="sourceCode"><pre>This is a generalized version of `torch_hann_window`.
</pre></div>

<hr>
<h2 id='torch_hann_window'>Hann_window</h2><span id='topic+torch_hann_window'></span>

<h3>Description</h3>

<p>Hann_window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_hann_window(
  window_length,
  periodic = TRUE,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_hann_window_+3A_window_length">window_length</code></td>
<td>
<p>(int) the size of returned window</p>
</td></tr>
<tr><td><code id="torch_hann_window_+3A_periodic">periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a window to be used as periodic        function. If False, return a symmetric window.</p>
</td></tr>
<tr><td><code id="torch_hann_window_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). Only floating point types are supported.</p>
</td></tr>
<tr><td><code id="torch_hann_window_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned window tensor. Only          <code>torch_strided</code> (dense layout) is supported.</p>
</td></tr>
<tr><td><code id="torch_hann_window_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_hann_window_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>hann_window(window_length, periodic=TRUE, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Hann window function.
</p>
<p style="text-align: center;"><code class="reqn">
    w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] =
            \sin^2 \left( \frac{\pi n}{N - 1} \right),
</code>
</p>

<p>where <code class="reqn">N</code> is the full window size.
</p>
<p>The input <code>window_length</code> is a positive integer controlling the
returned window size. <code>periodic</code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<code>torch_stft</code>. Therefore, if <code>periodic</code> is true, the <code class="reqn">N</code> in
above formula is in fact <code class="reqn">\mbox{window\_length} + 1</code>. Also, we always have
<code>torch_hann_window(L, periodic=TRUE)</code> equal to
<code style="white-space: pre;">&#8288;torch_hann_window(L + 1, periodic=False)[:-1])&#8288;</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `window_length` \eqn{=1}, the returned window contains a single value 1.
</pre></div>

<hr>
<h2 id='torch_heaviside'>Heaviside</h2><span id='topic+torch_heaviside'></span>

<h3>Description</h3>

<p>Heaviside
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_heaviside(self, values)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_heaviside_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_heaviside_+3A_values">values</code></td>
<td>
<p>(Tensor) The values to use where <code>input</code> is zero.</p>
</td></tr>
</table>


<h3>heaviside(input, values, *, out=None) -&gt; Tensor </h3>

<p>Computes the Heaviside step function for each element in <code>input</code>.
The Heaviside step function is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{{heaviside}}(input, values) = \begin{array}{ll}
 0, &amp; \mbox{if input &lt; 0}\\
 values, &amp; \mbox{if input == 0}\\
 1, &amp; \mbox{if input &gt; 0}
 \end{array}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input &lt;- torch_tensor(c(-1.5, 0, 2.0))
values &lt;- torch_tensor(c(0.5))
torch_heaviside(input, values)
values &lt;- torch_tensor(c(1.2, -2.0, 3.5))
torch_heaviside(input, values)
}
</code></pre>

<hr>
<h2 id='torch_histc'>Histc</h2><span id='topic+torch_histc'></span>

<h3>Description</h3>

<p>Histc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_histc(self, bins = 100L, min = 0L, max = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_histc_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_histc_+3A_bins">bins</code></td>
<td>
<p>(int) number of histogram bins</p>
</td></tr>
<tr><td><code id="torch_histc_+3A_min">min</code></td>
<td>
<p>(int) lower end of the range (inclusive)</p>
</td></tr>
<tr><td><code id="torch_histc_+3A_max">max</code></td>
<td>
<p>(int) upper end of the range (inclusive)</p>
</td></tr>
</table>


<h3>histc(input, bins=100, min=0, max=0, out=NULL) -&gt; Tensor </h3>

<p>Computes the histogram of a tensor.
</p>
<p>The elements are sorted into equal width bins between <code>min</code> and
<code>max</code>. If <code>min</code> and <code>max</code> are both zero, the minimum and
maximum values of the data are used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_histc(torch_tensor(c(1., 2, 1)), bins=4, min=0, max=3)
}
</code></pre>

<hr>
<h2 id='torch_hstack'>Hstack</h2><span id='topic+torch_hstack'></span>

<h3>Description</h3>

<p>Hstack
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_hstack(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_hstack_+3A_tensors">tensors</code></td>
<td>
<p>(sequence of Tensors) sequence of tensors to concatenate</p>
</td></tr>
</table>


<h3>hstack(tensors, *, out=None) -&gt; Tensor </h3>

<p>Stack tensors in sequence horizontally (column wise).
</p>
<p>This is equivalent to concatenation along the first axis for 1-D tensors, and
along the second axis for all other tensors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2, 3))
b &lt;- torch_tensor(c(4, 5, 6))
torch_hstack(list(a,b))
a &lt;- torch_tensor(rbind(1,2,3))
b &lt;- torch_tensor(rbind(4,5,6))
torch_hstack(list(a,b))
}
</code></pre>

<hr>
<h2 id='torch_hypot'>Hypot</h2><span id='topic+torch_hypot'></span>

<h3>Description</h3>

<p>Hypot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_hypot(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_hypot_+3A_self">self</code></td>
<td>
<p>(Tensor) the first input tensor</p>
</td></tr>
<tr><td><code id="torch_hypot_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>hypot(input, other, *, out=None) -&gt; Tensor </h3>

<p>Given the legs of a right triangle, return its hypotenuse.
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{out}_{i} = \sqrt{\mbox{input}_{i}^{2} + \mbox{other}_{i}^{2}}
</code>
</p>

<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_hypot(torch_tensor(c(4.0)), torch_tensor(c(3.0, 4.0, 5.0)))
}
</code></pre>

<hr>
<h2 id='torch_i0'>I0</h2><span id='topic+torch_i0'></span>

<h3>Description</h3>

<p>I0
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_i0(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_i0_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
</table>


<h3>i0(input, *, out=None) -&gt; Tensor </h3>

<p>Computes the zeroth order modified Bessel function of the first kind for each element of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{out}_{i} = I_0(\mbox{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\mbox{input}_{i}^2/4)^k}{(k!)^2}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_i0(torch_arange(start = 0, end = 5, dtype=torch_float32()))
}
</code></pre>

<hr>
<h2 id='torch_iinfo'>Integer type info</h2><span id='topic+torch_iinfo'></span>

<h3>Description</h3>

<p>A list that represents the numerical properties of a integer
type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_iinfo(dtype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_iinfo_+3A_dtype">dtype</code></td>
<td>
<p>dtype to get information from.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_imag'>Imag</h2><span id='topic+torch_imag'></span>

<h3>Description</h3>

<p>Imag
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_imag(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_imag_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>imag(input) -&gt; Tensor </h3>

<p>Returns the imaginary part of the <code>input</code> tensor.
</p>


<h3>Warning</h3>

<p>Not yet implemented.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = imag(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
torch_imag(torch_tensor(c(-1 + 1i, -2 + 2i, 3 - 3i)))

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_index'>Index torch tensors</h2><span id='topic+torch_index'></span>

<h3>Description</h3>

<p>Helper functions to index tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_index(self, indices)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_index_+3A_self">self</code></td>
<td>
<p>(Tensor) Tensor that will be indexed.</p>
</td></tr>
<tr><td><code id="torch_index_+3A_indices">indices</code></td>
<td>
<p>(<code>List[Tensor]</code>) List of indices. Indices are torch tensors with
<code>torch_long()</code> dtype.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_index_put'>Modify values selected by <code>indices</code>.</h2><span id='topic+torch_index_put'></span>

<h3>Description</h3>

<p>Modify values selected by <code>indices</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_index_put(self, indices, values, accumulate = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_index_put_+3A_self">self</code></td>
<td>
<p>(Tensor) Tensor that will be indexed.</p>
</td></tr>
<tr><td><code id="torch_index_put_+3A_indices">indices</code></td>
<td>
<p>(<code>List[Tensor]</code>) List of indices. Indices are torch tensors with
<code>torch_long()</code> dtype.</p>
</td></tr>
<tr><td><code id="torch_index_put_+3A_values">values</code></td>
<td>
<p>(Tensor) values that will be replaced the indexed location. Used
for <code>torch_index_put</code> and <code>torch_index_put_</code>.</p>
</td></tr>
<tr><td><code id="torch_index_put_+3A_accumulate">accumulate</code></td>
<td>
<p>(bool) Wether instead of replacing the current values with <code>values</code>,
you want to add them.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_index_put_'>In-place version of <code>torch_index_put</code>.</h2><span id='topic+torch_index_put_'></span>

<h3>Description</h3>

<p>In-place version of <code>torch_index_put</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_index_put_(self, indices, values, accumulate = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_index_put__+3A_self">self</code></td>
<td>
<p>(Tensor) Tensor that will be indexed.</p>
</td></tr>
<tr><td><code id="torch_index_put__+3A_indices">indices</code></td>
<td>
<p>(<code>List[Tensor]</code>) List of indices. Indices are torch tensors with
<code>torch_long()</code> dtype.</p>
</td></tr>
<tr><td><code id="torch_index_put__+3A_values">values</code></td>
<td>
<p>(Tensor) values that will be replaced the indexed location. Used
for <code>torch_index_put</code> and <code>torch_index_put_</code>.</p>
</td></tr>
<tr><td><code id="torch_index_put__+3A_accumulate">accumulate</code></td>
<td>
<p>(bool) Wether instead of replacing the current values with <code>values</code>,
you want to add them.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_index_select'>Index_select</h2><span id='topic+torch_index_select'></span>

<h3>Description</h3>

<p>Index_select
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_index_select(self, dim, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_index_select_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_index_select_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension in which we index</p>
</td></tr>
<tr><td><code id="torch_index_select_+3A_index">index</code></td>
<td>
<p>(LongTensor) the 1-D tensor containing the indices to index</p>
</td></tr>
</table>


<h3>index_select(input, dim, index, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor which indexes the <code>input</code> tensor along dimension
<code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.
</p>
<p>The returned tensor has the same number of dimensions as the original tensor
(<code>input</code>).  The <code>dim</code>\ th dimension has the same size as the length
of <code>index</code>; other dimensions have the same size as in the original tensor.
</p>


<h3>Note</h3>

<p>The returned tensor does <strong>not</strong> use the same storage as the original
tensor.  If <code>out</code> has a different shape than expected, we
silently change it to the correct shape, reallocating the underlying
storage if necessary.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(3, 4))
x
indices = torch_tensor(c(1, 3), dtype = torch_int64())
torch_index_select(x, 1, indices)
torch_index_select(x, 2, indices)
}
</code></pre>

<hr>
<h2 id='torch_install_path'>A simple exported version of install_path
Returns the torch installation path.</h2><span id='topic+torch_install_path'></span>

<h3>Description</h3>

<p>A simple exported version of install_path
Returns the torch installation path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_install_path()
</code></pre>

<hr>
<h2 id='torch_inverse'>Inverse</h2><span id='topic+torch_inverse'></span>

<h3>Description</h3>

<p>Inverse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_inverse(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_inverse_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code class="reqn">(*, n, n)</code> where <code>*</code> is zero or more                    batch dimensions</p>
</td></tr>
</table>


<h3>inverse(input, out=NULL) -&gt; Tensor </h3>

<p>Takes the inverse of the square matrix <code>input</code>. <code>input</code> can be batches
of 2D square tensors, in which case this function would return a tensor composed of
individual inverses.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>Irrespective of the original strides, the returned tensors will be
transposed, i.e. with strides like `input.contiguous().transpose(-2, -1).stride()`
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
x = torch_rand(c(4, 4))
y = torch_inverse(x)
z = torch_mm(x, y)
z
torch_max(torch_abs(z - torch_eye(4))) # Max non-zero
# Batched inverse example
x = torch_randn(c(2, 3, 4, 4))
y = torch_inverse(x)
z = torch_matmul(x, y)
torch_max(torch_abs(z - torch_eye(4)$expand_as(x))) # Max non-zero

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_is_complex'>Is_complex</h2><span id='topic+torch_is_complex'></span>

<h3>Description</h3>

<p>Is_complex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_is_complex(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_is_complex_+3A_self">self</code></td>
<td>
<p>(Tensor) the PyTorch tensor to test</p>
</td></tr>
</table>


<h3>is_complex(input) -&gt; (bool) </h3>

<p>Returns TRUE if the data type of <code>input</code> is a complex data type i.e.,
one of <code>torch_complex64</code>, and <code>torch.complex128</code>.
</p>

<hr>
<h2 id='torch_is_floating_point'>Is_floating_point</h2><span id='topic+torch_is_floating_point'></span>

<h3>Description</h3>

<p>Is_floating_point
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_is_floating_point(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_is_floating_point_+3A_self">self</code></td>
<td>
<p>(Tensor) the PyTorch tensor to test</p>
</td></tr>
</table>


<h3>is_floating_point(input) -&gt; (bool) </h3>

<p>Returns TRUE if the data type of <code>input</code> is a floating point data type i.e.,
one of <code>torch_float64</code>, <code>torch.float32</code> and <code>torch.float16</code>.
</p>

<hr>
<h2 id='torch_is_installed'>Verifies if torch is installed</h2><span id='topic+torch_is_installed'></span>

<h3>Description</h3>

<p>Verifies if torch is installed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_is_installed()
</code></pre>

<hr>
<h2 id='torch_is_nonzero'>Is_nonzero</h2><span id='topic+torch_is_nonzero'></span>

<h3>Description</h3>

<p>Is_nonzero
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_is_nonzero(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_is_nonzero_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>is_nonzero(input) -&gt; (bool) </h3>

<p>Returns TRUE if the <code>input</code> is a single element tensor which is not equal to zero
after type conversions.
i.e. not equal to <code>torch_tensor(c(0))</code> or <code>torch_tensor(c(0))</code> or
<code>torch_tensor(c(FALSE))</code>.
Throws a <code>RuntimeError</code> if <code>torch_numel() != 1</code> (even in case
of sparse tensors).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_is_nonzero(torch_tensor(c(0.)))
torch_is_nonzero(torch_tensor(c(1.5)))
torch_is_nonzero(torch_tensor(c(FALSE)))
torch_is_nonzero(torch_tensor(c(3)))
if (FALSE) {
torch_is_nonzero(torch_tensor(c(1, 3, 5)))
torch_is_nonzero(torch_tensor(c()))
}
}
</code></pre>

<hr>
<h2 id='torch_isclose'>Isclose</h2><span id='topic+torch_isclose'></span>

<h3>Description</h3>

<p>Isclose
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isclose(self, other, rtol = 1e-05, atol = 1e-08, equal_nan = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isclose_+3A_self">self</code></td>
<td>
<p>(Tensor) first tensor to compare</p>
</td></tr>
<tr><td><code id="torch_isclose_+3A_other">other</code></td>
<td>
<p>(Tensor) second tensor to compare</p>
</td></tr>
<tr><td><code id="torch_isclose_+3A_rtol">rtol</code></td>
<td>
<p>(float, optional) relative tolerance. Default: 1e-05</p>
</td></tr>
<tr><td><code id="torch_isclose_+3A_atol">atol</code></td>
<td>
<p>(float, optional) absolute tolerance. Default: 1e-08</p>
</td></tr>
<tr><td><code id="torch_isclose_+3A_equal_nan">equal_nan</code></td>
<td>
<p>(bool, optional) if <code>TRUE</code>, then two <code>NaN</code> s will be
considered equal. Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=FALSE) -&gt; Tensor </h3>

<p>Returns a new tensor with boolean elements representing if each element of
<code>input</code> is &quot;close&quot; to the corresponding element of <code>other</code>.
Closeness is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    \vert \mbox{input} - \mbox{other} \vert \leq \mbox{atol} + \mbox{rtol} \times \vert \mbox{other} \vert
</code>
</p>

<p>where <code>input</code> and <code>other</code> are finite. Where <code>input</code>
and/or <code>other</code> are nonfinite they are close if and only if
they are equal, with NaNs being considered equal to each other when
<code>equal_nan</code> is TRUE.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_isclose(torch_tensor(c(1., 2, 3)), torch_tensor(c(1 + 1e-10, 3, 4)))
torch_isclose(torch_tensor(c(Inf, 4)), torch_tensor(c(Inf, 6)), rtol=.5)
}
</code></pre>

<hr>
<h2 id='torch_isfinite'>Isfinite</h2><span id='topic+torch_isfinite'></span>

<h3>Description</h3>

<p>Isfinite
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isfinite(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isfinite_+3A_self">self</code></td>
<td>
<p>(Tensor) A tensor to check</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Returns a new tensor with boolean elements representing if each element is <code>Finite</code> or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_isfinite(torch_tensor(c(1, Inf, 2, -Inf, NaN)))
}
</code></pre>

<hr>
<h2 id='torch_isinf'>Isinf</h2><span id='topic+torch_isinf'></span>

<h3>Description</h3>

<p>Isinf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isinf(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isinf_+3A_self">self</code></td>
<td>
<p>(Tensor) A tensor to check</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Returns a new tensor with boolean elements representing if each element is <code style="white-space: pre;">&#8288;+/-INF&#8288;</code> or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_isinf(torch_tensor(c(1, Inf, 2, -Inf, NaN)))
}
</code></pre>

<hr>
<h2 id='torch_isnan'>Isnan</h2><span id='topic+torch_isnan'></span>

<h3>Description</h3>

<p>Isnan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isnan(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isnan_+3A_self">self</code></td>
<td>
<p>(Tensor) A tensor to check</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Returns a new tensor with boolean elements representing if each element is <code>NaN</code> or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_isnan(torch_tensor(c(1, NaN, 2)))
}
</code></pre>

<hr>
<h2 id='torch_isneginf'>Isneginf</h2><span id='topic+torch_isneginf'></span>

<h3>Description</h3>

<p>Isneginf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isneginf(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isneginf_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>isneginf(input, *, out=None) -&gt; Tensor </h3>

<p>Tests if each element of <code>input</code> is negative infinity or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(-Inf, Inf, 1.2))
torch_isneginf(a)
}
</code></pre>

<hr>
<h2 id='torch_isposinf'>Isposinf</h2><span id='topic+torch_isposinf'></span>

<h3>Description</h3>

<p>Isposinf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isposinf(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isposinf_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>isposinf(input, *, out=None) -&gt; Tensor </h3>

<p>Tests if each element of <code>input</code> is positive infinity or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(-Inf, Inf, 1.2))
torch_isposinf(a)
}
</code></pre>

<hr>
<h2 id='torch_isreal'>Isreal</h2><span id='topic+torch_isreal'></span>

<h3>Description</h3>

<p>Isreal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_isreal(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_isreal_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>isreal(input) -&gt; Tensor </h3>

<p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is real-valued or not.
All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
if (FALSE) {
torch_isreal(torch_tensor(c(1, 1+1i, 2+0i)))
}
}
</code></pre>

<hr>
<h2 id='torch_istft'>Istft</h2><span id='topic+torch_istft'></span>

<h3>Description</h3>

<p>Inverse short time Fourier Transform. This is expected to be the inverse of <code><a href="#topic+torch_stft">torch_stft()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_istft(
  self,
  n_fft,
  hop_length = NULL,
  win_length = NULL,
  window = list(),
  center = TRUE,
  normalized = FALSE,
  onesided = NULL,
  length = NULL,
  return_complex = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_istft_+3A_self">self</code></td>
<td>
<p>(Tensor) The input tensor. Expected to be output of <code><a href="#topic+torch_stft">torch_stft()</a></code>,
can either be complex (<code>channel</code>, <code>fft_size</code>, <code>n_frame</code>), or real
(<code>channel</code>, <code>fft_size</code>, <code>n_frame</code>, 2) where the <code>channel</code> dimension is
optional.</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_n_fft">n_fft</code></td>
<td>
<p>(int) Size of Fourier transform</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_hop_length">hop_length</code></td>
<td>
<p>(Optional<code style="white-space: pre;">&#8288;[int]&#8288;</code>) The distance between neighboring sliding window frames.
(Default: <code>n_fft %% 4</code>)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_win_length">win_length</code></td>
<td>
<p>(Optional<code style="white-space: pre;">&#8288;[int]&#8288;</code>) The size of window frame and STFT filter.
(Default: <code>n_fft</code>)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_window">window</code></td>
<td>
<p>(Optional(torch.Tensor)) The optional window function.
(Default: <code>torch_ones(win_length)</code>)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_center">center</code></td>
<td>
<p>(bool) Whether <code>input</code> was padded on both sides so that the
<code class="reqn">t</code>-th frame is centered at time <code class="reqn">t \times \mbox{hop\_length}</code>.
(Default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_normalized">normalized</code></td>
<td>
<p>(bool) Whether the STFT was normalized. (Default: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_onesided">onesided</code></td>
<td>
<p>(Optional(bool)) Whether the STFT was onesided.
(Default: <code>TRUE</code> if <code>n_fft != fft_size</code> in the input size)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_length">length</code></td>
<td>
<p>(Optional(int)]) The amount to trim the signal by (i.e. the
original signal length). (Default: whole signal)</p>
</td></tr>
<tr><td><code id="torch_istft_+3A_return_complex">return_complex</code></td>
<td>
<p>(Optional(bool)) Whether the output should be complex,
or if the input should be assumed to derive from a real signal and window.
Note that this is incompatible with <code>onesided=TRUE</code>. (Default: <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It has the same parameters (+ additional optional parameter of <code>length</code>) and it should return the
least squares estimation of the original signal. The algorithm will check using the NOLA
condition (nonzero overlap).
</p>
<p>Important consideration in the parameters <code>window</code> and <code>center</code> so that the envelop
created by the summation of all the windows is never zero at certain point in time. Specifically,
<code class="reqn">\sum_{t=-\infty}^{\infty} |w|^2(n-t\times hop_length) \neq 0</code>.
</p>
<p>Since <code><a href="#topic+torch_stft">torch_stft()</a></code> discards elements at the end of the signal if they do not fit in a frame,
<code>istft</code> may return a shorter signal than the original signal (can occur if <code>center</code> is FALSE
since the signal isn't padded).
</p>
<p>If <code>center</code> is <code>TRUE</code>, then there will be padding e.g. <code>'constant'</code>, <code>'reflect'</code>, etc.
Left padding can be trimmed off exactly because they can be calculated but right
padding cannot be calculated without additional information.
</p>
<p>Example: Suppose the last window is:
<code style="white-space: pre;">&#8288;[c(17, 18, 0, 0, 0)&#8288;</code> vs <code>c(18, 0, 0, 0, 0)</code>
</p>
<p>The <code>n_fft</code>, <code>hop_length</code>, <code>win_length</code> are all the same which prevents the calculation
of right padding. These additional values could be zeros or a reflection of the signal so providing
<code>length</code> could be useful. If <code>length</code> is <code>None</code> then padding will be aggressively removed
(some loss of signal).
</p>
<p>D. W. Griffin and J. S. Lim, &quot;Signal estimation from modified short-time Fourier transform,&quot;
IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.
</p>

<hr>
<h2 id='torch_kaiser_window'>Kaiser_window</h2><span id='topic+torch_kaiser_window'></span>

<h3>Description</h3>

<p>Kaiser_window
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_kaiser_window(
  window_length,
  periodic,
  beta,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_kaiser_window_+3A_window_length">window_length</code></td>
<td>
<p>(int) length of the window.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_periodic">periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a periodic window suitable for use in spectral analysis.        If FALSE, returns a symmetric window suitable for use in filter design.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_beta">beta</code></td>
<td>
<p>(float, optional) shape parameter for the window.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). If <code>dtype</code> is not given, infer the data type from the other input        arguments. If any of <code>start</code>, <code>end</code>, or <code>stop</code> are floating-point, the        <code>dtype</code> is inferred to be the default dtype, see        <code>~torch.get_default_dtype</code>. Otherwise, the <code>dtype</code> is inferred to        be <code>torch.int64</code>.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_kaiser_window_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>kaiser_window(window_length, periodic=TRUE, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=FALSE) -&gt; Tensor </h3>

<p>Computes the Kaiser window with window length <code>window_length</code> and shape parameter <code>beta</code>.
</p>
<p>Let I_0 be the zeroth order modified Bessel function of the first kind (see <code><a href="#topic+torch_i0">torch_i0()</a></code>) and
<code>N = L - 1</code> if <code>periodic</code> is FALSE and <code>L</code> if <code>periodic</code> is TRUE,
where <code>L</code> is the <code>window_length</code>. This function computes:
</p>
<p style="text-align: center;"><code class="reqn">
    out_i = I_0 \left( \beta \sqrt{1 - \left( {\frac{i - N/2}{N/2}} \right) ^2 } \right) / I_0( \beta )
</code>
</p>

<p>Calling <code>torch_kaiser_window(L, B, periodic=TRUE)</code> is equivalent to calling
<code style="white-space: pre;">&#8288;torch_kaiser_window(L + 1, B, periodic=FALSE)[:-1])&#8288;</code>.
The <code>periodic</code> argument is intended as a helpful shorthand
to produce a periodic window as input to functions like <code><a href="#topic+torch_stft">torch_stft()</a></code>.
</p>


<h3>Note</h3>

<p>If <code>window_length</code> is one, then the returned window is a single element
tensor containing a one.
</p>

<hr>
<h2 id='torch_kron'>Kronecker product</h2><span id='topic+torch_kron'></span>

<h3>Description</h3>

<p>Computes the Kronecker product of <code>self</code> and <code>other</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_kron(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_kron_+3A_self">self</code></td>
<td>
<p>(<code>Tensor</code>) input Tensor</p>
</td></tr>
<tr><td><code id="torch_kron_+3A_other">other</code></td>
<td>
<p>(<code>Tensor</code>) other tensor.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_kthvalue'>Kthvalue</h2><span id='topic+torch_kthvalue'></span>

<h3>Description</h3>

<p>Kthvalue
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_kthvalue(self, k, dim = -1L, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_kthvalue_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_kthvalue_+3A_k">k</code></td>
<td>
<p>(int) k for the k-th smallest element</p>
</td></tr>
<tr><td><code id="torch_kthvalue_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension to find the kth value along</p>
</td></tr>
<tr><td><code id="torch_kthvalue_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>kthvalue(input, k, dim=NULL, keepdim=False, out=NULL) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the <code>k</code> th
smallest element of each row of the <code>input</code> tensor in the given dimension
<code>dim</code>. And <code>indices</code> is the index location of each element found.
</p>
<p>If <code>dim</code> is not given, the last dimension of the <code>input</code> is chosen.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, both the <code>values</code> and <code>indices</code> tensors
are the same size as <code>input</code>, except in the dimension <code>dim</code> where
they are of size 1. Otherwise, <code>dim</code> is squeezed
(see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in both the <code>values</code> and
<code>indices</code> tensors having 1 fewer dimension than the <code>input</code> tensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(1, 6)
x
torch_kthvalue(x, 4)
x &lt;- torch_arange(1,6)$resize_(c(2,3))
x
torch_kthvalue(x, 2, 1, TRUE)
}
</code></pre>

<hr>
<h2 id='torch_layout'>Creates the corresponding layout</h2><span id='topic+torch_layout'></span><span id='topic+torch_strided'></span><span id='topic+torch_sparse_coo'></span>

<h3>Description</h3>

<p>Creates the corresponding layout
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_strided()

torch_sparse_coo()
</code></pre>

<hr>
<h2 id='torch_lcm'>Lcm</h2><span id='topic+torch_lcm'></span>

<h3>Description</h3>

<p>Lcm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lcm(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lcm_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_lcm_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>lcm(input, other, *, out=None) -&gt; Tensor </h3>

<p>Computes the element-wise least common multiple (LCM) of <code>input</code> and <code>other</code>.
</p>
<p>Both <code>input</code> and <code>other</code> must have integer types.
</p>


<h3>Note</h3>

<p>This defines <code class="reqn">lcm(0, 0) = 0</code> and <code class="reqn">lcm(0, a) = 0</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

if (torch::cuda_is_available()) {
a &lt;- torch_tensor(c(5, 10, 15), dtype = torch_long(), device = "cuda")
b &lt;- torch_tensor(c(3, 4, 5), dtype = torch_long(), device = "cuda")
torch_lcm(a, b)
c &lt;- torch_tensor(c(3L), device = "cuda")
torch_lcm(a, c)
}
}
</code></pre>

<hr>
<h2 id='torch_le'>Le</h2><span id='topic+torch_le'></span>

<h3>Description</h3>

<p>Le
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_le(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_le_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_le_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>le(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes <code class="reqn">\mbox{input} \leq \mbox{other}</code> element-wise.
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_le(torch_tensor(matrix(1:4, ncol = 2, byrow=TRUE)), 
         torch_tensor(matrix(c(1,1,4,4), ncol = 2, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_lerp'>Lerp</h2><span id='topic+torch_lerp'></span>

<h3>Description</h3>

<p>Lerp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lerp(self, end, weight)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lerp_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor with the starting points</p>
</td></tr>
<tr><td><code id="torch_lerp_+3A_end">end</code></td>
<td>
<p>(Tensor) the tensor with the ending points</p>
</td></tr>
<tr><td><code id="torch_lerp_+3A_weight">weight</code></td>
<td>
<p>(float or tensor) the weight for the interpolation formula</p>
</td></tr>
</table>


<h3>lerp(input, end, weight, out=NULL) </h3>

<p>Does a linear interpolation of two tensors <code>start</code> (given by <code>input</code>) and <code>end</code> based
on a scalar or tensor <code>weight</code> and returns the resulting <code>out</code> tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{start}_i + \mbox{weight}_i \times (\mbox{end}_i - \mbox{start}_i)
</code>
</p>

<p>The shapes of <code>start</code> and <code>end</code> must be
broadcastable . If <code>weight</code> is a tensor, then
the shapes of <code>weight</code>, <code>start</code>, and <code>end</code> must be broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

start = torch_arange(1, 4)
end = torch_empty(4)$fill_(10)
start
end
torch_lerp(start, end, 0.5)
torch_lerp(start, end, torch_full_like(start, 0.5))
}
</code></pre>

<hr>
<h2 id='torch_less'>Less</h2><span id='topic+torch_less'></span>

<h3>Description</h3>

<p>Less
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_less(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_less_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_less_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>less(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_lt">torch_lt()</a></code>.
</p>

<hr>
<h2 id='torch_less_equal'>Less_equal</h2><span id='topic+torch_less_equal'></span>

<h3>Description</h3>

<p>Less_equal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_less_equal(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_less_equal_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_less_equal_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>less_equal(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_le">torch_le()</a></code>.
</p>

<hr>
<h2 id='torch_lgamma'>Lgamma</h2><span id='topic+torch_lgamma'></span>

<h3>Description</h3>

<p>Lgamma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lgamma(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lgamma_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>lgamma(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the logarithm of the gamma function on <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \log \Gamma(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_arange(0.5, 2, 0.5)
torch_lgamma(a)
}
</code></pre>

<hr>
<h2 id='torch_linspace'>Linspace</h2><span id='topic+torch_linspace'></span>

<h3>Description</h3>

<p>Linspace
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_linspace(
  start,
  end,
  steps = 100,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_linspace_+3A_start">start</code></td>
<td>
<p>(float) the starting value for the set of points</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_end">end</code></td>
<td>
<p>(float) the ending value for the set of points</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_steps">steps</code></td>
<td>
<p>(int) number of points to sample between <code>start</code>        and <code>end</code>. Default: <code>100</code>.</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_linspace_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>linspace(start, end, steps=100, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a one-dimensional tensor of <code>steps</code>
equally spaced points between <code>start</code> and <code>end</code>.
</p>
<p>The output tensor is 1-D of size <code>steps</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_linspace(3, 10, steps=5)
torch_linspace(-10, 10, steps=5)
torch_linspace(start=-10, end=10, steps=5)
torch_linspace(start=-10, end=10, steps=1)
}
</code></pre>

<hr>
<h2 id='torch_load'>Loads a saved object</h2><span id='topic+torch_load'></span>

<h3>Description</h3>

<p>Loads a saved object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_load(path, device = "cpu")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_load_+3A_path">path</code></td>
<td>
<p>a path to the saved object</p>
</td></tr>
<tr><td><code id="torch_load_+3A_device">device</code></td>
<td>
<p>a device to load tensors to. By default we load to the <code>cpu</code> but you can also
load them to any <code>cuda</code> device. If <code>NULL</code> then the device where the tensor has been saved will
be reused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other torch_save: 
<code><a href="#topic+torch_save">torch_save</a>()</code>,
<code><a href="#topic+torch_serialize">torch_serialize</a>()</code>
</p>

<hr>
<h2 id='torch_log'>Log</h2><span id='topic+torch_log'></span>

<h3>Description</h3>

<p>Log
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_log(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_log_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>log(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the natural logarithm of the elements
of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = \log_{e} (x_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(5))
a
torch_log(a)
}
</code></pre>

<hr>
<h2 id='torch_log10'>Log10</h2><span id='topic+torch_log10'></span>

<h3>Description</h3>

<p>Log10
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_log10(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_log10_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>log10(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the logarithm to the base 10 of the elements
of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = \log_{10} (x_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_rand(5)
a
torch_log10(a)
}
</code></pre>

<hr>
<h2 id='torch_log1p'>Log1p</h2><span id='topic+torch_log1p'></span>

<h3>Description</h3>

<p>Log1p
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_log1p(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_log1p_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>log1p(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the natural logarithm of (1 + <code>input</code>).
</p>
<p style="text-align: center;"><code class="reqn">
    y_i = \log_{e} (x_i + 1)
</code>
</p>



<h3>Note</h3>

<p>This function is more accurate than <code><a href="#topic+torch_log">torch_log</a></code> for small
values of <code>input</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(5))
a
torch_log1p(a)
}
</code></pre>

<hr>
<h2 id='torch_log2'>Log2</h2><span id='topic+torch_log2'></span>

<h3>Description</h3>

<p>Log2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_log2(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_log2_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>log2(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the logarithm to the base 2 of the elements
of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = \log_{2} (x_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_rand(5)
a
torch_log2(a)
}
</code></pre>

<hr>
<h2 id='torch_logaddexp'>Logaddexp</h2><span id='topic+torch_logaddexp'></span>

<h3>Description</h3>

<p>Logaddexp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logaddexp(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logaddexp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logaddexp_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>logaddexp(input, other, *, out=None) -&gt; Tensor </h3>

<p>Logarithm of the sum of exponentiations of the inputs.
</p>
<p>Calculates pointwise <code class="reqn">\log\left(e^x + e^y\right)</code>. This function is useful
in statistics where the calculated probabilities of events may be so small as to
exceed the range of normal floating point numbers. In such cases the logarithm
of the calculated probability is stored. This function allows adding
probabilities stored in such a fashion.
</p>
<p>This op should be disambiguated with <code><a href="#topic+torch_logsumexp">torch_logsumexp()</a></code> which performs a
reduction on a single tensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logaddexp(torch_tensor(c(-1.0)), torch_tensor(c(-1.0, -2, -3)))
torch_logaddexp(torch_tensor(c(-100.0, -200, -300)), torch_tensor(c(-1.0, -2, -3)))
torch_logaddexp(torch_tensor(c(1.0, 2000, 30000)), torch_tensor(c(-1.0, -2, -3)))
}
</code></pre>

<hr>
<h2 id='torch_logaddexp2'>Logaddexp2</h2><span id='topic+torch_logaddexp2'></span>

<h3>Description</h3>

<p>Logaddexp2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logaddexp2(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logaddexp2_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logaddexp2_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>logaddexp2(input, other, *, out=None) -&gt; Tensor </h3>

<p>Logarithm of the sum of exponentiations of the inputs in base-2.
</p>
<p>Calculates pointwise <code class="reqn">\log_2\left(2^x + 2^y\right)</code>. See
<code><a href="#topic+torch_logaddexp">torch_logaddexp()</a></code> for more details.
</p>

<hr>
<h2 id='torch_logcumsumexp'>Logcumsumexp</h2><span id='topic+torch_logcumsumexp'></span>

<h3>Description</h3>

<p>Logcumsumexp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logcumsumexp(self, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logcumsumexp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logcumsumexp_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to do the operation over</p>
</td></tr>
</table>


<h3>logcumsumexp(input, dim, *, out=None) -&gt; Tensor </h3>

<p>Returns the logarithm of the cumulative summation of the exponentiation of
elements of <code>input</code> in the dimension <code>dim</code>.
</p>
<p>For summation index <code class="reqn">j</code> given by <code>dim</code> and other indices <code class="reqn">i</code>, the result is
</p>
<p style="text-align: center;"><code class="reqn">
        \mbox{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(10))
torch_logcumsumexp(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_logdet'>Logdet</h2><span id='topic+torch_logdet'></span>

<h3>Description</h3>

<p>Logdet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logdet(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logdet_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more                batch dimensions.</p>
</td></tr>
</table>


<h3>logdet(input) -&gt; Tensor </h3>

<p>Calculates log determinant of a square matrix or batches of square matrices.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>Result is `-inf` if `input` has zero log determinant, and is `NaN` if
`input` has negative determinant.
</pre></div>
<div class="sourceCode"><pre>Backward through `logdet` internally uses SVD results when `input`
is not invertible. In this case, double backward through `logdet` will
be unstable in when `input` doesn't have distinct singular values. See
`~torch.svd` for details.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A = torch_randn(c(3, 3))
torch_det(A)
torch_logdet(A)
A
A$det()
A$det()$log()
}
</code></pre>

<hr>
<h2 id='torch_logical_and'>Logical_and</h2><span id='topic+torch_logical_and'></span>

<h3>Description</h3>

<p>Logical_and
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logical_and(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logical_and_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logical_and_+3A_other">other</code></td>
<td>
<p>(Tensor) the tensor to compute AND with</p>
</td></tr>
</table>


<h3>logical_and(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise logical AND of the given input tensors. Zeros are treated as <code>FALSE</code> and nonzeros are
treated as <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logical_and(torch_tensor(c(TRUE, FALSE, TRUE)), torch_tensor(c(TRUE, FALSE, FALSE)))
a = torch_tensor(c(0, 1, 10, 0), dtype=torch_int8())
b = torch_tensor(c(4, 0, 1, 0), dtype=torch_int8())
torch_logical_and(a, b)
## Not run: 
torch_logical_and(a, b, out=torch_empty(4, dtype=torch_bool()))

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_logical_not'>Logical_not</h2><span id='topic+torch_logical_not'></span>

<h3>Description</h3>

<p>Logical_not
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logical_not_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>logical_not(input, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool
dtype. If the input tensor is not a bool tensor, zeros are treated as <code>FALSE</code> and non-zeros are treated as <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logical_not(torch_tensor(c(TRUE, FALSE)))
torch_logical_not(torch_tensor(c(0, 1, -10), dtype=torch_int8()))
torch_logical_not(torch_tensor(c(0., 1.5, -10.), dtype=torch_double()))
}
</code></pre>

<hr>
<h2 id='torch_logical_or'>Logical_or</h2><span id='topic+torch_logical_or'></span>

<h3>Description</h3>

<p>Logical_or
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logical_or(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logical_or_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logical_or_+3A_other">other</code></td>
<td>
<p>(Tensor) the tensor to compute OR with</p>
</td></tr>
</table>


<h3>logical_or(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise logical OR of the given input tensors. Zeros are treated as <code>FALSE</code> and nonzeros are
treated as <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logical_or(torch_tensor(c(TRUE, FALSE, TRUE)), torch_tensor(c(TRUE, FALSE, FALSE)))
a = torch_tensor(c(0, 1, 10, 0), dtype=torch_int8())
b = torch_tensor(c(4, 0, 1, 0), dtype=torch_int8())
torch_logical_or(a, b)
## Not run: 
torch_logical_or(a$double(), b$double())
torch_logical_or(a$double(), b)
torch_logical_or(a, b, out=torch_empty(4, dtype=torch_bool()))

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_logical_xor'>Logical_xor</h2><span id='topic+torch_logical_xor'></span>

<h3>Description</h3>

<p>Logical_xor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logical_xor(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logical_xor_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logical_xor_+3A_other">other</code></td>
<td>
<p>(Tensor) the tensor to compute XOR with</p>
</td></tr>
</table>


<h3>logical_xor(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as <code>FALSE</code> and nonzeros are
treated as <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logical_xor(torch_tensor(c(TRUE, FALSE, TRUE)), torch_tensor(c(TRUE, FALSE, FALSE)))
a = torch_tensor(c(0, 1, 10, 0), dtype=torch_int8())
b = torch_tensor(c(4, 0, 1, 0), dtype=torch_int8())
torch_logical_xor(a, b)
torch_logical_xor(a$to(dtype=torch_double()), b$to(dtype=torch_double()))
torch_logical_xor(a$to(dtype=torch_double()), b)
}
</code></pre>

<hr>
<h2 id='torch_logit'>Logit</h2><span id='topic+torch_logit'></span>

<h3>Description</h3>

<p>Logit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logit(self, eps = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logit_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logit_+3A_eps">eps</code></td>
<td>
<p>(float, optional) the epsilon for input clamp bound. Default: <code>None</code></p>
</td></tr>
</table>


<h3>logit(input, eps=None, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with the logit of the elements of <code>input</code>.
<code>input</code> is clamped to <code style="white-space: pre;">&#8288;[eps, 1 - eps]&#8288;</code> when eps is not None.
When eps is None and <code>input</code> &lt; 0 or <code>input</code> &gt; 1, the function will yields NaN.
</p>
<p style="text-align: center;"><code class="reqn">
    y_{i} = \ln(\frac{z_{i}}{1 - z_{i}}) \\
    z_{i} = \begin{array}{ll}
        x_{i} &amp; \mbox{if eps is None} \\
        \mbox{eps} &amp; \mbox{if } x_{i} &lt; \mbox{eps} \\
        x_{i} &amp; \mbox{if } \mbox{eps} \leq x_{i} \leq 1 - \mbox{eps} \\
        1 - \mbox{eps} &amp; \mbox{if } x_{i} &gt; 1 - \mbox{eps}
    \end{array}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_rand(5)
a
torch_logit(a, eps=1e-6)
}
</code></pre>

<hr>
<h2 id='torch_logspace'>Logspace</h2><span id='topic+torch_logspace'></span>

<h3>Description</h3>

<p>Logspace
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logspace(
  start,
  end,
  steps = 100,
  base = 10,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logspace_+3A_start">start</code></td>
<td>
<p>(float) the starting value for the set of points</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_end">end</code></td>
<td>
<p>(float) the ending value for the set of points</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_steps">steps</code></td>
<td>
<p>(int) number of points to sample between <code>start</code>        and <code>end</code>. Default: <code>100</code>.</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_base">base</code></td>
<td>
<p>(float) base of the logarithm function. Default: <code>10.0</code>.</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_logspace_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>logspace(start, end, steps=100, base=10.0, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a one-dimensional tensor of <code>steps</code> points
logarithmically spaced with base <code>base</code> between
<code class="reqn">{\mbox{base}}^{\mbox{start}}</code> and <code class="reqn">{\mbox{base}}^{\mbox{end}}</code>.
</p>
<p>The output tensor is 1-D of size <code>steps</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_logspace(start=-10, end=10, steps=5)
torch_logspace(start=0.1, end=1.0, steps=5)
torch_logspace(start=0.1, end=1.0, steps=1)
torch_logspace(start=2, end=2, steps=1, base=2)
}
</code></pre>

<hr>
<h2 id='torch_logsumexp'>Logsumexp</h2><span id='topic+torch_logsumexp'></span>

<h3>Description</h3>

<p>Logsumexp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_logsumexp(self, dim, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_logsumexp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_logsumexp_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_logsumexp_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>logsumexp(input, dim, keepdim=False, out=NULL) </h3>

<p>Returns the log of summed exponentials of each row of the <code>input</code>
tensor in the given dimension <code>dim</code>. The computation is numerically
stabilized.
</p>
<p>For summation index <code class="reqn">j</code> given by <code>dim</code> and other indices <code class="reqn">i</code>, the result is
</p>
<p style="text-align: center;"><code class="reqn">
        \mbox{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})
</code>
</p>

<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
torch_logsumexp(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_lstsq'>Lstsq</h2><span id='topic+torch_lstsq'></span>

<h3>Description</h3>

<p>Lstsq
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lstsq_+3A_self">self</code></td>
<td>
<p>(Tensor) the matrix <code class="reqn">B</code></p>
</td></tr>
<tr><td><code id="torch_lstsq_+3A_a">A</code></td>
<td>
<p>(Tensor) the <code class="reqn">m</code> by <code class="reqn">n</code> matrix <code class="reqn">A</code></p>
</td></tr>
</table>


<h3>lstsq(input, A, out=NULL) -&gt; Tensor </h3>

<p>Computes the solution to the least squares and least norm problems for a full
rank matrix <code class="reqn">A</code> of size <code class="reqn">(m \times n)</code> and a matrix <code class="reqn">B</code> of
size <code class="reqn">(m \times k)</code>.
</p>
<p>If <code class="reqn">m \geq n</code>, <code><a href="#topic+torch_lstsq">torch_lstsq()</a></code> solves the least-squares problem:
</p>
<p style="text-align: center;"><code class="reqn">
   \begin{array}{ll}
   \min_X &amp; \|AX-B\|_2.
   \end{array}
</code>
</p>

<p>If <code class="reqn">m &lt; n</code>, <code><a href="#topic+torch_lstsq">torch_lstsq()</a></code> solves the least-norm problem:
</p>
<p style="text-align: center;"><code class="reqn">
   \begin{array}{llll}
   \min_X &amp; \|X\|_2 &amp; \mbox{subject to} &amp; AX = B.
   \end{array}
</code>
</p>

<p>Returned tensor <code class="reqn">X</code> has shape <code class="reqn">(\mbox{max}(m, n) \times k)</code>. The first <code class="reqn">n</code>
rows of <code class="reqn">X</code> contains the solution. If <code class="reqn">m \geq n</code>, the residual sum of squares
for the solution in each column is given by the sum of squares of elements in the
remaining <code class="reqn">m - n</code> rows of that column.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>The case when \eqn{m &lt; n} is not supported on the GPU.
</pre></div>

<hr>
<h2 id='torch_lt'>Lt</h2><span id='topic+torch_lt'></span>

<h3>Description</h3>

<p>Lt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lt(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lt_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_lt_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>lt(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes <code class="reqn">\mbox{input} &lt; \mbox{other}</code> element-wise.
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_lt(torch_tensor(matrix(1:4, ncol = 2, byrow=TRUE)), 
         torch_tensor(matrix(c(1,1,4,4), ncol = 2, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_lu'>LU</h2><span id='topic+torch_lu'></span>

<h3>Description</h3>

<p>Computes the LU factorization of a matrix or batches of matrices A. Returns a
tuple containing the LU factorization and pivots of A. Pivoting is done if pivot
is set to True.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lu(A, pivot = TRUE, get_infos = FALSE, out = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lu_+3A_a">A</code></td>
<td>
<p>(Tensor) the tensor to factor of size (<em>, m, n)(</em>,m,n)</p>
</td></tr>
<tr><td><code id="torch_lu_+3A_pivot">pivot</code></td>
<td>
<p>(bool, optional)  controls whether pivoting is done. Default: TRUE</p>
</td></tr>
<tr><td><code id="torch_lu_+3A_get_infos">get_infos</code></td>
<td>
<p>(bool, optional)  if set to True, returns an info IntTensor. Default: FALSE</p>
</td></tr>
<tr><td><code id="torch_lu_+3A_out">out</code></td>
<td>
<p>(tuple, optional)  optional output tuple. If get_infos is True, then the elements
in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the
elements in the tuple are Tensor, IntTensor. Default: NULL</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A &lt;- torch_randn(c(2, 3, 3))
torch_lu(A)
}
</code></pre>

<hr>
<h2 id='torch_lu_solve'>Lu_solve</h2><span id='topic+torch_lu_solve'></span>

<h3>Description</h3>

<p>Lu_solve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lu_solve(self, LU_data, LU_pivots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lu_solve_+3A_self">self</code></td>
<td>
<p>(Tensor) the RHS tensor of size <code class="reqn">(*, m, k)</code>, where <code class="reqn">*</code>                is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="torch_lu_solve_+3A_lu_data">LU_data</code></td>
<td>
<p>(Tensor) the pivoted LU factorization of A from <code>torch_lu</code> of size <code class="reqn">(*, m, m)</code>,                       where <code class="reqn">*</code> is zero or more batch dimensions.</p>
</td></tr>
<tr><td><code id="torch_lu_solve_+3A_lu_pivots">LU_pivots</code></td>
<td>
<p>(IntTensor) the pivots of the LU factorization from <code>torch_lu</code> of size <code class="reqn">(*, m)</code>,                           where <code class="reqn">*</code> is zero or more batch dimensions.                           The batch dimensions of <code>LU_pivots</code> must be equal to the batch dimensions of                           <code>LU_data</code>.</p>
</td></tr>
</table>


<h3>lu_solve(input, LU_data, LU_pivots, out=NULL) -&gt; Tensor </h3>

<p>Returns the LU solve of the linear system <code class="reqn">Ax = b</code> using the partially pivoted
LU factorization of A from <code>torch_lu</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
A = torch_randn(c(2, 3, 3))
b = torch_randn(c(2, 3, 1))
out = torch_lu(A)
x = torch_lu_solve(b, out[[1]], out[[2]])
torch_norm(torch_bmm(A, x) - b)
}
</code></pre>

<hr>
<h2 id='torch_lu_unpack'>Lu_unpack</h2><span id='topic+torch_lu_unpack'></span>

<h3>Description</h3>

<p>Lu_unpack
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_lu_unpack(LU_data, LU_pivots, unpack_data = TRUE, unpack_pivots = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_lu_unpack_+3A_lu_data">LU_data</code></td>
<td>
<p>(Tensor)  the packed LU factorization data</p>
</td></tr>
<tr><td><code id="torch_lu_unpack_+3A_lu_pivots">LU_pivots</code></td>
<td>
<p>(Tensor)  the packed LU factorization pivots</p>
</td></tr>
<tr><td><code id="torch_lu_unpack_+3A_unpack_data">unpack_data</code></td>
<td>
<p>(logical)  flag indicating if the data should be unpacked. If FALSE, then the returned L and U are NULL Default: TRUE</p>
</td></tr>
<tr><td><code id="torch_lu_unpack_+3A_unpack_pivots">unpack_pivots</code></td>
<td>
<p>(logical)  flag indicating if the pivots should be unpacked into a permutation matrix P. If FALSE, then the returned P is None. Default: TRUE</p>
</td></tr>
</table>


<h3>lu_unpack(LU_data, LU_pivots, unpack_data = TRUE, unpack_pivots=TRUE) -&gt; Tensor </h3>

<p>Unpacks the data and pivots from a LU factorization of a tensor into tensors <code>L</code> and <code>U</code> and
a permutation tensor <code>P</code> such that <code>LU_data_and_pivots &lt;- torch_lu(P$matmul(L)$matmul(U))</code>.
Returns a list of tensors as <code style="white-space: pre;">&#8288;list(the P tensor (permutation matrix), the L tensor, the U tensor)&#8288;</code>
</p>

<hr>
<h2 id='torch_manual_seed'>Sets the seed for generating random numbers.</h2><span id='topic+torch_manual_seed'></span><span id='topic+local_torch_manual_seed'></span><span id='topic+with_torch_manual_seed'></span>

<h3>Description</h3>

<p>Sets the seed for generating random numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_manual_seed(seed)

local_torch_manual_seed(seed, .env = parent.frame())

with_torch_manual_seed(code, ..., seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_manual_seed_+3A_seed">seed</code></td>
<td>
<p>integer seed.</p>
</td></tr>
<tr><td><code id="torch_manual_seed_+3A_.env">.env</code></td>
<td>
<p>environment that will take the modifications from manual_seed.</p>
</td></tr>
<tr><td><code id="torch_manual_seed_+3A_code">code</code></td>
<td>
<p>expression to run in the context of the seed</p>
</td></tr>
<tr><td><code id="torch_manual_seed_+3A_...">...</code></td>
<td>
<p>unused currently.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>local_torch_manual_seed()</code>: Modifies the torch seed in the environment scope.
</p>
</li>
<li> <p><code>with_torch_manual_seed()</code>: A with context to change the seed during the function execution.
</p>
</li></ul>


<h3>Note</h3>

<p>Currently the <code>local_torch_manual_seed</code> and <code>with_torch_manual_seed</code> won't
work with Tensors in the MPS device. You can sample the tensors on CPU and
move them to MPS if reproducibility is required.
</p>

<hr>
<h2 id='torch_masked_select'>Masked_select</h2><span id='topic+torch_masked_select'></span>

<h3>Description</h3>

<p>Masked_select
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_masked_select(self, mask)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_masked_select_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_masked_select_+3A_mask">mask</code></td>
<td>
<p>(BoolTensor) the tensor containing the binary mask to index with</p>
</td></tr>
</table>


<h3>masked_select(input, mask, out=NULL) -&gt; Tensor </h3>

<p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to
the boolean mask <code>mask</code> which is a <code>BoolTensor</code>.
</p>
<p>The shapes of the <code>mask</code> tensor and the <code>input</code> tensor don't need
to match, but they must be broadcastable .
</p>


<h3>Note</h3>

<p>The returned tensor does <strong>not</strong> use the same storage
as the original tensor
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(3, 4))
x
mask = x$ge(0.5)
mask
torch_masked_select(x, mask)
}
</code></pre>

<hr>
<h2 id='torch_matmul'>Matmul</h2><span id='topic+torch_matmul'></span>

<h3>Description</h3>

<p>Matmul
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_matmul(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_matmul_+3A_self">self</code></td>
<td>
<p>(Tensor) the first tensor to be multiplied</p>
</td></tr>
<tr><td><code id="torch_matmul_+3A_other">other</code></td>
<td>
<p>(Tensor) the second tensor to be multiplied</p>
</td></tr>
</table>


<h3>matmul(input, other, out=NULL) -&gt; Tensor </h3>

<p>Matrix product of two tensors.
</p>
<p>The behavior depends on the dimensionality of the tensors as follows:
</p>

<ul>
<li><p> If both tensors are 1-dimensional, the dot product (scalar) is returned.
</p>
</li>
<li><p> If both arguments are 2-dimensional, the matrix-matrix product is returned.
</p>
</li>
<li><p> If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.
</p>
</li>
<li><p> If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.
</p>
</li>
<li><p> If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are broadcasted  (and thus
must be broadcastable).  For example, if <code>input</code> is a
<code class="reqn">(j \times 1 \times n \times m)</code> tensor and <code>other</code> is a <code class="reqn">(k \times m \times p)</code>
tensor, <code>out</code> will be an <code class="reqn">(j \times k \times n \times p)</code> tensor.
</p>
</li></ul>



<h3>Note</h3>

<div class="sourceCode"><pre>The 1-dimensional dot product version of this function does not support an `out` parameter.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

# vector x vector
tensor1 = torch_randn(c(3))
tensor2 = torch_randn(c(3))
torch_matmul(tensor1, tensor2)
# matrix x vector
tensor1 = torch_randn(c(3, 4))
tensor2 = torch_randn(c(4))
torch_matmul(tensor1, tensor2)
# batched matrix x broadcasted vector
tensor1 = torch_randn(c(10, 3, 4))
tensor2 = torch_randn(c(4))
torch_matmul(tensor1, tensor2)
# batched matrix x batched matrix
tensor1 = torch_randn(c(10, 3, 4))
tensor2 = torch_randn(c(10, 4, 5))
torch_matmul(tensor1, tensor2)
# batched matrix x broadcasted matrix
tensor1 = torch_randn(c(10, 3, 4))
tensor2 = torch_randn(c(4, 5))
torch_matmul(tensor1, tensor2)
}
</code></pre>

<hr>
<h2 id='torch_matrix_exp'>Matrix_exp</h2><span id='topic+torch_matrix_exp'></span>

<h3>Description</h3>

<p>Matrix_exp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_matrix_exp(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_matrix_exp_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>matrix_power(input) -&gt; Tensor </h3>

<p>Returns the matrix exponential. Supports batched input.
For a matrix <code>A</code>, the matrix exponential is defined as
</p>
<p style="text-align: center;"><code class="reqn">
    \exp^A = \sum_{k=0}^\infty A^k / k!.
</code>
</p>

<p>The implementation is based on:
Bader, P.; Blanes, S.; Casas, F.
Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.
Mathematics 2019, 7, 1174.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(2, 2, 2))
a[1, , ] &lt;- torch_eye(2, 2)
a[2, , ] &lt;- 2 * torch_eye(2, 2)
a
torch_matrix_exp(a)

x &lt;- torch_tensor(rbind(c(0, pi/3), c(-pi/3, 0)))
x$matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]
}
</code></pre>

<hr>
<h2 id='torch_matrix_power'>Matrix_power</h2><span id='topic+torch_matrix_power'></span>

<h3>Description</h3>

<p>Matrix_power
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_matrix_power(self, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_matrix_power_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_matrix_power_+3A_n">n</code></td>
<td>
<p>(int) the power to raise the matrix to</p>
</td></tr>
</table>


<h3>matrix_power(input, n) -&gt; Tensor </h3>

<p>Returns the matrix raised to the power <code>n</code> for square matrices.
For batch of matrices, each individual matrix is raised to the power <code>n</code>.
</p>
<p>If <code>n</code> is negative, then the inverse of the matrix (if invertible) is
raised to the power <code>n</code>.  For a batch of matrices, the batched inverse
(if invertible) is raised to the power <code>n</code>. If <code>n</code> is 0, then an identity matrix
is returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(2, 2, 2))
a
torch_matrix_power(a, 3)
}
</code></pre>

<hr>
<h2 id='torch_matrix_rank'>Matrix_rank</h2><span id='topic+torch_matrix_rank'></span>

<h3>Description</h3>

<p>Matrix_rank
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_matrix_rank_+3A_self">self</code></td>
<td>
<p>(Tensor) the input 2-D tensor</p>
</td></tr>
<tr><td><code id="torch_matrix_rank_+3A_tol">tol</code></td>
<td>
<p>(float, optional) the tolerance value. Default: <code>NULL</code></p>
</td></tr>
<tr><td><code id="torch_matrix_rank_+3A_symmetric">symmetric</code></td>
<td>
<p>(bool, optional) indicates whether <code>input</code> is symmetric.                               Default: <code>FALSE</code></p>
</td></tr>
</table>


<h3>matrix_rank(input, tol=NULL, symmetric=False) -&gt; Tensor </h3>

<p>Returns the numerical rank of a 2-D tensor. The method to compute the
matrix rank is done using SVD by default. If <code>symmetric</code> is <code>TRUE</code>,
then <code>input</code> is assumed to be symmetric, and the computation of the
rank is done by obtaining the eigenvalues.
</p>
<p><code>tol</code> is the threshold below which the singular values (or the eigenvalues
when <code>symmetric</code> is <code>TRUE</code>) are considered to be 0. If <code>tol</code> is not
specified, <code>tol</code> is set to <code>S.max() * max(S.size()) * eps</code> where <code>S</code> is the
singular values (or the eigenvalues when <code>symmetric</code> is <code>TRUE</code>), and <code>eps</code>
is the epsilon value for the datatype of <code>input</code>.
</p>

<hr>
<h2 id='torch_max'>Max</h2><span id='topic+torch_max'></span>

<h3>Description</h3>

<p>Max
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_max_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_max_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_max_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_max_+3A_out">out</code></td>
<td>
<p>(tuple, optional) the result tuple of two output tensors (max, max_indices)</p>
</td></tr>
<tr><td><code id="torch_max_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>max(input) -&gt; Tensor </h3>

<p>Returns the maximum value of all elements in the <code>input</code> tensor.
</p>


<h3>max(input, dim, keepdim=False, out=NULL) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the maximum
value of each row of the <code>input</code> tensor in the given dimension
<code>dim</code>. And <code>indices</code> is the index location of each maximum value found
(argmax).
</p>


<h3>Warning</h3>

<p><code>indices</code> does not necessarily contain the first occurrence of each
maximal value found, unless it is unique.
The exact implementation details are device-specific.
Do not expect the same result when run on CPU and GPU in general.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size
as <code>input</code> except in the dimension <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting
in the output tensors having 1 fewer dimension than <code>input</code>.
</p>


<h3>max(input, other, out=NULL) -&gt; Tensor </h3>

<p>Each element of the tensor <code>input</code> is compared with the corresponding
element of the tensor <code>other</code> and an element-wise maximum is taken.
</p>
<p>The shapes of <code>input</code> and <code>other</code> don't need to match,
but they must be broadcastable .
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \max(\mbox{tensor}_i, \mbox{other}_i)
</code>
</p>



<h3>Note</h3>

<p>When the shapes do not match, the shape of the returned output tensor
follows the broadcasting rules .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_max(a)


a = torch_randn(c(4, 4))
a
torch_max(a, dim = 1)


a = torch_randn(c(4))
a
b = torch_randn(c(4))
b
torch_max(a, other = b)
}
</code></pre>

<hr>
<h2 id='torch_maximum'>Maximum</h2><span id='topic+torch_maximum'></span>

<h3>Description</h3>

<p>Maximum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_maximum(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_maximum_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_maximum_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>maximum(input, other, *, out=None) -&gt; Tensor </h3>

<p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.
</p>


<h3>Note</h3>

<p>If one of the elements being compared is a NaN, then that element is returned.
<code><a href="#topic+torch_maximum">torch_maximum()</a></code> is not supported for tensors with complex dtypes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2, -1))
b &lt;- torch_tensor(c(3, 0, 4))
torch_maximum(a, b)
}
</code></pre>

<hr>
<h2 id='torch_mean'>Mean</h2><span id='topic+torch_mean'></span>

<h3>Description</h3>

<p>Mean
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mean(self, dim, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mean_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_mean_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_mean_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_mean_+3A_dtype">dtype</code></td>
<td>
<p>the resulting data type.</p>
</td></tr>
</table>


<h3>mean(input) -&gt; Tensor </h3>

<p>Returns the mean value of all elements in the <code>input</code> tensor.
</p>


<h3>mean(input, dim, keepdim=False, out=NULL) -&gt; Tensor </h3>

<p>Returns the mean value of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>. If <code>dim</code> is a list of dimensions,
reduce over all of them.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_mean(a)


a = torch_randn(c(4, 4))
a
torch_mean(a, 1)
torch_mean(a, 1, TRUE)
}
</code></pre>

<hr>
<h2 id='torch_median'>Median</h2><span id='topic+torch_median'></span>

<h3>Description</h3>

<p>Median
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_median(self, dim, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_median_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_median_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_median_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>median(input) -&gt; Tensor </h3>

<p>Returns the median value of all elements in the <code>input</code> tensor.
</p>


<h3>median(input, dim=-1, keepdim=False, out=NULL) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the median
value of each row of the <code>input</code> tensor in the given dimension
<code>dim</code>. And <code>indices</code> is the index location of each median value found.
</p>
<p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size
as <code>input</code> except in the dimension <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in
the outputs tensor having 1 fewer dimension than <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_median(a)


a = torch_randn(c(4, 5))
a
torch_median(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_memory_format'>Memory format</h2><span id='topic+torch_memory_format'></span><span id='topic+torch_contiguous_format'></span><span id='topic+torch_preserve_format'></span><span id='topic+torch_channels_last_format'></span>

<h3>Description</h3>

<p>Returns the correspondent memory format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_contiguous_format()

torch_preserve_format()

torch_channels_last_format()
</code></pre>

<hr>
<h2 id='torch_meshgrid'>Meshgrid</h2><span id='topic+torch_meshgrid'></span>

<h3>Description</h3>

<p>Take <code class="reqn">N</code> tensors, each of which can be either scalar or 1-dimensional
vector, and create <code class="reqn">N</code> N-dimensional grids, where the <code class="reqn">i</code> <code>th</code> grid is defined by
expanding the <code class="reqn">i</code> <code>th</code> input over dimensions defined by other inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_meshgrid(tensors, indexing)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_meshgrid_+3A_tensors">tensors</code></td>
<td>
<p>(list of Tensor) list of scalars or 1 dimensional tensors. Scalars will be
treated (1,).</p>
</td></tr>
<tr><td><code id="torch_meshgrid_+3A_indexing">indexing</code></td>
<td>
<p>(str, optional): the indexing mode, either xy or ij, defaults to ij.
See warning for future changes.
If xy is selected, the first dimension corresponds to the cardinality of
the second input and the second dimension corresponds to the cardinality of the
first input.
If ij is selected, the dimensions are in the same order as the cardinality
of the inputs.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>In the future <code>torch_meshgrid</code> will transition to indexing=xy as the default.
This <a href="https://github.com/pytorch/pytorch/issues/50276">issue</a> tracks this issue
with the goal of migrating to NumPys behavior.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_tensor(c(1, 2, 3))
y = torch_tensor(c(4, 5, 6))
out = torch_meshgrid(list(x, y))
out
}
</code></pre>

<hr>
<h2 id='torch_min'>Min</h2><span id='topic+torch_min'></span>

<h3>Description</h3>

<p>Min
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_min_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_min_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_min_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_min_+3A_out">out</code></td>
<td>
<p>(tuple, optional) the tuple of two output tensors (min, min_indices)</p>
</td></tr>
<tr><td><code id="torch_min_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>min(input) -&gt; Tensor </h3>

<p>Returns the minimum value of all elements in the <code>input</code> tensor.
</p>


<h3>min(input, dim, keepdim=False, out=NULL) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the minimum
value of each row of the <code>input</code> tensor in the given dimension
<code>dim</code>. And <code>indices</code> is the index location of each minimum value found
(argmin).
</p>


<h3>Warning</h3>

<p><code>indices</code> does not necessarily contain the first occurrence of each
minimal value found, unless it is unique.
The exact implementation details are device-specific.
Do not expect the same result when run on CPU and GPU in general.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size as
<code>input</code> except in the dimension <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in
the output tensors having 1 fewer dimension than <code>input</code>.
</p>


<h3>min(input, other, out=NULL) -&gt; Tensor </h3>

<p>Each element of the tensor <code>input</code> is compared with the corresponding
element of the tensor <code>other</code> and an element-wise minimum is taken.
The resulting tensor is returned.
</p>
<p>The shapes of <code>input</code> and <code>other</code> don't need to match,
but they must be broadcastable .
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \min(\mbox{tensor}_i, \mbox{other}_i)
</code>
</p>



<h3>Note</h3>

<p>When the shapes do not match, the shape of the returned output tensor
follows the broadcasting rules .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_min(a)


a = torch_randn(c(4, 4))
a
torch_min(a, dim = 1)


a = torch_randn(c(4))
a
b = torch_randn(c(4))
b
torch_min(a, other = b)
}
</code></pre>

<hr>
<h2 id='torch_minimum'>Minimum</h2><span id='topic+torch_minimum'></span>

<h3>Description</h3>

<p>Minimum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_minimum(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_minimum_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_minimum_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>minimum(input, other, *, out=None) -&gt; Tensor </h3>

<p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.
</p>


<h3>Note</h3>

<p>If one of the elements being compared is a NaN, then that element is returned.
<code><a href="#topic+torch_minimum">torch_minimum()</a></code> is not supported for tensors with complex dtypes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2, -1))
b &lt;- torch_tensor(c(3, 0, 4))
torch_minimum(a, b)
}
</code></pre>

<hr>
<h2 id='torch_mm'>Mm</h2><span id='topic+torch_mm'></span>

<h3>Description</h3>

<p>Mm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mm(self, mat2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mm_+3A_self">self</code></td>
<td>
<p>(Tensor) the first matrix to be multiplied</p>
</td></tr>
<tr><td><code id="torch_mm_+3A_mat2">mat2</code></td>
<td>
<p>(Tensor) the second matrix to be multiplied</p>
</td></tr>
</table>


<h3>mm(input, mat2, out=NULL) -&gt; Tensor </h3>

<p>Performs a matrix multiplication of the matrices <code>input</code> and <code>mat2</code>.
</p>
<p>If <code>input</code> is a <code class="reqn">(n \times m)</code> tensor, <code>mat2</code> is a
<code class="reqn">(m \times p)</code> tensor, <code>out</code> will be a <code class="reqn">(n \times p)</code> tensor.
</p>


<h3>Note</h3>

<p>This function does not broadcast .
For broadcasting matrix products, see <code><a href="#topic+torch_matmul">torch_matmul</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

mat1 = torch_randn(c(2, 3))
mat2 = torch_randn(c(3, 3))
torch_mm(mat1, mat2)
}
</code></pre>

<hr>
<h2 id='torch_mode'>Mode</h2><span id='topic+torch_mode'></span>

<h3>Description</h3>

<p>Mode
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mode(self, dim = -1L, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mode_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_mode_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_mode_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>mode(input, dim=-1, keepdim=False, out=NULL) -&gt; (Tensor, LongTensor) </h3>

<p>Returns a namedtuple <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> where <code>values</code> is the mode
value of each row of the <code>input</code> tensor in the given dimension
<code>dim</code>, i.e. a value which appears most often
in that row, and <code>indices</code> is the index location of each mode value found.
</p>
<p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size as
<code>input</code> except in the dimension <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting
in the output tensors having 1 fewer dimension than <code>input</code>.
</p>


<h3>Note</h3>

<p>This function is not defined for <code>torch_cuda.Tensor</code> yet.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randint(0, 50, size = list(5))
a
torch_mode(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_movedim'>Movedim</h2><span id='topic+torch_movedim'></span>

<h3>Description</h3>

<p>Movedim
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_movedim(self, source, destination)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_movedim_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_movedim_+3A_source">source</code></td>
<td>
<p>(int or tuple of ints) Original positions of the dims to move. These must be unique.</p>
</td></tr>
<tr><td><code id="torch_movedim_+3A_destination">destination</code></td>
<td>
<p>(int or tuple of ints) Destination positions for each of the original dims. These must also be unique.</p>
</td></tr>
</table>


<h3>movedim(input, source, destination) -&gt; Tensor </h3>

<p>Moves the dimension(s) of <code>input</code> at the position(s) in <code>source</code>
to the position(s) in <code>destination</code>.
</p>
<p>Other dimensions of <code>input</code> that are not explicitly moved remain in
their original order and appear at the positions not specified in <code>destination</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t &lt;- torch_randn(c(3,2,1))
t
torch_movedim(t, 2, 1)$shape
torch_movedim(t, 2, 1)
torch_movedim(t, c(2, 3), c(1, 2))$shape
torch_movedim(t, c(2, 3), c(1, 2))
}
</code></pre>

<hr>
<h2 id='torch_mul'>Mul</h2><span id='topic+torch_mul'></span>

<h3>Description</h3>

<p>Mul
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mul(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mul_+3A_self">self</code></td>
<td>
<p>(Tensor) the first multiplicand tensor</p>
</td></tr>
<tr><td><code id="torch_mul_+3A_other">other</code></td>
<td>
<p>(Tensor) the second multiplicand tensor</p>
</td></tr>
</table>


<h3>mul(input, other, out=NULL) </h3>

<p>Multiplies each element of the input <code>input</code> with the scalar
<code>other</code> and returns a new resulting tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{other} \times \mbox{input}_i
</code>
</p>

<p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>other</code>
should be a real number, otherwise it should be an integer
</p>
<p>Each element of the tensor <code>input</code> is multiplied by the corresponding
element of the Tensor <code>other</code>. The resulting tensor is returned.
</p>
<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{input}_i \times \mbox{other}_i
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3))
a
torch_mul(a, 100)


a = torch_randn(c(4, 1))
a
b = torch_randn(c(1, 4))
b
torch_mul(a, b)
}
</code></pre>

<hr>
<h2 id='torch_multinomial'>Multinomial</h2><span id='topic+torch_multinomial'></span>

<h3>Description</h3>

<p>Multinomial
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_multinomial(self, num_samples, replacement = FALSE, generator = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_multinomial_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor containing probabilities</p>
</td></tr>
<tr><td><code id="torch_multinomial_+3A_num_samples">num_samples</code></td>
<td>
<p>(int) number of samples to draw</p>
</td></tr>
<tr><td><code id="torch_multinomial_+3A_replacement">replacement</code></td>
<td>
<p>(bool, optional) whether to draw with replacement or not</p>
</td></tr>
<tr><td><code id="torch_multinomial_+3A_generator">generator</code></td>
<td>
<p>(<code>torch.Generator</code>, optional) a pseudorandom number generator for sampling</p>
</td></tr>
</table>


<h3>multinomial(input, num_samples, replacement=False, *, generator=NULL, out=NULL) -&gt; LongTensor </h3>

<p>Returns a tensor where each row contains <code>num_samples</code> indices sampled
from the multinomial probability distribution located in the corresponding row
of tensor <code>input</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>The rows of `input` do not need to sum to one (in which case we use
the values as weights), but must be non-negative, finite and have
a non-zero sum.
</pre></div>
<p>Indices are ordered from left to right according to when each was sampled
(first samples are placed in first column).
</p>
<p>If <code>input</code> is a vector, <code>out</code> is a vector of size <code>num_samples</code>.
</p>
<p>If <code>input</code> is a matrix with <code>m</code> rows, <code>out</code> is an matrix of shape
<code class="reqn">(m \times \mbox{num\_samples})</code>.
</p>
<p>If replacement is <code>TRUE</code>, samples are drawn with replacement.
</p>
<p>If not, they are drawn without replacement, which means that when a
sample index is drawn for a row, it cannot be drawn again for that row.
</p>
<div class="sourceCode"><pre>When drawn without replacement, `num_samples` must be lower than
number of non-zero elements in `input` (or the min number of non-zero
elements in each row of `input` if it is a matrix).
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

weights = torch_tensor(c(0, 10, 3, 0), dtype=torch_float()) # create a tensor of weights
torch_multinomial(weights, 2)
torch_multinomial(weights, 4, replacement=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_multiply'>Multiply</h2><span id='topic+torch_multiply'></span>

<h3>Description</h3>

<p>Multiply
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_multiply(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_multiply_+3A_self">self</code></td>
<td>
<p>(Tensor) the first multiplicand tensor</p>
</td></tr>
<tr><td><code id="torch_multiply_+3A_other">other</code></td>
<td>
<p>(Tensor) the second multiplicand tensor</p>
</td></tr>
</table>


<h3>multiply(input, other, *, out=None) </h3>

<p>Alias for <code><a href="#topic+torch_mul">torch_mul()</a></code>.
</p>

<hr>
<h2 id='torch_mv'>Mv</h2><span id='topic+torch_mv'></span>

<h3>Description</h3>

<p>Mv
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mv(self, vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mv_+3A_self">self</code></td>
<td>
<p>(Tensor) matrix to be multiplied</p>
</td></tr>
<tr><td><code id="torch_mv_+3A_vec">vec</code></td>
<td>
<p>(Tensor) vector to be multiplied</p>
</td></tr>
</table>


<h3>mv(input, vec, out=NULL) -&gt; Tensor </h3>

<p>Performs a matrix-vector product of the matrix <code>input</code> and the vector
<code>vec</code>.
</p>
<p>If <code>input</code> is a <code class="reqn">(n \times m)</code> tensor, <code>vec</code> is a 1-D tensor of
size <code class="reqn">m</code>, <code>out</code> will be 1-D of size <code class="reqn">n</code>.
</p>


<h3>Note</h3>

<p>This function does not broadcast .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

mat = torch_randn(c(2, 3))
vec = torch_randn(c(3))
torch_mv(mat, vec)
}
</code></pre>

<hr>
<h2 id='torch_mvlgamma'>Mvlgamma</h2><span id='topic+torch_mvlgamma'></span>

<h3>Description</h3>

<p>Mvlgamma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_mvlgamma(self, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_mvlgamma_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compute the multivariate log-gamma function</p>
</td></tr>
<tr><td><code id="torch_mvlgamma_+3A_p">p</code></td>
<td>
<p>(int) the number of dimensions</p>
</td></tr>
</table>


<h3>mvlgamma(input, p) -&gt; Tensor </h3>

<p>Computes the <code style="white-space: pre;">&#8288;multivariate log-gamma function &lt;https://en.wikipedia.org/wiki/Multivariate_gamma_function&gt;&#8288;</code>_) with dimension
<code class="reqn">p</code> element-wise, given by
</p>
<p style="text-align: center;"><code class="reqn">
    \log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)
</code>
</p>

<p>where <code class="reqn">C = \log(\pi) \times \frac{p (p - 1)}{4}</code> and <code class="reqn">\Gamma(\cdot)</code> is the Gamma function.
</p>
<p>All elements must be greater than <code class="reqn">\frac{p - 1}{2}</code>, otherwise an error would be thrown.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_empty(c(2, 3))$uniform_(1, 2)
a
torch_mvlgamma(a, 2)
}
</code></pre>

<hr>
<h2 id='torch_nanquantile'>Nanquantile</h2><span id='topic+torch_nanquantile'></span>

<h3>Description</h3>

<p>Nanquantile
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_nanquantile(
  self,
  q,
  dim = NULL,
  keepdim = FALSE,
  interpolation = "linear"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_nanquantile_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_nanquantile_+3A_q">q</code></td>
<td>
<p>(float or Tensor) a scalar or 1D tensor of quantile values in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></p>
</td></tr>
<tr><td><code id="torch_nanquantile_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_nanquantile_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_nanquantile_+3A_interpolation">interpolation</code></td>
<td>
<p>The interpolation method.</p>
</td></tr>
</table>


<h3>nanquantile(input, q, dim=None, keepdim=FALSE, *, out=None) -&gt; Tensor </h3>

<p>This is a variant of <code><a href="#topic+torch_quantile">torch_quantile()</a></code> that &quot;ignores&quot; <code>NaN</code> values,
computing the quantiles <code>q</code> as if <code>NaN</code> values in <code>input</code> did
not exist. If all values in a reduced row are <code>NaN</code> then the quantiles for
that reduction will be <code>NaN</code>. See the documentation for <code><a href="#topic+torch_quantile">torch_quantile()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

t &lt;- torch_tensor(c(NaN, 1, 2))
t$quantile(0.5)
t$nanquantile(0.5)
t &lt;- torch_tensor(rbind(c(NaN, NaN), c(1, 2)))
t
t$nanquantile(0.5, dim=1)
t$nanquantile(0.5, dim=2)
torch_nanquantile(t, 0.5, dim = 1)
torch_nanquantile(t, 0.5, dim = 2)
}
</code></pre>

<hr>
<h2 id='torch_nansum'>Nansum</h2><span id='topic+torch_nansum'></span>

<h3>Description</h3>

<p>Nansum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_nansum(self, dim = NULL, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_nansum_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_nansum_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_nansum_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_nansum_+3A_dtype">dtype</code></td>
<td>
<p>the desired data type of returned tensor. If specified, the
input tensor is casted to dtype before the operation is performed. This is
useful for preventing data type overflows. Default: <code>NULL</code>.</p>
</td></tr>
</table>


<h3>nansum(input, *, dtype=None) -&gt; Tensor </h3>

<p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.
</p>


<h3>nansum(input, dim, keepdim=FALSE, *, dtype=None) -&gt; Tensor </h3>

<p>Returns the sum of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>, treating Not a Numbers (NaNs) as zero.
If <code>dim</code> is a list of dimensions, reduce over all of them.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1., 2., NaN, 4.))
torch_nansum(a)


torch_nansum(torch_tensor(c(1., NaN)))
a &lt;- torch_tensor(rbind(c(1, 2), c(3., NaN)))
torch_nansum(a)
torch_nansum(a, dim=1)
torch_nansum(a, dim=2)
}
</code></pre>

<hr>
<h2 id='torch_narrow'>Narrow</h2><span id='topic+torch_narrow'></span>

<h3>Description</h3>

<p>Narrow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_narrow(self, dim, start, length)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_narrow_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to narrow</p>
</td></tr>
<tr><td><code id="torch_narrow_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension along which to narrow</p>
</td></tr>
<tr><td><code id="torch_narrow_+3A_start">start</code></td>
<td>
<p>(int) the starting dimension</p>
</td></tr>
<tr><td><code id="torch_narrow_+3A_length">length</code></td>
<td>
<p>(int) the distance to the ending dimension</p>
</td></tr>
</table>


<h3>narrow(input, dim, start, length) -&gt; Tensor </h3>

<p>Returns a new tensor that is a narrowed version of <code>input</code> tensor. The
dimension <code>dim</code> is input from <code>start</code> to <code>start + length</code>. The
returned tensor and <code>input</code> tensor share the same underlying storage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_tensor(matrix(c(1:9), ncol = 3, byrow= TRUE))
torch_narrow(x, 1, 1, 2)
torch_narrow(x, 2, 2, 2)
}
</code></pre>

<hr>
<h2 id='torch_ne'>Ne</h2><span id='topic+torch_ne'></span>

<h3>Description</h3>

<p>Ne
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ne(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ne_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_ne_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>ne(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes <code class="reqn">input \neq other</code> element-wise.
</p>
<p>The second argument can be a number or a tensor whose shape is
broadcastable  with the first argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_ne(torch_tensor(matrix(1:4, ncol = 2, byrow=TRUE)), 
         torch_tensor(matrix(rep(c(1,4), each = 2), ncol = 2, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_neg'>Neg</h2><span id='topic+torch_neg'></span>

<h3>Description</h3>

<p>Neg
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_neg(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_neg_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>neg(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the negative of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = -1 \times \mbox{input}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(5))
a
torch_neg(a)
}
</code></pre>

<hr>
<h2 id='torch_negative'>Negative</h2><span id='topic+torch_negative'></span>

<h3>Description</h3>

<p>Negative
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_negative(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_negative_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>negative(input, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_neg">torch_neg()</a></code>
</p>

<hr>
<h2 id='torch_nextafter'>Nextafter</h2><span id='topic+torch_nextafter'></span>

<h3>Description</h3>

<p>Nextafter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_nextafter(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_nextafter_+3A_self">self</code></td>
<td>
<p>(Tensor) the first input tensor</p>
</td></tr>
<tr><td><code id="torch_nextafter_+3A_other">other</code></td>
<td>
<p>(Tensor) the second input tensor</p>
</td></tr>
</table>


<h3>nextafter(input, other, *, out=None) -&gt; Tensor </h3>

<p>Return the next floating-point value after <code>input</code> towards <code>other</code>, elementwise.
</p>
<p>The shapes of <code>input</code> and <code>other</code> must be
broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

eps &lt;- torch_finfo(torch_float32())$eps
torch_nextafter(torch_tensor(c(1, 2)), torch_tensor(c(2, 1))) == torch_tensor(c(eps + 1, 2 - eps))
}
</code></pre>

<hr>
<h2 id='torch_nonzero'>Nonzero</h2><span id='topic+torch_nonzero'></span>

<h3>Description</h3>

<p>Nonzero elements of tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_nonzero(self, as_list = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_nonzero_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_nonzero_+3A_as_list">as_list</code></td>
<td>
<p>If <code>FALSE</code>, the output tensor containing indices. If <code>TRUE</code>, one
1-D tensor for each dimension, containing the indices of each nonzero element
along that dimension.
</p>
<p><strong>When</strong> <code>as_list</code> <strong>is <code>FALSE</code> (default)</strong>:
</p>
<p>Returns a tensor containing the indices of all non-zero elements of
<code>input</code>.  Each row in the result contains the indices of a non-zero
element in <code>input</code>. The result is sorted lexicographically, with
the last index changing the fastest (C-style).
</p>
<p>If <code>input</code> has <code class="reqn">n</code> dimensions, then the resulting indices tensor
<code>out</code> is of size <code class="reqn">(z \times n)</code>, where <code class="reqn">z</code> is the total number of
non-zero elements in the <code>input</code> tensor.
</p>
<p><strong>When</strong> <code>as_list</code> <strong>is <code>TRUE</code></strong>:
</p>
<p>Returns a tuple of 1-D tensors, one for each dimension in <code>input</code>,
each containing the indices (in that dimension) of all non-zero elements of
<code>input</code> .
</p>
<p>If <code>input</code> has <code class="reqn">n</code> dimensions, then the resulting tuple contains <code class="reqn">n</code>
tensors of size <code class="reqn">z</code>, where <code class="reqn">z</code> is the total number of
non-zero elements in the <code>input</code> tensor.
</p>
<p>As a special case, when <code>input</code> has zero dimensions and a nonzero scalar
value, it is treated as a one-dimensional tensor with one element.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_nonzero(torch_tensor(c(1, 1, 1, 0, 1)))
}
</code></pre>

<hr>
<h2 id='torch_norm'>Norm</h2><span id='topic+torch_norm'></span>

<h3>Description</h3>

<p>Norm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_norm(self, p = 2L, dim, keepdim = FALSE, dtype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_norm_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_norm_+3A_p">p</code></td>
<td>
<p>(int, float, inf, -inf, 'fro', 'nuc', optional) the order of norm. Default: <code>'fro'</code>        The following norms can be calculated:        =====  ============================  ==========================        ord    matrix norm                   vector norm        =====  ============================  ==========================        NULL   Frobenius norm                2-norm        'fro'  Frobenius norm                &ndash;        'nuc'  nuclear norm                  &ndash;        Other  as vec norm when dim is NULL  sum(abs(x)<strong>ord)</strong>(1./ord)        =====  ============================  ==========================</p>
</td></tr>
<tr><td><code id="torch_norm_+3A_dim">dim</code></td>
<td>
<p>(int, 2-tuple of ints, 2-list of ints, optional) If it is an int,        vector norm will be calculated, if it is 2-tuple of ints, matrix norm        will be calculated. If the value is NULL, matrix norm will be calculated        when the input tensor only has two dimensions, vector norm will be        calculated when the input tensor only has one dimension. If the input        tensor has more than two dimensions, the vector norm will be applied to        last dimension.</p>
</td></tr>
<tr><td><code id="torch_norm_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool, optional) whether the output tensors have <code>dim</code>        retained or not. Ignored if <code>dim</code> = <code>NULL</code> and        <code>out</code> = <code>NULL</code>. Default: <code>FALSE</code>
Ignored if        <code>dim</code> = <code>NULL</code> and <code>out</code> = <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="torch_norm_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of        returned tensor. If specified, the input tensor is casted to        'dtype' while performing the operation. Default: NULL.</p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Returns the matrix norm or vector norm of a given tensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_arange(1, 9, dtype = torch_float())
b &lt;- a$reshape(list(3, 3))
torch_norm(a)
torch_norm(b)
torch_norm(a, Inf)
torch_norm(b, Inf)

}
</code></pre>

<hr>
<h2 id='torch_normal'>Normal</h2><span id='topic+torch_normal'></span>

<h3>Description</h3>

<p>Normal
</p>
<p>Normal distributed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_normal(mean, std, size = NULL, generator = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_normal_+3A_mean">mean</code></td>
<td>
<p>(tensor or scalar double) Mean of the normal distribution.
If this is a <code><a href="#topic+torch_tensor">torch_tensor()</a></code> then the output has the same dim as <code>mean</code>
and it represents the per-element mean. If it's a scalar value, it's reused
for all elements.</p>
</td></tr>
<tr><td><code id="torch_normal_+3A_std">std</code></td>
<td>
<p>(tensor or scalar double) The standard deviation of the normal
distribution. If this is a <code><a href="#topic+torch_tensor">torch_tensor()</a></code> then the output has the same size as <code>std</code>
and it represents the per-element standard deviation. If it's a scalar value,
it's reused for all elements.</p>
</td></tr>
<tr><td><code id="torch_normal_+3A_size">size</code></td>
<td>
<p>(integers, optional) only used if both <code>mean</code> and <code>std</code> are scalars.</p>
</td></tr>
<tr><td><code id="torch_normal_+3A_generator">generator</code></td>
<td>
<p>a random number generator created with <code><a href="#topic+torch_generator">torch_generator()</a></code>. If <code>NULL</code>
a default generator is used.</p>
</td></tr>
<tr><td><code id="torch_normal_+3A_...">...</code></td>
<td>
<p>Tensor option parameters like <code>dtype</code>, <code>layout</code>, and <code>device</code>.
Can only be used when <code>mean</code> and <code>std</code> are both scalar numerics.</p>
</td></tr>
</table>


<h3>normal(mean, std, *) -&gt; Tensor </h3>

<p>Returns a tensor of random numbers drawn from separate normal distributions
whose mean and standard deviation are given.
</p>
<p>The <code>mean</code> is a tensor with the mean of
each output element's normal distribution
</p>
<p>The <code>std</code> is a tensor with the standard deviation of
each output element's normal distribution
</p>
<p>The shapes of <code>mean</code> and <code>std</code> don't need to match, but the
total number of elements in each tensor need to be the same.
</p>


<h3>normal(mean=0.0, std) -&gt; Tensor </h3>

<p>Similar to the function above, but the means are shared among all drawn
elements.
</p>


<h3>normal(mean, std=1.0) -&gt; Tensor </h3>

<p>Similar to the function above, but the standard-deviations are shared among
all drawn elements.
</p>


<h3>normal(mean, std, size, *) -&gt; Tensor </h3>

<p>Similar to the function above, but the means and standard deviations are shared
among all drawn elements. The resulting tensor has size given by <code>size</code>.
</p>


<h3>Note</h3>

<p>When the shapes do not match, the shape of <code>mean</code>
is used as the shape for the returned output tensor
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_normal(mean=0, std=torch_arange(1, 0, -0.1) + 1e-6)
torch_normal(mean=0.5, std=torch_arange(1., 6.))
torch_normal(mean=torch_arange(1., 6.))
torch_normal(2, 3, size=c(1, 4))

}
</code></pre>

<hr>
<h2 id='torch_not_equal'>Not_equal</h2><span id='topic+torch_not_equal'></span>

<h3>Description</h3>

<p>Not_equal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_not_equal(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_not_equal_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to compare</p>
</td></tr>
<tr><td><code id="torch_not_equal_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the tensor or value to compare</p>
</td></tr>
</table>


<h3>not_equal(input, other, *, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_ne">torch_ne()</a></code>.
</p>

<hr>
<h2 id='torch_ones'>Ones</h2><span id='topic+torch_ones'></span>

<h3>Description</h3>

<p>Ones
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ones(
  ...,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ones_+3A_...">...</code></td>
<td>
<p>(int...) a sequence of integers defining the shape of the output tensor.        Can be a variable number of arguments or a collection like a list or tuple.</p>
</td></tr>
<tr><td><code id="torch_ones_+3A_names">names</code></td>
<td>
<p>optional names for the dimensions</p>
</td></tr>
<tr><td><code id="torch_ones_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_ones_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_ones_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_ones_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>ones(*size, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with the scalar value <code>1</code>, with the shape defined
by the variable argument <code>size</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_ones(c(2, 3))
torch_ones(c(5))
}
</code></pre>

<hr>
<h2 id='torch_ones_like'>Ones_like</h2><span id='topic+torch_ones_like'></span>

<h3>Description</h3>

<p>Ones_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ones_like(
  input,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ones_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_ones_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_ones_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_ones_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_ones_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_ones_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>ones_like(input, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns a tensor filled with the scalar value <code>1</code>, with the same size as
<code>input</code>. <code>torch_ones_like(input)</code> is equivalent to
<code>torch_ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>


<h3>Warning</h3>

<p>As of 0.4, this function does not support an <code>out</code> keyword. As an alternative,
the old <code>torch_ones_like(input, out=output)</code> is equivalent to
<code>torch_ones(input.size(), out=output)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_empty(c(2, 3))
torch_ones_like(input)
}
</code></pre>

<hr>
<h2 id='torch_orgqr'>Orgqr</h2><span id='topic+torch_orgqr'></span>

<h3>Description</h3>

<p>Orgqr
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_orgqr(self, input2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_orgqr_+3A_self">self</code></td>
<td>
<p>(Tensor) the <code>a</code> from <code><a href="#topic+torch_geqrf">torch_geqrf</a></code>.</p>
</td></tr>
<tr><td><code id="torch_orgqr_+3A_input2">input2</code></td>
<td>
<p>(Tensor) the <code>tau</code> from <code><a href="#topic+torch_geqrf">torch_geqrf</a></code>.</p>
</td></tr>
</table>


<h3>orgqr(input, input2) -&gt; Tensor </h3>

<p>Computes the orthogonal matrix <code>Q</code> of a QR factorization, from the <code style="white-space: pre;">&#8288;(input, input2)&#8288;</code>
tuple returned by <code><a href="#topic+torch_geqrf">torch_geqrf</a></code>.
</p>
<p>This directly calls the underlying LAPACK function <code>?orgqr</code>.
See <code style="white-space: pre;">&#8288;LAPACK documentation for orgqr&#8288;</code>_ for further details.
</p>

<hr>
<h2 id='torch_ormqr'>Ormqr</h2><span id='topic+torch_ormqr'></span>

<h3>Description</h3>

<p>Ormqr
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_ormqr(self, input2, input3, left = TRUE, transpose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_ormqr_+3A_self">self</code></td>
<td>
<p>(Tensor) the <code>a</code> from <code><a href="#topic+torch_geqrf">torch_geqrf</a></code>.</p>
</td></tr>
<tr><td><code id="torch_ormqr_+3A_input2">input2</code></td>
<td>
<p>(Tensor) the <code>tau</code> from <code><a href="#topic+torch_geqrf">torch_geqrf</a></code>.</p>
</td></tr>
<tr><td><code id="torch_ormqr_+3A_input3">input3</code></td>
<td>
<p>(Tensor) the matrix to be multiplied.</p>
</td></tr>
<tr><td><code id="torch_ormqr_+3A_left">left</code></td>
<td>
<p>see LAPACK documentation</p>
</td></tr>
<tr><td><code id="torch_ormqr_+3A_transpose">transpose</code></td>
<td>
<p>see LAPACK documentation</p>
</td></tr>
</table>


<h3>ormqr(input, input2, input3, left=TRUE, transpose=False) -&gt; Tensor </h3>

<p>Multiplies <code>mat</code> (given by <code>input3</code>) by the orthogonal <code>Q</code> matrix of the QR factorization
formed by <code><a href="#topic+torch_geqrf">torch_geqrf()</a></code> that is represented by <code style="white-space: pre;">&#8288;(a, tau)&#8288;</code> (given by (<code>input</code>, <code>input2</code>)).
</p>
<p>This directly calls the underlying LAPACK function <code>?ormqr</code>.
</p>

<hr>
<h2 id='torch_outer'>Outer</h2><span id='topic+torch_outer'></span>

<h3>Description</h3>

<p>Outer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_outer(self, vec2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_outer_+3A_self">self</code></td>
<td>
<p>(Tensor) 1-D input vector</p>
</td></tr>
<tr><td><code id="torch_outer_+3A_vec2">vec2</code></td>
<td>
<p>(Tensor) 1-D input vector</p>
</td></tr>
</table>


<h3>outer(input, vec2, *, out=None) -&gt; Tensor </h3>

<p>Outer product of <code>input</code> and <code>vec2</code>.
If <code>input</code> is a vector of size <code class="reqn">n</code> and <code>vec2</code> is a vector of
size <code class="reqn">m</code>, then <code>out</code> must be a matrix of size <code class="reqn">(n \times m)</code>.
</p>


<h3>Note</h3>

<p>This function does not broadcast.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

v1 &lt;- torch_arange(1., 5.)
v2 &lt;- torch_arange(1., 4.)
torch_outer(v1, v2)
}
</code></pre>

<hr>
<h2 id='torch_pdist'>Pdist</h2><span id='topic+torch_pdist'></span>

<h3>Description</h3>

<p>Pdist
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_pdist(self, p = 2L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_pdist_+3A_self">self</code></td>
<td>
<p>NA input tensor of shape <code class="reqn">N \times M</code>.</p>
</td></tr>
<tr><td><code id="torch_pdist_+3A_p">p</code></td>
<td>
<p>NA p value for the p-norm distance to calculate between each vector pair        <code class="reqn">\in [0, \infty]</code>.</p>
</td></tr>
</table>


<h3>pdist(input, p=2) -&gt; Tensor </h3>

<p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<code style="white-space: pre;">&#8288;torch_norm(input[:, NULL] - input, dim=2, p=p)&#8288;</code>. This function will be faster
if the rows are contiguous.
</p>
<p>If input has shape <code class="reqn">N \times M</code> then the output will have shape
<code class="reqn">\frac{1}{2} N (N - 1)</code>.
</p>
<p>This function is equivalent to <code>scipy.spatial.distance.pdist(input, 'minkowski', p=p)</code> if <code class="reqn">p \in (0, \infty)</code>. When <code class="reqn">p = 0</code> it is
equivalent to <code>scipy.spatial.distance.pdist(input, 'hamming') * M</code>.
When <code class="reqn">p = \infty</code>, the closest scipy function is
<code style="white-space: pre;">&#8288;scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())&#8288;</code>.
</p>

<hr>
<h2 id='torch_pinverse'>Pinverse</h2><span id='topic+torch_pinverse'></span>

<h3>Description</h3>

<p>Pinverse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_pinverse(self, rcond = 1e-15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_pinverse_+3A_self">self</code></td>
<td>
<p>(Tensor) The input tensor of size <code class="reqn">(*, m, n)</code> where <code class="reqn">*</code> is zero or more batch dimensions</p>
</td></tr>
<tr><td><code id="torch_pinverse_+3A_rcond">rcond</code></td>
<td>
<p>(float) A floating point value to determine the cutoff for small singular values.                   Default: 1e-15</p>
</td></tr>
</table>


<h3>pinverse(input, rcond=1e-15) -&gt; Tensor </h3>

<p>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.
Please look at <code style="white-space: pre;">&#8288;Moore-Penrose inverse&#8288;</code>_ for more details
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>This method is implemented using the Singular Value Decomposition.
</pre></div>
<div class="sourceCode"><pre>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix `[1]`_.
Therefore, derivatives are not always existent, and exist for a constant rank only `[2]`_.
However, this method is backprop-able due to the implementation by using SVD results, and
could be unstable. Double-backward will also be unstable due to the usage of SVD internally.
See `~torch.svd` for more details.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_randn(c(3, 5))
input
torch_pinverse(input)
# Batched pinverse example
a = torch_randn(c(2,6,3))
b = torch_pinverse(a)
torch_matmul(b, a)
}
</code></pre>

<hr>
<h2 id='torch_pixel_shuffle'>Pixel_shuffle</h2><span id='topic+torch_pixel_shuffle'></span>

<h3>Description</h3>

<p>Pixel_shuffle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_pixel_shuffle(self, upscale_factor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_pixel_shuffle_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_pixel_shuffle_+3A_upscale_factor">upscale_factor</code></td>
<td>
<p>(int) factor to increase spatial resolution by</p>
</td></tr>
</table>


<h3>Rearranges elements in a tensor of shape </h3>

<p>math:<code style="white-space: pre;">&#8288;(*, C \times r^2, H, W)&#8288;</code> to a :
</p>
<p>Rearranges elements in a tensor of shape <code class="reqn">(*, C \times r^2, H, W)</code> to a
tensor of shape <code class="reqn">(*, C, H \times r, W \times r)</code>.
</p>
<p>See <code>~torch.nn.PixelShuffle</code> for details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_randn(c(1, 9, 4, 4))
output = nnf_pixel_shuffle(input, 3)
print(output$size())
}
</code></pre>

<hr>
<h2 id='torch_poisson'>Poisson</h2><span id='topic+torch_poisson'></span>

<h3>Description</h3>

<p>Poisson
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_poisson(self, generator = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_poisson_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor containing the rates of the Poisson distribution</p>
</td></tr>
<tr><td><code id="torch_poisson_+3A_generator">generator</code></td>
<td>
<p>(<code>torch.Generator</code>, optional) a pseudorandom number generator for sampling</p>
</td></tr>
</table>


<h3>poisson(input *, generator=NULL) -&gt; Tensor </h3>

<p>Returns a tensor of the same size as <code>input</code> with each element
sampled from a Poisson distribution with rate parameter given by the corresponding
element in <code>input</code> i.e.,
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i \sim \mbox{Poisson}(\mbox{input}_i)
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

rates = torch_rand(c(4, 4)) * 5  # rate parameter between 0 and 5
torch_poisson(rates)
}
</code></pre>

<hr>
<h2 id='torch_polar'>Polar</h2><span id='topic+torch_polar'></span>

<h3>Description</h3>

<p>Polar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_polar(abs, angle)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_polar_+3A_abs">abs</code></td>
<td>
<p>(Tensor) The absolute value the complex tensor. Must be float or
double.</p>
</td></tr>
<tr><td><code id="torch_polar_+3A_angle">angle</code></td>
<td>
<p>(Tensor) The angle of the complex tensor. Must be same dtype as
<code>abs</code>.</p>
</td></tr>
</table>


<h3>polar(abs, angle, *, out=None) -&gt; Tensor </h3>

<p>Constructs a complex tensor whose elements are Cartesian coordinates
corresponding to the polar coordinates with absolute value <code>abs</code> and angle
<code>angle</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out} = \mbox{abs} \cdot \cos(\mbox{angle}) + \mbox{abs} \cdot \sin(\mbox{angle}) \cdot j
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

abs &lt;- torch_tensor(c(1, 2), dtype=torch_float64())
angle &lt;- torch_tensor(c(pi / 2, 5 * pi / 4), dtype=torch_float64())
z &lt;- torch_polar(abs, angle)
z
}
</code></pre>

<hr>
<h2 id='torch_polygamma'>Polygamma</h2><span id='topic+torch_polygamma'></span>

<h3>Description</h3>

<p>Polygamma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_polygamma(n, input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_polygamma_+3A_n">n</code></td>
<td>
<p>(int) the order of the polygamma function</p>
</td></tr>
<tr><td><code id="torch_polygamma_+3A_input">input</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>polygamma(n, input, out=NULL) -&gt; Tensor </h3>

<p>Computes the <code class="reqn">n^{th}</code> derivative of the digamma function on <code>input</code>.
<code class="reqn">n \geq 0</code> is called the order of the polygamma function.
</p>
<p style="text-align: center;"><code class="reqn">
    \psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)
</code>
</p>



<h3>Note</h3>

<div class="sourceCode"><pre>This function is not implemented for \eqn{n \geq 2}.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
a = torch_tensor(c(1, 0.5))
torch_polygamma(1, a)

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_pow'>Pow</h2><span id='topic+torch_pow'></span>

<h3>Description</h3>

<p>Pow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_pow(self, exponent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_pow_+3A_self">self</code></td>
<td>
<p>(float) the scalar base value for the power operation</p>
</td></tr>
<tr><td><code id="torch_pow_+3A_exponent">exponent</code></td>
<td>
<p>(float or tensor) the exponent value</p>
</td></tr>
</table>


<h3>pow(input, exponent, out=NULL) -&gt; Tensor </h3>

<p>Takes the power of each element in <code>input</code> with <code>exponent</code> and
returns a tensor with the result.
</p>
<p><code>exponent</code> can be either a single <code>float</code> number or a <code>Tensor</code>
with the same number of elements as <code>input</code>.
</p>
<p>When <code>exponent</code> is a scalar value, the operation applied is:
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = x_i^{\mbox{exponent}}
</code>
</p>

<p>When <code>exponent</code> is a tensor, the operation applied is:
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = x_i^{\mbox{exponent}_i}
</code>
</p>

<p>When <code>exponent</code> is a tensor, the shapes of <code>input</code>
and <code>exponent</code> must be broadcastable .
</p>


<h3>pow(self, exponent, out=NULL) -&gt; Tensor </h3>

<p><code>self</code> is a scalar <code>float</code> value, and <code>exponent</code> is a tensor.
The returned tensor <code>out</code> is of the same shape as <code>exponent</code>
</p>
<p>The operation applied is:
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \mbox{self} ^ {\mbox{exponent}_i}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_pow(a, 2)
exp &lt;- torch_arange(1, 5)
a &lt;- torch_arange(1, 5)
a
exp
torch_pow(a, exp)


exp &lt;- torch_arange(1, 5)
base &lt;- 2
torch_pow(base, exp)
}
</code></pre>

<hr>
<h2 id='torch_prod'>Prod</h2><span id='topic+torch_prod'></span>

<h3>Description</h3>

<p>Prod
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_prod(self, dim, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_prod_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_prod_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_prod_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_prod_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        If specified, the input tensor is casted to <code>dtype</code> before the operation        is performed. This is useful for preventing data type overflows. Default: NULL.</p>
</td></tr>
</table>


<h3>prod(input, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the product of all elements in the <code>input</code> tensor.
</p>


<h3>prod(input, dim, keepdim=False, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the product of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in
the output tensor having 1 fewer dimension than <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_prod(a)


a = torch_randn(c(4, 2))
a
torch_prod(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_promote_types'>Promote_types</h2><span id='topic+torch_promote_types'></span>

<h3>Description</h3>

<p>Promote_types
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_promote_types(type1, type2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_promote_types_+3A_type1">type1</code></td>
<td>
<p>(<code>torch.dtype</code>)</p>
</td></tr>
<tr><td><code id="torch_promote_types_+3A_type2">type2</code></td>
<td>
<p>(<code>torch.dtype</code>)</p>
</td></tr>
</table>


<h3>promote_types(type1, type2) -&gt; dtype </h3>

<p>Returns the <code>torch_dtype</code> with the smallest size and scalar kind that is
not smaller nor of lower kind than either <code>type1</code> or <code>type2</code>. See type promotion
documentation  for more information on the type
promotion logic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_promote_types(torch_int32(), torch_float32())
torch_promote_types(torch_uint8(), torch_long())
}
</code></pre>

<hr>
<h2 id='torch_qr'>Qr</h2><span id='topic+torch_qr'></span>

<h3>Description</h3>

<p>Qr
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_qr(self, some = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_qr_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code class="reqn">(*, m, n)</code> where <code>*</code> is zero or more                batch dimensions consisting of matrices of dimension <code class="reqn">m \times n</code>.</p>
</td></tr>
<tr><td><code id="torch_qr_+3A_some">some</code></td>
<td>
<p>(bool, optional) Set to <code>TRUE</code> for reduced QR decomposition and <code>FALSE</code> for                complete QR decomposition.</p>
</td></tr>
</table>


<h3>qr(input, some=TRUE, out=NULL) -&gt; (Tensor, Tensor) </h3>

<p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>,
and returns a namedtuple (Q, R) of tensors such that <code class="reqn">\mbox{input} = Q R</code>
with <code class="reqn">Q</code> being an orthogonal matrix or batch of orthogonal matrices and
<code class="reqn">R</code> being an upper triangular matrix or batch of upper triangular matrices.
</p>
<p>If <code>some</code> is <code>TRUE</code>, then this function returns the thin (reduced) QR factorization.
Otherwise, if <code>some</code> is <code>FALSE</code>, this function returns the complete QR factorization.
</p>


<h3>Note</h3>

<p>precision may be lost if the magnitudes of the elements of <code>input</code>
are large
</p>
<p>While it should always give you a valid decomposition, it may not
give you the same one across platforms - it will depend on your
LAPACK implementation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_tensor(matrix(c(12., -51, 4, 6, 167, -68, -4, 24, -41), ncol = 3, byrow = TRUE))
out = torch_qr(a)
q = out[[1]]
r = out[[2]]
torch_mm(q, r)$round()
torch_mm(q$t(), q)$round()
}
</code></pre>

<hr>
<h2 id='torch_qscheme'>Creates the corresponding Scheme object</h2><span id='topic+torch_qscheme'></span><span id='topic+torch_per_channel_affine'></span><span id='topic+torch_per_tensor_affine'></span><span id='topic+torch_per_channel_symmetric'></span><span id='topic+torch_per_tensor_symmetric'></span>

<h3>Description</h3>

<p>Creates the corresponding Scheme object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_per_channel_affine()

torch_per_tensor_affine()

torch_per_channel_symmetric()

torch_per_tensor_symmetric()
</code></pre>

<hr>
<h2 id='torch_quantile'>Quantile</h2><span id='topic+torch_quantile'></span>

<h3>Description</h3>

<p>Quantile
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_quantile(self, q, dim = NULL, keepdim = FALSE, interpolation = "linear")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_quantile_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_quantile_+3A_q">q</code></td>
<td>
<p>(float or Tensor) a scalar or 1D tensor of quantile values in the range <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></p>
</td></tr>
<tr><td><code id="torch_quantile_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to reduce.</p>
</td></tr>
<tr><td><code id="torch_quantile_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_quantile_+3A_interpolation">interpolation</code></td>
<td>
<p>The interpolation method.</p>
</td></tr>
</table>


<h3>quantile(input, q) -&gt; Tensor </h3>

<p>Returns the q-th quantiles of all elements in the <code>input</code> tensor, doing a linear
interpolation when the q-th quantile lies between two data points.
</p>


<h3>quantile(input, q, dim=None, keepdim=FALSE, *, out=None) -&gt; Tensor </h3>

<p>Returns the q-th quantiles of each row of the <code>input</code> tensor along the dimension
<code>dim</code>, doing a linear interpolation when the q-th quantile lies between two
data points. By default, <code>dim</code> is <code>None</code> resulting in the <code>input</code> tensor
being flattened before computation.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output dimensions are of the same size as <code>input</code>
except in the dimensions being reduced (<code>dim</code> or all if <code>dim</code> is <code>NULL</code>) where they
have size 1. Otherwise, the dimensions being reduced are squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>).
If <code>q</code> is a 1D tensor, an extra dimension is prepended to the output tensor with the same
size as <code>q</code> which represents the quantiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_randn(c(1, 3))
a
q &lt;- torch_tensor(c(0, 0.5, 1))
torch_quantile(a, q)


a &lt;- torch_randn(c(2, 3))
a
q &lt;- torch_tensor(c(0.25, 0.5, 0.75))
torch_quantile(a, q, dim=1, keepdim=TRUE)
torch_quantile(a, q, dim=1, keepdim=TRUE)$shape
}
</code></pre>

<hr>
<h2 id='torch_quantize_per_channel'>Quantize_per_channel</h2><span id='topic+torch_quantize_per_channel'></span>

<h3>Description</h3>

<p>Quantize_per_channel
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_quantize_per_channel(self, scales, zero_points, axis, dtype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_quantize_per_channel_+3A_self">self</code></td>
<td>
<p>(Tensor) float tensor to quantize</p>
</td></tr>
<tr><td><code id="torch_quantize_per_channel_+3A_scales">scales</code></td>
<td>
<p>(Tensor) float 1D tensor of scales to use, size should match <code>input.size(axis)</code></p>
</td></tr>
<tr><td><code id="torch_quantize_per_channel_+3A_zero_points">zero_points</code></td>
<td>
<p>(int) integer 1D tensor of offset to use, size should match <code>input.size(axis)</code></p>
</td></tr>
<tr><td><code id="torch_quantize_per_channel_+3A_axis">axis</code></td>
<td>
<p>(int) dimension on which apply per-channel quantization</p>
</td></tr>
<tr><td><code id="torch_quantize_per_channel_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>) the desired data type of returned tensor.        Has to be one of the quantized dtypes: <code>torch_quint8</code>, <code>torch.qint8</code>, <code>torch.qint32</code></p>
</td></tr>
</table>


<h3>quantize_per_channel(input, scales, zero_points, axis, dtype) -&gt; Tensor </h3>

<p>Converts a float tensor to per-channel quantized tensor with given scales and zero points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x = torch_tensor(matrix(c(-1.0, 0.0, 1.0, 2.0), ncol = 2, byrow = TRUE))
torch_quantize_per_channel(x, torch_tensor(c(0.1, 0.01)), 
                           torch_tensor(c(10L, 0L)), 0, torch_quint8())
torch_quantize_per_channel(x, torch_tensor(c(0.1, 0.01)), 
                           torch_tensor(c(10L, 0L)), 0, torch_quint8())$int_repr()
}
</code></pre>

<hr>
<h2 id='torch_quantize_per_tensor'>Quantize_per_tensor</h2><span id='topic+torch_quantize_per_tensor'></span>

<h3>Description</h3>

<p>Quantize_per_tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_quantize_per_tensor(self, scale, zero_point, dtype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_quantize_per_tensor_+3A_self">self</code></td>
<td>
<p>(Tensor) float tensor to quantize</p>
</td></tr>
<tr><td><code id="torch_quantize_per_tensor_+3A_scale">scale</code></td>
<td>
<p>(float) scale to apply in quantization formula</p>
</td></tr>
<tr><td><code id="torch_quantize_per_tensor_+3A_zero_point">zero_point</code></td>
<td>
<p>(int) offset in integer value that maps to float zero</p>
</td></tr>
<tr><td><code id="torch_quantize_per_tensor_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>) the desired data type of returned tensor.        Has to be one of the quantized dtypes: <code>torch_quint8</code>, <code>torch.qint8</code>, <code>torch.qint32</code></p>
</td></tr>
</table>


<h3>quantize_per_tensor(input, scale, zero_point, dtype) -&gt; Tensor </h3>

<p>Converts a float tensor to quantized tensor with given scale and zero point.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
torch_quantize_per_tensor(torch_tensor(c(-1.0, 0.0, 1.0, 2.0)), 0.1, 10, torch_quint8())
torch_quantize_per_tensor(torch_tensor(c(-1.0, 0.0, 1.0, 2.0)), 0.1, 10, torch_quint8())$int_repr()
}
</code></pre>

<hr>
<h2 id='torch_rad2deg'>Rad2deg</h2><span id='topic+torch_rad2deg'></span>

<h3>Description</h3>

<p>Rad2deg
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rad2deg(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rad2deg_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>rad2deg(input, *, out=None) -&gt; Tensor </h3>

<p>Returns a new tensor with each of the elements of <code>input</code>
converted from angles in radians to degrees.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(rbind(c(3.142, -3.142), c(6.283, -6.283), c(1.570, -1.570)))
torch_rad2deg(a)
}
</code></pre>

<hr>
<h2 id='torch_rand'>Rand</h2><span id='topic+torch_rand'></span>

<h3>Description</h3>

<p>Rand
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rand(
  ...,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rand_+3A_...">...</code></td>
<td>
<p>(int...) a sequence of integers defining the shape of the output tensor.        Can be a variable number of arguments or a collection like a list or tuple.</p>
</td></tr>
<tr><td><code id="torch_rand_+3A_names">names</code></td>
<td>
<p>optional dimension names</p>
</td></tr>
<tr><td><code id="torch_rand_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_rand_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_rand_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_rand_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>rand(*size, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with random numbers from a uniform distribution
on the interval <code class="reqn">[0, 1)</code>
</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_rand(4)
torch_rand(c(2, 3))
}
</code></pre>

<hr>
<h2 id='torch_rand_like'>Rand_like</h2><span id='topic+torch_rand_like'></span>

<h3>Description</h3>

<p>Rand_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rand_like(
  input,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rand_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_rand_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_rand_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_rand_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_rand_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_rand_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>rand_like(input, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns a tensor with the same size as <code>input</code> that is filled with
random numbers from a uniform distribution on the interval <code class="reqn">[0, 1)</code>.
<code>torch_rand_like(input)</code> is equivalent to
<code>torch_rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>

<hr>
<h2 id='torch_randint'>Randint</h2><span id='topic+torch_randint'></span>

<h3>Description</h3>

<p>Randint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_randint(
  low,
  high,
  size,
  generator = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_randint_+3A_low">low</code></td>
<td>
<p>(int, optional) Lowest integer to be drawn from the distribution. Default: 0.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_high">high</code></td>
<td>
<p>(int) One above the highest integer to be drawn from the distribution.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_size">size</code></td>
<td>
<p>(tuple) a tuple defining the shape of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_generator">generator</code></td>
<td>
<p>(<code>torch.Generator</code>, optional) a pseudorandom number generator for sampling</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_randint_+3A_memory_format">memory_format</code></td>
<td>
<p>memory format for the resulting tensor.</p>
</td></tr>
</table>


<h3>randint(low=0, high, size, *, generator=NULL, out=NULL, \ </h3>

<p>dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor
</p>
<p>Returns a tensor filled with random integers generated uniformly
between <code>low</code> (inclusive) and <code>high</code> (exclusive).
</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.
</p>
<p>.. note:
With the global dtype default (<code>torch_float32</code>), this function returns
a tensor with dtype <code>torch_int64</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_randint(3, 5, list(3))
torch_randint(0, 10, size = list(2, 2))
torch_randint(3, 10, list(2, 2))
}
</code></pre>

<hr>
<h2 id='torch_randint_like'>Randint_like</h2><span id='topic+torch_randint_like'></span>

<h3>Description</h3>

<p>Randint_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_randint_like(
  input,
  low,
  high,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_randint_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_low">low</code></td>
<td>
<p>(int, optional) Lowest integer to be drawn from the distribution. Default: 0.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_high">high</code></td>
<td>
<p>(int) One above the highest integer to be drawn from the distribution.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randint_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>randint_like(input, low=0, high, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False, </h3>

<p>memory_format=torch.preserve_format) -&gt; Tensor
</p>
<p>Returns a tensor with the same shape as Tensor <code>input</code> filled with
random integers generated uniformly between <code>low</code> (inclusive) and
<code>high</code> (exclusive).
</p>
<p>.. note:
With the global dtype default (<code>torch_float32</code>), this function returns
a tensor with dtype <code>torch_int64</code>.
</p>

<hr>
<h2 id='torch_randn'>Randn</h2><span id='topic+torch_randn'></span>

<h3>Description</h3>

<p>Randn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_randn(
  ...,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_randn_+3A_...">...</code></td>
<td>
<p>(int...) a sequence of integers defining the shape of the output tensor.        Can be a variable number of arguments or a collection like a list or tuple.</p>
</td></tr>
<tr><td><code id="torch_randn_+3A_names">names</code></td>
<td>
<p>optional names for the dimensions</p>
</td></tr>
<tr><td><code id="torch_randn_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_randn_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_randn_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_randn_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>randn(*size, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with random numbers from a normal distribution
with mean <code>0</code> and variance <code>1</code> (also called the standard normal
distribution).
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} \sim \mathcal{N}(0, 1)
</code>
</p>

<p>The shape of the tensor is defined by the variable argument <code>size</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_randn(c(4))
torch_randn(c(2, 3))
}
</code></pre>

<hr>
<h2 id='torch_randn_like'>Randn_like</h2><span id='topic+torch_randn_like'></span>

<h3>Description</h3>

<p>Randn_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_randn_like(
  input,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_randn_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_randn_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randn_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randn_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_randn_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_randn_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>randn_like(input, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns a tensor with the same size as <code>input</code> that is filled with
random numbers from a normal distribution with mean 0 and variance 1.
<code>torch_randn_like(input)</code> is equivalent to
<code>torch_randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>

<hr>
<h2 id='torch_randperm'>Randperm</h2><span id='topic+torch_randperm'></span>

<h3>Description</h3>

<p>Randperm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_randperm(
  n,
  dtype = torch_int64(),
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_randperm_+3A_n">n</code></td>
<td>
<p>(int) the upper bound (exclusive)</p>
</td></tr>
<tr><td><code id="torch_randperm_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: <code>torch_int64</code>.</p>
</td></tr>
<tr><td><code id="torch_randperm_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_randperm_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_randperm_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>randperm(n, out=NULL, dtype=torch.int64, layout=torch.strided, device=NULL, requires_grad=False) -&gt; LongTensor </h3>

<p>Returns a random permutation of integers from <code>0</code> to <code>n - 1</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_randperm(4)
}
</code></pre>

<hr>
<h2 id='torch_range'>Range</h2><span id='topic+torch_range'></span>

<h3>Description</h3>

<p>Range
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_range(
  start,
  end,
  step = 1,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_range_+3A_start">start</code></td>
<td>
<p>(float) the starting value for the set of points. Default: <code>0</code>.</p>
</td></tr>
<tr><td><code id="torch_range_+3A_end">end</code></td>
<td>
<p>(float) the ending value for the set of points</p>
</td></tr>
<tr><td><code id="torch_range_+3A_step">step</code></td>
<td>
<p>(float) the gap between each pair of adjacent points. Default: <code>1</code>.</p>
</td></tr>
<tr><td><code id="torch_range_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). If <code>dtype</code> is not given, infer the data type from the other input        arguments. If any of <code>start</code>, <code>end</code>, or <code>stop</code> are floating-point, the        <code>dtype</code> is inferred to be the default dtype, see        <code>~torch.get_default_dtype</code>. Otherwise, the <code>dtype</code> is inferred to        be <code>torch.int64</code>.</p>
</td></tr>
<tr><td><code id="torch_range_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_range_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_range_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>range(start=0, end, step=1, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a 1-D tensor of size <code class="reqn">\left\lfloor \frac{\mbox{end} - \mbox{start}}{\mbox{step}} \right\rfloor + 1</code>
with values from <code>start</code> to <code>end</code> with step <code>step</code>. Step is
the gap between two values in the tensor.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i+1} = \mbox{out}_i + \mbox{step}.
</code>
</p>



<h3>Warning</h3>

<p>This function is deprecated in favor of <code><a href="#topic+torch_arange">torch_arange</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_range(1, 4)
torch_range(1, 4, 0.5)
}
</code></pre>

<hr>
<h2 id='torch_real'>Real</h2><span id='topic+torch_real'></span>

<h3>Description</h3>

<p>Real
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_real(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_real_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>real(input) -&gt; Tensor </h3>

<p>Returns the real part of the <code>input</code> tensor. If
<code>input</code> is a real (non-complex) tensor, this function just
returns it.
</p>


<h3>Warning</h3>

<p>Not yet implemented for complex tensors.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = real(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
torch_real(torch_tensor(c(-1 + 1i, -2 + 2i, 3 - 3i)))

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_reciprocal'>Reciprocal</h2><span id='topic+torch_reciprocal'></span>

<h3>Description</h3>

<p>Reciprocal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_reciprocal(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_reciprocal_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>reciprocal(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the reciprocal of the elements of <code>input</code>
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \frac{1}{\mbox{input}_{i}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_reciprocal(a)
}
</code></pre>

<hr>
<h2 id='torch_reduction'>Creates the reduction objet</h2><span id='topic+torch_reduction'></span><span id='topic+torch_reduction_sum'></span><span id='topic+torch_reduction_mean'></span><span id='topic+torch_reduction_none'></span>

<h3>Description</h3>

<p>Creates the reduction objet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_reduction_sum()

torch_reduction_mean()

torch_reduction_none()
</code></pre>

<hr>
<h2 id='torch_relu'>Relu</h2><span id='topic+torch_relu'></span>

<h3>Description</h3>

<p>Relu
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_relu(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_relu_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
</table>


<h3>relu(input) -&gt; Tensor </h3>

<p>Computes the relu tranformation.
</p>

<hr>
<h2 id='torch_relu_'>Relu_</h2><span id='topic+torch_relu_'></span>

<h3>Description</h3>

<p>Relu_
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_relu_(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_relu__+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
</table>


<h3>relu_(input) -&gt; Tensor </h3>

<p>In-place version of <code><a href="#topic+torch_relu">torch_relu()</a></code>.
</p>

<hr>
<h2 id='torch_remainder'>Remainder</h2><span id='topic+torch_remainder'></span>

<h3>Description</h3>

<p>Remainder
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_remainder(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_remainder_+3A_self">self</code></td>
<td>
<p>(Tensor) the dividend</p>
</td></tr>
<tr><td><code id="torch_remainder_+3A_other">other</code></td>
<td>
<p>(Tensor or float) the divisor that may be either a number or a                               Tensor of the same shape as the dividend</p>
</td></tr>
</table>


<h3>remainder(input, other, out=NULL) -&gt; Tensor </h3>

<p>Computes the element-wise remainder of division.
</p>
<p>The divisor and dividend may contain both for integer and floating point
numbers. The remainder has the same sign as the divisor.
</p>
<p>When <code>other</code> is a tensor, the shapes of <code>input</code> and
<code>other</code> must be broadcastable .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_remainder(torch_tensor(c(-3., -2, -1, 1, 2, 3)), 2)
torch_remainder(torch_tensor(c(1., 2, 3, 4, 5)), 1.5)
}
</code></pre>

<hr>
<h2 id='torch_renorm'>Renorm</h2><span id='topic+torch_renorm'></span>

<h3>Description</h3>

<p>Renorm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_renorm(self, p, dim, maxnorm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_renorm_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_renorm_+3A_p">p</code></td>
<td>
<p>(float) the power for the norm computation</p>
</td></tr>
<tr><td><code id="torch_renorm_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to slice over to get the sub-tensors</p>
</td></tr>
<tr><td><code id="torch_renorm_+3A_maxnorm">maxnorm</code></td>
<td>
<p>(float) the maximum norm to keep each sub-tensor under</p>
</td></tr>
</table>


<h3>renorm(input, p, dim, maxnorm, out=NULL) -&gt; Tensor </h3>

<p>Returns a tensor where each sub-tensor of <code>input</code> along dimension
<code>dim</code> is normalized such that the <code>p</code>-norm of the sub-tensor is lower
than the value <code>maxnorm</code>
</p>


<h3>Note</h3>

<p>If the norm of a row is lower than <code>maxnorm</code>, the row is unchanged
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x = torch_ones(c(3, 3))
x[2,]$fill_(2)
x[3,]$fill_(3)
x
torch_renorm(x, 1, 1, 5)
}
</code></pre>

<hr>
<h2 id='torch_repeat_interleave'>Repeat_interleave</h2><span id='topic+torch_repeat_interleave'></span>

<h3>Description</h3>

<p>Repeat_interleave
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_repeat_interleave(self, repeats, dim = NULL, output_size = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_repeat_interleave_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_repeat_interleave_+3A_repeats">repeats</code></td>
<td>
<p>(Tensor or int) The number of repetitions for each element.        repeats is broadcasted to fit the shape of the given axis.</p>
</td></tr>
<tr><td><code id="torch_repeat_interleave_+3A_dim">dim</code></td>
<td>
<p>(int, optional) The dimension along which to repeat values.        By default, use the flattened input array, and return a flat output        array.</p>
</td></tr>
<tr><td><code id="torch_repeat_interleave_+3A_output_size">output_size</code></td>
<td>
<p>(int, optional)  Total output size for the given axis
( e.g. sum of repeats). If given, it will avoid stream syncronization needed
to calculate output shape of the tensor.</p>
</td></tr>
</table>


<h3>repeat_interleave(input, repeats, dim=NULL) -&gt; Tensor </h3>

<p>Repeat elements of a tensor.
</p>


<h3>Warning</h3>

<div class="sourceCode"><pre>This is different from `torch_Tensor.repeat` but similar to `numpy.repeat`.
</pre></div>


<h3>repeat_interleave(repeats) -&gt; Tensor </h3>

<p>If the <code>repeats</code> is <code style="white-space: pre;">&#8288;tensor([n1, n2, n3, ...])&#8288;</code>, then the output will be
<code style="white-space: pre;">&#8288;tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])&#8288;</code> where <code>0</code> appears <code>n1</code> times,
<code>1</code> appears <code>n2</code> times, <code>2</code> appears <code>n3</code> times, etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
x = torch_tensor(c(1, 2, 3))
x$repeat_interleave(2)
y = torch_tensor(matrix(c(1, 2, 3, 4), ncol = 2, byrow=TRUE))
torch_repeat_interleave(y, 2)
torch_repeat_interleave(y, 3, dim=1)
torch_repeat_interleave(y, torch_tensor(c(1, 2)), dim=1)

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_reshape'>Reshape</h2><span id='topic+torch_reshape'></span>

<h3>Description</h3>

<p>Reshape
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_reshape(self, shape)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_reshape_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to be reshaped</p>
</td></tr>
<tr><td><code id="torch_reshape_+3A_shape">shape</code></td>
<td>
<p>(tuple of ints) the new shape</p>
</td></tr>
</table>


<h3>reshape(input, shape) -&gt; Tensor </h3>

<p>Returns a tensor with the same data and number of elements as <code>input</code>,
but with the specified shape. When possible, the returned tensor will be a view
of <code>input</code>. Otherwise, it will be a copy. Contiguous inputs and inputs
with compatible strides can be reshaped without copying, but you should not
depend on the copying vs. viewing behavior.
</p>
<p>See <code>torch_Tensor.view</code> on when it is possible to return a view.
</p>
<p>A single dimension may be -1, in which case it's inferred from the remaining
dimensions and the number of elements in <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_arange(0, 3)
torch_reshape(a, list(2, 2))
b &lt;- torch_tensor(matrix(c(0, 1, 2, 3), ncol = 2, byrow=TRUE))
torch_reshape(b, list(-1))
}
</code></pre>

<hr>
<h2 id='torch_result_type'>Result_type</h2><span id='topic+torch_result_type'></span>

<h3>Description</h3>

<p>Result_type
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_result_type(tensor1, tensor2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_result_type_+3A_tensor1">tensor1</code></td>
<td>
<p>(Tensor or Number) an input tensor or number</p>
</td></tr>
<tr><td><code id="torch_result_type_+3A_tensor2">tensor2</code></td>
<td>
<p>(Tensor or Number) an input tensor or number</p>
</td></tr>
</table>


<h3>result_type(tensor1, tensor2) -&gt; dtype </h3>

<p>Returns the <code>torch_dtype</code> that would result from performing an arithmetic
operation on the provided input tensors. See type promotion documentation
for more information on the type promotion logic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_result_type(tensor1 = torch_tensor(c(1, 2), dtype=torch_int()), tensor2 = 1)
}
</code></pre>

<hr>
<h2 id='torch_roll'>Roll</h2><span id='topic+torch_roll'></span>

<h3>Description</h3>

<p>Roll
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_roll(self, shifts, dims = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_roll_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_roll_+3A_shifts">shifts</code></td>
<td>
<p>(int or tuple of ints) The number of places by which the elements        of the tensor are shifted. If shifts is a tuple, dims must be a tuple of        the same size, and each dimension will be rolled by the corresponding        value</p>
</td></tr>
<tr><td><code id="torch_roll_+3A_dims">dims</code></td>
<td>
<p>(int or tuple of ints) Axis along which to roll</p>
</td></tr>
</table>


<h3>roll(input, shifts, dims=NULL) -&gt; Tensor </h3>

<p>Roll the tensor along the given dimension(s). Elements that are shifted beyond the
last position are re-introduced at the first position. If a dimension is not
specified, the tensor will be flattened before rolling and then restored
to the original shape.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_tensor(c(1, 2, 3, 4, 5, 6, 7, 8))$view(c(4, 2))
x
torch_roll(x, 1, 1)
torch_roll(x, -1, 1)
torch_roll(x, shifts=list(2, 1), dims=list(1, 2))
}
</code></pre>

<hr>
<h2 id='torch_rot90'>Rot90</h2><span id='topic+torch_rot90'></span>

<h3>Description</h3>

<p>Rot90
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rot90(self, k = 1L, dims = c(0, 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rot90_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_rot90_+3A_k">k</code></td>
<td>
<p>(int) number of times to rotate</p>
</td></tr>
<tr><td><code id="torch_rot90_+3A_dims">dims</code></td>
<td>
<p>(a list or tuple) axis to rotate</p>
</td></tr>
</table>


<h3>rot90(input, k, dims) -&gt; Tensor </h3>

<p>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.
Rotation direction is from the first towards the second axis if k &gt; 0, and from the second towards the first for k &lt; 0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(1, 4)$view(c(2, 2))
x
torch_rot90(x, 1, c(1, 2))
x &lt;- torch_arange(1, 8)$view(c(2, 2, 2))
x
torch_rot90(x, 1, c(1, 2))
}
</code></pre>

<hr>
<h2 id='torch_round'>Round</h2><span id='topic+torch_round'></span>

<h3>Description</h3>

<p>Round
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_round(self, decimals)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_round_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_round_+3A_decimals">decimals</code></td>
<td>
<p>Number of decimal places to round to (default: 0).
If decimals is negative, it specifies the number of positions to
the left of the decimal point.</p>
</td></tr>
</table>


<h3>round(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with each of the elements of <code>input</code> rounded
to the closest integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_round(a)
}
</code></pre>

<hr>
<h2 id='torch_rrelu_'>Rrelu_</h2><span id='topic+torch_rrelu_'></span>

<h3>Description</h3>

<p>Rrelu_
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rrelu_(
  self,
  lower = 0.125,
  upper = 0.333333333333333,
  training = FALSE,
  generator = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rrelu__+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
<tr><td><code id="torch_rrelu__+3A_lower">lower</code></td>
<td>
<p>lower bound of the uniform distribution. Default: 1/8</p>
</td></tr>
<tr><td><code id="torch_rrelu__+3A_upper">upper</code></td>
<td>
<p>upper bound of the uniform distribution. Default: 1/3</p>
</td></tr>
<tr><td><code id="torch_rrelu__+3A_training">training</code></td>
<td>
<p>bool wether it's a training pass. DEfault: FALSE</p>
</td></tr>
<tr><td><code id="torch_rrelu__+3A_generator">generator</code></td>
<td>
<p>random number generator</p>
</td></tr>
</table>


<h3>rrelu_(input, lower=1./8, upper=1./3, training=False) -&gt; Tensor </h3>

<p>In-place version of <code>torch_rrelu</code>.
</p>

<hr>
<h2 id='torch_rsqrt'>Rsqrt</h2><span id='topic+torch_rsqrt'></span>

<h3>Description</h3>

<p>Rsqrt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_rsqrt(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_rsqrt_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>rsqrt(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the reciprocal of the square-root of each of
the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \frac{1}{\sqrt{\mbox{input}_{i}}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_rsqrt(a)
}
</code></pre>

<hr>
<h2 id='torch_save'>Saves an object to a disk file.</h2><span id='topic+torch_save'></span>

<h3>Description</h3>

<p>This function is experimental, don't use for long
term storage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_save(obj, path, ..., compress = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_save_+3A_obj">obj</code></td>
<td>
<p>the saved object</p>
</td></tr>
<tr><td><code id="torch_save_+3A_path">path</code></td>
<td>
<p>a connection or the name of the file to save.</p>
</td></tr>
<tr><td><code id="torch_save_+3A_...">...</code></td>
<td>
<p>not currently used.</p>
</td></tr>
<tr><td><code id="torch_save_+3A_compress">compress</code></td>
<td>
<p>a logical specifying whether saving to a named file is to use
&quot;gzip&quot; compression, or one of &quot;gzip&quot;, &quot;bzip2&quot; or &quot;xz&quot; to indicate the type of
compression to be used. Ignored if file is a connection.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other torch_save: 
<code><a href="#topic+torch_load">torch_load</a>()</code>,
<code><a href="#topic+torch_serialize">torch_serialize</a>()</code>
</p>

<hr>
<h2 id='torch_scalar_tensor'>Scalar tensor</h2><span id='topic+torch_scalar_tensor'></span>

<h3>Description</h3>

<p>Creates a singleton dimension tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_scalar_tensor(value, dtype = NULL, device = NULL, requires_grad = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_scalar_tensor_+3A_value">value</code></td>
<td>
<p>the value you want to use</p>
</td></tr>
<tr><td><code id="torch_scalar_tensor_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_scalar_tensor_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_scalar_tensor_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_searchsorted'>Searchsorted</h2><span id='topic+torch_searchsorted'></span>

<h3>Description</h3>

<p>Searchsorted
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_searchsorted(
  sorted_sequence,
  self,
  out_int32 = FALSE,
  right = FALSE,
  side = NULL,
  sorter = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_searchsorted_+3A_sorted_sequence">sorted_sequence</code></td>
<td>
<p>(Tensor) N-D or 1-D tensor, containing monotonically increasing
sequence on the <em>innermost</em> dimension.</p>
</td></tr>
<tr><td><code id="torch_searchsorted_+3A_self">self</code></td>
<td>
<p>(Tensor or Scalar) N-D tensor or a Scalar containing the search value(s).</p>
</td></tr>
<tr><td><code id="torch_searchsorted_+3A_out_int32">out_int32</code></td>
<td>
<p>(bool, optional)  indicate the output data type. <code><a href="#topic+torch_int32">torch_int32()</a></code>
if True, <code><a href="#topic+torch_int64">torch_int64()</a></code> otherwise. Default value is FALSE, i.e. default output
data type is <code><a href="#topic+torch_int64">torch_int64()</a></code>.</p>
</td></tr>
<tr><td><code id="torch_searchsorted_+3A_right">right</code></td>
<td>
<p>(bool, optional)  if False, return the first suitable location
that is found. If True, return the last such index. If no suitable index found,
return 0 for non-numerical value (eg. nan, inf) or the size of boundaries
(one pass the last index). In other words, if False, gets the lower bound index
for each value in input from boundaries. If True, gets the upper bound index
instead. Default value is False.</p>
</td></tr>
<tr><td><code id="torch_searchsorted_+3A_side">side</code></td>
<td>
<p>the same as right but preferred. left corresponds to <code>FALSE</code> for right
and right corresponds to <code>TRUE</code> for right. It will error if this is set to
left while right is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="torch_searchsorted_+3A_sorter">sorter</code></td>
<td>
<p>if provided, a tensor matching the shape of the unsorted <code>sorted_sequence</code>
containing a sequence of indices that sort it in the ascending order on the
innermost dimension.</p>
</td></tr>
</table>


<h3>searchsorted(sorted_sequence, values, *, out_int32=FALSE, right=FALSE, out=None) -&gt; Tensor </h3>

<p>Find the indices from the <em>innermost</em> dimension of <code>sorted_sequence</code> such that, if the
corresponding values in <code>values</code> were inserted before the indices, the order of the
corresponding <em>innermost</em> dimension within <code>sorted_sequence</code> would be preserved.
Return a new tensor with the same size as <code>values</code>. If <code>right</code> is FALSE (default),
then the left boundary of <code>sorted_sequence</code> is closed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

sorted_sequence &lt;- torch_tensor(rbind(c(1, 3, 5, 7, 9), c(2, 4, 6, 8, 10)))
sorted_sequence
values &lt;- torch_tensor(rbind(c(3, 6, 9), c(3, 6, 9)))
values
torch_searchsorted(sorted_sequence, values)
torch_searchsorted(sorted_sequence, values, right=TRUE)
sorted_sequence_1d &lt;- torch_tensor(c(1, 3, 5, 7, 9))
sorted_sequence_1d
torch_searchsorted(sorted_sequence_1d, values)
}
</code></pre>

<hr>
<h2 id='torch_selu'>Selu</h2><span id='topic+torch_selu'></span>

<h3>Description</h3>

<p>Selu
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_selu(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_selu_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
</table>


<h3>selu(input) -&gt; Tensor </h3>

<p>Computes the selu transformation.
</p>

<hr>
<h2 id='torch_selu_'>Selu_</h2><span id='topic+torch_selu_'></span>

<h3>Description</h3>

<p>Selu_
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_selu_(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_selu__+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
</table>


<h3>selu_(input) -&gt; Tensor </h3>

<p>In-place version of <code><a href="#topic+torch_selu">torch_selu()</a></code>.
</p>

<hr>
<h2 id='torch_serialize'>Serialize a torch object returning a raw object</h2><span id='topic+torch_serialize'></span>

<h3>Description</h3>

<p>It's just a wraper around <code><a href="#topic+torch_save">torch_save()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_serialize(obj, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_serialize_+3A_obj">obj</code></td>
<td>
<p>the saved object</p>
</td></tr>
<tr><td><code id="torch_serialize_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+torch_save">torch_save()</a></code>. <code>obj</code> and <code>path</code> are
not accepted as they are set by <code><a href="#topic+torch_serialize">torch_serialize()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A raw vector containing the serialized object. Can be reloaded using
<code><a href="#topic+torch_load">torch_load()</a></code>.
</p>


<h3>See Also</h3>

<p>Other torch_save: 
<code><a href="#topic+torch_load">torch_load</a>()</code>,
<code><a href="#topic+torch_save">torch_save</a>()</code>
</p>

<hr>
<h2 id='torch_set_default_dtype'>Gets and sets the default floating point dtype.</h2><span id='topic+torch_set_default_dtype'></span><span id='topic+torch_get_default_dtype'></span>

<h3>Description</h3>

<p>Gets and sets the default floating point dtype.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_set_default_dtype(d)

torch_get_default_dtype()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_set_default_dtype_+3A_d">d</code></td>
<td>
<p>The default floating point dtype to set. Initially set to
<code>torch_float()</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='torch_sgn'>Sgn</h2><span id='topic+torch_sgn'></span>

<h3>Description</h3>

<p>Sgn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sgn(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sgn_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sgn(input, *, out=None) -&gt; Tensor </h3>

<p>For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the
elements of <code>input</code> and absolute value 1. For a non-complex tensor, this function
returns the signs of the elements of <code>input</code> (see <code><a href="#topic+torch_sign">torch_sign</a></code>).
</p>
<p><code class="reqn">\mbox{out}_{i} = 0</code>, if <code class="reqn">|{\mbox{{input}}_i}| == 0</code>
<code class="reqn">\mbox{out}_{i} = \frac{{\mbox{{input}}_i}}{|{\mbox{{input}}_i}|}</code>, otherwise
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
if (FALSE) {
x &lt;- torch_tensor(c(3+4i, 7-24i, 0, 1+2i))
x$sgn()
torch_sgn(x)
}
}
</code></pre>

<hr>
<h2 id='torch_sigmoid'>Sigmoid</h2><span id='topic+torch_sigmoid'></span>

<h3>Description</h3>

<p>Sigmoid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sigmoid(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sigmoid_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sigmoid(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the sigmoid of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \frac{1}{1 + e^{-\mbox{input}_{i}}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_sigmoid(a)
}
</code></pre>

<hr>
<h2 id='torch_sign'>Sign</h2><span id='topic+torch_sign'></span>

<h3>Description</h3>

<p>Sign
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sign(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sign_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sign(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the signs of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \mbox{sgn}(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_tensor(c(0.7, -1.2, 0., 2.3))
a
torch_sign(a)
}
</code></pre>

<hr>
<h2 id='torch_signbit'>Signbit</h2><span id='topic+torch_signbit'></span>

<h3>Description</h3>

<p>Signbit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_signbit(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_signbit_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>signbit(input, *, out=None) -&gt; Tensor </h3>

<p>Tests if each element of <code>input</code> has its sign bit set (is less than zero) or not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(0.7, -1.2, 0., 2.3))
torch_signbit(a)
}
</code></pre>

<hr>
<h2 id='torch_sin'>Sin</h2><span id='topic+torch_sin'></span>

<h3>Description</h3>

<p>Sin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sin(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sin_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sin(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the sine of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \sin(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_sin(a)
}
</code></pre>

<hr>
<h2 id='torch_sinh'>Sinh</h2><span id='topic+torch_sinh'></span>

<h3>Description</h3>

<p>Sinh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sinh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sinh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sinh(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the hyperbolic sine of the elements of
<code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \sinh(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_sinh(a)
}
</code></pre>

<hr>
<h2 id='torch_slogdet'>Slogdet</h2><span id='topic+torch_slogdet'></span>

<h3>Description</h3>

<p>Slogdet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_slogdet(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_slogdet_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code style="white-space: pre;">&#8288;(*, n, n)&#8288;</code> where <code>*</code> is zero or more                batch dimensions.</p>
</td></tr>
</table>


<h3>slogdet(input) -&gt; (Tensor, Tensor) </h3>

<p>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `input` has zero determinant, this returns `(0, -inf)`.
</pre></div>
<div class="sourceCode"><pre>Backward through `slogdet` internally uses SVD results when `input`
is not invertible. In this case, double backward through `slogdet`
will be unstable in when `input` doesn't have distinct singular values.
See `~torch.svd` for details.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A = torch_randn(c(3, 3))
A
torch_det(A)
torch_logdet(A)
torch_slogdet(A)
}
</code></pre>

<hr>
<h2 id='torch_sort'>Sort</h2><span id='topic+torch_sort'></span>

<h3>Description</h3>

<p>Sort
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sort_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_sort_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension to sort along</p>
</td></tr>
<tr><td><code id="torch_sort_+3A_descending">descending</code></td>
<td>
<p>(bool, optional) controls the sorting order (ascending or descending)</p>
</td></tr>
<tr><td><code id="torch_sort_+3A_stable">stable</code></td>
<td>
<p>(bool, optional)  makes the sorting routine stable, which guarantees
that the order of equivalent elements is preserved.</p>
</td></tr>
</table>


<h3>sort(input, dim=-1, descending=FALSE) -&gt; (Tensor, LongTensor) </h3>

<p>Sorts the elements of the <code>input</code> tensor along a given dimension
in ascending order by value.
</p>
<p>If <code>dim</code> is not given, the last dimension of the <code>input</code> is chosen.
</p>
<p>If <code>descending</code> is <code>TRUE</code> then the elements are sorted in descending
order by value.
</p>
<p>A namedtuple of (values, indices) is returned, where the <code>values</code> are the
sorted values and <code>indices</code> are the indices of the elements in the original
<code>input</code> tensor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(3, 4))
out = torch_sort(x)
out
out = torch_sort(x, 1)
out
}
</code></pre>

<hr>
<h2 id='torch_sparse_coo_tensor'>Sparse_coo_tensor</h2><span id='topic+torch_sparse_coo_tensor'></span>

<h3>Description</h3>

<p>Sparse_coo_tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sparse_coo_tensor(
  indices,
  values,
  size = NULL,
  dtype = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sparse_coo_tensor_+3A_indices">indices</code></td>
<td>
<p>(array_like) Initial data for the tensor. Can be a list, tuple,        NumPy <code>ndarray</code>, scalar, and other types. Will be cast to a <code>torch_LongTensor</code>        internally. The indices are the coordinates of the non-zero values in the matrix, and thus        should be two-dimensional where the first dimension is the number of tensor dimensions and        the second dimension is the number of non-zero values.</p>
</td></tr>
<tr><td><code id="torch_sparse_coo_tensor_+3A_values">values</code></td>
<td>
<p>(array_like) Initial values for the tensor. Can be a list, tuple,        NumPy <code>ndarray</code>, scalar, and other types.</p>
</td></tr>
<tr><td><code id="torch_sparse_coo_tensor_+3A_size">size</code></td>
<td>
<p>(list, tuple, or <code>torch.Size</code>, optional) Size of the sparse tensor. If not        provided the size will be inferred as the minimum size big enough to hold all non-zero        elements.</p>
</td></tr>
<tr><td><code id="torch_sparse_coo_tensor_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if NULL, infers data type from <code>values</code>.</p>
</td></tr>
<tr><td><code id="torch_sparse_coo_tensor_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if NULL, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_sparse_coo_tensor_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>sparse_coo_tensor(indices, values, size=NULL, dtype=NULL, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given <code>indices</code>
with the given <code>values</code>. A sparse tensor can be <code>uncoalesced</code>, in that case, there are duplicate
coordinates in the indices, and the value at that index is the sum of all duplicate value entries:
<code>torch_sparse</code>_.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

i = torch_tensor(matrix(c(1, 2, 2, 3, 1, 3), ncol = 3, byrow = TRUE), dtype=torch_int64())
v = torch_tensor(c(3, 4, 5), dtype=torch_float32())
torch_sparse_coo_tensor(i, v)
torch_sparse_coo_tensor(i, v, c(2, 4))

# create empty sparse tensors
S = torch_sparse_coo_tensor(
  torch_empty(c(1, 0), dtype = torch_int64()), 
  torch_tensor(numeric(), dtype = torch_float32()), 
  c(1)
)
S = torch_sparse_coo_tensor(
  torch_empty(c(1, 0), dtype = torch_int64()), 
  torch_empty(c(0, 2)), 
  c(1, 2)
)
}
</code></pre>

<hr>
<h2 id='torch_split'>Split</h2><span id='topic+torch_split'></span>

<h3>Description</h3>

<p>Splits the tensor into chunks. Each chunk is a view of the original tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_split(self, split_size, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_split_+3A_self">self</code></td>
<td>
<p>(Tensor) tensor to split.</p>
</td></tr>
<tr><td><code id="torch_split_+3A_split_size">split_size</code></td>
<td>
<p>(int) size of a single chunk or
list of sizes for each chunk</p>
</td></tr>
<tr><td><code id="torch_split_+3A_dim">dim</code></td>
<td>
<p>(int) dimension along which to split the tensor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>split_size</code> is an integer type, then <code>tensor</code> will
be split into equally sized chunks (if possible). Last chunk will be smaller if
the tensor size along the given dimension <code>dim</code> is not divisible by
<code>split_size</code>.
</p>
<p>If <code>split_size</code> is a list, then <code>tensor</code> will be split
into <code>length(split_size)</code> chunks with sizes in <code>dim</code> according
to <code>split_size_or_sections</code>.
</p>

<hr>
<h2 id='torch_sqrt'>Sqrt</h2><span id='topic+torch_sqrt'></span>

<h3>Description</h3>

<p>Sqrt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sqrt(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sqrt_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>sqrt(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the square-root of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \sqrt{\mbox{input}_{i}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_sqrt(a)
}
</code></pre>

<hr>
<h2 id='torch_square'>Square</h2><span id='topic+torch_square'></span>

<h3>Description</h3>

<p>Square
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_square(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_square_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>square(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the square of the elements of <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_square(a)
}
</code></pre>

<hr>
<h2 id='torch_squeeze'>Squeeze</h2><span id='topic+torch_squeeze'></span>

<h3>Description</h3>

<p>Squeeze
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_squeeze(self, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_squeeze_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_squeeze_+3A_dim">dim</code></td>
<td>
<p>(int, optional) if given, the input will be squeezed only in           this dimension</p>
</td></tr>
</table>


<h3>squeeze(input, dim=NULL, out=NULL) -&gt; Tensor </h3>

<p>Returns a tensor with all the dimensions of <code>input</code> of size <code>1</code> removed.
</p>
<p>For example, if <code>input</code> is of shape:
<code class="reqn">(A \times 1 \times B \times C \times 1 \times D)</code> then the <code>out</code> tensor
will be of shape: <code class="reqn">(A \times B \times C \times D)</code>.
</p>
<p>When <code>dim</code> is given, a squeeze operation is done only in the given
dimension. If <code>input</code> is of shape: <code class="reqn">(A \times 1 \times B)</code>,
<code>squeeze(input, 0)</code> leaves the tensor unchanged, but <code>squeeze(input, 1)</code>
will squeeze the tensor to the shape <code class="reqn">(A \times B)</code>.
</p>


<h3>Note</h3>

<p>The returned tensor shares the storage with the input tensor,
so changing the contents of one will change the contents of the other.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_zeros(c(2, 1, 2, 1, 2))
x
y = torch_squeeze(x)
y
y = torch_squeeze(x, 1)
y
y = torch_squeeze(x, 2)
y
}
</code></pre>

<hr>
<h2 id='torch_stack'>Stack</h2><span id='topic+torch_stack'></span>

<h3>Description</h3>

<p>Stack
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_stack(tensors, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_stack_+3A_tensors">tensors</code></td>
<td>
<p>(sequence of Tensors) sequence of tensors to concatenate</p>
</td></tr>
<tr><td><code id="torch_stack_+3A_dim">dim</code></td>
<td>
<p>(int) dimension to insert. Has to be between 0 and the number        of dimensions of concatenated tensors (inclusive)</p>
</td></tr>
</table>


<h3>stack(tensors, dim=0, out=NULL) -&gt; Tensor </h3>

<p>Concatenates sequence of tensors along a new dimension.
</p>
<p>All tensors need to be of the same size.
</p>

<hr>
<h2 id='torch_std'>Std</h2><span id='topic+torch_std'></span>

<h3>Description</h3>

<p>Std
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_std(self, dim, unbiased = TRUE, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_std_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_std_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_std_+3A_unbiased">unbiased</code></td>
<td>
<p>(bool) whether to use the unbiased estimation or not</p>
</td></tr>
<tr><td><code id="torch_std_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>std(input, unbiased=TRUE) -&gt; Tensor </h3>

<p>Returns the standard-deviation of all elements in the <code>input</code> tensor.
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>std(input, dim, unbiased=TRUE, keepdim=False, out=NULL) -&gt; Tensor </h3>

<p>Returns the standard-deviation of each row of the <code>input</code> tensor in the
dimension <code>dim</code>. If <code>dim</code> is a list of dimensions,
reduce over all of them.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_std(a)


a = torch_randn(c(4, 4))
a
torch_std(a, dim=1)
}
</code></pre>

<hr>
<h2 id='torch_std_mean'>Std_mean</h2><span id='topic+torch_std_mean'></span>

<h3>Description</h3>

<p>Std_mean
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_std_mean(self, dim, unbiased = TRUE, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_std_mean_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_std_mean_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_std_mean_+3A_unbiased">unbiased</code></td>
<td>
<p>(bool) whether to use the unbiased estimation or not</p>
</td></tr>
<tr><td><code id="torch_std_mean_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>std_mean(input, unbiased=TRUE) -&gt; (Tensor, Tensor) </h3>

<p>Returns the standard-deviation and mean of all elements in the <code>input</code> tensor.
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>std_mean(input, dim, unbiased=TRUE, keepdim=False) -&gt; (Tensor, Tensor) </h3>

<p>Returns the standard-deviation and mean of each row of the <code>input</code> tensor in the
dimension <code>dim</code>. If <code>dim</code> is a list of dimensions,
reduce over all of them.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_std_mean(a)


a = torch_randn(c(4, 4))
a
torch_std_mean(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_stft'>Stft</h2><span id='topic+torch_stft'></span>

<h3>Description</h3>

<p>Stft
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_stft(
  input,
  n_fft,
  hop_length = NULL,
  win_length = NULL,
  window = NULL,
  center = TRUE,
  pad_mode = "reflect",
  normalized = FALSE,
  onesided = NULL,
  return_complex = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_stft_+3A_input">input</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_stft_+3A_n_fft">n_fft</code></td>
<td>
<p>(int) size of Fourier transform</p>
</td></tr>
<tr><td><code id="torch_stft_+3A_hop_length">hop_length</code></td>
<td>
<p>(int, optional) the distance between neighboring sliding window
frames. Default: <code>NULL</code> (treated as equal to <code>floor(n_fft / 4)</code>)</p>
</td></tr>
<tr><td><code id="torch_stft_+3A_win_length">win_length</code></td>
<td>
<p>(int, optional) the size of window frame and STFT filter.
Default: <code>NULL</code>  (treated as equal to <code>n_fft</code>)</p>
</td></tr>
<tr><td><code id="torch_stft_+3A_window">window</code></td>
<td>
<p>(Tensor, optional) the optional window function.
Default: <code>NULL</code> (treated as window of all <code class="reqn">1</code> s)</p>
</td></tr>
<tr><td><code id="torch_stft_+3A_center">center</code></td>
<td>
<p>(bool, optional) whether to pad <code>input</code> on both sides so
that the <code class="reqn">t</code>-th frame is centered at time <code class="reqn">t \times \mbox{hop\_length}</code>.
Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="torch_stft_+3A_pad_mode">pad_mode</code></td>
<td>
<p>(string, optional) controls the padding method used when
<code>center</code> is <code>TRUE</code>. Default: <code>"reflect"</code></p>
</td></tr>
<tr><td><code id="torch_stft_+3A_normalized">normalized</code></td>
<td>
<p>(bool, optional) controls whether to return the normalized
STFT results Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="torch_stft_+3A_onesided">onesided</code></td>
<td>
<p>(bool, optional) controls whether to return half of results to
avoid redundancy Default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="torch_stft_+3A_return_complex">return_complex</code></td>
<td>
<p>(bool, optional) controls whether to return complex tensors
or not.</p>
</td></tr>
</table>


<h3>Short-time Fourier transform (STFT). </h3>

<p>Short-time Fourier transform (STFT).
</p>
<div class="sourceCode"><pre>Ignoring the optional batch dimension, this method computes the following
expression:
</pre></div>
<p style="text-align: center;"><code class="reqn">
        X[m, \omega] = \sum_{k = 0}^{\mbox{win\_length-1}}%
                            \mbox{window}[k]\ \mbox{input}[m \times \mbox{hop\_length} + k]\ %
                            \exp\left(- j \frac{2 \pi \cdot \omega k}{\mbox{win\_length}}\right),
</code>
</p>

<p>where <code class="reqn">m</code> is the index of the sliding window, and <code class="reqn">\omega</code> is
the frequency that <code class="reqn">0 \leq \omega &lt; \mbox{n\_fft}</code>. When
<code>onesided</code> is the default value <code>TRUE</code>,
</p>
<div class="sourceCode"><pre>* `input` must be either a 1-D time sequence or a 2-D batch of time
  sequences.

* If `hop_length` is `NULL` (default), it is treated as equal to
  `floor(n_fft / 4)`.

* If `win_length` is `NULL` (default), it is treated as equal to
  `n_fft`.

* `window` can be a 1-D tensor of size `win_length`, e.g., from
  `torch_hann_window`. If `window` is `NULL` (default), it is
  treated as if having \eqn{1} everywhere in the window. If
  \eqn{\mbox{win\_length} &lt; \mbox{n\_fft}}, `window` will be padded on
  both sides to length `n_fft` before being applied.

* If `center` is `TRUE` (default), `input` will be padded on
  both sides so that the \eqn{t}-th frame is centered at time
  \eqn{t \times \mbox{hop\_length}}. Otherwise, the \eqn{t}-th frame
  begins at time  \eqn{t \times \mbox{hop\_length}}.

* `pad_mode` determines the padding method used on `input` when
  `center` is `TRUE`. See `torch_nn.functional.pad` for
  all available options. Default is `"reflect"`.

* If `onesided` is `TRUE` (default), only values for \eqn{\omega}
  in \eqn{\left[0, 1, 2, \dots, \left\lfloor \frac{\mbox{n\_fft}}{2} \right\rfloor + 1\right]}
  are returned because the real-to-complex Fourier transform satisfies the
  conjugate symmetry, i.e., \eqn{X[m, \omega] = X[m, \mbox{n\_fft} - \omega]^*}.

* If `normalized` is `TRUE` (default is `FALSE`), the function
  returns the normalized STFT results, i.e., multiplied by \eqn{(\mbox{frame\_length})^{-0.5}}.

Returns the real and the imaginary parts together as one tensor of size
\eqn{(* \times N \times T \times 2)}, where \eqn{*} is the optional
batch size of `input`, \eqn{N} is the number of frequencies where
STFT is applied, \eqn{T} is the total number of frames used, and each pair
in the last dimension represents a complex number as the real part and the
imaginary part.
</pre></div>


<h3>Warning</h3>

<p>This function changed signature at version 0.4.1. Calling with the
previous signature may cause error or return incorrect result.
</p>

<hr>
<h2 id='torch_sub'>Sub</h2><span id='topic+torch_sub'></span>

<h3>Description</h3>

<p>Sub
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sub(self, other, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sub_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_sub_+3A_other">other</code></td>
<td>
<p>(Tensor or Scalar) the tensor or scalar to subtract from <code>input</code></p>
</td></tr>
<tr><td><code id="torch_sub_+3A_alpha">alpha</code></td>
<td>
<p>the scalar multiplier for other</p>
</td></tr>
</table>


<h3>sub(input, other, *, alpha=1, out=None) -&gt; Tensor </h3>

<p>Subtracts <code>other</code>, scaled by <code>alpha</code>, from <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{{out}}_i = \mbox{{input}}_i - \mbox{{alpha}} \times \mbox{{other}}_i
</code>
</p>

<p>Supports broadcasting to a common shape ,
type promotion , and integer, float, and complex inputs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2))
b &lt;- torch_tensor(c(0, 1))
torch_sub(a, b, alpha=2)
}
</code></pre>

<hr>
<h2 id='torch_subtract'>Subtract</h2><span id='topic+torch_subtract'></span>

<h3>Description</h3>

<p>Subtract
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_subtract(self, other, alpha = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_subtract_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_subtract_+3A_other">other</code></td>
<td>
<p>(Tensor or Scalar) the tensor or scalar to subtract from <code>input</code></p>
</td></tr>
<tr><td><code id="torch_subtract_+3A_alpha">alpha</code></td>
<td>
<p>the scalar multiplier for other</p>
</td></tr>
</table>


<h3>subtract(input, other, *, alpha=1, out=None) -&gt; Tensor </h3>

<p>Alias for <code><a href="#topic+torch_sub">torch_sub()</a></code>.
</p>

<hr>
<h2 id='torch_sum'>Sum</h2><span id='topic+torch_sum'></span>

<h3>Description</h3>

<p>Sum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_sum(self, dim, keepdim = FALSE, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_sum_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_sum_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_sum_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
<tr><td><code id="torch_sum_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        If specified, the input tensor is casted to <code>dtype</code> before the operation        is performed. This is useful for preventing data type overflows. Default: NULL.</p>
</td></tr>
</table>


<h3>sum(input, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the sum of all elements in the <code>input</code> tensor.
</p>


<h3>sum(input, dim, keepdim=False, dtype=NULL) -&gt; Tensor </h3>

<p>Returns the sum of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>. If <code>dim</code> is a list of dimensions,
reduce over all of them.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_sum(a)


a &lt;- torch_randn(c(4, 4))
a
torch_sum(a, 1)
b &lt;- torch_arange(1, 4 * 5 * 6)$view(c(4, 5, 6))
torch_sum(b, list(2, 1))
}
</code></pre>

<hr>
<h2 id='torch_svd'>Svd</h2><span id='topic+torch_svd'></span>

<h3>Description</h3>

<p>Svd
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_svd(self, some = TRUE, compute_uv = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_svd_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor of size <code class="reqn">(*, m, n)</code> where <code>*</code> is zero or more                    batch dimensions consisting of <code class="reqn">m \times n</code> matrices.</p>
</td></tr>
<tr><td><code id="torch_svd_+3A_some">some</code></td>
<td>
<p>(bool, optional) controls the shape of returned <code>U</code> and <code>V</code></p>
</td></tr>
<tr><td><code id="torch_svd_+3A_compute_uv">compute_uv</code></td>
<td>
<p>(bool, optional) option whether to compute <code>U</code> and <code>V</code> or not</p>
</td></tr>
</table>


<h3>svd(input, some=TRUE, compute_uv=TRUE) -&gt; (Tensor, Tensor, Tensor) </h3>

<p>This function returns a namedtuple <code style="white-space: pre;">&#8288;(U, S, V)&#8288;</code> which is the singular value
decomposition of a input real matrix or batches of real matrices <code>input</code> such that
<code class="reqn">input = U \times diag(S) \times V^T</code>.
</p>
<p>If <code>some</code> is <code>TRUE</code> (default), the method returns the reduced singular value decomposition
i.e., if the last two dimensions of <code>input</code> are <code>m</code> and <code>n</code>, then the returned
<code>U</code> and <code>V</code> matrices will contain only <code class="reqn">min(n, m)</code> orthonormal columns.
</p>
<p>If <code>compute_uv</code> is <code>FALSE</code>, the returned <code>U</code> and <code>V</code> matrices will be zero matrices
of shape <code class="reqn">(m \times m)</code> and <code class="reqn">(n \times n)</code> respectively. <code>some</code> will be ignored here.
</p>


<h3>Note</h3>

<p>The singular values are returned in descending order. If <code>input</code> is a batch of matrices,
then the singular values of each matrix in the batch is returned in descending order.
</p>
<p>The implementation of SVD on CPU uses the LAPACK routine <code>?gesdd</code> (a divide-and-conquer
algorithm) instead of <code>?gesvd</code> for speed. Analogously, the SVD on GPU uses the MAGMA routine
<code>gesdd</code> as well.
</p>
<p>Irrespective of the original strides, the returned matrix <code>U</code>
will be transposed, i.e. with strides <code style="white-space: pre;">&#8288;U.contiguous().transpose(-2, -1).stride()&#8288;</code>
</p>
<p>Extra care needs to be taken when backward through <code>U</code> and <code>V</code>
outputs. Such operation is really only stable when <code>input</code> is
full rank with all distinct singular values. Otherwise, <code>NaN</code> can
appear as the gradients are not properly defined. Also, notice that
double backward will usually do an additional backward through <code>U</code> and
<code>V</code> even if the original backward is only on <code>S</code>.
</p>
<p>When <code>some</code> = <code>FALSE</code>, the gradients on <code style="white-space: pre;">&#8288;U[..., :, min(m, n):]&#8288;</code>
and <code style="white-space: pre;">&#8288;V[..., :, min(m, n):]&#8288;</code> will be ignored in backward as those vectors
can be arbitrary bases of the subspaces.
</p>
<p>When <code>compute_uv</code> = <code>FALSE</code>, backward cannot be performed since <code>U</code> and <code>V</code>
from the forward pass is required for the backward operation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(5, 3))
a
out = torch_svd(a)
u = out[[1]]
s = out[[2]]
v = out[[3]]
torch_dist(a, torch_mm(torch_mm(u, torch_diag(s)), v$t()))
a_big = torch_randn(c(7, 5, 3))
out = torch_svd(a_big)
u = out[[1]]
s = out[[2]]
v = out[[3]]
torch_dist(a_big, torch_matmul(torch_matmul(u, torch_diag_embed(s)), v$transpose(-2, -1)))
}
</code></pre>

<hr>
<h2 id='torch_t'>T</h2><span id='topic+torch_t'></span>

<h3>Description</h3>

<p>T
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_t(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_t_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>t(input) -&gt; Tensor </h3>

<p>Expects <code>input</code> to be &lt;= 2-D tensor and transposes dimensions 0
and 1.
</p>
<p>0-D and 1-D tensors are returned as is. When input is a 2-D tensor this
is equivalent to <code>transpose(input, 0, 1)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(2,3))
x
torch_t(x)
x = torch_randn(c(3))
x
torch_t(x)
x = torch_randn(c(2, 3))
x
torch_t(x)
}
</code></pre>

<hr>
<h2 id='torch_take'>Take</h2><span id='topic+torch_take'></span>

<h3>Description</h3>

<p>Take
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_take(self, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_take_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_take_+3A_index">index</code></td>
<td>
<p>(LongTensor) the indices into tensor</p>
</td></tr>
</table>


<h3>take(input, index) -&gt; Tensor </h3>

<p>Returns a new tensor with the elements of <code>input</code> at the given indices.
The input tensor is treated as if it were viewed as a 1-D tensor. The result
takes the same shape as the indices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

src = torch_tensor(matrix(c(4,3,5,6,7,8), ncol = 3, byrow = TRUE))
torch_take(src, torch_tensor(c(1, 2, 5), dtype = torch_int64()))
}
</code></pre>

<hr>
<h2 id='torch_tan'>Tan</h2><span id='topic+torch_tan'></span>

<h3>Description</h3>

<p>Tan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tan(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tan_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>tan(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the tangent of the elements of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \tan(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_tan(a)
}
</code></pre>

<hr>
<h2 id='torch_tanh'>Tanh</h2><span id='topic+torch_tanh'></span>

<h3>Description</h3>

<p>Tanh
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tanh(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tanh_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>tanh(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the hyperbolic tangent of the elements
of <code>input</code>.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_{i} = \tanh(\mbox{input}_{i})
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_tanh(a)
}
</code></pre>

<hr>
<h2 id='torch_tensor'>Converts R objects to a torch tensor</h2><span id='topic+torch_tensor'></span>

<h3>Description</h3>

<p>Converts R objects to a torch tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tensor(
  data,
  dtype = NULL,
  device = NULL,
  requires_grad = FALSE,
  pin_memory = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tensor_+3A_data">data</code></td>
<td>
<p>an R atomic vector, matrix or array</p>
</td></tr>
<tr><td><code id="torch_tensor_+3A_dtype">dtype</code></td>
<td>
<p>a <a href="#topic+torch_dtype">torch_dtype</a> instance</p>
</td></tr>
<tr><td><code id="torch_tensor_+3A_device">device</code></td>
<td>
<p>a device creted with <code><a href="#topic+torch_device">torch_device()</a></code></p>
</td></tr>
<tr><td><code id="torch_tensor_+3A_requires_grad">requires_grad</code></td>
<td>
<p>if autograd should record operations on the returned tensor.</p>
</td></tr>
<tr><td><code id="torch_tensor_+3A_pin_memory">pin_memory</code></td>
<td>
<p>If set, returned tensor would be allocated in the pinned memory.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
torch_tensor(c(1, 2, 3, 4))
torch_tensor(c(1, 2, 3, 4), dtype = torch_int())
}
</code></pre>

<hr>
<h2 id='torch_tensor_from_buffer'>Creates a tensor from a buffer of memory</h2><span id='topic+torch_tensor_from_buffer'></span><span id='topic+buffer_from_torch_tensor'></span>

<h3>Description</h3>

<p>It creates a tensor without taking ownership of the memory it points to.
You must call <code>clone</code> if you want to copy the memory over a new tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tensor_from_buffer(buffer, shape, dtype = "float")

buffer_from_torch_tensor(tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tensor_from_buffer_+3A_buffer">buffer</code></td>
<td>
<p>An R atomic object containing the data in a contiguous array.</p>
</td></tr>
<tr><td><code id="torch_tensor_from_buffer_+3A_shape">shape</code></td>
<td>
<p>The shape of the resulting tensor.</p>
</td></tr>
<tr><td><code id="torch_tensor_from_buffer_+3A_dtype">dtype</code></td>
<td>
<p>A torch data type for the tresulting tensor.</p>
</td></tr>
<tr><td><code id="torch_tensor_from_buffer_+3A_tensor">tensor</code></td>
<td>
<p>Tensor object that will be converted into a buffer.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>buffer_from_torch_tensor()</code>: Creates a raw vector containing the tensor data. Causes a data copy.
</p>
</li></ul>

<hr>
<h2 id='torch_tensordot'>Tensordot</h2><span id='topic+torch_tensordot'></span>

<h3>Description</h3>

<p>Returns a contraction of a and b over multiple dimensions.
<code>tensordot</code> implements a generalized matrix product.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tensordot(a, b, dims = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tensordot_+3A_a">a</code></td>
<td>
<p>(Tensor) Left tensor to contract</p>
</td></tr>
<tr><td><code id="torch_tensordot_+3A_b">b</code></td>
<td>
<p>(Tensor) Right tensor to contract</p>
</td></tr>
<tr><td><code id="torch_tensordot_+3A_dims">dims</code></td>
<td>
<p>(int or tuple of two lists of integers) number of dimensions to     contract or explicit lists of dimensions for <code>a</code> and     <code>b</code> respectively</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_arange(start = 1, end = 60)$reshape(c(3, 4, 5))
b &lt;- torch_arange(start = 1, end = 24)$reshape(c(4, 3, 2))
torch_tensordot(a, b, dims = list(c(2, 1), c(1, 2)))
## Not run: 
a = torch_randn(3, 4, 5, device='cuda')
b = torch_randn(4, 5, 6, device='cuda')
c = torch_tensordot(a, b, dims=2)$cpu()

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_threshold_'>Threshold_</h2><span id='topic+torch_threshold_'></span>

<h3>Description</h3>

<p>Threshold_
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_threshold_(self, threshold, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_threshold__+3A_self">self</code></td>
<td>
<p>input tensor</p>
</td></tr>
<tr><td><code id="torch_threshold__+3A_threshold">threshold</code></td>
<td>
<p>The value to threshold at</p>
</td></tr>
<tr><td><code id="torch_threshold__+3A_value">value</code></td>
<td>
<p>The value to replace with</p>
</td></tr>
</table>


<h3>threshold_(input, threshold, value) -&gt; Tensor </h3>

<p>In-place version of <code>torch_threshold</code>.
</p>

<hr>
<h2 id='torch_topk'>Topk</h2><span id='topic+torch_topk'></span>

<h3>Description</h3>

<p>Topk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_topk(self, k, dim = -1L, largest = TRUE, sorted = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_topk_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_topk_+3A_k">k</code></td>
<td>
<p>(int) the k in &quot;top-k&quot;</p>
</td></tr>
<tr><td><code id="torch_topk_+3A_dim">dim</code></td>
<td>
<p>(int, optional) the dimension to sort along</p>
</td></tr>
<tr><td><code id="torch_topk_+3A_largest">largest</code></td>
<td>
<p>(bool, optional) controls whether to return largest or           smallest elements</p>
</td></tr>
<tr><td><code id="torch_topk_+3A_sorted">sorted</code></td>
<td>
<p>(bool, optional) controls whether to return the elements           in sorted order</p>
</td></tr>
</table>


<h3>topk(input, k, dim=NULL, largest=TRUE, sorted=TRUE) -&gt; (Tensor, LongTensor) </h3>

<p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along
a given dimension.
</p>
<p>If <code>dim</code> is not given, the last dimension of the <code>input</code> is chosen.
</p>
<p>If <code>largest</code> is <code>FALSE</code> then the <code>k</code> smallest elements are returned.
</p>
<p>A namedtuple of <code style="white-space: pre;">&#8288;(values, indices)&#8288;</code> is returned, where the <code>indices</code> are the indices
of the elements in the original <code>input</code> tensor.
</p>
<p>The boolean option <code>sorted</code> if <code>TRUE</code>, will make sure that the returned
<code>k</code> elements are themselves sorted
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_arange(1., 6.)
x
torch_topk(x, 3)
}
</code></pre>

<hr>
<h2 id='torch_trace'>Trace</h2><span id='topic+torch_trace'></span>

<h3>Description</h3>

<p>Trace
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_trace(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_trace_+3A_self">self</code></td>
<td>
<p>the input tensor</p>
</td></tr>
</table>


<h3>trace(input) -&gt; Tensor </h3>

<p>Returns the sum of the elements of the diagonal of the input 2-D matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_arange(1, 9)$view(c(3, 3))
x
torch_trace(x)
}
</code></pre>

<hr>
<h2 id='torch_transpose'>Transpose</h2><span id='topic+torch_transpose'></span>

<h3>Description</h3>

<p>Transpose
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_transpose(self, dim0, dim1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_transpose_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_transpose_+3A_dim0">dim0</code></td>
<td>
<p>(int) the first dimension to be transposed</p>
</td></tr>
<tr><td><code id="torch_transpose_+3A_dim1">dim1</code></td>
<td>
<p>(int) the second dimension to be transposed</p>
</td></tr>
</table>


<h3>transpose(input, dim0, dim1) -&gt; Tensor </h3>

<p>Returns a tensor that is a transposed version of <code>input</code>.
The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.
</p>
<p>The resulting <code>out</code> tensor shares it's underlying storage with the
<code>input</code> tensor, so changing the content of one would change the content
of the other.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_randn(c(2, 3))
x
torch_transpose(x, 1, 2)
}
</code></pre>

<hr>
<h2 id='torch_trapz'>Trapz</h2><span id='topic+torch_trapz'></span>

<h3>Description</h3>

<p>Trapz
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_trapz(y, dx = 1L, x, dim = -1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_trapz_+3A_y">y</code></td>
<td>
<p>(Tensor) The values of the function to integrate</p>
</td></tr>
<tr><td><code id="torch_trapz_+3A_dx">dx</code></td>
<td>
<p>(float) The distance between points at which <code>y</code> is sampled.</p>
</td></tr>
<tr><td><code id="torch_trapz_+3A_x">x</code></td>
<td>
<p>(Tensor) The points at which the function <code>y</code> is sampled.        If <code>x</code> is not in ascending order, intervals on which it is decreasing        contribute negatively to the estimated integral (i.e., the convention        <code class="reqn">\int_a^b f = -\int_b^a f</code> is followed).</p>
</td></tr>
<tr><td><code id="torch_trapz_+3A_dim">dim</code></td>
<td>
<p>(int) The dimension along which to integrate.        By default, use the last dimension.</p>
</td></tr>
</table>


<h3>trapz(y, x, *, dim=-1) -&gt; Tensor </h3>

<p>Estimate <code class="reqn">\int y\,dx</code> along <code>dim</code>, using the trapezoid rule.
</p>


<h3>trapz(y, *, dx=1, dim=-1) -&gt; Tensor </h3>

<p>As above, but the sample points are spaced uniformly at a distance of <code>dx</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

y = torch_randn(list(2, 3))
y
x = torch_tensor(matrix(c(1, 3, 4, 1, 2, 3), ncol = 3, byrow=TRUE))
torch_trapz(y, x = x)

}
</code></pre>

<hr>
<h2 id='torch_triangular_solve'>Triangular_solve</h2><span id='topic+torch_triangular_solve'></span>

<h3>Description</h3>

<p>Triangular_solve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_triangular_solve(
  self,
  A,
  upper = TRUE,
  transpose = FALSE,
  unitriangular = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_triangular_solve_+3A_self">self</code></td>
<td>
<p>(Tensor) multiple right-hand sides of size <code class="reqn">(*, m, k)</code> where                <code class="reqn">*</code> is zero of more batch dimensions (<code class="reqn">b</code>)</p>
</td></tr>
<tr><td><code id="torch_triangular_solve_+3A_a">A</code></td>
<td>
<p>(Tensor) the input triangular coefficient matrix of size <code class="reqn">(*, m, m)</code>                where <code class="reqn">*</code> is zero or more batch dimensions</p>
</td></tr>
<tr><td><code id="torch_triangular_solve_+3A_upper">upper</code></td>
<td>
<p>(bool, optional) whether to solve the upper-triangular system        of equations (default) or the lower-triangular system of equations. Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="torch_triangular_solve_+3A_transpose">transpose</code></td>
<td>
<p>(bool, optional) whether <code class="reqn">A</code> should be transposed before        being sent into the solver. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_triangular_solve_+3A_unitriangular">unitriangular</code></td>
<td>
<p>(bool, optional) whether <code class="reqn">A</code> is unit triangular.        If TRUE, the diagonal elements of <code class="reqn">A</code> are assumed to be        1 and not referenced from <code class="reqn">A</code>. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>triangular_solve(input, A, upper=TRUE, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor) </h3>

<p>Solves a system of equations with a triangular coefficient matrix <code class="reqn">A</code>
and multiple right-hand sides <code class="reqn">b</code>.
</p>
<p>In particular, solves <code class="reqn">AX = b</code> and assumes <code class="reqn">A</code> is upper-triangular
with the default keyword arguments.
</p>
<p><code>torch_triangular_solve(b, A)</code> can take in 2D inputs <code style="white-space: pre;">&#8288;b, A&#8288;</code> or inputs that are
batches of 2D matrices. If the inputs are batches, then returns
batched outputs <code>X</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

A = torch_randn(c(2, 2))$triu()
A
b = torch_randn(c(2, 3))
b
torch_triangular_solve(b, A)
}
</code></pre>

<hr>
<h2 id='torch_tril'>Tril</h2><span id='topic+torch_tril'></span>

<h3>Description</h3>

<p>Tril
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tril(self, diagonal = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tril_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_tril_+3A_diagonal">diagonal</code></td>
<td>
<p>(int, optional) the diagonal to consider</p>
</td></tr>
</table>


<h3>tril(input, diagonal=0, out=NULL) -&gt; Tensor </h3>

<p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices
<code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.
</p>
<p>The lower triangular part of the matrix is defined as the elements on and
below the diagonal.
</p>
<p>The argument <code>diagonal</code> controls which diagonal to consider. If
<code>diagonal</code> = 0, all elements on and below the main diagonal are
retained. A positive value includes just as many diagonals above the main
diagonal, and similarly a negative value excludes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<code class="reqn">\lbrace (i, i) \rbrace</code> for <code class="reqn">i \in [0, \min\{d_{1}, d_{2}\} - 1]</code> where
<code class="reqn">d_{1}, d_{2}</code> are the dimensions of the matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
a
torch_tril(a)
b = torch_randn(c(4, 6))
b
torch_tril(b, diagonal=1)
torch_tril(b, diagonal=-1)
}
</code></pre>

<hr>
<h2 id='torch_tril_indices'>Tril_indices</h2><span id='topic+torch_tril_indices'></span>

<h3>Description</h3>

<p>Tril_indices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_tril_indices(
  row,
  col,
  offset = 0,
  dtype = NULL,
  device = NULL,
  layout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_tril_indices_+3A_row">row</code></td>
<td>
<p>(<code>int</code>) number of rows in the 2-D matrix.</p>
</td></tr>
<tr><td><code id="torch_tril_indices_+3A_col">col</code></td>
<td>
<p>(<code>int</code>) number of columns in the 2-D matrix.</p>
</td></tr>
<tr><td><code id="torch_tril_indices_+3A_offset">offset</code></td>
<td>
<p>(<code>int</code>) diagonal offset from the main diagonal.        Default: if not provided, 0.</p>
</td></tr>
<tr><td><code id="torch_tril_indices_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, <code>torch_long</code>.</p>
</td></tr>
<tr><td><code id="torch_tril_indices_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_tril_indices_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) currently only support <code>torch_strided</code>.</p>
</td></tr>
</table>


<h3>tril_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor </h3>

<p>Returns the indices of the lower triangular part of a <code>row</code>-by-
<code>col</code> matrix in a 2-by-N Tensor, where the first row contains row
coordinates of all indices and the second row contains column coordinates.
Indices are ordered based on rows and then columns.
</p>
<p>The lower triangular part of the matrix is defined as the elements on and
below the diagonal.
</p>
<p>The argument <code>offset</code> controls which diagonal to consider. If
<code>offset</code> = 0, all elements on and below the main diagonal are
retained. A positive value includes just as many diagonals above the main
diagonal, and similarly a negative value excludes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<code class="reqn">\lbrace (i, i) \rbrace</code> for <code class="reqn">i \in [0, \min\{d_{1}, d_{2}\} - 1]</code>
where <code class="reqn">d_{1}, d_{2}</code> are the dimensions of the matrix.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>When running on CUDA, `row * col` must be less than \eqn{2^{59}} to
prevent overflow during calculation.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
a = torch_tril_indices(3, 3)
a
a = torch_tril_indices(4, 3, -1)
a
a = torch_tril_indices(4, 3, 1)
a

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_triu'>Triu</h2><span id='topic+torch_triu'></span>

<h3>Description</h3>

<p>Triu
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_triu(self, diagonal = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_triu_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_triu_+3A_diagonal">diagonal</code></td>
<td>
<p>(int, optional) the diagonal to consider</p>
</td></tr>
</table>


<h3>triu(input, diagonal=0, out=NULL) -&gt; Tensor </h3>

<p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices
<code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.
</p>
<p>The upper triangular part of the matrix is defined as the elements on and
above the diagonal.
</p>
<p>The argument <code>diagonal</code> controls which diagonal to consider. If
<code>diagonal</code> = 0, all elements on and above the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<code class="reqn">\lbrace (i, i) \rbrace</code> for <code class="reqn">i \in [0, \min\{d_{1}, d_{2}\} - 1]</code> where
<code class="reqn">d_{1}, d_{2}</code> are the dimensions of the matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(3, 3))
a
torch_triu(a)
torch_triu(a, diagonal=1)
torch_triu(a, diagonal=-1)
b = torch_randn(c(4, 6))
b
torch_triu(b, diagonal=1)
torch_triu(b, diagonal=-1)
}
</code></pre>

<hr>
<h2 id='torch_triu_indices'>Triu_indices</h2><span id='topic+torch_triu_indices'></span>

<h3>Description</h3>

<p>Triu_indices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_triu_indices(
  row,
  col,
  offset = 0,
  dtype = NULL,
  device = NULL,
  layout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_triu_indices_+3A_row">row</code></td>
<td>
<p>(<code>int</code>) number of rows in the 2-D matrix.</p>
</td></tr>
<tr><td><code id="torch_triu_indices_+3A_col">col</code></td>
<td>
<p>(<code>int</code>) number of columns in the 2-D matrix.</p>
</td></tr>
<tr><td><code id="torch_triu_indices_+3A_offset">offset</code></td>
<td>
<p>(<code>int</code>) diagonal offset from the main diagonal.        Default: if not provided, 0.</p>
</td></tr>
<tr><td><code id="torch_triu_indices_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, <code>torch_long</code>.</p>
</td></tr>
<tr><td><code id="torch_triu_indices_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_triu_indices_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) currently only support <code>torch_strided</code>.</p>
</td></tr>
</table>


<h3>triu_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor </h3>

<p>Returns the indices of the upper triangular part of a <code>row</code> by
<code>col</code> matrix in a 2-by-N Tensor, where the first row contains row
coordinates of all indices and the second row contains column coordinates.
Indices are ordered based on rows and then columns.
</p>
<p>The upper triangular part of the matrix is defined as the elements on and
above the diagonal.
</p>
<p>The argument <code>offset</code> controls which diagonal to consider. If
<code>offset</code> = 0, all elements on and above the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<code class="reqn">\lbrace (i, i) \rbrace</code> for <code class="reqn">i \in [0, \min\{d_{1}, d_{2}\} - 1]</code>
where <code class="reqn">d_{1}, d_{2}</code> are the dimensions of the matrix.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>When running on CUDA, `row * col` must be less than \eqn{2^{59}} to
prevent overflow during calculation.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
## Not run: 
a = torch_triu_indices(3, 3)
a
a = torch_triu_indices(4, 3, -1)
a
a = torch_triu_indices(4, 3, 1)
a

## End(Not run)
}
</code></pre>

<hr>
<h2 id='torch_true_divide'>TRUE_divide</h2><span id='topic+torch_true_divide'></span>

<h3>Description</h3>

<p>TRUE_divide
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_true_divide(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_true_divide_+3A_self">self</code></td>
<td>
<p>(Tensor) the dividend</p>
</td></tr>
<tr><td><code id="torch_true_divide_+3A_other">other</code></td>
<td>
<p>(Tensor or Scalar) the divisor</p>
</td></tr>
</table>


<h3>true_divide(dividend, divisor) -&gt; Tensor </h3>

<p>Performs &quot;true division&quot; that always computes the division
in floating point. Analogous to division in Python 3 and equivalent to
<code><a href="#topic+torch_div">torch_div</a></code> except when both inputs have bool or integer scalar types,
in which case they are cast to the default (floating) scalar type before the division.
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \frac{\mbox{dividend}_i}{\mbox{divisor}}
</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

dividend = torch_tensor(c(5, 3), dtype=torch_int())
divisor = torch_tensor(c(3, 2), dtype=torch_int())
torch_true_divide(dividend, divisor)
torch_true_divide(dividend, 2)
}
</code></pre>

<hr>
<h2 id='torch_trunc'>Trunc</h2><span id='topic+torch_trunc'></span>

<h3>Description</h3>

<p>Trunc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_trunc(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_trunc_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>trunc(input, out=NULL) -&gt; Tensor </h3>

<p>Returns a new tensor with the truncated integer values of
the elements of <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(4))
a
torch_trunc(a)
}
</code></pre>

<hr>
<h2 id='torch_unbind'>Unbind</h2><span id='topic+torch_unbind'></span>

<h3>Description</h3>

<p>Unbind
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_unbind(self, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_unbind_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to unbind</p>
</td></tr>
<tr><td><code id="torch_unbind_+3A_dim">dim</code></td>
<td>
<p>(int) dimension to remove</p>
</td></tr>
</table>


<h3>unbind(input, dim=0) -&gt; seq </h3>

<p>Removes a tensor dimension.
</p>
<p>Returns a tuple of all slices along a given dimension, already without it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_unbind(torch_tensor(matrix(1:9, ncol = 3, byrow=TRUE)))
}
</code></pre>

<hr>
<h2 id='torch_unique_consecutive'>Unique_consecutive</h2><span id='topic+torch_unique_consecutive'></span>

<h3>Description</h3>

<p>Unique_consecutive
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_unique_consecutive(
  self,
  return_inverse = FALSE,
  return_counts = FALSE,
  dim = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_unique_consecutive_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor</p>
</td></tr>
<tr><td><code id="torch_unique_consecutive_+3A_return_inverse">return_inverse</code></td>
<td>
<p>(bool) Whether to also return the indices for where        elements in the original input ended up in the returned unique list.</p>
</td></tr>
<tr><td><code id="torch_unique_consecutive_+3A_return_counts">return_counts</code></td>
<td>
<p>(bool) Whether to also return the counts for each unique        element.</p>
</td></tr>
<tr><td><code id="torch_unique_consecutive_+3A_dim">dim</code></td>
<td>
<p>(int) the dimension to apply unique. If <code>NULL</code>, the unique of the        flattened input is returned. default: <code>NULL</code></p>
</td></tr>
</table>


<h3>TEST </h3>

<p>Eliminates all but the first element from every consecutive group of equivalent elements.
</p>
<div class="sourceCode"><pre>.. note:: This function is different from [`torch_unique`] in the sense that this function
    only eliminates consecutive duplicate values. This semantics is similar to `std::unique`
    in C++.
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x = torch_tensor(c(1, 1, 2, 2, 3, 1, 1, 2))
output = torch_unique_consecutive(x)
output
torch_unique_consecutive(x, return_inverse=TRUE)
torch_unique_consecutive(x, return_counts=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_unsafe_chunk'>Unsafe_chunk</h2><span id='topic+torch_unsafe_chunk'></span>

<h3>Description</h3>

<p>Unsafe_chunk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_unsafe_chunk(self, chunks, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_unsafe_chunk_+3A_self">self</code></td>
<td>
<p>(Tensor) the tensor to split</p>
</td></tr>
<tr><td><code id="torch_unsafe_chunk_+3A_chunks">chunks</code></td>
<td>
<p>(int) number of chunks to return</p>
</td></tr>
<tr><td><code id="torch_unsafe_chunk_+3A_dim">dim</code></td>
<td>
<p>(int) dimension along which to split the tensor</p>
</td></tr>
</table>


<h3>unsafe_chunk(input, chunks, dim=0) -&gt; List of Tensors </h3>

<p>Works like <code><a href="#topic+torch_chunk">torch_chunk()</a></code> but without enforcing the autograd restrictions
on inplace modification of the outputs.
</p>


<h3>Warning</h3>

<p>This function is safe to use as long as only the input, or only the outputs
are modified inplace after calling this function. It is user's
responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
</p>

<hr>
<h2 id='torch_unsafe_split'>Unsafe_split</h2><span id='topic+torch_unsafe_split'></span>

<h3>Description</h3>

<p>Unsafe_split
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_unsafe_split(self, split_size, dim = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_unsafe_split_+3A_self">self</code></td>
<td>
<p>(Tensor) tensor to split.</p>
</td></tr>
<tr><td><code id="torch_unsafe_split_+3A_split_size">split_size</code></td>
<td>
<p>(int) size of a single chunk or
list of sizes for each chunk</p>
</td></tr>
<tr><td><code id="torch_unsafe_split_+3A_dim">dim</code></td>
<td>
<p>(int) dimension along which to split the tensor.</p>
</td></tr>
</table>


<h3>unsafe_split(tensor, split_size_or_sections, dim=0) -&gt; List of Tensors </h3>

<p>Works like <code><a href="#topic+torch_split">torch_split()</a></code> but without enforcing the autograd restrictions
on inplace modification of the outputs.
</p>


<h3>Warning</h3>

<p>This function is safe to use as long as only the input, or only the outputs
are modified inplace after calling this function. It is user's
responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
</p>

<hr>
<h2 id='torch_unsqueeze'>Unsqueeze</h2><span id='topic+torch_unsqueeze'></span>

<h3>Description</h3>

<p>Unsqueeze
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_unsqueeze(self, dim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_unsqueeze_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_unsqueeze_+3A_dim">dim</code></td>
<td>
<p>(int) the index at which to insert the singleton dimension</p>
</td></tr>
</table>


<h3>unsqueeze(input, dim) -&gt; Tensor </h3>

<p>Returns a new tensor with a dimension of size one inserted at the
specified position.
</p>
<p>The returned tensor shares the same underlying data with this tensor.
</p>
<p>A <code>dim</code> value within the range <code style="white-space: pre;">&#8288;[-input.dim() - 1, input.dim() + 1)&#8288;</code>
can be used. Negative <code>dim</code> will correspond to <code>unsqueeze</code>
applied at <code>dim</code> = <code>dim + input.dim() + 1</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x = torch_tensor(c(1, 2, 3, 4))
torch_unsqueeze(x, 1)
torch_unsqueeze(x, 2)
}
</code></pre>

<hr>
<h2 id='torch_vander'>Vander</h2><span id='topic+torch_vander'></span>

<h3>Description</h3>

<p>Vander
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_vander(x, N = NULL, increasing = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_vander_+3A_x">x</code></td>
<td>
<p>(Tensor) 1-D input tensor.</p>
</td></tr>
<tr><td><code id="torch_vander_+3A_n">N</code></td>
<td>
<p>(int, optional) Number of columns in the output. If N is not specified,
a square array is returned <code class="reqn">(N = len(x))</code>.</p>
</td></tr>
<tr><td><code id="torch_vander_+3A_increasing">increasing</code></td>
<td>
<p>(bool, optional) Order of the powers of the columns. If TRUE,
the powers increase from left to right, if FALSE (the default) they are reversed.</p>
</td></tr>
</table>


<h3>vander(x, N=None, increasing=FALSE) -&gt; Tensor </h3>

<p>Generates a Vandermonde matrix.
</p>
<p>The columns of the output matrix are elementwise powers of the input vector
<code class="reqn">x^{(N-1)}, x^{(N-2)}, ..., x^0</code>.
If increasing is TRUE, the order of the columns is reversed
<code class="reqn">x^0, x^1, ..., x^{(N-1)}</code>. Such a
matrix with a geometric progression in each row is
named for Alexandre-Theophile Vandermonde.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_tensor(c(1, 2, 3, 5))
torch_vander(x)
torch_vander(x, N=3)
torch_vander(x, N=3, increasing=TRUE)
}
</code></pre>

<hr>
<h2 id='torch_var'>Var</h2><span id='topic+torch_var'></span>

<h3>Description</h3>

<p>Var
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_var(self, dim, unbiased = TRUE, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_var_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_var_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_var_+3A_unbiased">unbiased</code></td>
<td>
<p>(bool) whether to use the unbiased estimation or not</p>
</td></tr>
<tr><td><code id="torch_var_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>var(input, unbiased=TRUE) -&gt; Tensor </h3>

<p>Returns the variance of all elements in the <code>input</code> tensor.
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>var(input, dim, keepdim=False, unbiased=TRUE, out=NULL) -&gt; Tensor </h3>

<p>Returns the variance of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_var(a)


a = torch_randn(c(4, 4))
a
torch_var(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_var_mean'>Var_mean</h2><span id='topic+torch_var_mean'></span>

<h3>Description</h3>

<p>Var_mean
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_var_mean(self, dim, unbiased = TRUE, keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_var_mean_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
<tr><td><code id="torch_var_mean_+3A_dim">dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td></tr>
<tr><td><code id="torch_var_mean_+3A_unbiased">unbiased</code></td>
<td>
<p>(bool) whether to use the unbiased estimation or not</p>
</td></tr>
<tr><td><code id="torch_var_mean_+3A_keepdim">keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td></tr>
</table>


<h3>var_mean(input, unbiased=TRUE) -&gt; (Tensor, Tensor) </h3>

<p>Returns the variance and mean of all elements in the <code>input</code> tensor.
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>var_mean(input, dim, keepdim=False, unbiased=TRUE) -&gt; (Tensor, Tensor) </h3>

<p>Returns the variance and mean of each row of the <code>input</code> tensor in the given
dimension <code>dim</code>.
</p>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensor is of the same size
as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1.
Otherwise, <code>dim</code> is squeezed (see <code><a href="#topic+torch_squeeze">torch_squeeze</a></code>), resulting in the
output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).
</p>
<p>If <code>unbiased</code> is <code>FALSE</code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel's correction will be used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a = torch_randn(c(1, 3))
a
torch_var_mean(a)


a = torch_randn(c(4, 4))
a
torch_var_mean(a, 1)
}
</code></pre>

<hr>
<h2 id='torch_vdot'>Vdot</h2><span id='topic+torch_vdot'></span>

<h3>Description</h3>

<p>Vdot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_vdot(self, other)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_vdot_+3A_self">self</code></td>
<td>
<p>(Tensor) first tensor in the dot product. Its conjugate is used
if it's complex.</p>
</td></tr>
<tr><td><code id="torch_vdot_+3A_other">other</code></td>
<td>
<p>(Tensor) second tensor in the dot product.</p>
</td></tr>
</table>


<h3>vdot(input, other, *, out=None) -&gt; Tensor </h3>

<p>Computes the dot product (inner product) of two tensors. The vdot(a, b) function
handles complex numbers differently than dot(a, b). If the first argument is complex,
the complex conjugate of the first argument is used for the calculation of the dot product.
</p>


<h3>Note</h3>

<p>This function does not broadcast .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_vdot(torch_tensor(c(2, 3)), torch_tensor(c(2, 1)))
if (FALSE) {
a &lt;- torch_tensor(list(1 +2i, 3 - 1i))
b &lt;- torch_tensor(list(2 +1i, 4 - 0i))
torch_vdot(a, b)
torch_vdot(b, a)
}
}
</code></pre>

<hr>
<h2 id='torch_view_as_complex'>View_as_complex</h2><span id='topic+torch_view_as_complex'></span>

<h3>Description</h3>

<p>View_as_complex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_view_as_complex(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_view_as_complex_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>view_as_complex(input) -&gt; Tensor </h3>

<p>Returns a view of <code>input</code> as a complex tensor. For an input complex
tensor of <code>size</code> <code class="reqn">m1, m2, \dots, mi, 2</code>, this function returns a
new complex tensor of <code>size</code> <code class="reqn">m1, m2, \dots, mi</code> where the last
dimension of the input tensor is expected to represent the real and imaginary
components of complex numbers.
</p>


<h3>Warning</h3>

<p><a href="#topic+torch_view_as_complex">torch_view_as_complex</a> is only supported for tensors with
<code>torch_dtype</code> <code>torch_float64()</code> and <code>torch_float32()</code>.  The input is
expected to have the last dimension of <code>size</code> 2. In addition, the
tensor must have a <code>stride</code> of 1 for its last dimension. The strides of all
other dimensions must be even numbers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
if (FALSE) {
x=torch_randn(c(4, 2))
x
torch_view_as_complex(x)
}
}
</code></pre>

<hr>
<h2 id='torch_view_as_real'>View_as_real</h2><span id='topic+torch_view_as_real'></span>

<h3>Description</h3>

<p>View_as_real
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_view_as_real(self)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_view_as_real_+3A_self">self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td></tr>
</table>


<h3>view_as_real(input) -&gt; Tensor </h3>

<p>Returns a view of <code>input</code> as a real tensor. For an input complex tensor of
<code>size</code> <code class="reqn">m1, m2, \dots, mi</code>, this function returns a new
real tensor of size <code class="reqn">m1, m2, \dots, mi, 2</code>, where the last dimension of size 2
represents the real and imaginary components of complex numbers.
</p>


<h3>Warning</h3>

<p><code><a href="#topic+torch_view_as_real">torch_view_as_real()</a></code> is only supported for tensors with <code style="white-space: pre;">&#8288;complex dtypes&#8288;</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

if (FALSE) {
x &lt;- torch_randn(4, dtype=torch_cfloat())
x
torch_view_as_real(x)
}
}
</code></pre>

<hr>
<h2 id='torch_vstack'>Vstack</h2><span id='topic+torch_vstack'></span>

<h3>Description</h3>

<p>Vstack
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_vstack(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_vstack_+3A_tensors">tensors</code></td>
<td>
<p>(sequence of Tensors) sequence of tensors to concatenate</p>
</td></tr>
</table>


<h3>vstack(tensors, *, out=None) -&gt; Tensor </h3>

<p>Stack tensors in sequence vertically (row wise).
</p>
<p>This is equivalent to concatenation along the first axis after all 1-D tensors
have been reshaped by <code><a href="#topic+torch_atleast_2d">torch_atleast_2d()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

a &lt;- torch_tensor(c(1, 2, 3))
b &lt;- torch_tensor(c(4, 5, 6))
torch_vstack(list(a,b))
a &lt;- torch_tensor(rbind(1,2,3))
b &lt;- torch_tensor(rbind(4,5,6))
torch_vstack(list(a,b))
}
</code></pre>

<hr>
<h2 id='torch_where'>Where</h2><span id='topic+torch_where'></span>

<h3>Description</h3>

<p>Where
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_where(condition, self = NULL, other = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_where_+3A_condition">condition</code></td>
<td>
<p>(BoolTensor) When TRUE (nonzero), yield x, otherwise yield y</p>
</td></tr>
<tr><td><code id="torch_where_+3A_self">self</code></td>
<td>
<p>(Tensor) values selected at indices where <code>condition</code> is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="torch_where_+3A_other">other</code></td>
<td>
<p>(Tensor) values selected at indices where <code>condition</code> is <code>FALSE</code></p>
</td></tr>
</table>


<h3>where(condition, x, y) -&gt; Tensor </h3>

<p>Return a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>condition</code>.
</p>
<p>The operation is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{out}_i = \left\{ \begin{array}{ll}
        \mbox{x}_i &amp; \mbox{if } \mbox{condition}_i \\
        \mbox{y}_i &amp; \mbox{otherwise} \\
    \end{array}
    \right.
</code>
</p>



<h3>where(condition) -&gt; tuple of LongTensor </h3>

<p><code>torch_where(condition)</code> is identical to
<code>torch_nonzero(condition, as_tuple=TRUE)</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>The tensors `condition`, `x`, `y` must be broadcastable .
</pre></div>
<p>See also <code><a href="#topic+torch_nonzero">torch_nonzero()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

## Not run: 
x = torch_randn(c(3, 2))
y = torch_ones(c(3, 2))
x
torch_where(x &gt; 0, x, y)

## End(Not run)



}
</code></pre>

<hr>
<h2 id='torch_zeros'>Zeros</h2><span id='topic+torch_zeros'></span>

<h3>Description</h3>

<p>Zeros
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_zeros(
  ...,
  names = NULL,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_zeros_+3A_...">...</code></td>
<td>
<p>a sequence of integers defining the shape of the output tensor.        Can be a variable number of arguments or a collection like a list or tuple.</p>
</td></tr>
<tr><td><code id="torch_zeros_+3A_names">names</code></td>
<td>
<p>optional dimension names</p>
</td></tr>
<tr><td><code id="torch_zeros_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>).</p>
</td></tr>
<tr><td><code id="torch_zeros_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned Tensor.        Default: <code>torch_strided</code>.</p>
</td></tr>
<tr><td><code id="torch_zeros_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td></tr>
<tr><td><code id="torch_zeros_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>zeros(*size, out=NULL, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Returns a tensor filled with the scalar value <code>0</code>, with the shape defined
by the variable argument <code>size</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

torch_zeros(c(2, 3))
torch_zeros(c(5))
}
</code></pre>

<hr>
<h2 id='torch_zeros_like'>Zeros_like</h2><span id='topic+torch_zeros_like'></span>

<h3>Description</h3>

<p>Zeros_like
</p>


<h3>Usage</h3>

<pre><code class='language-R'>torch_zeros_like(
  input,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE,
  memory_format = torch_preserve_format()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="torch_zeros_like_+3A_input">input</code></td>
<td>
<p>(Tensor) the size of <code>input</code> will determine size of the output tensor.</p>
</td></tr>
<tr><td><code id="torch_zeros_like_+3A_dtype">dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned Tensor.        Default: if <code>NULL</code>, defaults to the dtype of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_zeros_like_+3A_layout">layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned tensor.        Default: if <code>NULL</code>, defaults to the layout of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_zeros_like_+3A_device">device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, defaults to the device of <code>input</code>.</p>
</td></tr>
<tr><td><code id="torch_zeros_like_+3A_requires_grad">requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="torch_zeros_like_+3A_memory_format">memory_format</code></td>
<td>
<p>(<code>torch.memory_format</code>, optional) the desired memory format of        returned Tensor. Default: <code>torch_preserve_format</code>.</p>
</td></tr>
</table>


<h3>zeros_like(input, dtype=NULL, layout=NULL, device=NULL, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor </h3>

<p>Returns a tensor filled with the scalar value <code>0</code>, with the same size as
<code>input</code>. <code>torch_zeros_like(input)</code> is equivalent to
<code>torch_zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.
</p>


<h3>Warning</h3>

<p>As of 0.4, this function does not support an <code>out</code> keyword. As an alternative,
the old <code>torch_zeros_like(input, out=output)</code> is equivalent to
<code>torch_zeros(input.size(), out=output)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

input = torch_empty(c(2, 3))
torch_zeros_like(input)
}
</code></pre>

<hr>
<h2 id='with_detect_anomaly'>Context-manager that enable anomaly detection for the autograd engine.</h2><span id='topic+with_detect_anomaly'></span>

<h3>Description</h3>

<p>This does two things:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>with_detect_anomaly(code)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="with_detect_anomaly_+3A_code">code</code></td>
<td>
<p>Code that will be executed in the detect anomaly context.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function.
</p>
</li>
<li><p> Any backward computation that generate &quot;nan&quot; value will raise an error.
</p>
</li></ul>



<h3>Warning</h3>

<p>This mode should be enabled only for debugging as the different tests
will slow down your program execution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_randn(2, requires_grad = TRUE)
y &lt;- torch_randn(1)
b &lt;- (x^y)$sum()
y$add_(1)

try({
  b$backward()

  with_detect_anomaly({
    b$backward()
  })
})
}
</code></pre>

<hr>
<h2 id='with_enable_grad'>Enable grad</h2><span id='topic+with_enable_grad'></span><span id='topic+local_enable_grad'></span>

<h3>Description</h3>

<p>Context-manager that enables gradient calculation.
Enables gradient calculation, if it has been disabled via <a href="#topic+with_no_grad">with_no_grad</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>with_enable_grad(code)

local_enable_grad(.env = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="with_enable_grad_+3A_code">code</code></td>
<td>
<p>code to be executed with gradient recording.</p>
</td></tr>
<tr><td><code id="with_enable_grad_+3A_.env">.env</code></td>
<td>
<p>The environment to use for scoping.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This context manager is thread local; it will not affect computation in
other threads.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>local_enable_grad()</code>: Locally enable gradient computations.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {

x &lt;- torch_tensor(1, requires_grad = TRUE)
with_no_grad({
  with_enable_grad({
    y &lt;- x * 2
  })
})
y$backward()
x$grad
}
</code></pre>

<hr>
<h2 id='with_no_grad'>Temporarily modify gradient recording.</h2><span id='topic+with_no_grad'></span><span id='topic+local_no_grad'></span>

<h3>Description</h3>

<p>Temporarily modify gradient recording.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>with_no_grad(code)

local_no_grad(.env = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="with_no_grad_+3A_code">code</code></td>
<td>
<p>code to be executed with no gradient recording.</p>
</td></tr>
<tr><td><code id="with_no_grad_+3A_.env">.env</code></td>
<td>
<p>The environment to use for scoping.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>local_no_grad()</code>: Disable autograd until it goes out of scope
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch_is_installed()) {
x &lt;- torch_tensor(runif(5), requires_grad = TRUE)
with_no_grad({
  x$sub_(torch_tensor(as.numeric(1:5)))
})
x
x$grad
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
