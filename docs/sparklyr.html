<!DOCTYPE html><html lang="en"><head><title>Help for package sparklyr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparklyr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+5B.tbl_spark'><p>Subsetting operator for Spark dataframe</p></a></li>
<li><a href='#+25-+26gt+3B+25'><p>Infix operator for composing a lambda expression</p></a></li>
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#arrow_enabled_object'><p>Determine whether arrow is able to serialize the given R object</p></a></li>
<li><a href='#checkpoint_directory'><p>Set/Get Spark checkpoint directory</p></a></li>
<li><a href='#collect'><p>Collect</p></a></li>
<li><a href='#collect_from_rds'><p>Collect Spark data serialized in RDS format into R</p></a></li>
<li><a href='#compile_package_jars'><p>Compile Scala sources into a Java Archive (jar)</p></a></li>
<li><a href='#connection_config'><p>Read configuration values for a connection</p></a></li>
<li><a href='#connection_is_open'><p>Check whether the connection is open</p></a></li>
<li><a href='#connection_spark_shinyapp'><p>A Shiny app that can be used to construct a <code>spark_connect</code> statement</p></a></li>
<li><a href='#copy_to'><p>Copy To</p></a></li>
<li><a href='#copy_to.spark_connection'><p>Copy an R Data Frame to Spark</p></a></li>
<li><a href='#DBISparkResult-class'><p>DBI Spark Result.</p></a></li>
<li><a href='#distinct'><p>Distinct</p></a></li>
<li><a href='#download_scalac'><p>Downloads default Scala Compilers</p></a></li>
<li><a href='#dplyr_hof'><p>dplyr wrappers for Apache Spark higher order functions</p></a></li>
<li><a href='#ensure'><p>Enforce Specific Structure for R Objects</p></a></li>
<li><a href='#fill'><p>Fill</p></a></li>
<li><a href='#filter'><p>Filter</p></a></li>
<li><a href='#find_scalac'><p>Discover the Scala Compiler</p></a></li>
<li><a href='#ft_binarizer'><p>Feature Transformation &ndash; Binarizer (Transformer)</p></a></li>
<li><a href='#ft_bucketizer'><p>Feature Transformation &ndash; Bucketizer (Transformer)</p></a></li>
<li><a href='#ft_chisq_selector'><p>Feature Transformation &ndash; ChiSqSelector (Estimator)</p></a></li>
<li><a href='#ft_count_vectorizer'><p>Feature Transformation &ndash; CountVectorizer (Estimator)</p></a></li>
<li><a href='#ft_dct'><p>Feature Transformation &ndash; Discrete Cosine Transform (DCT) (Transformer)</p></a></li>
<li><a href='#ft_elementwise_product'><p>Feature Transformation &ndash; ElementwiseProduct (Transformer)</p></a></li>
<li><a href='#ft_feature_hasher'><p>Feature Transformation &ndash; FeatureHasher (Transformer)</p></a></li>
<li><a href='#ft_hashing_tf'><p>Feature Transformation &ndash; HashingTF (Transformer)</p></a></li>
<li><a href='#ft_idf'><p>Feature Transformation &ndash; IDF (Estimator)</p></a></li>
<li><a href='#ft_imputer'><p>Feature Transformation &ndash; Imputer (Estimator)</p></a></li>
<li><a href='#ft_index_to_string'><p>Feature Transformation &ndash; IndexToString (Transformer)</p></a></li>
<li><a href='#ft_interaction'><p>Feature Transformation &ndash; Interaction (Transformer)</p></a></li>
<li><a href='#ft_lsh'><p>Feature Transformation &ndash; LSH (Estimator)</p></a></li>
<li><a href='#ft_lsh_utils'><p>Utility functions for LSH models</p></a></li>
<li><a href='#ft_max_abs_scaler'><p>Feature Transformation &ndash; MaxAbsScaler (Estimator)</p></a></li>
<li><a href='#ft_min_max_scaler'><p>Feature Transformation &ndash; MinMaxScaler (Estimator)</p></a></li>
<li><a href='#ft_ngram'><p>Feature Transformation &ndash; NGram (Transformer)</p></a></li>
<li><a href='#ft_normalizer'><p>Feature Transformation &ndash; Normalizer (Transformer)</p></a></li>
<li><a href='#ft_one_hot_encoder'><p>Feature Transformation &ndash; OneHotEncoder (Transformer)</p></a></li>
<li><a href='#ft_one_hot_encoder_estimator'><p>Feature Transformation &ndash; OneHotEncoderEstimator (Estimator)</p></a></li>
<li><a href='#ft_pca'><p>Feature Transformation &ndash; PCA (Estimator)</p></a></li>
<li><a href='#ft_polynomial_expansion'><p>Feature Transformation &ndash; PolynomialExpansion (Transformer)</p></a></li>
<li><a href='#ft_quantile_discretizer'><p>Feature Transformation &ndash; QuantileDiscretizer (Estimator)</p></a></li>
<li><a href='#ft_r_formula'><p>Feature Transformation &ndash; RFormula (Estimator)</p></a></li>
<li><a href='#ft_regex_tokenizer'><p>Feature Transformation &ndash; RegexTokenizer (Transformer)</p></a></li>
<li><a href='#ft_robust_scaler'><p>Feature Transformation &ndash; RobustScaler (Estimator)</p></a></li>
<li><a href='#ft_sql_transformer'><p>Feature Transformation &ndash; SQLTransformer</p></a></li>
<li><a href='#ft_standard_scaler'><p>Feature Transformation &ndash; StandardScaler (Estimator)</p></a></li>
<li><a href='#ft_stop_words_remover'><p>Feature Transformation &ndash; StopWordsRemover (Transformer)</p></a></li>
<li><a href='#ft_string_indexer'><p>Feature Transformation &ndash; StringIndexer (Estimator)</p></a></li>
<li><a href='#ft_tokenizer'><p>Feature Transformation &ndash; Tokenizer (Transformer)</p></a></li>
<li><a href='#ft_vector_assembler'><p>Feature Transformation &ndash; VectorAssembler (Transformer)</p></a></li>
<li><a href='#ft_vector_indexer'><p>Feature Transformation &ndash; VectorIndexer (Estimator)</p></a></li>
<li><a href='#ft_vector_slicer'><p>Feature Transformation &ndash; VectorSlicer (Transformer)</p></a></li>
<li><a href='#ft_word2vec'><p>Feature Transformation &ndash; Word2Vec (Estimator)</p></a></li>
<li><a href='#full_join'><p>Full join</p></a></li>
<li><a href='#generic_call_interface'><p>Generic Call Interface</p></a></li>
<li><a href='#get_spark_sql_catalog_implementation'><p>Retrieve the Spark connection's SQL catalog implementation property</p></a></li>
<li><a href='#hive_context_config'><p>Runtime configuration interface for Hive</p></a></li>
<li><a href='#hof_aggregate'><p>Apply Aggregate Function to Array Column</p></a></li>
<li><a href='#hof_array_sort'><p>Sorts array using a custom comparator</p></a></li>
<li><a href='#hof_exists'><p>Determine Whether Some Element Exists in an Array Column</p></a></li>
<li><a href='#hof_filter'><p>Filter Array Column</p></a></li>
<li><a href='#hof_forall'><p>Checks whether all elements in an array satisfy a predicate</p></a></li>
<li><a href='#hof_map_filter'><p>Filters a map</p></a></li>
<li><a href='#hof_map_zip_with'><p>Merges two maps into one</p></a></li>
<li><a href='#hof_transform'><p>Transform Array Column</p></a></li>
<li><a href='#hof_transform_keys'><p>Transforms keys of a map</p></a></li>
<li><a href='#hof_transform_values'><p>Transforms values of a map</p></a></li>
<li><a href='#hof_zip_with'><p>Combines 2 Array Columns</p></a></li>
<li><a href='#inner_join'><p>Inner join</p></a></li>
<li><a href='#invoke'><p>Invoke a Method on a JVM Object</p></a></li>
<li><a href='#invoke_method'><p>Generic Call Interface</p></a></li>
<li><a href='#j_invoke'><p>Invoke a Java function.</p></a></li>
<li><a href='#j_invoke_method'><p>Generic Call Interface</p></a></li>
<li><a href='#jarray'><p>Instantiate a Java array with a specific element type.</p></a></li>
<li><a href='#jfloat'><p>Instantiate a Java float type.</p></a></li>
<li><a href='#jfloat_array'><p>Instantiate an Array[Float].</p></a></li>
<li><a href='#jobj_class'><p>Superclasses of object</p></a></li>
<li><a href='#jobj_set_param'><p>Parameter Setting for JVM Objects</p></a></li>
<li><a href='#join.tbl_spark'><p>Join Spark tbls.</p></a></li>
<li><a href='#left_join'><p>Left join</p></a></li>
<li><a href='#list_sparklyr_jars'><p>list all sparklyr-*.jar files that have been built</p></a></li>
<li><a href='#livy_config'><p>Create a Spark Configuration for Livy</p></a></li>
<li><a href='#livy_install'><p>Install Livy</p></a></li>
<li><a href='#livy_service_start'><p>Start Livy</p></a></li>
<li><a href='#ml_add_stage'><p>Add a Stage to a Pipeline</p></a></li>
<li><a href='#ml_aft_survival_regression'><p>Spark ML &ndash; Survival Regression</p></a></li>
<li><a href='#ml_als'><p>Spark ML &ndash; ALS</p></a></li>
<li><a href='#ml_als_tidiers'><p>Tidying methods for Spark ML ALS</p></a></li>
<li><a href='#ml_bisecting_kmeans'><p>Spark ML &ndash; Bisecting K-Means Clustering</p></a></li>
<li><a href='#ml_call_constructor'><p>Wrap a Spark ML JVM object</p></a></li>
<li><a href='#ml_chisquare_test'><p>Chi-square hypothesis testing for categorical data.</p></a></li>
<li><a href='#ml_clustering_evaluator'><p>Spark ML - Clustering Evaluator</p></a></li>
<li><a href='#ml_corr'><p>Compute correlation matrix</p></a></li>
<li><a href='#ml_decision_tree_classifier'><p>Spark ML &ndash; Decision Trees</p></a></li>
<li><a href='#ml_default_stop_words'><p>Default stop words</p></a></li>
<li><a href='#ml_evaluate'><p>Evaluate the Model on a Validation Set</p></a></li>
<li><a href='#ml_evaluator'><p>Spark ML - Evaluators</p></a></li>
<li><a href='#ml_feature_importances'><p>Spark ML - Feature Importance for Tree Models</p></a></li>
<li><a href='#ml_fpgrowth'><p>Frequent Pattern Mining &ndash; FPGrowth</p></a></li>
<li><a href='#ml_gaussian_mixture'><p>Spark ML &ndash; Gaussian Mixture clustering.</p></a></li>
<li><a href='#ml_gbt_classifier'><p>Spark ML &ndash; Gradient Boosted Trees</p></a></li>
<li><a href='#ml_generalized_linear_regression'><p>Spark ML &ndash; Generalized Linear Regression</p></a></li>
<li><a href='#ml_glm_tidiers'><p>Tidying methods for Spark ML linear models</p></a></li>
<li><a href='#ml_isotonic_regression'><p>Spark ML &ndash; Isotonic Regression</p></a></li>
<li><a href='#ml_isotonic_regression_tidiers'><p>Tidying methods for Spark ML Isotonic Regression</p></a></li>
<li><a href='#ml_kmeans'><p>Spark ML &ndash; K-Means Clustering</p></a></li>
<li><a href='#ml_kmeans_cluster_eval'><p>Evaluate a K-mean clustering</p></a></li>
<li><a href='#ml_lda'><p>Spark ML &ndash; Latent Dirichlet Allocation</p></a></li>
<li><a href='#ml_lda_tidiers'><p>Tidying methods for Spark ML LDA models</p></a></li>
<li><a href='#ml_linear_regression'><p>Spark ML &ndash; Linear Regression</p></a></li>
<li><a href='#ml_linear_svc'><p>Spark ML &ndash; LinearSVC</p></a></li>
<li><a href='#ml_linear_svc_tidiers'><p>Tidying methods for Spark ML linear svc</p></a></li>
<li><a href='#ml_logistic_regression'><p>Spark ML &ndash; Logistic Regression</p></a></li>
<li><a href='#ml_logistic_regression_tidiers'><p>Tidying methods for Spark ML Logistic Regression</p></a></li>
<li><a href='#ml_metrics_binary'><p>Extracts metrics from a fitted table</p></a></li>
<li><a href='#ml_metrics_multiclass'><p>Extracts metrics from a fitted table</p></a></li>
<li><a href='#ml_metrics_regression'><p>Extracts metrics from a fitted table</p></a></li>
<li><a href='#ml_model_data'><p>Extracts data associated with a Spark ML model</p></a></li>
<li><a href='#ml_multilayer_perceptron_classifier'><p>Spark ML &ndash; Multilayer Perceptron</p></a></li>
<li><a href='#ml_multilayer_perceptron_tidiers'><p>Tidying methods for Spark ML MLP</p></a></li>
<li><a href='#ml_naive_bayes'><p>Spark ML &ndash; Naive-Bayes</p></a></li>
<li><a href='#ml_naive_bayes_tidiers'><p>Tidying methods for Spark ML Naive Bayes</p></a></li>
<li><a href='#ml_one_vs_rest'><p>Spark ML &ndash; OneVsRest</p></a></li>
<li><a href='#ml_pca_tidiers'><p>Tidying methods for Spark ML Principal Component Analysis</p></a></li>
<li><a href='#ml_pipeline'><p>Spark ML &ndash; Pipelines</p></a></li>
<li><a href='#ml_power_iteration'><p>Spark ML &ndash; Power Iteration Clustering</p></a></li>
<li><a href='#ml_prefixspan'><p>Frequent Pattern Mining &ndash; PrefixSpan</p></a></li>
<li><a href='#ml_random_forest_classifier'><p>Spark ML &ndash; Random Forest</p></a></li>
<li><a href='#ml_stage'><p>Spark ML &ndash; Pipeline stage extraction</p></a></li>
<li><a href='#ml_standardize_formula'><p>Standardize Formula Input for 'ml_model'</p></a></li>
<li><a href='#ml_summary'><p>Spark ML &ndash; Extraction of summary metrics</p></a></li>
<li><a href='#ml_supervised_pipeline'><p>Constructors for 'ml_model' Objects</p></a></li>
<li><a href='#ml_survival_regression_tidiers'><p>Tidying methods for Spark ML Survival Regression</p></a></li>
<li><a href='#ml_tree_tidiers'><p>Tidying methods for Spark ML tree models</p></a></li>
<li><a href='#ml_uid'><p>Spark ML &ndash; UID</p></a></li>
<li><a href='#ml_unsupervised_tidiers'><p>Tidying methods for Spark ML unsupervised models</p></a></li>
<li><a href='#ml-constructors'><p>Constructors for Pipeline Stages</p></a></li>
<li><a href='#ml-params'><p>Spark ML &ndash; ML Params</p></a></li>
<li><a href='#ml-persistence'><p>Spark ML &ndash; Model Persistence</p></a></li>
<li><a href='#ml-transform-methods'><p>Spark ML &ndash; Transform, fit, and predict methods (ml_ interface)</p></a></li>
<li><a href='#ml-tuning'><p>Spark ML &ndash; Tuning</p></a></li>
<li><a href='#mutate'><p>Mutate</p></a></li>
<li><a href='#na.replace'><p>Replace Missing Values in Objects</p></a></li>
<li><a href='#nest'><p>Nest</p></a></li>
<li><a href='#pivot_longer'><p>Pivot longer</p></a></li>
<li><a href='#pivot_wider'><p>Pivot wider</p></a></li>
<li><a href='#print_jobj'><p>Generic method for print jobj for a connection type</p></a></li>
<li><a href='#quote_sql_name'><p>Translate input character vector or symbol to a SQL identifier</p></a></li>
<li><a href='#random_string'><p>Random string generation</p></a></li>
<li><a href='#reactiveSpark'><p>Reactive spark reader</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#register_extension'><p>Register a Package that Implements a Spark Extension</p></a></li>
<li><a href='#registerDoSpark'><p>Register a Parallel Backend</p></a></li>
<li><a href='#replace_na'><p>Replace NA</p></a></li>
<li><a href='#right_join'><p>Right join</p></a></li>
<li><a href='#sdf_along'><p>Create DataFrame for along Object</p></a></li>
<li><a href='#sdf_bind'><p>Bind multiple Spark DataFrames by row and column</p></a></li>
<li><a href='#sdf_broadcast'><p>Broadcast hint</p></a></li>
<li><a href='#sdf_checkpoint'><p>Checkpoint a Spark DataFrame</p></a></li>
<li><a href='#sdf_coalesce'><p>Coalesces a Spark DataFrame</p></a></li>
<li><a href='#sdf_collect'><p>Collect a Spark DataFrame into R.</p></a></li>
<li><a href='#sdf_copy_to'><p>Copy an Object into Spark</p></a></li>
<li><a href='#sdf_crosstab'><p>Cross Tabulation</p></a></li>
<li><a href='#sdf_debug_string'><p>Debug Info for Spark DataFrame</p></a></li>
<li><a href='#sdf_describe'><p>Compute summary statistics for columns of a data frame</p></a></li>
<li><a href='#sdf_dim'><p>Support for Dimension Operations</p></a></li>
<li><a href='#sdf_distinct'><p>Invoke distinct on a Spark DataFrame</p></a></li>
<li><a href='#sdf_drop_duplicates'><p>Remove duplicates from a Spark DataFrame</p></a></li>
<li><a href='#sdf_expand_grid'><p>Create a Spark dataframe containing all combinations of inputs</p></a></li>
<li><a href='#sdf_fast_bind_cols'><p>Fast cbind for Spark DataFrames</p></a></li>
<li><a href='#sdf_from_avro'><p>Convert column(s) from avro format</p></a></li>
<li><a href='#sdf_is_streaming'><p>Spark DataFrame is Streaming</p></a></li>
<li><a href='#sdf_last_index'><p>Returns the last index of a Spark DataFrame</p></a></li>
<li><a href='#sdf_len'><p>Create DataFrame for Length</p></a></li>
<li><a href='#sdf_num_partitions'><p>Gets number of partitions of a Spark DataFrame</p></a></li>
<li><a href='#sdf_partition_sizes'><p>Compute the number of records within each partition of a Spark DataFrame</p></a></li>
<li><a href='#sdf_persist'><p>Persist a Spark DataFrame</p></a></li>
<li><a href='#sdf_pivot'><p>Pivot a Spark DataFrame</p></a></li>
<li><a href='#sdf_project'><p>Project features onto principal components</p></a></li>
<li><a href='#sdf_quantile'><p>Compute (Approximate) Quantiles with a Spark DataFrame</p></a></li>
<li><a href='#sdf_random_split'><p>Partition a Spark Dataframe</p></a></li>
<li><a href='#sdf_rbeta'><p>Generate random samples from a Beta distribution</p></a></li>
<li><a href='#sdf_rbinom'><p>Generate random samples from a binomial distribution</p></a></li>
<li><a href='#sdf_rcauchy'><p>Generate random samples from a Cauchy distribution</p></a></li>
<li><a href='#sdf_rchisq'><p>Generate random samples from a chi-squared distribution</p></a></li>
<li><a href='#sdf_read_column'><p>Read a Column from a Spark DataFrame</p></a></li>
<li><a href='#sdf_register'><p>Register a Spark DataFrame</p></a></li>
<li><a href='#sdf_repartition'><p>Repartition a Spark DataFrame</p></a></li>
<li><a href='#sdf_residuals.ml_model_generalized_linear_regression'><p>Model Residuals</p></a></li>
<li><a href='#sdf_rexp'><p>Generate random samples from an exponential distribution</p></a></li>
<li><a href='#sdf_rgamma'><p>Generate random samples from a Gamma distribution</p></a></li>
<li><a href='#sdf_rgeom'><p>Generate random samples from a geometric distribution</p></a></li>
<li><a href='#sdf_rhyper'><p>Generate random samples from a hypergeometric distribution</p></a></li>
<li><a href='#sdf_rlnorm'><p>Generate random samples from a log normal distribution</p></a></li>
<li><a href='#sdf_rnorm'><p>Generate random samples from the standard normal distribution</p></a></li>
<li><a href='#sdf_rpois'><p>Generate random samples from a Poisson distribution</p></a></li>
<li><a href='#sdf_rt'><p>Generate random samples from a t-distribution</p></a></li>
<li><a href='#sdf_runif'><p>Generate random samples from the uniform distribution U(0, 1).</p></a></li>
<li><a href='#sdf_rweibull'><p>Generate random samples from a Weibull distribution.</p></a></li>
<li><a href='#sdf_sample'><p>Randomly Sample Rows from a Spark DataFrame</p></a></li>
<li><a href='#sdf_schema'><p>Read the Schema of a Spark DataFrame</p></a></li>
<li><a href='#sdf_separate_column'><p>Separate a Vector Column into Scalar Columns</p></a></li>
<li><a href='#sdf_seq'><p>Create DataFrame for Range</p></a></li>
<li><a href='#sdf_sort'><p>Sort a Spark DataFrame</p></a></li>
<li><a href='#sdf_sql'><p>Spark DataFrame from SQL</p></a></li>
<li><a href='#sdf_to_avro'><p>Convert column(s) to avro format</p></a></li>
<li><a href='#sdf_unnest_longer'><p>Unnest longer</p></a></li>
<li><a href='#sdf_unnest_wider'><p>Unnest wider</p></a></li>
<li><a href='#sdf_weighted_sample'><p>Perform Weighted Random Sampling on a Spark DataFrame</p></a></li>
<li><a href='#sdf_with_sequential_id'><p>Add a Sequential ID Column to a Spark DataFrame</p></a></li>
<li><a href='#sdf_with_unique_id'><p>Add a Unique ID Column to a Spark DataFrame</p></a></li>
<li><a href='#sdf-saveload'><p>Save / Load a Spark DataFrame</p></a></li>
<li><a href='#sdf-transform-methods'><p>Spark ML &ndash; Transform, fit, and predict methods (sdf_ interface)</p></a></li>
<li><a href='#select'><p>Select</p></a></li>
<li><a href='#separate'><p>Separate</p></a></li>
<li><a href='#spark_adaptive_query_execution'><p>Retrieves or sets status of Spark AQE</p></a></li>
<li><a href='#spark_advisory_shuffle_partition_size'><p>Retrieves or sets advisory size of the shuffle partition</p></a></li>
<li><a href='#spark_apply'><p>Apply an R Function in Spark</p></a></li>
<li><a href='#spark_apply_bundle'><p>Create Bundle for Spark Apply</p></a></li>
<li><a href='#spark_apply_log'><p>Log Writer for Spark Apply</p></a></li>
<li><a href='#spark_auto_broadcast_join_threshold'><p>Retrieves or sets the auto broadcast join threshold</p></a></li>
<li><a href='#spark_coalesce_initial_num_partitions'><p>Retrieves or sets initial number of shuffle partitions before coalescing</p></a></li>
<li><a href='#spark_coalesce_min_num_partitions'><p>Retrieves or sets the minimum number of shuffle partitions after coalescing</p></a></li>
<li><a href='#spark_coalesce_shuffle_partitions'><p>Retrieves or sets whether coalescing contiguous shuffle partitions is enabled</p></a></li>
<li><a href='#spark_compilation_spec'><p>Define a Spark Compilation Specification</p></a></li>
<li><a href='#spark_compile'><p>Compile Scala sources into a Java Archive</p></a></li>
<li><a href='#spark_config'><p>Read Spark Configuration</p></a></li>
<li><a href='#spark_config_exists'><p>A helper function to check value exist under <code>spark_config()</code></p></a></li>
<li><a href='#spark_config_kubernetes'><p>Kubernetes Configuration</p></a></li>
<li><a href='#spark_config_packages'><p>Creates Spark Configuration</p></a></li>
<li><a href='#spark_config_settings'><p>Retrieve Available Settings</p></a></li>
<li><a href='#spark_config_value'><p>A helper function to retrieve values from <code>spark_config()</code></p></a></li>
<li><a href='#spark_connect_method'><p>Function that negotiates the connection with the Spark back-end</p></a></li>
<li><a href='#spark_connection'><p>Retrieve the Spark Connection Associated with an R Object</p></a></li>
<li><a href='#spark_connection_find'><p>Find Spark Connection</p></a></li>
<li><a href='#spark_connection-class'><p>spark_connection class</p></a></li>
<li><a href='#spark_context_config'><p>Runtime configuration interface for the Spark Context.</p></a></li>
<li><a href='#spark_dataframe'><p>Retrieve a Spark DataFrame</p></a></li>
<li><a href='#spark_default_compilation_spec'><p>Default Compilation Specification for Spark Extensions</p></a></li>
<li><a href='#spark_default_version'><p>determine the version that will be used by default if version is NULL</p></a></li>
<li><a href='#spark_dependency'><p>Define a Spark dependency</p></a></li>
<li><a href='#spark_dependency_fallback'><p>Fallback to Spark Dependency</p></a></li>
<li><a href='#spark_extension'><p>Create Spark Extension</p></a></li>
<li><a href='#spark_get_java'><p>Find path to Java</p></a></li>
<li><a href='#spark_home_dir'><p>Find the SPARK_HOME directory for a version of Spark</p></a></li>
<li><a href='#spark_home_set'><p>Set the SPARK_HOME environment variable</p></a></li>
<li><a href='#spark_ide_connection_open'><p>Set of functions to provide integration with the RStudio IDE</p></a></li>
<li><a href='#spark_insert_table'><p>Inserts a Spark DataFrame into a Spark table</p></a></li>
<li><a href='#spark_install'><p>Download and install various versions of Spark</p></a></li>
<li><a href='#spark_install_find'><p>Find a given Spark installation by version.</p></a></li>
<li><a href='#spark_install_sync'><p>helper function to sync sparkinstall project to sparklyr</p></a></li>
<li><a href='#spark_integ_test_skip'><p>It lets the package know if it should test a particular functionality or not</p></a></li>
<li><a href='#spark_jobj'><p>Retrieve a Spark JVM Object Reference</p></a></li>
<li><a href='#spark_jobj-class'><p>spark_jobj class</p></a></li>
<li><a href='#spark_last_error'><p>Surfaces the last error from Spark captured by internal 'spark_error' function</p></a></li>
<li><a href='#spark_load_table'><p>Reads from a Spark Table into a Spark DataFrame.</p></a></li>
<li><a href='#spark_log'><p>View Entries in the Spark Log</p></a></li>
<li><a href='#spark_pipeline_stage'><p>Create a Pipeline Stage Object</p></a></li>
<li><a href='#spark_read'><p>Read file(s) into a Spark DataFrame using a custom reader</p></a></li>
<li><a href='#spark_read_avro'><p>Read Apache Avro data into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_binary'><p>Read binary data into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_csv'><p>Read a CSV file into a Spark DataFrame</p></a></li>
<li><a href='#spark_read_delta'><p>Read from Delta Lake into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_image'><p>Read image data into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_jdbc'><p>Read from JDBC connection into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_json'><p>Read a JSON file into a Spark DataFrame</p></a></li>
<li><a href='#spark_read_libsvm'><p>Read libsvm file into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_orc'><p>Read a ORC file into a Spark DataFrame</p></a></li>
<li><a href='#spark_read_parquet'><p>Read a Parquet file into a Spark DataFrame</p></a></li>
<li><a href='#spark_read_source'><p>Read from a generic source into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_table'><p>Reads from a Spark Table into a Spark DataFrame.</p></a></li>
<li><a href='#spark_read_text'><p>Read a Text file into a Spark DataFrame</p></a></li>
<li><a href='#spark_save_table'><p>Saves a Spark DataFrame as a Spark table</p></a></li>
<li><a href='#spark_session_config'><p>Runtime configuration interface for the Spark Session</p></a></li>
<li><a href='#spark_statistical_routines'><p>Generate random samples from some distribution</p></a></li>
<li><a href='#spark_table_name'><p>Generate a Table Name from Expression</p></a></li>
<li><a href='#spark_version'><p>Get the Spark Version Associated with a Spark Connection</p></a></li>
<li><a href='#spark_version_from_home'><p>Get the Spark Version Associated with a Spark Installation</p></a></li>
<li><a href='#spark_versions'><p>Returns a data frame of available Spark versions that can be installed.</p></a></li>
<li><a href='#spark_web'><p>Open the Spark web interface</p></a></li>
<li><a href='#spark_write'><p>Write Spark DataFrame to file using a custom writer</p></a></li>
<li><a href='#spark_write_avro'><p>Serialize a Spark DataFrame into Apache Avro format</p></a></li>
<li><a href='#spark_write_csv'><p>Write a Spark DataFrame to a CSV</p></a></li>
<li><a href='#spark_write_delta'><p>Writes a Spark DataFrame into Delta Lake</p></a></li>
<li><a href='#spark_write_jdbc'><p>Writes a Spark DataFrame into a JDBC table</p></a></li>
<li><a href='#spark_write_json'><p>Write a Spark DataFrame to a JSON file</p></a></li>
<li><a href='#spark_write_orc'><p>Write a Spark DataFrame to a ORC file</p></a></li>
<li><a href='#spark_write_parquet'><p>Write a Spark DataFrame to a Parquet file</p></a></li>
<li><a href='#spark_write_rds'><p>Write Spark DataFrame to RDS files</p></a></li>
<li><a href='#spark_write_source'><p>Writes a Spark DataFrame into a generic source</p></a></li>
<li><a href='#spark_write_table'><p>Writes a Spark DataFrame into a Spark table</p></a></li>
<li><a href='#spark_write_text'><p>Write a Spark DataFrame to a Text file</p></a></li>
<li><a href='#spark-api'><p>Access the Spark API</p></a></li>
<li><a href='#spark-connections'><p>Manage Spark Connections</p></a></li>
<li><a href='#sparklyr_get_backend_port'><p>Return the port number of a 'sparklyr' backend.</p></a></li>
<li><a href='#src_databases'><p>Show database list</p></a></li>
<li><a href='#stream_find'><p>Find Stream</p></a></li>
<li><a href='#stream_generate_test'><p>Generate Test Stream</p></a></li>
<li><a href='#stream_id'><p>Spark Stream's Identifier</p></a></li>
<li><a href='#stream_lag'><p>Apply lag function to columns of a Spark Streaming DataFrame</p></a></li>
<li><a href='#stream_name'><p>Spark Stream's Name</p></a></li>
<li><a href='#stream_read_csv'><p>Read files created by the stream</p></a></li>
<li><a href='#stream_render'><p>Render Stream</p></a></li>
<li><a href='#stream_stats'><p>Stream Statistics</p></a></li>
<li><a href='#stream_stop'><p>Stops a Spark Stream</p></a></li>
<li><a href='#stream_trigger_continuous'><p>Spark Stream Continuous Trigger</p></a></li>
<li><a href='#stream_trigger_interval'><p>Spark Stream Interval Trigger</p></a></li>
<li><a href='#stream_view'><p>View Stream</p></a></li>
<li><a href='#stream_watermark'><p>Watermark Stream</p></a></li>
<li><a href='#stream_write_csv'><p>Write files to the stream</p></a></li>
<li><a href='#stream_write_memory'><p>Write Memory Stream</p></a></li>
<li><a href='#stream_write_table'><p>Write Stream to Table</p></a></li>
<li><a href='#tbl_cache'><p>Cache a Spark Table</p></a></li>
<li><a href='#tbl_change_db'><p>Use specific database</p></a></li>
<li><a href='#tbl_uncache'><p>Uncache a Spark Table</p></a></li>
<li><a href='#transform_sdf'><p>transform a subset of column(s) in a Spark Dataframe</p></a></li>
<li><a href='#unite'><p>Unite</p></a></li>
<li><a href='#unnest'><p>Unnest</p></a></li>
<li><a href='#worker_spark_apply_unbundle'><p>Extracts a bundle of dependencies required by <code>spark_apply()</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>R Interface to Apache Spark</td>
</tr>
<tr>
<td>Version:</td>
<td>1.9.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edgar Ruiz &lt;edgar@rstudio.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>R interface to Apache Spark, a fast and general
    engine for big data processing, see <a href="https://spark.apache.org/">https://spark.apache.org/</a>. This
    package supports connecting to local and remote Apache Spark clusters,
    provides a 'dplyr' compatible back-end, and provides an interface to
    Spark's built-in machine learning algorithms.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://spark.posit.co/">https://spark.posit.co/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/sparklyr/sparklyr/issues">https://github.com/sparklyr/sparklyr/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>config (&ge; 0.2), DBI (&ge; 1.0.0), dbplyr (&ge; 2.5.0), dplyr (&ge;
1.0.9), generics, globals, glue, httr (&ge; 1.2.1), jsonlite (&ge;
1.4), methods, openssl (&ge; 0.8), purrr, rlang (&ge; 0.1.4),
rstudioapi (&ge; 0.10), tidyr (&ge; 1.2.0), tidyselect, uuid,
vctrs, withr, xml2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arrow (&ge; 0.17.0), broom, diffobj, foreach, ggplot2,
iterators, janeaustenr, Lahman, mlbench, nnet, nycflights13,
R6, r2d3, RCurl, reshape2, shiny (&ge; 1.0.1), parsnip, testthat,
rprojroot</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Spark: 2.x, or 3.x, or 4.x</td>
</tr>
<tr>
<td>Collate:</td>
<td>'spark_data_build_types.R' 'arrow_data.R' 'spark_invoke.R'
'browse_url.R' 'spark_connection.R' 'avro_utils.R'
'config_settings.R' 'config_spark.R' 'connection_instances.R'
'connection_progress.R' 'connection_shinyapp.R'
'spark_version.R' 'connection_spark.R' 'core_arrow.R'
'core_config.R' 'core_connection.R' 'core_deserialize.R'
'core_gateway.R' 'core_invoke.R' 'core_jobj.R'
'core_serialize.R' 'core_utils.R' 'core_worker_config.R'
'utils.R' 'sql_utils.R' 'data_copy.R' 'data_csv.R'
'spark_schema_from_rdd.R' 'spark_apply_bundle.R'
'spark_apply.R' 'tables_spark.R' 'tbl_spark.R' 'spark_sql.R'
'spark_dataframe.R' 'dplyr_spark.R' 'sdf_interface.R'
'data_interface.R' 'databricks_connection.R'
'dbi_spark_connection.R' 'dbi_spark_result.R'
'dbi_spark_table.R' 'do_spark.R' 'dplyr_do.R' 'dplyr_hof.R'
'dplyr_join.R' 'dplyr_spark_data.R' 'dplyr_spark_table.R'
'stratified_sample.R' 'sdf_sql.R' 'dplyr_sql.R'
'dplyr_sql_translation.R' 'dplyr_verbs.R' 'imports.R'
'install_spark.R' 'install_spark_versions.R'
'install_spark_windows.R' 'install_tools.R' 'java.R'
'jobs_api.R' 'kubernetes_config.R' 'shell_connection.R'
'livy_connection.R' 'livy_install.R' 'livy_invoke.R'
'livy_service.R' 'ml_clustering.R'
'ml_classification_decision_tree_classifier.R'
'ml_classification_gbt_classifier.R'
'ml_classification_linear_svc.R'
'ml_classification_logistic_regression.R'
'ml_classification_multilayer_perceptron_classifier.R'
'ml_classification_naive_bayes.R'
'ml_classification_one_vs_rest.R'
'ml_classification_random_forest_classifier.R'
'ml_model_helpers.R' 'ml_clustering_bisecting_kmeans.R'
'ml_clustering_gaussian_mixture.R' 'ml_clustering_kmeans.R'
'ml_clustering_lda.R' 'ml_clustering_power_iteration.R'
'ml_constructor_utils.R' 'ml_evaluate.R'
'ml_evaluation_clustering.R' 'ml_evaluation_prediction.R'
'ml_evaluator.R' 'ml_feature_binarizer.R'
'ml_feature_bucketed_random_projection_lsh.R'
'ml_feature_bucketizer.R' 'ml_feature_chisq_selector.R'
'ml_feature_count_vectorizer.R' 'ml_feature_dct.R'
'ml_feature_sql_transformer.R' 'ml_feature_dplyr_transformer.R'
'ml_feature_elementwise_product.R'
'ml_feature_feature_hasher.R' 'ml_feature_hashing_tf.R'
'ml_feature_idf.R' 'ml_feature_imputer.R'
'ml_feature_index_to_string.R' 'ml_feature_interaction.R'
'ml_feature_lsh_utils.R' 'ml_feature_max_abs_scaler.R'
'ml_feature_min_max_scaler.R' 'ml_feature_minhash_lsh.R'
'ml_feature_ngram.R' 'ml_feature_normalizer.R'
'ml_feature_one_hot_encoder.R'
'ml_feature_one_hot_encoder_estimator.R' 'ml_feature_pca.R'
'ml_feature_polynomial_expansion.R'
'ml_feature_quantile_discretizer.R' 'ml_feature_r_formula.R'
'ml_feature_regex_tokenizer.R' 'ml_feature_robust_scaler.R'
'ml_feature_standard_scaler.R'
'ml_feature_stop_words_remover.R' 'ml_feature_string_indexer.R'
'ml_feature_string_indexer_model.R' 'ml_feature_tokenizer.R'
'ml_feature_vector_assembler.R' 'ml_feature_vector_indexer.R'
'ml_feature_vector_slicer.R' 'ml_feature_word2vec.R'
'ml_fpm_fpgrowth.R' 'ml_fpm_prefixspan.R' 'ml_helpers.R'
'ml_mapping_tables.R' 'ml_metrics.R' 'ml_model_als.R'
'ml_model_bisecting_kmeans.R' 'ml_model_constructors.R'
'ml_model_decision_tree.R' 'ml_model_gaussian_mixture.R'
'ml_model_generalized_linear_regression.R'
'ml_model_gradient_boosted_trees.R'
'ml_model_isotonic_regression.R' 'ml_model_kmeans.R'
'ml_model_lda.R' 'ml_model_linear_regression.R'
'ml_model_linear_svc.R' 'ml_model_logistic_regression.R'
'ml_model_naive_bayes.R' 'ml_model_one_vs_rest.R'
'ml_model_random_forest.R' 'ml_model_utils.R'
'ml_param_utils.R' 'ml_persistence.R' 'ml_pipeline.R'
'ml_pipeline_utils.R' 'ml_print_utils.R'
'ml_recommendation_als.R'
'ml_regression_aft_survival_regression.R'
'ml_regression_decision_tree_regressor.R'
'ml_regression_gbt_regressor.R'
'ml_regression_generalized_linear_regression.R'
'ml_regression_isotonic_regression.R'
'ml_regression_linear_regression.R'
'ml_regression_random_forest_regressor.R' 'ml_stat.R'
'ml_summary.R' 'ml_transformation_methods.R'
'ml_transformer_and_estimator.R' 'ml_tuning.R'
'ml_tuning_cross_validator.R'
'ml_tuning_train_validation_split.R' 'ml_utils.R'
'ml_validator_utils.R' 'mutation.R' 'na_actions.R'
'new_model_multilayer_perceptron.R' 'params_validator.R'
'precondition.R' 'project_template.R' 'qubole_connection.R'
'reexports.R' 'sdf_dim.R' 'sdf_distinct.R' 'sdf_ml.R'
'sdf_saveload.R' 'sdf_sequence.R' 'sdf_stat.R'
'sdf_streaming.R' 'tidyr_utils.R' 'sdf_unnest_longer.R'
'sdf_wrapper.R' 'sdf_unnest_wider.R' 'sdf_utils.R'
'spark_compile.R' 'spark_context_config.R' 'spark_extensions.R'
'spark_gateway.R' 'spark_gen_embedded_sources.R'
'spark_globals.R' 'spark_hive.R' 'spark_home.R' 'spark_ide.R'
'spark_submit.R' 'spark_update_embedded_sources.R'
'spark_utils.R' 'spark_verify_embedded_sources.R'
'stream_data.R' 'stream_job.R' 'stream_operations.R'
'stream_shiny.R' 'stream_view.R' 'synapse_connection.R'
'test_connection.R' 'tidiers_ml_aft_survival_regression.R'
'tidiers_ml_als.R' 'tidiers_ml_isotonic_regression.R'
'tidiers_ml_lda.R' 'tidiers_ml_linear_models.R'
'tidiers_ml_logistic_regression.R'
'tidiers_ml_multilayer_perceptron.R' 'tidiers_ml_naive_bayes.R'
'tidiers_ml_svc_models.R' 'tidiers_ml_tree_models.R'
'tidiers_ml_unsupervised_models.R' 'tidiers_pca.R'
'tidiers_utils.R' 'tidyr_fill.R' 'tidyr_nest.R'
'tidyr_pivot_utils.R' 'tidyr_pivot_longer.R'
'tidyr_pivot_wider.R' 'tidyr_separate.R' 'tidyr_unite.R'
'tidyr_unnest.R' 'worker_apply.R' 'worker_connect.R'
'worker_connection.R' 'worker_invoke.R' 'worker_log.R'
'worker_main.R' 'yarn_cluster.R' 'yarn_config.R' 'yarn_ui.R'
'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-18 12:18:54 UTC; edgar</td>
</tr>
<tr>
<td>Author:</td>
<td>Javier Luraschi [aut],
  Kevin Kuo <a href="https://orcid.org/0000-0001-7803-7901"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Kevin Ushey [aut],
  JJ Allaire [aut],
  Samuel Macedo [ctb],
  Hossein Falaki [aut],
  Lu Wang [aut],
  Andy Zhang [aut],
  Yitao Li <a href="https://orcid.org/0000-0002-1261-905X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jozef Hajnala [ctb],
  Maciej Szymkiewicz
    <a href="https://orcid.org/0000-0003-1469-9396"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Wil Davis [ctb],
  Edgar Ruiz [aut, cre],
  RStudio [cph],
  The Apache Software Foundation [aut, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-18 13:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+5B.tbl_spark'>Subsetting operator for Spark dataframe</h2><span id='topic++5B.tbl_spark'></span>

<h3>Description</h3>

<p>Susetting operator for Spark dataframe allowing a subset of column(s) to be
selected using syntaxes similar to those supported by R dataframes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tbl_spark'
x[i]
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B5B.tbl_spark_+3A_x">x</code></td>
<td>
<p>The Spark dataframe</p>
</td></tr>
<tr><td><code id="+2B5B.tbl_spark_+3A_i">i</code></td>
<td>
<p>Expression specifying subset of column(s) to include or exclude
from the result (e.g., '[&quot;col1&quot;]', '[c(&quot;col1&quot;, &quot;col2&quot;)]', '[1:10]', '[-1]',
'[NULL]', or '[]')</p>
</td></tr>
</table>

<hr>
<h2 id='+25-+26gt+3B+25'>Infix operator for composing a lambda expression</h2><span id='topic++25-+3E+25'></span>

<h3>Description</h3>

<p>Infix operator that allows a lambda expression to be composed in R and be
translated to Spark SQL equivalent using ' <code>dbplyr::translate_sql</code> functionalities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params %-&gt;% ...
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25-+2B26gt+2B3B+2B25_+3A_params">params</code></td>
<td>
<p>Parameter(s) of the lambda expression, can be either a single
parameter or a comma separated listed of parameters in the form of
<code>.(param1, param2, ... )</code> (see examples)</p>
</td></tr>
<tr><td><code id="+2B25-+2B26gt+2B3B+2B25_+3A_...">...</code></td>
<td>
<p>Body of the lambda expression, *must be within parentheses*</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Notice when composing a lambda expression in R, the body of the lambda expression
*must always be surrounded with parentheses*, otherwise a parsing error will occur.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

a %-&gt;% (mean(a) + 1) # translates to &lt;SQL&gt; `a` -&gt; (AVG(`a`) OVER () + 1.0)

.(a, b) %-&gt;% (a &lt; 1 &amp;&amp; b &gt; 1) # translates to &lt;SQL&gt; `a`,`b` -&gt; (`a` &lt; 1.0 AND `b` &gt; 1.0)

## End(Not run)
</code></pre>

<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+reexports">%&gt;%</a></code> for more details.
</p>

<hr>
<h2 id='arrow_enabled_object'>Determine whether arrow is able to serialize the given R object</h2><span id='topic+arrow_enabled_object'></span>

<h3>Description</h3>

<p>If the given R object is not serializable by arrow due to some known
limitations of arrow, then return FALSE, otherwise return TRUE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_enabled_object(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="arrow_enabled_object_+3A_object">object</code></td>
<td>
<p>The object to be serialized</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

df &lt;- dplyr::tibble(x = seq(5))
arrow_enabled_object(df)

## End(Not run)

</code></pre>

<hr>
<h2 id='checkpoint_directory'>Set/Get Spark checkpoint directory</h2><span id='topic+checkpoint_directory'></span><span id='topic+spark_set_checkpoint_dir'></span><span id='topic+spark_get_checkpoint_dir'></span>

<h3>Description</h3>

<p>Set/Get Spark checkpoint directory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_set_checkpoint_dir(sc, dir)

spark_get_checkpoint_dir(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="checkpoint_directory_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="checkpoint_directory_+3A_dir">dir</code></td>
<td>
<p>checkpoint directory, must be HDFS path of running on cluster</p>
</td></tr>
</table>

<hr>
<h2 id='collect'>Collect</h2><span id='topic+collect'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+compute">collect</a></code> for more details.
</p>

<hr>
<h2 id='collect_from_rds'>Collect Spark data serialized in RDS format into R</h2><span id='topic+collect_from_rds'></span>

<h3>Description</h3>

<p>Deserialize Spark data that is serialized using 'spark_write_rds()' into a R
dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collect_from_rds(path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="collect_from_rds_+3A_path">path</code></td>
<td>
<p>Path to a local RDS file that is produced by 'spark_write_rds()'
(RDS files stored in HDFS will need to be downloaded to local filesystem
first (e.g., by running 'hadoop fs -copyToLocal ...' or similar)</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='compile_package_jars'>Compile Scala sources into a Java Archive (jar)</h2><span id='topic+compile_package_jars'></span>

<h3>Description</h3>

<p>Compile the <code>scala</code> source files contained within an <span class="rlang"><b>R</b></span> package
into a Java Archive (<code>jar</code>) file that can be loaded and used within
a Spark environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compile_package_jars(..., spec = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compile_package_jars_+3A_...">...</code></td>
<td>
<p>Optional compilation specifications, as generated by
<code>spark_compilation_spec</code>. When no arguments are passed,
<code>spark_default_compilation_spec</code> is used instead.</p>
</td></tr>
<tr><td><code id="compile_package_jars_+3A_spec">spec</code></td>
<td>
<p>An optional list of compilation specifications. When
set, this option takes precedence over arguments passed to
<code>...</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='connection_config'>Read configuration values for a connection</h2><span id='topic+connection_config'></span>

<h3>Description</h3>

<p>Read configuration values for a connection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>connection_config(sc, prefix, not_prefix = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="connection_config_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
<tr><td><code id="connection_config_+3A_prefix">prefix</code></td>
<td>
<p>Prefix to read parameters for
(e.g. <code>spark.context.</code>, <code>spark.sql.</code>, etc.)</p>
</td></tr>
<tr><td><code id="connection_config_+3A_not_prefix">not_prefix</code></td>
<td>
<p>Prefix to not include.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of config parameters (note that if a prefix was
specified then the names will not include the prefix)
</p>

<hr>
<h2 id='connection_is_open'>Check whether the connection is open</h2><span id='topic+connection_is_open'></span>

<h3>Description</h3>

<p>Check whether the connection is open
</p>


<h3>Usage</h3>

<pre><code class='language-R'>connection_is_open(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="connection_is_open_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
</table>

<hr>
<h2 id='connection_spark_shinyapp'>A Shiny app that can be used to construct a <code>spark_connect</code> statement</h2><span id='topic+connection_spark_shinyapp'></span>

<h3>Description</h3>

<p>A Shiny app that can be used to construct a <code>spark_connect</code> statement
</p>


<h3>Usage</h3>

<pre><code class='language-R'>connection_spark_shinyapp()
</code></pre>

<hr>
<h2 id='copy_to'>Copy To</h2><span id='topic+copy_to'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+copy_to">copy_to</a></code> for more details.
</p>

<hr>
<h2 id='copy_to.spark_connection'>Copy an R Data Frame to Spark</h2><span id='topic+copy_to.spark_connection'></span>

<h3>Description</h3>

<p>Copy an R <code>data.frame</code> to Spark, and return a reference to the
generated Spark DataFrame as a <code>tbl_spark</code>. The returned object will
act as a <code>dplyr</code>-compatible interface to the underlying Spark table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spark_connection'
copy_to(
  dest,
  df,
  name = spark_table_name(substitute(df)),
  overwrite = FALSE,
  memory = TRUE,
  repartition = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="copy_to.spark_connection_+3A_dest">dest</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_df">df</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_name">name</code></td>
<td>
<p>The name to assign to the copied table in Spark.</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite a pre-existing table with the name <code>name</code>
if one already exists?</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the table be cached into memory?</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
table across the Spark cluster. The default (0) can be used to avoid
partitioning.</p>
</td></tr>
<tr><td><code id="copy_to.spark_connection_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tbl_spark</code>, representing a <code>dplyr</code>-compatible interface
to a Spark DataFrame.
</p>

<hr>
<h2 id='DBISparkResult-class'>DBI Spark Result.</h2><span id='topic+DBISparkResult-class'></span>

<h3>Description</h3>

<p>DBI Spark Result.
</p>


<h3>Slots</h3>


<dl>
<dt><code>sql</code></dt><dd><p>character.</p>
</dd>
<dt><code>sdf</code></dt><dd><p>spark_jobj.</p>
</dd>
<dt><code>conn</code></dt><dd><p>spark_connection.</p>
</dd>
<dt><code>state</code></dt><dd><p>environment.</p>
</dd>
</dl>

<hr>
<h2 id='distinct'>Distinct</h2><span id='topic+distinct'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+distinct">distinct</a></code> for more details.
</p>

<hr>
<h2 id='download_scalac'>Downloads default Scala Compilers</h2><span id='topic+download_scalac'></span>

<h3>Description</h3>

<p><code>compile_package_jars</code> requires several versions of the
scala compiler to work, this is to match Spark scala versions.
To help setup your environment, this function will download the
required compilers under the default search path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_scalac(dest_path = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_scalac_+3A_dest_path">dest_path</code></td>
<td>
<p>The destination path where scalac will be
downloaded to.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>find_scalac</code> for a list of paths searched and used by
this function to install the required compilers.
</p>

<hr>
<h2 id='dplyr_hof'>dplyr wrappers for Apache Spark higher order functions</h2><span id='topic+dplyr_hof'></span>

<h3>Description</h3>

<p>These methods implement dplyr grammars for Apache Spark higher order functions
</p>

<hr>
<h2 id='ensure'>Enforce Specific Structure for R Objects</h2><span id='topic+ensure'></span>

<h3>Description</h3>

<p>These routines are useful when preparing to pass objects to
a Spark routine, as it is often necessary to ensure certain
parameters are scalar integers, or scalar doubles, and so on.
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ensure_+3A_object">object</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="ensure_+3A_allow.na">allow.na</code></td>
<td>
<p>Are <code>NA</code> values permitted for this object?</p>
</td></tr>
<tr><td><code id="ensure_+3A_allow.null">allow.null</code></td>
<td>
<p>Are <code>NULL</code> values permitted for this object?</p>
</td></tr>
<tr><td><code id="ensure_+3A_default">default</code></td>
<td>
<p>If <code>object</code> is <code>NULL</code>, what value should
be used in its place? If <code>default</code> is specified, <code>allow.null</code>
is ignored (and assumed to be <code>TRUE</code>).</p>
</td></tr>
</table>

<hr>
<h2 id='fill'>Fill</h2><span id='topic+fill'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+fill">fill</a></code> for more details.
</p>

<hr>
<h2 id='filter'>Filter</h2><span id='topic+filter'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+filter">filter</a></code> for more details.
</p>

<hr>
<h2 id='find_scalac'>Discover the Scala Compiler</h2><span id='topic+find_scalac'></span>

<h3>Description</h3>

<p>Find the <code>scalac</code> compiler for a particular version of
<code>scala</code>, by scanning some common directories containing
<code>scala</code> installations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_scalac(version, locations = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_scalac_+3A_version">version</code></td>
<td>
<p>The <code>scala</code> version to search for. Versions
of the form <code>major.minor</code> will be matched against the
<code>scalac</code> installation with version <code>major.minor.patch</code>;
if multiple compilers are discovered the most recent one will be
used.</p>
</td></tr>
<tr><td><code id="find_scalac_+3A_locations">locations</code></td>
<td>
<p>Additional locations to scan. By default, the
directories <code>/opt/scala</code> and <code>/usr/local/scala</code> will
be scanned.</p>
</td></tr>
</table>

<hr>
<h2 id='ft_binarizer'>Feature Transformation &ndash; Binarizer (Transformer)</h2><span id='topic+ft_binarizer'></span>

<h3>Description</h3>

<p>Apply thresholding to a column, such that values less than or equal to the
<code>threshold</code> are assigned the value 0.0, and values greater than the
threshold are assigned the value 1.0. Column output is numeric for
compatibility with other modeling functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_binarizer(
  x,
  input_col,
  output_col,
  threshold = 0,
  uid = random_string("binarizer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_binarizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_binarizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_binarizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_binarizer_+3A_threshold">threshold</code></td>
<td>
<p>Threshold used to binarize continuous features.</p>
</td></tr>
<tr><td><code id="ft_binarizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_binarizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  ft_binarizer(
    input_col = "Sepal_Length",
    output_col = "Sepal_Length_bin",
    threshold = 5
  ) %&gt;%
  select(Sepal_Length, Sepal_Length_bin, Species)

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_bucketizer'>Feature Transformation &ndash; Bucketizer (Transformer)</h2><span id='topic+ft_bucketizer'></span>

<h3>Description</h3>

<p>Similar to <span class="rlang"><b>R</b></span>'s <code><a href="base.html#topic+cut">cut</a></code> function, this transforms a numeric column
into a discretized column, with breaks specified through the <code>splits</code>
parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_bucketizer(
  x,
  input_col = NULL,
  output_col = NULL,
  splits = NULL,
  input_cols = NULL,
  output_cols = NULL,
  splits_array = NULL,
  handle_invalid = "error",
  uid = random_string("bucketizer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_bucketizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_splits">splits</code></td>
<td>
<p>A numeric vector of cutpoints, indicating the bucket boundaries.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_input_cols">input_cols</code></td>
<td>
<p>Names of input columns.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_output_cols">output_cols</code></td>
<td>
<p>Names of output columns.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_splits_array">splits_array</code></td>
<td>
<p>Parameter for specifying multiple splits parameters. Each
element in this array can be used to map continuous features into buckets.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_bucketizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  ft_bucketizer(
    input_col = "Sepal_Length",
    output_col = "Sepal_Length_bucket",
    splits = c(0, 4.5, 5, 8)
  ) %&gt;%
  select(Sepal_Length, Sepal_Length_bucket, Species)

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_chisq_selector'>Feature Transformation &ndash; ChiSqSelector (Estimator)</h2><span id='topic+ft_chisq_selector'></span>

<h3>Description</h3>

<p>Chi-Squared feature selection, which selects categorical features to use for predicting a categorical label
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_chisq_selector(
  x,
  features_col = "features",
  output_col = NULL,
  label_col = "label",
  selector_type = "numTopFeatures",
  fdr = 0.05,
  fpr = 0.05,
  fwe = 0.05,
  num_top_features = 50,
  percentile = 0.1,
  uid = random_string("chisq_selector_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_chisq_selector_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_selector_type">selector_type</code></td>
<td>
<p>(Spark 2.1.0+) The selector type of the ChisqSelector. Supported options: &quot;numTopFeatures&quot; (default), &quot;percentile&quot;, &quot;fpr&quot;, &quot;fdr&quot;, &quot;fwe&quot;.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_fdr">fdr</code></td>
<td>
<p>(Spark 2.2.0+) The upper bound of the expected false discovery rate. Only applicable when selector_type = &quot;fdr&quot;. Default value is 0.05.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_fpr">fpr</code></td>
<td>
<p>(Spark 2.1.0+) The highest p-value for features to be kept. Only applicable when selector_type= &quot;fpr&quot;. Default value is 0.05.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_fwe">fwe</code></td>
<td>
<p>(Spark 2.2.0+) The upper bound of the expected family-wise error rate. Only applicable when selector_type = &quot;fwe&quot;. Default value is 0.05.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_num_top_features">num_top_features</code></td>
<td>
<p>Number of features that selector will select, ordered by ascending p-value. If the number of features is less than <code>num_top_features</code>, then this will select all features. Only applicable when selector_type = &quot;numTopFeatures&quot;. The default value of <code>num_top_features</code> is 50.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_percentile">percentile</code></td>
<td>
<p>(Spark 2.1.0+) Percentile of features that selector will select, ordered by statistics value descending. Only applicable when selector_type = &quot;percentile&quot;. Default value is 0.1.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_chisq_selector_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_count_vectorizer'>Feature Transformation &ndash; CountVectorizer (Estimator)</h2><span id='topic+ft_count_vectorizer'></span><span id='topic+ml_vocabulary'></span>

<h3>Description</h3>

<p>Extracts a vocabulary from document collections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_count_vectorizer(
  x,
  input_col = NULL,
  output_col = NULL,
  binary = FALSE,
  min_df = 1,
  min_tf = 1,
  vocab_size = 2^18,
  uid = random_string("count_vectorizer_"),
  ...
)

ml_vocabulary(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_count_vectorizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_binary">binary</code></td>
<td>
<p>Binary toggle to control the output vector values.
If <code>TRUE</code>, all nonzero counts (after <code>min_tf</code> filter applied)
are set to 1. This is useful for discrete probabilistic models that
model binary events rather than integer counts. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_min_df">min_df</code></td>
<td>
<p>Specifies the minimum number of different documents a
term must appear in to be included in the vocabulary. If this is an
integer greater than or equal to 1, this specifies the number of
documents the term must appear in; if this is a double in [0,1), then
this specifies the fraction of documents. Default: 1.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_min_tf">min_tf</code></td>
<td>
<p>Filter to ignore rare words in a document. For each
document, terms with frequency/count less than the given threshold
are ignored. If this is an integer greater than or equal to 1, then
this specifies a count (of times the term must appear in the document);
if this is a double in [0,1), then this specifies a fraction (out of
the document's token count). Default: 1.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_vocab_size">vocab_size</code></td>
<td>
<p>Build a vocabulary that only considers the top
<code>vocab_size</code> terms ordered by term frequency across the corpus.
Default: <code>2^18</code>.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ft_count_vectorizer_+3A_model">model</code></td>
<td>
<p>A <code>ml_count_vectorizer_model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>
<p><code>ml_vocabulary()</code> returns a vector of vocabulary built.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_dct'>Feature Transformation &ndash; Discrete Cosine Transform (DCT) (Transformer)</h2><span id='topic+ft_dct'></span><span id='topic+ft_discrete_cosine_transform'></span>

<h3>Description</h3>

<p>A feature transformer that takes the 1D discrete cosine transform of a real
vector. No zero padding is performed on the input vector. It returns a real
vector of the same length representing the DCT. The return vector is scaled
such that the transform matrix is unitary (aka scaled DCT-II).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_dct(
  x,
  input_col = NULL,
  output_col = NULL,
  inverse = FALSE,
  uid = random_string("dct_"),
  ...
)

ft_discrete_cosine_transform(
  x,
  input_col,
  output_col,
  inverse = FALSE,
  uid = random_string("dct_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_dct_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_dct_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_dct_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_dct_+3A_inverse">inverse</code></td>
<td>
<p>Indicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).</p>
</td></tr>
<tr><td><code id="ft_dct_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_dct_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ft_discrete_cosine_transform()</code> is an alias for <code>ft_dct</code> for backwards compatibility.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_elementwise_product'>Feature Transformation &ndash; ElementwiseProduct (Transformer)</h2><span id='topic+ft_elementwise_product'></span>

<h3>Description</h3>

<p>Outputs the Hadamard product (i.e., the element-wise product) of each input vector
with a provided &quot;weight&quot; vector. In other words, it scales each column of the
dataset by a scalar multiplier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_elementwise_product(
  x,
  input_col = NULL,
  output_col = NULL,
  scaling_vec = NULL,
  uid = random_string("elementwise_product_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_elementwise_product_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_elementwise_product_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_elementwise_product_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_elementwise_product_+3A_scaling_vec">scaling_vec</code></td>
<td>
<p>the vector to multiply with input vectors</p>
</td></tr>
<tr><td><code id="ft_elementwise_product_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_elementwise_product_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_feature_hasher'>Feature Transformation &ndash; FeatureHasher (Transformer)</h2><span id='topic+ft_feature_hasher'></span>

<h3>Description</h3>

<p>Feature Transformation &ndash; FeatureHasher (Transformer)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_feature_hasher(
  x,
  input_cols = NULL,
  output_col = NULL,
  num_features = 2^18,
  categorical_cols = NULL,
  uid = random_string("feature_hasher_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_feature_hasher_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_input_cols">input_cols</code></td>
<td>
<p>Names of input columns.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_output_col">output_col</code></td>
<td>
<p>Name of output column.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_num_features">num_features</code></td>
<td>
<p>Number of features. Defaults to <code class="reqn">2^18</code>.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_categorical_cols">categorical_cols</code></td>
<td>
<p>Numeric columns to treat as categorical features.
By default only string and boolean columns are treated as categorical,
so this param can be used to explicitly specify the numerical columns to
treat as categorical.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_feature_hasher_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Feature hashing projects a set of categorical or numerical features into a
feature vector of specified dimension (typically substantially smaller than
that of the original feature space). This is done using the hashing trick
<a href="https://en.wikipedia.org/wiki/Feature_hashing">https://en.wikipedia.org/wiki/Feature_hashing</a> to map features to indices
in the feature vector.
</p>
<p>The FeatureHasher transformer operates on multiple columns. Each column may
contain either numeric or categorical features. Behavior and handling of
column data types is as follows: -Numeric columns: For numeric features,
the hash value of the column name is used to map the feature value to its
index in the feature vector. By default, numeric features are not treated
as categorical (even when they are integers). To treat them as categorical,
specify the relevant columns in categoricalCols. -String columns: For
categorical features, the hash value of the string &quot;column_name=value&quot;
is used to map to the vector index, with an indicator value of 1.0.
Thus, categorical features are &quot;one-hot&quot; encoded (similarly to using
OneHotEncoder with drop_last=FALSE). -Boolean columns: Boolean values
are treated in the same way as string columns. That is, boolean features
are represented as &quot;column_name=true&quot; or &quot;column_name=false&quot;, with an
indicator value of 1.0.
</p>
<p>Null (missing) values are ignored (implicitly zero in the resulting feature vector).
</p>
<p>The hash function used here is also the MurmurHash 3 used in HashingTF. Since a
simple modulo on the hashed value is used to determine the vector index, it is
advisable to use a power of two as the num_features parameter; otherwise the
features will not be mapped evenly to the vector indices.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_hashing_tf'>Feature Transformation &ndash; HashingTF (Transformer)</h2><span id='topic+ft_hashing_tf'></span>

<h3>Description</h3>

<p>Maps a sequence of terms to their term frequencies using the hashing trick.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_hashing_tf(
  x,
  input_col = NULL,
  output_col = NULL,
  binary = FALSE,
  num_features = 2^18,
  uid = random_string("hashing_tf_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_hashing_tf_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_binary">binary</code></td>
<td>
<p>Binary toggle to control term frequency counts.
If true, all non-zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts. (default = <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_num_features">num_features</code></td>
<td>
<p>Number of features. Should be greater than 0. (default = <code>2^18</code>)</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_hashing_tf_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_idf'>Feature Transformation &ndash; IDF (Estimator)</h2><span id='topic+ft_idf'></span>

<h3>Description</h3>

<p>Compute the Inverse Document Frequency (IDF) given a collection of documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_idf(
  x,
  input_col = NULL,
  output_col = NULL,
  min_doc_freq = 0,
  uid = random_string("idf_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_idf_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_idf_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_idf_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_idf_+3A_min_doc_freq">min_doc_freq</code></td>
<td>
<p>The minimum number of documents in which a term should appear. Default: 0</p>
</td></tr>
<tr><td><code id="ft_idf_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_idf_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_imputer'>Feature Transformation &ndash; Imputer (Estimator)</h2><span id='topic+ft_imputer'></span>

<h3>Description</h3>

<p>Imputation estimator for completing missing values, either using the mean or
the median of the columns in which the missing values are located. The input
columns should be of numeric type. This function requires Spark 2.2.0+.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_imputer(
  x,
  input_cols = NULL,
  output_cols = NULL,
  missing_value = NULL,
  strategy = "mean",
  uid = random_string("imputer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_imputer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_input_cols">input_cols</code></td>
<td>
<p>The names of the input columns</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_output_cols">output_cols</code></td>
<td>
<p>The names of the output columns.</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_missing_value">missing_value</code></td>
<td>
<p>The placeholder for the missing values. All occurrences of
<code>missing_value</code> will be imputed. Note that null values are always treated
as missing.</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_strategy">strategy</code></td>
<td>
<p>The imputation strategy. Currently only &quot;mean&quot; and &quot;median&quot; are
supported. If &quot;mean&quot;, then replace missing values using the mean value of the
feature. If &quot;median&quot;, then replace missing values using the approximate median
value of the feature. Default: mean</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_imputer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_index_to_string'>Feature Transformation &ndash; IndexToString (Transformer)</h2><span id='topic+ft_index_to_string'></span>

<h3>Description</h3>

<p>A Transformer that maps a column of indices back to a new column of
corresponding string values. The index-string mapping is either from
the ML attributes of the input column, or from user-supplied labels
(which take precedence over ML attributes). This function is the inverse
of <code><a href="#topic+ft_string_indexer">ft_string_indexer</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_index_to_string(
  x,
  input_col = NULL,
  output_col = NULL,
  labels = NULL,
  uid = random_string("index_to_string_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_index_to_string_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_index_to_string_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_index_to_string_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_index_to_string_+3A_labels">labels</code></td>
<td>
<p>Optional param for array of labels specifying index-string mapping.</p>
</td></tr>
<tr><td><code id="ft_index_to_string_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_index_to_string_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ft_string_indexer">ft_string_indexer</a></code>
</p>
<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_interaction'>Feature Transformation &ndash; Interaction (Transformer)</h2><span id='topic+ft_interaction'></span>

<h3>Description</h3>

<p>Implements the feature interaction transform. This transformer takes in Double and
Vector type columns and outputs a flattened vector of their feature interactions.
To handle interaction, we first one-hot encode any nominal features. Then, a
vector of the feature cross-products is produced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_interaction(
  x,
  input_cols = NULL,
  output_col = NULL,
  uid = random_string("interaction_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_interaction_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_interaction_+3A_input_cols">input_cols</code></td>
<td>
<p>The names of the input columns</p>
</td></tr>
<tr><td><code id="ft_interaction_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_interaction_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_interaction_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_lsh'>Feature Transformation &ndash; LSH (Estimator)</h2><span id='topic+ft_lsh'></span><span id='topic+ft_bucketed_random_projection_lsh'></span><span id='topic+ft_minhash_lsh'></span>

<h3>Description</h3>

<p>Locality Sensitive Hashing functions for Euclidean distance
(Bucketed Random Projection) and Jaccard distance (MinHash).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_bucketed_random_projection_lsh(
  x,
  input_col = NULL,
  output_col = NULL,
  bucket_length = NULL,
  num_hash_tables = 1,
  seed = NULL,
  uid = random_string("bucketed_random_projection_lsh_"),
  ...
)

ft_minhash_lsh(
  x,
  input_col = NULL,
  output_col = NULL,
  num_hash_tables = 1L,
  seed = NULL,
  uid = random_string("minhash_lsh_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_lsh_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_bucket_length">bucket_length</code></td>
<td>
<p>The length of each hash bucket, a larger bucket lowers the
false negative rate. The number of buckets will be (max L2 norm of input vectors) /
bucketLength.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_num_hash_tables">num_hash_tables</code></td>
<td>
<p>Number of hash tables used in LSH OR-amplification. LSH
OR-amplification can be used to reduce the false negative rate. Higher values
for this param lead to a reduced false negative rate, at the expense of added
computational complexity.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_lsh_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>ft_lsh_utils
</p>
<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_lsh_utils'>Utility functions for LSH models</h2><span id='topic+ft_lsh_utils'></span><span id='topic+ml_approx_nearest_neighbors'></span><span id='topic+ml_approx_similarity_join'></span>

<h3>Description</h3>

<p>Utility functions for LSH models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_approx_nearest_neighbors(
  model,
  dataset,
  key,
  num_nearest_neighbors,
  dist_col = "distCol"
)

ml_approx_similarity_join(
  model,
  dataset_a,
  dataset_b,
  threshold,
  dist_col = "distCol"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_lsh_utils_+3A_model">model</code></td>
<td>
<p>A fitted LSH model, returned by either <code>ft_minhash_lsh()</code>
or <code>ft_bucketed_random_projection_lsh()</code>.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_dataset">dataset</code></td>
<td>
<p>The dataset to search for nearest neighbors of the key.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_key">key</code></td>
<td>
<p>Feature vector representing the item to search for.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_num_nearest_neighbors">num_nearest_neighbors</code></td>
<td>
<p>The maximum number of nearest neighbors.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_dist_col">dist_col</code></td>
<td>
<p>Output column for storing the distance between each result row and the key.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_dataset_a">dataset_a</code></td>
<td>
<p>One of the datasets to join.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_dataset_b">dataset_b</code></td>
<td>
<p>Another dataset to join.</p>
</td></tr>
<tr><td><code id="ft_lsh_utils_+3A_threshold">threshold</code></td>
<td>
<p>The threshold for the distance of row pairs.</p>
</td></tr>
</table>

<hr>
<h2 id='ft_max_abs_scaler'>Feature Transformation &ndash; MaxAbsScaler (Estimator)</h2><span id='topic+ft_max_abs_scaler'></span>

<h3>Description</h3>

<p>Rescale each feature individually to range [-1, 1] by dividing through the
largest maximum absolute value in each feature. It does not shift/center the
data, and thus does not destroy any sparsity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_max_abs_scaler(
  x,
  input_col = NULL,
  output_col = NULL,
  uid = random_string("max_abs_scaler_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_max_abs_scaler_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_max_abs_scaler_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_max_abs_scaler_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_max_abs_scaler_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_max_abs_scaler_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  ft_vector_assembler(
    input_col = features,
    output_col = "features_temp"
  ) %&gt;%
  ft_max_abs_scaler(
    input_col = "features_temp",
    output_col = "features"
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_min_max_scaler'>Feature Transformation &ndash; MinMaxScaler (Estimator)</h2><span id='topic+ft_min_max_scaler'></span>

<h3>Description</h3>

<p>Rescale each feature individually to a common range [min, max] linearly using
column summary statistics, which is also known as min-max normalization or
Rescaling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_min_max_scaler(
  x,
  input_col = NULL,
  output_col = NULL,
  min = 0,
  max = 1,
  uid = random_string("min_max_scaler_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_min_max_scaler_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_min">min</code></td>
<td>
<p>Lower bound after transformation, shared by all features Default: 0.0</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_max">max</code></td>
<td>
<p>Upper bound after transformation, shared by all features Default: 1.0</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_min_max_scaler_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  ft_vector_assembler(
    input_col = features,
    output_col = "features_temp"
  ) %&gt;%
  ft_min_max_scaler(
    input_col = "features_temp",
    output_col = "features"
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_ngram'>Feature Transformation &ndash; NGram (Transformer)</h2><span id='topic+ft_ngram'></span>

<h3>Description</h3>

<p>A feature transformer that converts the input array of strings into an array of n-grams. Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_ngram(
  x,
  input_col = NULL,
  output_col = NULL,
  n = 2,
  uid = random_string("ngram_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_ngram_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_ngram_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_ngram_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_ngram_+3A_n">n</code></td>
<td>
<p>Minimum n-gram length, greater than or equal to 1. Default: 2, bigram features</p>
</td></tr>
<tr><td><code id="ft_ngram_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_ngram_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_normalizer'>Feature Transformation &ndash; Normalizer (Transformer)</h2><span id='topic+ft_normalizer'></span>

<h3>Description</h3>

<p>Normalize a vector to have unit norm using the given p-norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_normalizer(
  x,
  input_col = NULL,
  output_col = NULL,
  p = 2,
  uid = random_string("normalizer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_normalizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_normalizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_normalizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_normalizer_+3A_p">p</code></td>
<td>
<p>Normalization in L^p space. Must be &gt;= 1. Defaults to 2.</p>
</td></tr>
<tr><td><code id="ft_normalizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_normalizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_one_hot_encoder'>Feature Transformation &ndash; OneHotEncoder (Transformer)</h2><span id='topic+ft_one_hot_encoder'></span>

<h3>Description</h3>

<p>One-hot encoding maps a column of label indices to a column of binary
vectors, with at most a single one-value. This encoding allows algorithms
which expect continuous features, such as Logistic Regression, to use
categorical features. Typically, used with  <code>ft_string_indexer()</code> to
index a column first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_one_hot_encoder(
  x,
  input_cols = NULL,
  output_cols = NULL,
  handle_invalid = NULL,
  drop_last = TRUE,
  uid = random_string("one_hot_encoder_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_one_hot_encoder_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_input_cols">input_cols</code></td>
<td>
<p>The name of the input columns.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_output_cols">output_cols</code></td>
<td>
<p>The name of the output columns.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_drop_last">drop_last</code></td>
<td>
<p>Whether to drop the last category. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_one_hot_encoder_estimator'>Feature Transformation &ndash; OneHotEncoderEstimator (Estimator)</h2><span id='topic+ft_one_hot_encoder_estimator'></span>

<h3>Description</h3>

<p>A one-hot encoder that maps a column of category indices
to a column of binary vectors, with at most a single one-value
per row that indicates the input category index. For example
with 5 categories, an input value of 2.0 would map to an output
vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included
by default (configurable via dropLast), because it makes the
vector entries sum up to one, and hence linearly dependent. So
an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_one_hot_encoder_estimator(
  x,
  input_cols = NULL,
  output_cols = NULL,
  handle_invalid = "error",
  drop_last = TRUE,
  uid = random_string("one_hot_encoder_estimator_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_input_cols">input_cols</code></td>
<td>
<p>Names of input columns.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_output_cols">output_cols</code></td>
<td>
<p>Names of output columns.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_drop_last">drop_last</code></td>
<td>
<p>Whether to drop the last category. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_one_hot_encoder_estimator_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_pca'>Feature Transformation &ndash; PCA (Estimator)</h2><span id='topic+ft_pca'></span><span id='topic+ml_pca'></span>

<h3>Description</h3>

<p>PCA trains a model to project vectors to a lower dimensional space of the top k principal components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_pca(
  x,
  input_col = NULL,
  output_col = NULL,
  k = NULL,
  uid = random_string("pca_"),
  ...
)

ml_pca(x, features = tbl_vars(x), k = length(features), pc_prefix = "PC", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_pca_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_k">k</code></td>
<td>
<p>The number of principal components</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_features">features</code></td>
<td>
<p>The columns to use in the principal components
analysis. Defaults to all columns in <code>x</code>.</p>
</td></tr>
<tr><td><code id="ft_pca_+3A_pc_prefix">pc_prefix</code></td>
<td>
<p>Length-one character vector used to prepend names of components.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>
<p><code>ml_pca()</code> is a wrapper around <code>ft_pca()</code> that returns a
<code>ml_model</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  select(-Species) %&gt;%
  ml_pca(k = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_polynomial_expansion'>Feature Transformation &ndash; PolynomialExpansion (Transformer)</h2><span id='topic+ft_polynomial_expansion'></span>

<h3>Description</h3>

<p>Perform feature expansion in a polynomial space. E.g. take a 2-variable feature
vector as an example: (x, y), if we want to expand it with degree 2, then
we get (x, x * x, y, x * y, y * y).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_polynomial_expansion(
  x,
  input_col = NULL,
  output_col = NULL,
  degree = 2,
  uid = random_string("polynomial_expansion_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_polynomial_expansion_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_polynomial_expansion_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_polynomial_expansion_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_polynomial_expansion_+3A_degree">degree</code></td>
<td>
<p>The polynomial degree to expand, which should be greater
than equal to 1. A value of 1 means no expansion. Default: 2</p>
</td></tr>
<tr><td><code id="ft_polynomial_expansion_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_polynomial_expansion_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_quantile_discretizer'>Feature Transformation &ndash; QuantileDiscretizer (Estimator)</h2><span id='topic+ft_quantile_discretizer'></span>

<h3>Description</h3>

<p><code>ft_quantile_discretizer</code> takes a column with continuous features and outputs
a column with binned categorical features. The number of bins can be
set using the <code>num_buckets</code> parameter. It is possible that the number
of buckets used will be smaller than this value, for example, if there
are too few distinct values of the input to create enough distinct
quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_quantile_discretizer(
  x,
  input_col = NULL,
  output_col = NULL,
  num_buckets = 2,
  input_cols = NULL,
  output_cols = NULL,
  num_buckets_array = NULL,
  handle_invalid = "error",
  relative_error = 0.001,
  uid = random_string("quantile_discretizer_"),
  weight_column = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_quantile_discretizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_num_buckets">num_buckets</code></td>
<td>
<p>Number of buckets (quantiles, or categories) into which data
points are grouped. Must be greater than or equal to 2.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_input_cols">input_cols</code></td>
<td>
<p>Names of input columns.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_output_cols">output_cols</code></td>
<td>
<p>Names of output columns.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_num_buckets_array">num_buckets_array</code></td>
<td>
<p>Array of number of buckets (quantiles, or categories)
into which data points are grouped. Each value must be greater than or equal to 2.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_relative_error">relative_error</code></td>
<td>
<p>(Spark 2.0.0+) Relative error (see documentation for
org.apache.spark.sql.DataFrameStatFunctions.approxQuantile
<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions">here</a>
for description). Must be in the range [0, 1]. default: 0.001</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_weight_column">weight_column</code></td>
<td>
<p>If not NULL, then a generalized version of the Greenwald-Khanna algorithm will be run to compute
weighted percentiles, with each input having a relative weight specified by the corresponding value in 'weight_column'.
The weights can be considered as relative frequencies of sample inputs.</p>
</td></tr>
<tr><td><code id="ft_quantile_discretizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NaN handling: null and NaN values will be ignored from the column
during <code>QuantileDiscretizer</code> fitting. This will produce a <code>Bucketizer</code>
model for making predictions. During the transformation, <code>Bucketizer</code>
will raise an error when it finds NaN values in the dataset, but the
user can also choose to either keep or remove NaN values within the
dataset by setting <code>handle_invalid</code> If the user chooses to keep NaN values,
they will be handled specially and placed into their own bucket,
for example, if 4 buckets are used, then non-NaN data will be put
into buckets[0-3], but NaNs will be counted in a special bucket[4].
</p>
<p>Algorithm: The bin ranges are chosen using an approximate algorithm (see
the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile
<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions">here</a> for a detailed description). The precision of the approximation can be
controlled with the <code>relative_error</code> parameter. The lower and upper bin
bounds will be -Infinity and +Infinity, covering all real values.
</p>
<p>Note that the result may be different every time you run it, since the sample
strategy behind it is non-deterministic.
</p>
<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ft_bucketizer">ft_bucketizer</a></code>
</p>
<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_r_formula'>Feature Transformation &ndash; RFormula (Estimator)</h2><span id='topic+ft_r_formula'></span>

<h3>Description</h3>

<p>Implements the transforms required for fitting a dataset against an R model
formula. Currently we support a limited subset of the R operators,
including <code>~</code>, <code>.</code>, <code>:</code>, <code>+</code>, and <code>-</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_r_formula(
  x,
  formula = NULL,
  features_col = "features",
  label_col = "label",
  force_index_label = FALSE,
  uid = random_string("r_formula_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_r_formula_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_formula">formula</code></td>
<td>
<p>R formula as a character string or a formula. Formula objects are
converted to character strings directly and the environment is not captured.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_force_index_label">force_index_label</code></td>
<td>
<p>(Spark 2.1.0+) Force to index label whether it is numeric or
string type. Usually we index label only when it is string type. If
the formula was used by classification algorithms, we can force to index
label even it is numeric type by setting this param with true.
Default: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_r_formula_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic operators in the formula are:
</p>

<ul>
<li><p> ~ separate target and terms
</p>
</li>
<li><p> + concat terms, &quot;+ 0&quot; means removing intercept
</p>
</li>
<li><p> - remove a term, &quot;- 1&quot; means removing intercept
</p>
</li>
<li><p> : interaction (multiplication for numeric values, or binarized categorical values)
</p>
</li>
<li><p> . all columns except target
</p>
</li></ul>

<p>Suppose a and b are double columns, we use the following simple examples to illustrate the
effect of RFormula:
</p>

<ul>
<li> <p><code>y ~ a + b</code> means model <code>y ~ w0 + w1 * a + w2 * b</code>
where <code>w0</code> is the intercept and <code>w1, w2</code> are coefficients.
</p>
</li>
<li> <p><code>y ~ a + b + a:b - 1</code> means model <code>y ~ w1 * a + w2 * b + w3 * a * b</code>
where <code>w1, w2, w3</code> are coefficients.
</p>
</li></ul>

<p>RFormula produces a vector column of features and a double or string column
of label. Like when formulas are used in R for linear regression, string
input columns will be one-hot encoded, and numeric columns will be cast to
doubles. If the label column is of type string, it will be first transformed
to double with StringIndexer. If the label column does not exist in the
DataFrame, the output label column will be created from the specified
response variable in the formula.
</p>
<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_regex_tokenizer'>Feature Transformation &ndash; RegexTokenizer (Transformer)</h2><span id='topic+ft_regex_tokenizer'></span>

<h3>Description</h3>

<p>A regex based tokenizer that extracts tokens either by using the provided
regex pattern to split the text (default) or repeatedly matching the regex
(if <code>gaps</code> is false). Optional parameters also allow filtering tokens using a
minimal length. It returns an array of strings that can be empty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_regex_tokenizer(
  x,
  input_col = NULL,
  output_col = NULL,
  gaps = TRUE,
  min_token_length = 1,
  pattern = "\\s+",
  to_lower_case = TRUE,
  uid = random_string("regex_tokenizer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_regex_tokenizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_gaps">gaps</code></td>
<td>
<p>Indicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_min_token_length">min_token_length</code></td>
<td>
<p>Minimum token length, greater than or equal to 0.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_pattern">pattern</code></td>
<td>
<p>The regular expression pattern to be used.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_to_lower_case">to_lower_case</code></td>
<td>
<p>Indicates whether to convert all characters to lowercase before tokenizing.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_regex_tokenizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_robust_scaler'>Feature Transformation &ndash; RobustScaler (Estimator)</h2><span id='topic+ft_robust_scaler'></span>

<h3>Description</h3>

<p>RobustScaler removes the median and scales the data according to the quantile range.
The quantile range is by default IQR (Interquartile Range, quantile range between the
1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.
Centering and scaling happen independently on each feature by computing the relevant
statistics on the samples in the training set. Median and quantile range are then
stored to be used on later data using the transform method.
Note that missing values are ignored in the computation of medians and ranges.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_robust_scaler(
  x,
  input_col = NULL,
  output_col = NULL,
  lower = 0.25,
  upper = 0.75,
  with_centering = TRUE,
  with_scaling = TRUE,
  relative_error = 0.001,
  uid = random_string("ft_robust_scaler_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_robust_scaler_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_lower">lower</code></td>
<td>
<p>Lower quantile to calculate quantile range.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_upper">upper</code></td>
<td>
<p>Upper quantile to calculate quantile range.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_with_centering">with_centering</code></td>
<td>
<p>Whether to center data with median.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_with_scaling">with_scaling</code></td>
<td>
<p>Whether to scale the data to quantile range.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_relative_error">relative_error</code></td>
<td>
<p>The target relative error for quantile computation.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_robust_scaler_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_sql_transformer'>Feature Transformation &ndash; SQLTransformer</h2><span id='topic+ft_sql_transformer'></span><span id='topic+ft_dplyr_transformer'></span>

<h3>Description</h3>

<p>Implements the transformations which are defined by SQL statement. Currently we
only support SQL syntax like 'SELECT ... FROM __THIS__ ...' where '__THIS__' represents
the underlying table of the input dataset. The select clause specifies the
fields, constants, and expressions to display in the output, it can be any
select clause that Spark SQL supports. Users can also use Spark SQL built-in
function and UDFs to operate on these selected columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_sql_transformer(
  x,
  statement = NULL,
  uid = random_string("sql_transformer_"),
  ...
)

ft_dplyr_transformer(x, tbl, uid = random_string("dplyr_transformer_"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_sql_transformer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_sql_transformer_+3A_statement">statement</code></td>
<td>
<p>A SQL statement.</p>
</td></tr>
<tr><td><code id="ft_sql_transformer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_sql_transformer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ft_sql_transformer_+3A_tbl">tbl</code></td>
<td>
<p>A <code>tbl_spark</code> generated using <code>dplyr</code> transformations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ft_dplyr_transformer()</code> is mostly a wrapper around <code>ft_sql_transformer()</code> that
takes a <code>tbl_spark</code> instead of a SQL statement. Internally, the <code>ft_dplyr_transformer()</code>
extracts the <code>dplyr</code> transformations used to generate <code>tbl</code> as a SQL statement or a
sampling operation. Note that only single-table <code>dplyr</code> verbs are supported and that the
<code>sdf_</code> family of functions are not.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_standard_scaler'>Feature Transformation &ndash; StandardScaler (Estimator)</h2><span id='topic+ft_standard_scaler'></span>

<h3>Description</h3>

<p>Standardizes features by removing the mean and scaling to unit variance using
column summary statistics on the samples in the training set. The &quot;unit std&quot;
is computed using the corrected sample standard deviation, which is computed
as the square root of the unbiased sample variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_standard_scaler(
  x,
  input_col = NULL,
  output_col = NULL,
  with_mean = FALSE,
  with_std = TRUE,
  uid = random_string("standard_scaler_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_standard_scaler_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_with_mean">with_mean</code></td>
<td>
<p>Whether to center the data with mean before scaling. It will
build a dense output, so take care when applying to sparse input. Default: FALSE</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_with_std">with_std</code></td>
<td>
<p>Whether to scale the data to unit standard deviation. Default: TRUE</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_standard_scaler_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- c("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  ft_vector_assembler(
    input_col = features,
    output_col = "features_temp"
  ) %&gt;%
  ft_standard_scaler(
    input_col = "features_temp",
    output_col = "features",
    with_mean = TRUE
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='ft_stop_words_remover'>Feature Transformation &ndash; StopWordsRemover (Transformer)</h2><span id='topic+ft_stop_words_remover'></span>

<h3>Description</h3>

<p>A feature transformer that filters out stop words from input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_stop_words_remover(
  x,
  input_col = NULL,
  output_col = NULL,
  case_sensitive = FALSE,
  stop_words = ml_default_stop_words(spark_connection(x), "english"),
  uid = random_string("stop_words_remover_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_stop_words_remover_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>Whether to do a case sensitive comparison over the stop words.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_stop_words">stop_words</code></td>
<td>
<p>The words to be filtered out.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_stop_words_remover_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ml_default_stop_words">ml_default_stop_words</a></code>
</p>
<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_string_indexer'>Feature Transformation &ndash; StringIndexer (Estimator)</h2><span id='topic+ft_string_indexer'></span><span id='topic+ml_labels'></span><span id='topic+ft_string_indexer_model'></span>

<h3>Description</h3>

<p>A label indexer that maps a string column of labels to an ML column of
label indices. If the input column is numeric, we cast it to string and
index the string values. The indices are in <code>[0, numLabels)</code>, ordered by
label frequencies. So the most frequent label gets index 0. This function
is the inverse of <code><a href="#topic+ft_index_to_string">ft_index_to_string</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_string_indexer(
  x,
  input_col = NULL,
  output_col = NULL,
  handle_invalid = "error",
  string_order_type = "frequencyDesc",
  uid = random_string("string_indexer_"),
  ...
)

ml_labels(model)

ft_string_indexer_model(
  x,
  input_col = NULL,
  output_col = NULL,
  labels,
  handle_invalid = "error",
  uid = random_string("string_indexer_model_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_string_indexer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_string_order_type">string_order_type</code></td>
<td>
<p>(Spark 2.3+)How to order labels of string column.
The first label after ordering is assigned an index of 0. Options are
<code>"frequencyDesc"</code>, <code>"frequencyAsc"</code>, <code>"alphabetDesc"</code>, and <code>"alphabetAsc"</code>.
Defaults to <code>"frequencyDesc"</code>.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_model">model</code></td>
<td>
<p>A fitted StringIndexer model returned by <code>ft_string_indexer()</code></p>
</td></tr>
<tr><td><code id="ft_string_indexer_+3A_labels">labels</code></td>
<td>
<p>Vector of labels, corresponding to indices to be assigned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>
<p><code>ml_labels()</code> returns a vector of labels, corresponding to indices to be assigned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ft_index_to_string">ft_index_to_string</a></code>
</p>
<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_tokenizer'>Feature Transformation &ndash; Tokenizer (Transformer)</h2><span id='topic+ft_tokenizer'></span>

<h3>Description</h3>

<p>A tokenizer that converts the input string to lowercase and then splits it
by white spaces.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_tokenizer(
  x,
  input_col = NULL,
  output_col = NULL,
  uid = random_string("tokenizer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_tokenizer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_tokenizer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_tokenizer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_tokenizer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_tokenizer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_vector_assembler'>Feature Transformation &ndash; VectorAssembler (Transformer)</h2><span id='topic+ft_vector_assembler'></span>

<h3>Description</h3>

<p>Combine multiple vectors into a single row-vector; that is,
where each row element of the newly generated column is a
vector formed by concatenating each row element from the
specified input columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_vector_assembler(
  x,
  input_cols = NULL,
  output_col = NULL,
  uid = random_string("vector_assembler_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_vector_assembler_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_vector_assembler_+3A_input_cols">input_cols</code></td>
<td>
<p>The names of the input columns</p>
</td></tr>
<tr><td><code id="ft_vector_assembler_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_vector_assembler_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_vector_assembler_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_vector_indexer'>Feature Transformation &ndash; VectorIndexer (Estimator)</h2><span id='topic+ft_vector_indexer'></span>

<h3>Description</h3>

<p>Indexing categorical feature columns in a dataset of Vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_vector_indexer(
  x,
  input_col = NULL,
  output_col = NULL,
  handle_invalid = "error",
  max_categories = 20,
  uid = random_string("vector_indexer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_vector_indexer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_handle_invalid">handle_invalid</code></td>
<td>
<p>(Spark 2.1.0+) Param for how to handle invalid entries. Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). Default: &quot;error&quot;</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_max_categories">max_categories</code></td>
<td>
<p>Threshold for the number of values a categorical feature can take. If a feature is found to have &gt; <code>max_categories</code> values, then it is declared continuous. Must be greater than or equal to 2. Defaults to 20.</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_vector_indexer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_vector_slicer'>Feature Transformation &ndash; VectorSlicer (Transformer)</h2><span id='topic+ft_vector_slicer'></span>

<h3>Description</h3>

<p>Takes a feature vector and outputs a new feature vector with a subarray of the original features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_vector_slicer(
  x,
  input_col = NULL,
  output_col = NULL,
  indices = NULL,
  uid = random_string("vector_slicer_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_vector_slicer_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_vector_slicer_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_vector_slicer_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_vector_slicer_+3A_indices">indices</code></td>
<td>
<p>An vector of indices to select features from a vector column.
Note that the indices are 0-based.</p>
</td></tr>
<tr><td><code id="ft_vector_slicer_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_vector_slicer_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_word2vec">ft_word2vec</a>()</code>
</p>

<hr>
<h2 id='ft_word2vec'>Feature Transformation &ndash; Word2Vec (Estimator)</h2><span id='topic+ft_word2vec'></span><span id='topic+ml_find_synonyms'></span>

<h3>Description</h3>

<p>Word2Vec transforms a word into a code for further natural language processing or machine learning process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_word2vec(
  x,
  input_col = NULL,
  output_col = NULL,
  vector_size = 100,
  min_count = 5,
  max_sentence_length = 1000,
  num_partitions = 1,
  step_size = 0.025,
  max_iter = 1,
  seed = NULL,
  uid = random_string("word2vec_"),
  ...
)

ml_find_synonyms(model, word, num)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ft_word2vec_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_vector_size">vector_size</code></td>
<td>
<p>The dimension of the code that you want to transform from words. Default: 100</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_min_count">min_count</code></td>
<td>
<p>The minimum number of times a token must appear to be included in
the word2vec model's vocabulary. Default: 5</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_max_sentence_length">max_sentence_length</code></td>
<td>
<p>(Spark 2.0.0+) Sets the maximum length (in words) of each sentence
in the input data. Any sentence longer than this threshold will be divided into
chunks of up to <code>max_sentence_length</code> size. Default: 1000</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions for sentences of words. Default: 1</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_step_size">step_size</code></td>
<td>
<p>Param for Step size to be used for each iteration of optimization (&gt; 0).</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_model">model</code></td>
<td>
<p>A fitted <code>Word2Vec</code> model, returned by <code>ft_word2vec()</code>.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_word">word</code></td>
<td>
<p>A word, as a length-one character vector.</p>
</td></tr>
<tr><td><code id="ft_word2vec_+3A_num">num</code></td>
<td>
<p>Number of words closest in similarity to the given word to find.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>
<p><code>ml_find_synonyms()</code> returns a DataFrame of synonyms and cosine similarities
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code><a href="#topic+ft_binarizer">ft_binarizer</a>()</code>,
<code><a href="#topic+ft_bucketizer">ft_bucketizer</a>()</code>,
<code><a href="#topic+ft_chisq_selector">ft_chisq_selector</a>()</code>,
<code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a>()</code>,
<code><a href="#topic+ft_dct">ft_dct</a>()</code>,
<code><a href="#topic+ft_elementwise_product">ft_elementwise_product</a>()</code>,
<code><a href="#topic+ft_feature_hasher">ft_feature_hasher</a>()</code>,
<code><a href="#topic+ft_hashing_tf">ft_hashing_tf</a>()</code>,
<code><a href="#topic+ft_idf">ft_idf</a>()</code>,
<code><a href="#topic+ft_imputer">ft_imputer</a>()</code>,
<code><a href="#topic+ft_index_to_string">ft_index_to_string</a>()</code>,
<code><a href="#topic+ft_interaction">ft_interaction</a>()</code>,
<code><a href="#topic+ft_lsh">ft_lsh</a></code>,
<code><a href="#topic+ft_max_abs_scaler">ft_max_abs_scaler</a>()</code>,
<code><a href="#topic+ft_min_max_scaler">ft_min_max_scaler</a>()</code>,
<code><a href="#topic+ft_ngram">ft_ngram</a>()</code>,
<code><a href="#topic+ft_normalizer">ft_normalizer</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder">ft_one_hot_encoder</a>()</code>,
<code><a href="#topic+ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator</a>()</code>,
<code><a href="#topic+ft_pca">ft_pca</a>()</code>,
<code><a href="#topic+ft_polynomial_expansion">ft_polynomial_expansion</a>()</code>,
<code><a href="#topic+ft_quantile_discretizer">ft_quantile_discretizer</a>()</code>,
<code><a href="#topic+ft_r_formula">ft_r_formula</a>()</code>,
<code><a href="#topic+ft_regex_tokenizer">ft_regex_tokenizer</a>()</code>,
<code><a href="#topic+ft_robust_scaler">ft_robust_scaler</a>()</code>,
<code><a href="#topic+ft_sql_transformer">ft_sql_transformer</a>()</code>,
<code><a href="#topic+ft_standard_scaler">ft_standard_scaler</a>()</code>,
<code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a>()</code>,
<code><a href="#topic+ft_string_indexer">ft_string_indexer</a>()</code>,
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a>()</code>,
<code><a href="#topic+ft_vector_assembler">ft_vector_assembler</a>()</code>,
<code><a href="#topic+ft_vector_indexer">ft_vector_indexer</a>()</code>,
<code><a href="#topic+ft_vector_slicer">ft_vector_slicer</a>()</code>
</p>

<hr>
<h2 id='full_join'>Full join</h2><span id='topic+full_join'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+mutate-joins">full_join</a></code> for more details.
</p>

<hr>
<h2 id='generic_call_interface'>Generic Call Interface</h2><span id='topic+generic_call_interface'></span>

<h3>Description</h3>

<p>Generic Call Interface
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generic_call_interface_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
<tr><td><code id="generic_call_interface_+3A_static">static</code></td>
<td>
<p>Is this a static method call (including a constructor). If so
then the <code>object</code> parameter should be the name of a class (otherwise
it should be a spark_jobj instance).</p>
</td></tr>
<tr><td><code id="generic_call_interface_+3A_object">object</code></td>
<td>
<p>Object instance or name of class (for <code>static</code>)</p>
</td></tr>
<tr><td><code id="generic_call_interface_+3A_method">method</code></td>
<td>
<p>Name of method</p>
</td></tr>
<tr><td><code id="generic_call_interface_+3A_...">...</code></td>
<td>
<p>Call parameters</p>
</td></tr>
</table>

<hr>
<h2 id='get_spark_sql_catalog_implementation'>Retrieve the Spark connection's SQL catalog implementation property</h2><span id='topic+get_spark_sql_catalog_implementation'></span>

<h3>Description</h3>

<p>Retrieve the Spark connection's SQL catalog implementation property
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_spark_sql_catalog_implementation(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_spark_sql_catalog_implementation_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>spark.sql.catalogImplementation property from the connection's
runtime configuration
</p>

<hr>
<h2 id='hive_context_config'>Runtime configuration interface for Hive</h2><span id='topic+hive_context_config'></span>

<h3>Description</h3>

<p>Retrieves the runtime configuration interface for Hive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hive_context_config(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hive_context_config_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='hof_aggregate'>Apply Aggregate Function to Array Column</h2><span id='topic+hof_aggregate'></span>

<h3>Description</h3>

<p>Apply an element-wise aggregation function to an array column
(this is essentially a dplyr wrapper for the
<code>aggregate(array&lt;T&gt;, A, function&lt;A, T, A&gt;[, function&lt;A, R&gt;]): R</code>
built-in Spark SQL functions)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_aggregate(
  x,
  start,
  merge,
  finish = NULL,
  expr = NULL,
  dest_col = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_aggregate_+3A_x">x</code></td>
<td>
<p>The Spark data frame to run aggregation on</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_start">start</code></td>
<td>
<p>The starting value of the aggregation</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_merge">merge</code></td>
<td>
<p>The aggregation function</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_finish">finish</code></td>
<td>
<p>Optional param specifying a transformation to apply on the final value of the aggregation</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_expr">expr</code></td>
<td>
<p>The array being aggregated, could be any SQL expression evaluating to an array
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the aggregated result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_aggregate_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")
# concatenates all numbers of each array in `array_column` and add parentheses
# around the resulting string
copy_to(sc, dplyr::tibble(array_column = list(1:5, 21:25))) %&gt;%
  hof_aggregate(
    start = "",
    merge = ~ CONCAT(.y, .x),
    finish = ~ CONCAT("(", .x, ")")
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_array_sort'>Sorts array using a custom comparator</h2><span id='topic+hof_array_sort'></span>

<h3>Description</h3>

<p>Applies a custom comparator function to sort an array
(this is essentially a dplyr wrapper to the 'array_sort(expr, func)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_array_sort(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_array_sort_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_array_sort_+3A_func">func</code></td>
<td>
<p>The comparator function to apply (it should take 2 array elements as arguments
and return an integer, with a return value of -1 indicating the first element is less than
the second, 0 indicating equality, or 1 indicating the first element is greater than the
second)</p>
</td></tr>
<tr><td><code id="hof_array_sort_+3A_expr">expr</code></td>
<td>
<p>The array being sorted, could be any SQL expression evaluating to an array
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_array_sort_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the sorted result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_array_sort_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "3.0.0")
copy_to(
  sc,
  dplyr::tibble(
    # x contains 2 arrays each having elements in ascending order
    x = list(1:5, 6:10)
  )
) %&gt;%
  # now each array from x gets sorted in descending order
  hof_array_sort(~ as.integer(sign(.y - .x)))

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_exists'>Determine Whether Some Element Exists in an Array Column</h2><span id='topic+hof_exists'></span>

<h3>Description</h3>

<p>Determines whether an element satisfying the given predicate exists in each array from
an array column
(this is essentially a dplyr wrapper for the
<code>exists(array&lt;T&gt;, function&lt;T, Boolean&gt;): Boolean</code> built-in Spark SQL function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_exists(x, pred, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_exists_+3A_x">x</code></td>
<td>
<p>The Spark data frame to search</p>
</td></tr>
<tr><td><code id="hof_exists_+3A_pred">pred</code></td>
<td>
<p>A boolean predicate</p>
</td></tr>
<tr><td><code id="hof_exists_+3A_expr">expr</code></td>
<td>
<p>The array being searched (could be any SQL expression evaluating to an array)</p>
</td></tr>
<tr><td><code id="hof_exists_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the search result</p>
</td></tr>
<tr><td><code id="hof_exists_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>

<hr>
<h2 id='hof_filter'>Filter Array Column</h2><span id='topic+hof_filter'></span>

<h3>Description</h3>

<p>Apply an element-wise filtering function to an array column
(this is essentially a dplyr wrapper for the
<code>filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt;</code> built-in Spark SQL functions)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_filter(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_filter_+3A_x">x</code></td>
<td>
<p>The Spark data frame to filter</p>
</td></tr>
<tr><td><code id="hof_filter_+3A_func">func</code></td>
<td>
<p>The filtering function</p>
</td></tr>
<tr><td><code id="hof_filter_+3A_expr">expr</code></td>
<td>
<p>The array being filtered, could be any SQL expression evaluating to an array
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_filter_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the filtered result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_filter_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")
# only keep odd elements in each array in `array_column`
copy_to(sc, dplyr::tibble(array_column = list(1:5, 21:25))) %&gt;%
  hof_filter(~ .x %% 2 == 1)

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_forall'>Checks whether all elements in an array satisfy a predicate</h2><span id='topic+hof_forall'></span>

<h3>Description</h3>

<p>Checks whether the predicate specified holds for all elements in an array
(this is essentially a dplyr wrapper to the 'forall(expr, pred)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_forall(x, pred, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_forall_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_forall_+3A_pred">pred</code></td>
<td>
<p>The predicate to test (it should take an array element as argument and
return a boolean value)</p>
</td></tr>
<tr><td><code id="hof_forall_+3A_expr">expr</code></td>
<td>
<p>The array being tested, could be any SQL expression evaluating to an
array (default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_forall_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the boolean result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_forall_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

sc &lt;- spark_connect(master = "local", version = "3.0.0")
df &lt;- dplyr::tibble(
  x = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),
  y = list(c(1, 4, 2, 8, 5), c(7, 1, 4, 2, 8)),
)
sdf &lt;- sdf_copy_to(sc, df, overwrite = TRUE)

all_positive_tbl &lt;- sdf %&gt;%
  hof_forall(pred = ~ .x &gt; 0, expr = y, dest_col = all_positive) %&gt;%
  dplyr::select(all_positive)

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_map_filter'>Filters a map</h2><span id='topic+hof_map_filter'></span>

<h3>Description</h3>

<p>Filters entries in a map using the function specified
(this is essentially a dplyr wrapper to the 'map_filter(expr, func)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_map_filter(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_map_filter_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_map_filter_+3A_func">func</code></td>
<td>
<p>The filter function to apply (it should take (key, value) as arguments
and return a boolean value, with FALSE indicating the key-value pair should be discarded
and TRUE otherwise)</p>
</td></tr>
<tr><td><code id="hof_map_filter_+3A_expr">expr</code></td>
<td>
<p>The map being filtered, could be any SQL expression evaluating to a map
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_map_filter_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the filtered result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_map_filter_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "3.0.0")
sdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map(1, 0, 2, 2, 3, -1))
filtered_sdf &lt;- sdf %&gt;% hof_map_filter(~ .x &gt; .y)

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_map_zip_with'>Merges two maps into one</h2><span id='topic+hof_map_zip_with'></span>

<h3>Description</h3>

<p>Merges two maps into a single map by applying the function specified to pairs of
values with the same key
(this is essentially a dplyr wrapper to the 'map_zip_with(map1, map2, func)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_map_zip_with(x, func, dest_col = NULL, map1 = NULL, map2 = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_map_zip_with_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_map_zip_with_+3A_func">func</code></td>
<td>
<p>The function to apply (it should take (key, value1, value2) as arguments,
where (key, value1) is a key-value pair present in map1, (key, value2) is a key-value
pair present in map2, and return a transformed value associated with key in the
resulting map</p>
</td></tr>
<tr><td><code id="hof_map_zip_with_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the query result
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_map_zip_with_+3A_map1">map1</code></td>
<td>
<p>The first map being merged, could be any SQL expression evaluating to a
map (default: the first column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_map_zip_with_+3A_map2">map2</code></td>
<td>
<p>The second map being merged, could be any SQL expression evaluating to a
map (default: the second column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_map_zip_with_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "3.0.0")

# create a Spark dataframe with 2 columns of type MAP&lt;STRING, INT&gt;
two_maps_tbl &lt;- sdf_copy_to(
  sc,
  dplyr::tibble(
    m1 = c("{\"1\":2,\"3\":4,\"5\":6}", "{\"2\":1,\"4\":3,\"6\":5}"),
    m2 = c("{\"1\":1,\"3\":3,\"5\":5}", "{\"2\":2,\"4\":4,\"6\":6}")
  ),
  overwrite = TRUE
) %&gt;%
  dplyr::mutate(m1 = from_json(m1, "MAP&lt;STRING, INT&gt;"),
                m2 = from_json(m2, "MAP&lt;STRING, INT&gt;"))

# create a 3rd column containing MAP&lt;STRING, INT&gt; values derived from the
# first 2 columns

transformed_two_maps_tbl &lt;- two_maps_tbl %&gt;%
  hof_map_zip_with(
    func = .(k, v1, v2) %-&gt;% (CONCAT(k, "_", v1, "_", v2)),
    dest_col = m3
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_transform'>Transform Array Column</h2><span id='topic+hof_transform'></span>

<h3>Description</h3>

<p>Apply an element-wise transformation function to an array column
(this is essentially a dplyr wrapper for the
<code>transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt;</code> and the
<code>transform(array&lt;T&gt;, function&lt;T, Int, U&gt;): array&lt;U&gt;</code> built-in Spark SQL functions)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_transform(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_transform_+3A_x">x</code></td>
<td>
<p>The Spark data frame to transform</p>
</td></tr>
<tr><td><code id="hof_transform_+3A_func">func</code></td>
<td>
<p>The transformation to apply</p>
</td></tr>
<tr><td><code id="hof_transform_+3A_expr">expr</code></td>
<td>
<p>The array being transformed, could be any SQL expression evaluating to an array
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_transform_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the transformed result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_transform_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")
# applies the (x -&gt; x * x) transformation to elements of all arrays
copy_to(sc, dplyr::tibble(arr = list(1:5, 21:25))) %&gt;%
  hof_transform(~ .x * .x)

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_transform_keys'>Transforms keys of a map</h2><span id='topic+hof_transform_keys'></span>

<h3>Description</h3>

<p>Applies the transformation function specified to all keys of a map
(this is essentially a dplyr wrapper to the 'transform_keys(expr, func)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_transform_keys(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_transform_keys_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_transform_keys_+3A_func">func</code></td>
<td>
<p>The transformation function to apply (it should take (key, value) as
arguments and return a transformed key)</p>
</td></tr>
<tr><td><code id="hof_transform_keys_+3A_expr">expr</code></td>
<td>
<p>The map being transformed, could be any SQL expression evaluating to a map
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_transform_keys_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the transformed result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_transform_keys_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "3.0.0")
sdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map("a", 0L, "b", 2L, "c", -1L))
transformed_sdf &lt;- sdf %&gt;% hof_transform_keys(~ CONCAT(.x, " == ", .y))

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_transform_values'>Transforms values of a map</h2><span id='topic+hof_transform_values'></span>

<h3>Description</h3>

<p>Applies the transformation function specified to all values of a map
(this is essentially a dplyr wrapper to the 'transform_values(expr, func)' higher-
order function, which is supported since Spark 3.0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_transform_values(x, func, expr = NULL, dest_col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_transform_values_+3A_x">x</code></td>
<td>
<p>The Spark data frame to be processed</p>
</td></tr>
<tr><td><code id="hof_transform_values_+3A_func">func</code></td>
<td>
<p>The transformation function to apply (it should take (key, value) as
arguments and return a transformed value)</p>
</td></tr>
<tr><td><code id="hof_transform_values_+3A_expr">expr</code></td>
<td>
<p>The map being transformed, could be any SQL expression evaluating to a map
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_transform_values_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the transformed result (default: expr)</p>
</td></tr>
<tr><td><code id="hof_transform_values_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "3.0.0")
sdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map("a", 0L, "b", 2L, "c", -1L))
transformed_sdf &lt;- sdf %&gt;% hof_transform_values(~ CONCAT(.x, " == ", .y))

## End(Not run)

</code></pre>

<hr>
<h2 id='hof_zip_with'>Combines 2 Array Columns</h2><span id='topic+hof_zip_with'></span>

<h3>Description</h3>

<p>Applies an element-wise function to combine elements from 2 array columns
(this is essentially a dplyr wrapper for the
<code>zip_with(array&lt;T&gt;, array&lt;U&gt;, function&lt;T, U, R&gt;): array&lt;R&gt;</code>
built-in function in Spark SQL)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hof_zip_with(x, func, dest_col = NULL, left = NULL, right = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hof_zip_with_+3A_x">x</code></td>
<td>
<p>The Spark data frame to process</p>
</td></tr>
<tr><td><code id="hof_zip_with_+3A_func">func</code></td>
<td>
<p>Element-wise combining function to be applied</p>
</td></tr>
<tr><td><code id="hof_zip_with_+3A_dest_col">dest_col</code></td>
<td>
<p>Column to store the query result
(default: the last column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_zip_with_+3A_left">left</code></td>
<td>
<p>Any expression evaluating to an array
(default: the first column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_zip_with_+3A_right">right</code></td>
<td>
<p>Any expression evaluating to an array
(default: the second column of the Spark data frame)</p>
</td></tr>
<tr><td><code id="hof_zip_with_+3A_...">...</code></td>
<td>
<p>Additional params to dplyr::mutate</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")
# compute element-wise products of 2 arrays from each row of `left` and `right`
# and store the resuling array in `res`
copy_to(
  sc,
  dplyr::tibble(
    left = list(1:5, 21:25),
    right = list(6:10, 16:20),
    res = c(0, 0)
  )
) %&gt;%
  hof_zip_with(~ .x * .y)

## End(Not run)

</code></pre>

<hr>
<h2 id='inner_join'>Inner join</h2><span id='topic+inner_join'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+mutate-joins">inner_join</a></code> for more details.
</p>

<hr>
<h2 id='invoke'>Invoke a Method on a JVM Object</h2><span id='topic+invoke'></span><span id='topic+invoke_static'></span><span id='topic+invoke_new'></span>

<h3>Description</h3>

<p>Invoke methods on Java object references. These functions provide a
mechanism for invoking various Java object methods directly from <span class="rlang"><b>R</b></span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invoke(jobj, method, ...)

invoke_static(sc, class, method, ...)

invoke_new(sc, class, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="invoke_+3A_jobj">jobj</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object acting as a Java object reference (typically, a <code>spark_jobj</code>).</p>
</td></tr>
<tr><td><code id="invoke_+3A_method">method</code></td>
<td>
<p>The name of the method to be invoked.</p>
</td></tr>
<tr><td><code id="invoke_+3A_...">...</code></td>
<td>
<p>Optional arguments, currently unused.</p>
</td></tr>
<tr><td><code id="invoke_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="invoke_+3A_class">class</code></td>
<td>
<p>The name of the Java class whose methods should be invoked.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use each of these functions in the following scenarios:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>invoke</code> </td><td style="text-align: left;"> Execute a method on a Java object reference (typically, a <code>spark_jobj</code>). </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>invoke_static</code> </td><td style="text-align: left;"> Execute a static method associated with a Java class. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>invoke_new</code> </td><td style="text-align: left;"> Invoke a constructor associated with a Java class. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<hr>
<h2 id='invoke_method'>Generic Call Interface</h2><span id='topic+invoke_method'></span>

<h3>Description</h3>

<p>Generic Call Interface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invoke_method(sc, static, object, method, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="invoke_method_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
<tr><td><code id="invoke_method_+3A_static">static</code></td>
<td>
<p>Is this a static method call (including a constructor). If so
then the <code>object</code> parameter should be the name of a class (otherwise
it should be a spark_jobj instance).</p>
</td></tr>
<tr><td><code id="invoke_method_+3A_object">object</code></td>
<td>
<p>Object instance or name of class (for <code>static</code>)</p>
</td></tr>
<tr><td><code id="invoke_method_+3A_method">method</code></td>
<td>
<p>Name of method</p>
</td></tr>
<tr><td><code id="invoke_method_+3A_...">...</code></td>
<td>
<p>Call parameters</p>
</td></tr>
</table>

<hr>
<h2 id='j_invoke'>Invoke a Java function.</h2><span id='topic+j_invoke'></span><span id='topic+j_invoke_static'></span><span id='topic+j_invoke_new'></span>

<h3>Description</h3>

<p>Invoke a Java function and force return value of the call to be retrieved
as a Java object reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>j_invoke(jobj, method, ...)

j_invoke_static(sc, class, method, ...)

j_invoke_new(sc, class, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="j_invoke_+3A_jobj">jobj</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object acting as a Java object reference (typically, a <code>spark_jobj</code>).</p>
</td></tr>
<tr><td><code id="j_invoke_+3A_method">method</code></td>
<td>
<p>The name of the method to be invoked.</p>
</td></tr>
<tr><td><code id="j_invoke_+3A_...">...</code></td>
<td>
<p>Optional arguments, currently unused.</p>
</td></tr>
<tr><td><code id="j_invoke_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="j_invoke_+3A_class">class</code></td>
<td>
<p>The name of the Java class whose methods should be invoked.</p>
</td></tr>
</table>

<hr>
<h2 id='j_invoke_method'>Generic Call Interface</h2><span id='topic+j_invoke_method'></span>

<h3>Description</h3>

<p>Call a Java method and retrieve the return value through a JVM object
reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>j_invoke_method(sc, static, object, method, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="j_invoke_method_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code></p>
</td></tr>
<tr><td><code id="j_invoke_method_+3A_static">static</code></td>
<td>
<p>Is this a static method call (including a constructor). If so
then the <code>object</code> parameter should be the name of a class (otherwise
it should be a spark_jobj instance).</p>
</td></tr>
<tr><td><code id="j_invoke_method_+3A_object">object</code></td>
<td>
<p>Object instance or name of class (for <code>static</code>)</p>
</td></tr>
<tr><td><code id="j_invoke_method_+3A_method">method</code></td>
<td>
<p>Name of method</p>
</td></tr>
<tr><td><code id="j_invoke_method_+3A_...">...</code></td>
<td>
<p>Call parameters</p>
</td></tr>
</table>

<hr>
<h2 id='jarray'>Instantiate a Java array with a specific element type.</h2><span id='topic+jarray'></span>

<h3>Description</h3>

<p>Given a list of Java object references, instantiate an <code>Array[T]</code>
containing the same list of references, where <code>T</code> is a non-primitive
type that is more specific than <code>java.lang.Object</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jarray(sc, x, element_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jarray_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="jarray_+3A_x">x</code></td>
<td>
<p>A list of Java object references.</p>
</td></tr>
<tr><td><code id="jarray_+3A_element_type">element_type</code></td>
<td>
<p>A valid Java class name representing the generic type
parameter of the Java array to be instantiated. Each element of <code>x</code>
must refer to a Java object that is assignable to <code>element_type</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
sc &lt;- spark_connect(master = "spark://HOST:PORT")

string_arr &lt;- jarray(sc, letters, element_type = "java.lang.String")
# string_arr is now a reference to an array of type String[]


</code></pre>

<hr>
<h2 id='jfloat'>Instantiate a Java float type.</h2><span id='topic+jfloat'></span>

<h3>Description</h3>

<p>Instantiate a <code>java.lang.Float</code> object with the value specified.
NOTE: this method is useful when one has to invoke a Java/Scala method
requiring a float (instead of double) type for at least one of its
parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jfloat(sc, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jfloat_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="jfloat_+3A_x">x</code></td>
<td>
<p>A numeric value in R.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
sc &lt;- spark_connect(master = "spark://HOST:PORT")

jflt &lt;- jfloat(sc, 1.23e-8)
# jflt is now a reference to a java.lang.Float object


</code></pre>

<hr>
<h2 id='jfloat_array'>Instantiate an Array[Float].</h2><span id='topic+jfloat_array'></span>

<h3>Description</h3>

<p>Instantiate an <code>Array[Float]</code> object with the value specified.
NOTE: this method is useful when one has to invoke a Java/Scala method
requiring an <code>Array[Float]</code> as one of its parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jfloat_array(sc, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jfloat_array_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="jfloat_array_+3A_x">x</code></td>
<td>
<p>A numeric vector in R.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
sc &lt;- spark_connect(master = "spark://HOST:PORT")

jflt_arr &lt;- jfloat_array(sc, c(-1.23e-8, 0, -1.23e-8))
# jflt_arr is now a reference an array of java.lang.Float


</code></pre>

<hr>
<h2 id='jobj_class'>Superclasses of object</h2><span id='topic+jobj_class'></span>

<h3>Description</h3>

<p>Extract the classes that a Java object inherits from. This is the jobj equivalent of <code>class()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jobj_class(jobj, simple_name = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jobj_class_+3A_jobj">jobj</code></td>
<td>
<p>A <code>spark_jobj</code></p>
</td></tr>
<tr><td><code id="jobj_class_+3A_simple_name">simple_name</code></td>
<td>
<p>Whether to return simple names, defaults to TRUE</p>
</td></tr>
</table>

<hr>
<h2 id='jobj_set_param'>Parameter Setting for JVM Objects</h2><span id='topic+jobj_set_param'></span>

<h3>Description</h3>

<p>Sets a parameter value for a pipeline stage object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jobj_set_param(jobj, setter, value, min_version = NULL, default = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jobj_set_param_+3A_jobj">jobj</code></td>
<td>
<p>A pipeline stage jobj.</p>
</td></tr>
<tr><td><code id="jobj_set_param_+3A_setter">setter</code></td>
<td>
<p>The name of the setter method as a string.</p>
</td></tr>
<tr><td><code id="jobj_set_param_+3A_value">value</code></td>
<td>
<p>The value to be set.</p>
</td></tr>
<tr><td><code id="jobj_set_param_+3A_min_version">min_version</code></td>
<td>
<p>The minimum required Spark version for this parameter to be valid.</p>
</td></tr>
<tr><td><code id="jobj_set_param_+3A_default">default</code></td>
<td>
<p>The default value of the parameter, to be used together with 'min_version'.
An error is thrown if the user's Spark version is older than 'min_version' and 'value'
differs from 'default'.</p>
</td></tr>
</table>

<hr>
<h2 id='join.tbl_spark'>Join Spark tbls.</h2><span id='topic+join.tbl_spark'></span><span id='topic+inner_join.tbl_spark'></span><span id='topic+left_join.tbl_spark'></span><span id='topic+right_join.tbl_spark'></span><span id='topic+full_join.tbl_spark'></span>

<h3>Description</h3>

<p>These functions are wrappers around their 'dplyr' equivalents that set
Spark SQL-compliant values for the 'suffix' argument by replacing dots ('.')
with underscores ('_'). See [join] for a description of the general purpose
of the functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tbl_spark'
inner_join(
  x,
  y,
  by = NULL,
  copy = FALSE,
  suffix = c("_x", "_y"),
  auto_index = FALSE,
  ...,
  sql_on = NULL
)

## S3 method for class 'tbl_spark'
left_join(
  x,
  y,
  by = NULL,
  copy = FALSE,
  suffix = c("_x", "_y"),
  auto_index = FALSE,
  ...,
  sql_on = NULL
)

## S3 method for class 'tbl_spark'
right_join(
  x,
  y,
  by = NULL,
  copy = FALSE,
  suffix = c("_x", "_y"),
  auto_index = FALSE,
  ...,
  sql_on = NULL
)

## S3 method for class 'tbl_spark'
full_join(
  x,
  y,
  by = NULL,
  copy = FALSE,
  suffix = c("_x", "_y"),
  auto_index = FALSE,
  ...,
  sql_on = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="join.tbl_spark_+3A_x">x</code>, <code id="join.tbl_spark_+3A_y">y</code></td>
<td>
<p>A pair of lazy data frames backed by database queries.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_by">by</code></td>
<td>
<p>A join specification created with <code><a href="dplyr.html#topic+join_by">join_by()</a></code>, or a character
vector of variables to join by.
</p>
<p>If <code>NULL</code>, the default, <code style="white-space: pre;">&#8288;*_join()&#8288;</code> will perform a natural join, using all
variables in common across <code>x</code> and <code>y</code>. A message lists the variables so
that you can check they're correct; suppress the message by supplying <code>by</code>
explicitly.
</p>
<p>To join on different variables between <code>x</code> and <code>y</code>, use a <code><a href="dplyr.html#topic+join_by">join_by()</a></code>
specification. For example, <code>join_by(a == b)</code> will match <code>x$a</code> to <code>y$b</code>.
</p>
<p>To join by multiple variables, use a <code><a href="dplyr.html#topic+join_by">join_by()</a></code> specification with
multiple expressions. For example, <code>join_by(a == b, c == d)</code> will match
<code>x$a</code> to <code>y$b</code> and <code>x$c</code> to <code>y$d</code>. If the column names are the same between
<code>x</code> and <code>y</code>, you can shorten this by listing only the variable names, like
<code>join_by(a, c)</code>.
</p>
<p><code><a href="dplyr.html#topic+join_by">join_by()</a></code> can also be used to perform inequality, rolling, and overlap
joins. See the documentation at <a href="dplyr.html#topic+join_by">?join_by</a> for details on
these types of joins.
</p>
<p>For simple equality joins, you can alternatively specify a character vector
of variable names to join by. For example, <code>by = c("a", "b")</code> joins <code>x$a</code>
to <code>y$a</code> and <code>x$b</code> to <code>y$b</code>. If variable names differ between <code>x</code> and <code>y</code>,
use a named character vector like <code>by = c("x_a" = "y_a", "x_b" = "y_b")</code>.
</p>
<p>To perform a cross-join, generating all combinations of <code>x</code> and <code>y</code>, see
<code><a href="dplyr.html#topic+cross_join">cross_join()</a></code>.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_copy">copy</code></td>
<td>
<p>If <code>x</code> and <code>y</code> are not from the same data source,
and <code>copy</code> is <code>TRUE</code>, then <code>y</code> will be copied into a
temporary table in same database as <code>x</code>. <code style="white-space: pre;">&#8288;*_join()&#8288;</code> will automatically
run <code>ANALYZE</code> on the created table in the hope that this will make
you queries as efficient as possible by giving more data to the query
planner.
</p>
<p>This allows you to join tables across srcs, but it's potentially expensive
operation so you must opt into it.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_suffix">suffix</code></td>
<td>
<p>If there are non-joined duplicate variables in <code>x</code> and
<code>y</code>, these suffixes will be added to the output to disambiguate them.
Should be a character vector of length 2.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_auto_index">auto_index</code></td>
<td>
<p>if <code>copy</code> is <code>TRUE</code>, automatically create
indices for the variables in <code>by</code>. This may speed up the join if
there are matching indexes in <code>x</code>.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_...">...</code></td>
<td>
<p>Other parameters passed onto methods.</p>
</td></tr>
<tr><td><code id="join.tbl_spark_+3A_sql_on">sql_on</code></td>
<td>
<p>A custom join predicate as an SQL expression.
Usually joins use column equality, but you can perform more complex
queries by supply <code>sql_on</code> which should be a SQL expression that
uses <code>LHS</code> and <code>RHS</code> aliases to refer to the left-hand side or
right-hand side of the join respectively.</p>
</td></tr>
</table>

<hr>
<h2 id='left_join'>Left join</h2><span id='topic+left_join'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+mutate-joins">left_join</a></code> for more details.
</p>

<hr>
<h2 id='list_sparklyr_jars'>list all sparklyr-*.jar files that have been built</h2><span id='topic+list_sparklyr_jars'></span>

<h3>Description</h3>

<p>list all sparklyr-*.jar files that have been built
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_sparklyr_jars()
</code></pre>

<hr>
<h2 id='livy_config'>Create a Spark Configuration for Livy</h2><span id='topic+livy_config'></span>

<h3>Description</h3>

<p>Create a Spark Configuration for Livy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>livy_config(
  config = spark_config(),
  username = NULL,
  password = NULL,
  negotiate = FALSE,
  custom_headers = list(`X-Requested-By` = "sparklyr"),
  proxy = NULL,
  curl_opts = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="livy_config_+3A_config">config</code></td>
<td>
<p>Optional base configuration</p>
</td></tr>
<tr><td><code id="livy_config_+3A_username">username</code></td>
<td>
<p>The username to use in the Authorization header</p>
</td></tr>
<tr><td><code id="livy_config_+3A_password">password</code></td>
<td>
<p>The password to use in the Authorization header</p>
</td></tr>
<tr><td><code id="livy_config_+3A_negotiate">negotiate</code></td>
<td>
<p>Whether to use gssnegotiate method or not</p>
</td></tr>
<tr><td><code id="livy_config_+3A_custom_headers">custom_headers</code></td>
<td>
<p>List of custom headers to append to http requests. Defaults to <code>list("X-Requested-By" = "sparklyr")</code>.</p>
</td></tr>
<tr><td><code id="livy_config_+3A_proxy">proxy</code></td>
<td>
<p>Either NULL or a proxy specified by httr::use_proxy(). Defaults to NULL.</p>
</td></tr>
<tr><td><code id="livy_config_+3A_curl_opts">curl_opts</code></td>
<td>
<p>List of CURL options (e.g., verbose, connecttimeout, dns_cache_timeout, etc, see httr::httr_options() for a
list of valid options) &ndash; NOTE: these configurations are for libcurl only and separate from HTTP headers or Livy session
parameters.</p>
</td></tr>
<tr><td><code id="livy_config_+3A_...">...</code></td>
<td>
<p>additional Livy session parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extends a Spark <code>spark_config()</code> configuration with settings
for Livy. For instance, <code>username</code> and <code>password</code>
define the basic authentication settings for a Livy session.
</p>
<p>The default value of <code>"custom_headers"</code> is set to <code>list("X-Requested-By" = "sparklyr")</code>
in order to facilitate connection to Livy servers with CSRF protection enabled.
</p>
<p>Additional parameters for Livy sessions are:
</p>

<dl>
<dt><code>proxy_user</code></dt><dd><p>User to impersonate when starting the session</p>
</dd>
<dt><code>jars</code></dt><dd><p>jars to be used in this session</p>
</dd>
<dt><code>py_files</code></dt><dd><p>Python files to be used in this session</p>
</dd>
<dt><code>files</code></dt><dd><p>files to be used in this session</p>
</dd>
<dt><code>driver_memory</code></dt><dd><p>Amount of memory to use for the driver process</p>
</dd>
<dt><code>driver_cores</code></dt><dd><p>Number of cores to use for the driver process</p>
</dd>
<dt><code>executor_memory</code></dt><dd><p>Amount of memory to use per executor process</p>
</dd>
<dt><code>executor_cores</code></dt><dd><p>Number of cores to use for each executor</p>
</dd>
<dt><code>num_executors</code></dt><dd><p>Number of executors to launch for this session</p>
</dd>
<dt><code>archives</code></dt><dd><p>Archives to be used in this session</p>
</dd>
<dt><code>queue</code></dt><dd><p>The name of the YARN queue to which submitted</p>
</dd>
<dt><code>name</code></dt><dd><p>The name of this session</p>
</dd>
<dt><code>heartbeat_timeout</code></dt><dd><p>Timeout in seconds to which session be orphaned</p>
</dd>
<dt><code>conf</code></dt><dd><p>Spark configuration properties (Map of key=value)</p>
</dd>
</dl>

<p>Note that <code>queue</code> is supported only by version 0.4.0 of Livy or newer.
If you are using the older one, specify queue via <code>config</code> (e.g.
<code>config = spark_config(spark.yarn.queue = "my_queue")</code>).
</p>


<h3>Value</h3>

<p>Named list with configuration data
</p>

<hr>
<h2 id='livy_install'>Install Livy</h2><span id='topic+livy_install'></span><span id='topic+livy_available_versions'></span><span id='topic+livy_install_dir'></span><span id='topic+livy_installed_versions'></span><span id='topic+livy_home_dir'></span>

<h3>Description</h3>

<p>Automatically download and install <a href="https://livy.apache.org/">&lsquo;<span class="samp">&#8288;livy&#8288;</span>&rsquo;</a>.
&lsquo;<span class="samp">&#8288;livy&#8288;</span>&rsquo; provides a REST API to Spark.
</p>
<p>Find the LIVY_HOME directory for a given version of Livy that
was previously installed using <code><a href="#topic+livy_install">livy_install</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>livy_install(version = "0.6.0", spark_home = NULL, spark_version = NULL)

livy_available_versions()

livy_install_dir()

livy_installed_versions()

livy_home_dir(version = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="livy_install_+3A_version">version</code></td>
<td>
<p>Version of Livy</p>
</td></tr>
<tr><td><code id="livy_install_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to a Spark installation. The downloaded and
installed version of &lsquo;<span class="samp">&#8288;livy&#8288;</span>&rsquo; will then be associated with this Spark
installation. When unset (&lsquo;<span class="samp">&#8288;NULL&#8288;</span>&rsquo;), the value is inferred based on
the value of &lsquo;<span class="samp">&#8288;spark_version&#8288;</span>&rsquo; supplied.</p>
</td></tr>
<tr><td><code id="livy_install_+3A_spark_version">spark_version</code></td>
<td>
<p>The version of Spark to use. When unset (&lsquo;<span class="samp">&#8288;NULL&#8288;</span>&rsquo;),
the value is inferred based on the value of &lsquo;<span class="samp">&#8288;livy_version&#8288;</span>&rsquo; supplied.
A version of Spark known to be compatible with the requested version of
&lsquo;<span class="samp">&#8288;livy&#8288;</span>&rsquo; is chosen when possible.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Path to LIVY_HOME (or <code>NULL</code> if the specified version
was not found).
</p>

<hr>
<h2 id='livy_service_start'>Start Livy</h2><span id='topic+livy_service_start'></span><span id='topic+livy_service_stop'></span>

<h3>Description</h3>

<p>Starts the livy service.
</p>
<p>Stops the running instances of the livy service.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>livy_service_start(
  version = NULL,
  spark_version = NULL,
  stdout = "",
  stderr = "",
  ...
)

livy_service_stop()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="livy_service_start_+3A_version">version</code></td>
<td>
<p>The version of &lsquo;<span class="samp">&#8288;livy&#8288;</span>&rsquo; to use.</p>
</td></tr>
<tr><td><code id="livy_service_start_+3A_spark_version">spark_version</code></td>
<td>
<p>The version of &lsquo;<span class="samp">&#8288;spark&#8288;</span>&rsquo; to connect to.</p>
</td></tr>
<tr><td><code id="livy_service_start_+3A_stdout">stdout</code>, <code id="livy_service_start_+3A_stderr">stderr</code></td>
<td>
<p>where output to 'stdout' or 'stderr' should
be sent. Same options as <code>system2</code>.</p>
</td></tr>
<tr><td><code id="livy_service_start_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_add_stage'>Add a Stage to a Pipeline</h2><span id='topic+ml_add_stage'></span>

<h3>Description</h3>

<p>Adds a stage to a pipeline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_add_stage(x, stage)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_add_stage_+3A_x">x</code></td>
<td>
<p>A pipeline or a pipeline stage.</p>
</td></tr>
<tr><td><code id="ml_add_stage_+3A_stage">stage</code></td>
<td>
<p>A pipeline stage.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_aft_survival_regression'>Spark ML &ndash; Survival Regression</h2><span id='topic+ml_aft_survival_regression'></span><span id='topic+ml_survival_regression'></span>

<h3>Description</h3>

<p>Fit a parametric survival regression model named accelerated failure time (AFT) model (see <a href="https://en.wikipedia.org/wiki/Accelerated_failure_time_model">Accelerated failure time model (Wikipedia)</a>) based on the Weibull distribution of the survival time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_aft_survival_regression(
  x,
  formula = NULL,
  censor_col = "censor",
  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),
  fit_intercept = TRUE,
  max_iter = 100L,
  tol = 1e-06,
  aggregation_depth = 2,
  quantiles_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("aft_survival_regression_"),
  ...
)

ml_survival_regression(
  x,
  formula = NULL,
  censor_col = "censor",
  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),
  fit_intercept = TRUE,
  max_iter = 100L,
  tol = 1e-06,
  aggregation_depth = 2,
  quantiles_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("aft_survival_regression_"),
  response = NULL,
  features = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_aft_survival_regression_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_censor_col">censor_col</code></td>
<td>
<p>Censor column name. The value of this column could be 0 or
1. If the value is 1, it means the event has occurred i.e. uncensored;
otherwise censored.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_quantile_probabilities">quantile_probabilities</code></td>
<td>
<p>Quantile probabilities array. Values of the
quantile probabilities array should be in the range (0, 1) and the array
should be non-empty.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>Boolean; should the model be fit with an intercept term?</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_aggregation_depth">aggregation_depth</code></td>
<td>
<p>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_quantiles_col">quantiles_col</code></td>
<td>
<p>Quantiles column name. This column will output quantiles
of corresponding quantileProbabilities if it is set.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_response">response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td></tr>
<tr><td><code id="ml_aft_survival_regression_+3A_features">features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ml_survival_regression()</code> is an alias for <code>ml_aft_survival_regression()</code> for backwards compatibility.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(survival)
library(sparklyr)

sc &lt;- spark_connect(master = "local")
ovarian_tbl &lt;- sdf_copy_to(sc, ovarian, name = "ovarian_tbl", overwrite = TRUE)

partitions &lt;- ovarian_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

ovarian_training &lt;- partitions$training
ovarian_test &lt;- partitions$test

sur_reg &lt;- ovarian_training %&gt;%
  ml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = "fustat")

pred &lt;- ml_predict(sur_reg, ovarian_test)
pred

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_als'>Spark ML &ndash; ALS</h2><span id='topic+ml_als'></span><span id='topic+ml_recommend'></span>

<h3>Description</h3>

<p>Perform recommendation using Alternating Least Squares (ALS) matrix factorization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_als(
  x,
  formula = NULL,
  rating_col = "rating",
  user_col = "user",
  item_col = "item",
  rank = 10,
  reg_param = 0.1,
  implicit_prefs = FALSE,
  alpha = 1,
  nonnegative = FALSE,
  max_iter = 10,
  num_user_blocks = 10,
  num_item_blocks = 10,
  checkpoint_interval = 10,
  cold_start_strategy = "nan",
  intermediate_storage_level = "MEMORY_AND_DISK",
  final_storage_level = "MEMORY_AND_DISK",
  uid = random_string("als_"),
  ...
)

ml_recommend(model, type = c("items", "users"), n = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_als_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula.
This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.
The ALS model requires a specific formula format, please use <code>rating_col ~ user_col + item_col</code>.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_rating_col">rating_col</code></td>
<td>
<p>Column name for ratings. Default: &quot;rating&quot;</p>
</td></tr>
<tr><td><code id="ml_als_+3A_user_col">user_col</code></td>
<td>
<p>Column name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: &quot;user&quot;</p>
</td></tr>
<tr><td><code id="ml_als_+3A_item_col">item_col</code></td>
<td>
<p>Column name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: &quot;item&quot;</p>
</td></tr>
<tr><td><code id="ml_als_+3A_rank">rank</code></td>
<td>
<p>Rank of the matrix factorization (positive). Default: 10</p>
</td></tr>
<tr><td><code id="ml_als_+3A_reg_param">reg_param</code></td>
<td>
<p>Regularization parameter.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_implicit_prefs">implicit_prefs</code></td>
<td>
<p>Whether to use implicit preference. Default: FALSE.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_alpha">alpha</code></td>
<td>
<p>Alpha parameter in the implicit preference formulation (nonnegative).</p>
</td></tr>
<tr><td><code id="ml_als_+3A_nonnegative">nonnegative</code></td>
<td>
<p>Whether to apply nonnegativity constraints. Default: FALSE.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_num_user_blocks">num_user_blocks</code></td>
<td>
<p>Number of user blocks (positive). Default: 10</p>
</td></tr>
<tr><td><code id="ml_als_+3A_num_item_blocks">num_item_blocks</code></td>
<td>
<p>Number of item blocks (positive). Default: 10</p>
</td></tr>
<tr><td><code id="ml_als_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_cold_start_strategy">cold_start_strategy</code></td>
<td>
<p>(Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: - &quot;nan&quot;: predicted value for unknown ids will be NaN. - &quot;drop&quot;: rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. Default: &quot;nan&quot;.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_intermediate_storage_level">intermediate_storage_level</code></td>
<td>
<p>(Spark 2.0.0+) StorageLevel for intermediate datasets. Pass in a string representation of <code>StorageLevel</code>. Cannot be &quot;NONE&quot;. Default: &quot;MEMORY_AND_DISK&quot;.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_final_storage_level">final_storage_level</code></td>
<td>
<p>(Spark 2.0.0+) StorageLevel for ALS model factors. Pass in a string representation of <code>StorageLevel</code>. Default: &quot;MEMORY_AND_DISK&quot;.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml_als_+3A_model">model</code></td>
<td>
<p>An ALS model object</p>
</td></tr>
<tr><td><code id="ml_als_+3A_type">type</code></td>
<td>
<p>What to recommend, one of <code>items</code> or <code>users</code></p>
</td></tr>
<tr><td><code id="ml_als_+3A_n">n</code></td>
<td>
<p>Maximum number of recommendations to return</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ml_recommend()</code> returns the top <code>n</code> users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly.
</p>


<h3>Value</h3>

<p>ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called 'factor' matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.
</p>
<p>This is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as &quot;users&quot; and &quot;products&quot;) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user's feature vector. This is achieved by pre-computing some information about the ratings matrix to determine the &quot;out-links&quot; of each user (which blocks of products it will contribute to) and &quot;in-link&quot; information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users' ratings and update the products based on these messages.
</p>
<p>For implicit preference data, the algorithm used is based on &quot;Collaborative Filtering for Implicit Feedback Datasets&quot;, available at <a href="https://doi.org/10.1109/ICDM.2008.22">doi:10.1109/ICDM.2008.22</a>, adapted for the blocked approach used here.
</p>
<p>Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. The ratings then act as 'confidence' values related to strength of indicated user preferences rather than explicit ratings given to items.
</p>
<p>The object returned depends on the class of <code>x</code>.
</p>

<ul>
<li> <p><code>spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_als</code> recommender object, which is an Estimator.
</p>
</li>
<li> <p><code>ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
the recommender appended to the pipeline.
</p>
</li>
<li> <p><code>tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a recommender
estimator is constructed then immediately fit with the input
<code>tbl_spark</code>, returning a recommendation model, i.e. <code>ml_als_model</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")

movies &lt;- data.frame(
  user   = c(1, 2, 0, 1, 2, 0),
  item   = c(1, 1, 1, 2, 2, 0),
  rating = c(3, 1, 2, 4, 5, 4)
)
movies_tbl &lt;- sdf_copy_to(sc, movies)

model &lt;- ml_als(movies_tbl, rating ~ user + item)

ml_predict(model, movies_tbl)

ml_recommend(model, type = "item", 1)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_als_tidiers'>Tidying methods for Spark ML ALS</h2><span id='topic+ml_als_tidiers'></span><span id='topic+tidy.ml_model_als'></span><span id='topic+augment.ml_model_als'></span><span id='topic+glance.ml_model_als'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_als'
tidy(x, ...)

## S3 method for class 'ml_model_als'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_als'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_als_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_als_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_als_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_bisecting_kmeans'>Spark ML &ndash; Bisecting K-Means Clustering</h2><span id='topic+ml_bisecting_kmeans'></span>

<h3>Description</h3>

<p>A bisecting k-means algorithm based on the paper &quot;A comparison of document clustering techniques&quot; by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_bisecting_kmeans(
  x,
  formula = NULL,
  k = 4,
  max_iter = 20,
  seed = NULL,
  min_divisible_cluster_size = 1,
  features_col = "features",
  prediction_col = "prediction",
  uid = random_string("bisecting_bisecting_kmeans_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_bisecting_kmeans_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_k">k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_min_divisible_cluster_size">min_divisible_cluster_size</code></td>
<td>
<p>The minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster (default: 1.0).</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_bisecting_kmeans_+3A_...">...</code></td>
<td>
<p>Optional arguments, see Details.
#' @return The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  select(-Species) %&gt;%
  ml_bisecting_kmeans(k = 4, Species ~ .)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_call_constructor'>Wrap a Spark ML JVM object</h2><span id='topic+ml_call_constructor'></span>

<h3>Description</h3>

<p>Identifies the associated sparklyr ML constructor for the JVM object by inspecting its
class and performing a lookup. The lookup table is specified by the
'sparkml/class_mapping.json' files of sparklyr and the loaded extensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_call_constructor(jobj)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_call_constructor_+3A_jobj">jobj</code></td>
<td>
<p>The jobj for the pipeline stage.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_chisquare_test'>Chi-square hypothesis testing for categorical data.</h2><span id='topic+ml_chisquare_test'></span>

<h3>Description</h3>

<p>Conduct Pearson's independence test for every feature against the
label. For each feature, the (feature, label) pairs are converted
into a contingency matrix for which the Chi-squared statistic is
computed. All label and feature values must be categorical.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_chisquare_test(x, features, label)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_chisquare_test_+3A_x">x</code></td>
<td>
<p>A <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_chisquare_test_+3A_features">features</code></td>
<td>
<p>The name(s) of the feature columns. This can also be the name
of a single vector column created using <code>ft_vector_assembler()</code>.</p>
</td></tr>
<tr><td><code id="ml_chisquare_test_+3A_label">label</code></td>
<td>
<p>The name of the label column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with one row for each (feature, label) pair with p-values,
degrees of freedom, and test statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- c("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_chisquare_test(iris_tbl, features = features, label = "Species")

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_clustering_evaluator'>Spark ML - Clustering Evaluator</h2><span id='topic+ml_clustering_evaluator'></span>

<h3>Description</h3>

<p>Evaluator for clustering results. The metric computes the Silhouette measure using the squared
Euclidean distance. The Silhouette is a measure for the validation of the consistency
within clusters. It ranges between 1 and -1, where a value close to 1 means that the
points in a cluster are close to the other points in the same cluster and far from the
points of the other clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_clustering_evaluator(
  x,
  features_col = "features",
  prediction_col = "prediction",
  metric_name = "silhouette",
  uid = random_string("clustering_evaluator_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_clustering_evaluator_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code> object or a <code>tbl_spark</code> containing label and prediction columns. The latter should be the output of <code><a href="#topic+sdf_predict">sdf_predict</a></code>.</p>
</td></tr>
<tr><td><code id="ml_clustering_evaluator_+3A_features_col">features_col</code></td>
<td>
<p>Name of features column.</p>
</td></tr>
<tr><td><code id="ml_clustering_evaluator_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Name of the prediction column.</p>
</td></tr>
<tr><td><code id="ml_clustering_evaluator_+3A_metric_name">metric_name</code></td>
<td>
<p>The performance metric. Currently supports &quot;silhouette&quot;.</p>
</td></tr>
<tr><td><code id="ml_clustering_evaluator_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_clustering_evaluator_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The calculated performance metric
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

formula &lt;- Species ~ .

# Train the models
kmeans_model &lt;- ml_kmeans(iris_training, formula = formula)
b_kmeans_model &lt;- ml_bisecting_kmeans(iris_training, formula = formula)
gmm_model &lt;- ml_gaussian_mixture(iris_training, formula = formula)

# Predict
pred_kmeans &lt;- ml_predict(kmeans_model, iris_test)
pred_b_kmeans &lt;- ml_predict(b_kmeans_model, iris_test)
pred_gmm &lt;- ml_predict(gmm_model, iris_test)

# Evaluate
ml_clustering_evaluator(pred_kmeans)
ml_clustering_evaluator(pred_b_kmeans)
ml_clustering_evaluator(pred_gmm)

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_corr'>Compute correlation matrix</h2><span id='topic+ml_corr'></span>

<h3>Description</h3>

<p>Compute correlation matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_corr(x, columns = NULL, method = c("pearson", "spearman"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_corr_+3A_x">x</code></td>
<td>
<p>A <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_corr_+3A_columns">columns</code></td>
<td>
<p>The names of the columns to calculate correlations of. If only one
column is specified, it must be a vector column (for example, assembled using
<code>ft_vector_assember()</code>).</p>
</td></tr>
<tr><td><code id="ml_corr_+3A_method">method</code></td>
<td>
<p>The method to use, either <code>"pearson"</code> or <code>"spearman"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A correlation matrix organized as a data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- c("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_corr(iris_tbl, columns = features, method = "pearson")

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_decision_tree_classifier'>Spark ML &ndash; Decision Trees</h2><span id='topic+ml_decision_tree_classifier'></span><span id='topic+ml_decision_tree'></span><span id='topic+ml_decision_tree_regressor'></span>

<h3>Description</h3>

<p>Perform classification and regression using decision trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_decision_tree_classifier(
  x,
  formula = NULL,
  max_depth = 5,
  max_bins = 32,
  min_instances_per_node = 1,
  min_info_gain = 0,
  impurity = "gini",
  seed = NULL,
  thresholds = NULL,
  cache_node_ids = FALSE,
  checkpoint_interval = 10,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("decision_tree_classifier_"),
  ...
)

ml_decision_tree(
  x,
  formula = NULL,
  type = c("auto", "regression", "classification"),
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  variance_col = NULL,
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  checkpoint_interval = 10L,
  impurity = "auto",
  max_bins = 32L,
  max_depth = 5L,
  min_info_gain = 0,
  min_instances_per_node = 1L,
  seed = NULL,
  thresholds = NULL,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256L,
  uid = random_string("decision_tree_"),
  response = NULL,
  features = NULL,
  ...
)

ml_decision_tree_regressor(
  x,
  formula = NULL,
  max_depth = 5,
  max_bins = 32,
  min_instances_per_node = 1,
  min_info_gain = 0,
  impurity = "variance",
  seed = NULL,
  cache_node_ids = FALSE,
  checkpoint_interval = 10,
  max_memory_in_mb = 256,
  variance_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("decision_tree_regressor_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_decision_tree_classifier_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_max_bins">max_bins</code></td>
<td>
<p>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. More bins give higher granularity.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_min_instances_per_node">min_instances_per_node</code></td>
<td>
<p>Minimum number of instances each child must
have after split.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_min_info_gain">min_info_gain</code></td>
<td>
<p>Minimum information gain for a split to be considered
at a tree node. Should be &gt;= 0, defaults to 0.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_impurity">impurity</code></td>
<td>
<p>Criterion used for information gain calculation. Supported: &quot;entropy&quot;
and &quot;gini&quot; (default) for classification and &quot;variance&quot; (default) for regression. For
<code>ml_decision_tree</code>, setting <code>"auto"</code> will default to the appropriate
criterion based on model type.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_seed">seed</code></td>
<td>
<p>Seed for random numbers.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_cache_node_ids">cache_node_ids</code></td>
<td>
<p>If <code>FALSE</code>, the algorithm will pass trees to
executors to match instances with nodes. If <code>TRUE</code>, the algorithm will
cache node IDs for each instance. Caching can speed up training of deeper
trees. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10
iterations, defaults to 10.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_max_memory_in_mb">max_memory_in_mb</code></td>
<td>
<p>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration, and its aggregates
may exceed this size. Defaults to 256.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_type">type</code></td>
<td>
<p>The type of model to fit. <code>"regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. When <code>"auto"</code> is used, the model type is
inferred based on the response variable type &ndash; if it is a numeric type,
then regression is used; classification otherwise.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_variance_col">variance_col</code></td>
<td>
<p>(Optional) Column name for the biased sample variance of prediction.</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_response">response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td></tr>
<tr><td><code id="ml_decision_tree_classifier_+3A_features">features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ml_decision_tree</code> is a wrapper around <code>ml_decision_tree_regressor.tbl_spark</code> and <code>ml_decision_tree_classifier.tbl_spark</code> and calls the appropriate method based on model type.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

dt_model &lt;- iris_training %&gt;%
  ml_decision_tree(Species ~ .)

pred &lt;- ml_predict(dt_model, iris_test)

ml_multiclass_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_default_stop_words'>Default stop words</h2><span id='topic+ml_default_stop_words'></span>

<h3>Description</h3>

<p>Loads the default stop words for the given language.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_default_stop_words(
  sc,
  language = c("english", "danish", "dutch", "finnish", "french", "german", "hungarian",
    "italian", "norwegian", "portuguese", "russian", "spanish", "swedish", "turkish"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_default_stop_words_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code></p>
</td></tr>
<tr><td><code id="ml_default_stop_words_+3A_language">language</code></td>
<td>
<p>A character string.</p>
</td></tr>
<tr><td><code id="ml_default_stop_words_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supported languages: danish, dutch, english, finnish, french,
german, hungarian, italian, norwegian, portuguese, russian, spanish,
swedish, turkish. Defaults to English. See <a href="https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/">https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/</a>
for more details
</p>


<h3>Value</h3>

<p>A list of stop words.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ft_stop_words_remover">ft_stop_words_remover</a></code>
</p>

<hr>
<h2 id='ml_evaluate'>Evaluate the Model on a Validation Set</h2><span id='topic+ml_evaluate'></span><span id='topic+ml_evaluate.ml_model_logistic_regression'></span><span id='topic+ml_evaluate.ml_logistic_regression_model'></span><span id='topic+ml_evaluate.ml_model_linear_regression'></span><span id='topic+ml_evaluate.ml_linear_regression_model'></span><span id='topic+ml_evaluate.ml_model_generalized_linear_regression'></span><span id='topic+ml_evaluate.ml_generalized_linear_regression_model'></span><span id='topic+ml_evaluate.ml_model_clustering'></span><span id='topic+ml_evaluate.ml_model_classification'></span><span id='topic+ml_evaluate.ml_evaluator'></span>

<h3>Description</h3>

<p>Compute performance metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_evaluate(x, dataset)

## S3 method for class 'ml_model_logistic_regression'
ml_evaluate(x, dataset)

## S3 method for class 'ml_logistic_regression_model'
ml_evaluate(x, dataset)

## S3 method for class 'ml_model_linear_regression'
ml_evaluate(x, dataset)

## S3 method for class 'ml_linear_regression_model'
ml_evaluate(x, dataset)

## S3 method for class 'ml_model_generalized_linear_regression'
ml_evaluate(x, dataset)

## S3 method for class 'ml_generalized_linear_regression_model'
ml_evaluate(x, dataset)

## S3 method for class 'ml_model_clustering'
ml_evaluate(x, dataset)

## S3 method for class 'ml_model_classification'
ml_evaluate(x, dataset)

## S3 method for class 'ml_evaluator'
ml_evaluate(x, dataset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_evaluate_+3A_x">x</code></td>
<td>
<p>An ML model object or an evaluator object.</p>
</td></tr>
<tr><td><code id="ml_evaluate_+3A_dataset">dataset</code></td>
<td>
<p>The dataset to be validate the model on.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

ml_gaussian_mixture(iris_tbl, Species ~ .) %&gt;%
  ml_evaluate(iris_tbl)

ml_kmeans(iris_tbl, Species ~ .) %&gt;%
  ml_evaluate(iris_tbl)

ml_bisecting_kmeans(iris_tbl, Species ~ .) %&gt;%
  ml_evaluate(iris_tbl)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_evaluator'>Spark ML - Evaluators</h2><span id='topic+ml_evaluator'></span><span id='topic+ml_binary_classification_evaluator'></span><span id='topic+ml_binary_classification_eval'></span><span id='topic+ml_multiclass_classification_evaluator'></span><span id='topic+ml_classification_eval'></span><span id='topic+ml_regression_evaluator'></span>

<h3>Description</h3>

<p>A set of functions to calculate performance metrics for prediction models. Also see the Spark ML Documentation <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_binary_classification_evaluator(
  x,
  label_col = "label",
  raw_prediction_col = "rawPrediction",
  metric_name = "areaUnderROC",
  uid = random_string("binary_classification_evaluator_"),
  ...
)

ml_binary_classification_eval(
  x,
  label_col = "label",
  prediction_col = "prediction",
  metric_name = "areaUnderROC"
)

ml_multiclass_classification_evaluator(
  x,
  label_col = "label",
  prediction_col = "prediction",
  metric_name = "f1",
  uid = random_string("multiclass_classification_evaluator_"),
  ...
)

ml_classification_eval(
  x,
  label_col = "label",
  prediction_col = "prediction",
  metric_name = "f1"
)

ml_regression_evaluator(
  x,
  label_col = "label",
  prediction_col = "prediction",
  metric_name = "rmse",
  uid = random_string("regression_evaluator_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_evaluator_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code> object or a <code>tbl_spark</code> containing label and prediction columns. The latter should be the output of <code><a href="#topic+sdf_predict">sdf_predict</a></code>.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_label_col">label_col</code></td>
<td>
<p>Name of column string specifying which column contains the true labels or values.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_metric_name">metric_name</code></td>
<td>
<p>The performance metric. See details.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml_evaluator_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Name of the column that contains the predicted
label or value NOT the scored probability. Column should be of type
<code>Double</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following metrics are supported
</p>

<ul>
<li><p> Binary Classification: <code>areaUnderROC</code> (default) or <code>areaUnderPR</code> (not available in Spark 2.X.)
</p>
</li>
<li><p> Multiclass Classification: <code>f1</code> (default), <code>precision</code>, <code>recall</code>, <code>weightedPrecision</code>, <code>weightedRecall</code> or <code>accuracy</code>; for Spark 2.X: <code>f1</code> (default), <code>weightedPrecision</code>, <code>weightedRecall</code> or <code>accuracy</code>.
</p>
</li>
<li><p> Regression: <code>rmse</code> (root mean squared error, default),
<code>mse</code> (mean squared error), <code>r2</code>, or <code>mae</code> (mean absolute error.)
</p>
</li></ul>

<p><code>ml_binary_classification_eval()</code> is an alias for <code>ml_binary_classification_evaluator()</code> for backwards compatibility.
</p>
<p><code>ml_classification_eval()</code> is an alias for <code>ml_multiclass_classification_evaluator()</code> for backwards compatibility.
</p>


<h3>Value</h3>

<p>The calculated performance metric
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
mtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

# for multiclass classification
rf_model &lt;- mtcars_training %&gt;%
  ml_random_forest(cyl ~ ., type = "classification")

pred &lt;- ml_predict(rf_model, mtcars_test)

ml_multiclass_classification_evaluator(pred)

# for regression
rf_model &lt;- mtcars_training %&gt;%
  ml_random_forest(cyl ~ ., type = "regression")

pred &lt;- ml_predict(rf_model, mtcars_test)

ml_regression_evaluator(pred, label_col = "cyl")

# for binary classification
rf_model &lt;- mtcars_training %&gt;%
  ml_random_forest(am ~ gear + carb, type = "classification")

pred &lt;- ml_predict(rf_model, mtcars_test)

ml_binary_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_feature_importances'>Spark ML - Feature Importance for Tree Models</h2><span id='topic+ml_feature_importances'></span><span id='topic+ml_tree_feature_importance'></span>

<h3>Description</h3>

<p>Spark ML - Feature Importance for Tree Models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_feature_importances(model, ...)

ml_tree_feature_importance(model, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_feature_importances_+3A_model">model</code></td>
<td>
<p>A decision tree-based model.</p>
</td></tr>
<tr><td><code id="ml_feature_importances_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>ml_model</code>, a sorted data frame with feature labels and their relative importance.
For <code>ml_prediction_model</code>, a vector of relative importances.
</p>

<hr>
<h2 id='ml_fpgrowth'>Frequent Pattern Mining &ndash; FPGrowth</h2><span id='topic+ml_fpgrowth'></span><span id='topic+ml_association_rules'></span><span id='topic+ml_freq_itemsets'></span>

<h3>Description</h3>

<p>A parallel FP-growth algorithm to mine frequent itemsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_fpgrowth(
  x,
  items_col = "items",
  min_confidence = 0.8,
  min_support = 0.3,
  prediction_col = "prediction",
  uid = random_string("fpgrowth_"),
  ...
)

ml_association_rules(model)

ml_freq_itemsets(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_fpgrowth_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_items_col">items_col</code></td>
<td>
<p>Items column name. Default: &quot;items&quot;</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_min_confidence">min_confidence</code></td>
<td>
<p>Minimal confidence for generating Association Rule.
<code>min_confidence</code> will not affect the mining for frequent itemsets, but
will affect the association rules generation. Default: 0.8</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_min_support">min_support</code></td>
<td>
<p>Minimal support level of the frequent pattern. [0.0, 1.0].
Any pattern that appears more than (min_support * size-of-the-dataset) times
will be output in the frequent itemsets. Default: 0.3</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml_fpgrowth_+3A_model">model</code></td>
<td>
<p>A fitted FPGrowth model returned by <code>ml_fpgrowth()</code></p>
</td></tr>
</table>

<hr>
<h2 id='ml_gaussian_mixture'>Spark ML &ndash; Gaussian Mixture clustering.</h2><span id='topic+ml_gaussian_mixture'></span>

<h3>Description</h3>

<p>This class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated &quot;mixing&quot; weights specifying each's contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than <code>tol</code>, or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_gaussian_mixture(
  x,
  formula = NULL,
  k = 2,
  max_iter = 100,
  tol = 0.01,
  seed = NULL,
  features_col = "features",
  prediction_col = "prediction",
  probability_col = "probability",
  uid = random_string("gaussian_mixture_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_gaussian_mixture_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_k">k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_gaussian_mixture_+3A_...">...</code></td>
<td>
<p>Optional arguments, see Details.
#' @return The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

gmm_model &lt;- ml_gaussian_mixture(iris_tbl, Species ~ .)
pred &lt;- sdf_predict(iris_tbl, gmm_model)
ml_clustering_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_gbt_classifier'>Spark ML &ndash; Gradient Boosted Trees</h2><span id='topic+ml_gbt_classifier'></span><span id='topic+ml_gradient_boosted_trees'></span><span id='topic+ml_gbt_regressor'></span>

<h3>Description</h3>

<p>Perform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_gbt_classifier(
  x,
  formula = NULL,
  max_iter = 20,
  max_depth = 5,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  min_instances_per_node = 1L,
  max_bins = 32,
  min_info_gain = 0,
  loss_type = "logistic",
  seed = NULL,
  thresholds = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("gbt_classifier_"),
  ...
)

ml_gradient_boosted_trees(
  x,
  formula = NULL,
  type = c("auto", "regression", "classification"),
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  checkpoint_interval = 10,
  loss_type = c("auto", "logistic", "squared", "absolute"),
  max_bins = 32,
  max_depth = 5,
  max_iter = 20L,
  min_info_gain = 0,
  min_instances_per_node = 1,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  seed = NULL,
  thresholds = NULL,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  uid = random_string("gradient_boosted_trees_"),
  response = NULL,
  features = NULL,
  ...
)

ml_gbt_regressor(
  x,
  formula = NULL,
  max_iter = 20,
  max_depth = 5,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  min_instances_per_node = 1,
  max_bins = 32,
  min_info_gain = 0,
  loss_type = "squared",
  seed = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("gbt_regressor_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_gbt_classifier_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_max_iter">max_iter</code></td>
<td>
<p>Maxmimum number of iterations.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_step_size">step_size</code></td>
<td>
<p>Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1)</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_subsampling_rate">subsampling_rate</code></td>
<td>
<p>Fraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_feature_subset_strategy">feature_subset_strategy</code></td>
<td>
<p>The number of features to consider for splits at each tree node. See details for options.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_min_instances_per_node">min_instances_per_node</code></td>
<td>
<p>Minimum number of instances each child must
have after split.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_max_bins">max_bins</code></td>
<td>
<p>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. More bins give higher granularity.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_min_info_gain">min_info_gain</code></td>
<td>
<p>Minimum information gain for a split to be considered
at a tree node. Should be &gt;= 0, defaults to 0.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_loss_type">loss_type</code></td>
<td>
<p>Loss function which GBT tries to minimize. Supported: <code>"squared"</code> (L2) and <code>"absolute"</code> (L1) (default = squared) for regression and <code>"logistic"</code> (default) for classification. For <code>ml_gradient_boosted_trees</code>, setting <code>"auto"</code>
will default to the appropriate loss type based on model type.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_seed">seed</code></td>
<td>
<p>Seed for random numbers.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10
iterations, defaults to 10.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_cache_node_ids">cache_node_ids</code></td>
<td>
<p>If <code>FALSE</code>, the algorithm will pass trees to
executors to match instances with nodes. If <code>TRUE</code>, the algorithm will
cache node IDs for each instance. Caching can speed up training of deeper
trees. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_max_memory_in_mb">max_memory_in_mb</code></td>
<td>
<p>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration, and its aggregates
may exceed this size. Defaults to 256.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_type">type</code></td>
<td>
<p>The type of model to fit. <code>"regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. When <code>"auto"</code> is used, the model type is
inferred based on the response variable type &ndash; if it is a numeric type,
then regression is used; classification otherwise.</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_response">response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td></tr>
<tr><td><code id="ml_gbt_classifier_+3A_features">features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported options for <code>feature_subset_strategy</code> are
</p>

<ul>
<li> <p><code>"auto"</code>: Choose automatically for task: If <code>num_trees == 1</code>, set to <code>"all"</code>. If <code>num_trees &gt; 1</code> (forest), set to <code>"sqrt"</code> for classification and to <code>"onethird"</code> for regression.
</p>
</li>
<li> <p><code>"all"</code>: use all features
</p>
</li>
<li> <p><code>"onethird"</code>: use 1/3 of the features
</p>
</li>
<li> <p><code>"sqrt"</code>: use use sqrt(number of features)
</p>
</li>
<li> <p><code>"log2"</code>: use log2(number of features)
</p>
</li>
<li> <p><code>"n"</code>: when <code>n</code> is in the range (0, 1.0], use n * number of features. When <code>n</code> is in the range (1, number of features), use <code>n</code> features. (default = <code>"auto"</code>)
</p>
</li></ul>

<p><code>ml_gradient_boosted_trees</code> is a wrapper around <code>ml_gbt_regressor.tbl_spark</code> and <code>ml_gbt_classifier.tbl_spark</code> and calls the appropriate method based on model type.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

gbt_model &lt;- iris_training %&gt;%
  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)

pred &lt;- ml_predict(gbt_model, iris_test)

ml_regression_evaluator(pred, label_col = "Sepal_Length")

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_generalized_linear_regression'>Spark ML &ndash; Generalized Linear Regression</h2><span id='topic+ml_generalized_linear_regression'></span>

<h3>Description</h3>

<p>Perform regression using Generalized Linear Model (GLM).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_generalized_linear_regression(
  x,
  formula = NULL,
  family = "gaussian",
  link = NULL,
  fit_intercept = TRUE,
  offset_col = NULL,
  link_power = NULL,
  link_prediction_col = NULL,
  reg_param = 0,
  max_iter = 25,
  weight_col = NULL,
  solver = "irls",
  tol = 1e-06,
  variance_power = 0,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("generalized_linear_regression_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_generalized_linear_regression_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_family">family</code></td>
<td>
<p>Name of family which is a description of the error distribution to be used in the model. Supported options: &quot;gaussian&quot;, &quot;binomial&quot;, &quot;poisson&quot;, &quot;gamma&quot; and &quot;tweedie&quot;. Default is &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_link">link</code></td>
<td>
<p>Name of link function which provides the relationship between the linear predictor and the mean of the distribution function. See for supported link functions.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>Boolean; should the model be fit with an intercept term?</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_offset_col">offset_col</code></td>
<td>
<p>Offset column name. If this is not set, we treat all instance offsets as 0.0. The feature specified as offset has a constant coefficient of 1.0.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_link_power">link_power</code></td>
<td>
<p>Index in the power link function. Only applicable to the Tweedie family. Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. When not set, this value defaults to 1 - variancePower, which matches the R &quot;statmod&quot; package.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_link_prediction_col">link_prediction_col</code></td>
<td>
<p>Link prediction (linear predictor) column name. Default is not set, which means we do not output link prediction.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_reg_param">reg_param</code></td>
<td>
<p>Regularization parameter (aka lambda)</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_weight_col">weight_col</code></td>
<td>
<p>The name of the column to use as weights for the model fit.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_solver">solver</code></td>
<td>
<p>Solver algorithm for optimization.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_variance_power">variance_power</code></td>
<td>
<p>Power in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. (see <a href="https://en.wikipedia.org/wiki/Tweedie_distribution">Tweedie Distribution (Wikipedia)</a>) Supported values: 0 and [1, Inf). Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_generalized_linear_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Valid link functions for each family is listed below. The first link function of each family is the default one.
</p>

<ul>
<li><p> gaussian: &quot;identity&quot;, &quot;log&quot;, &quot;inverse&quot;
</p>
</li>
<li><p> binomial: &quot;logit&quot;, &quot;probit&quot;, &quot;loglog&quot;
</p>
</li>
<li><p> poisson: &quot;log&quot;, &quot;identity&quot;, &quot;sqrt&quot;
</p>
</li>
<li><p> gamma: &quot;inverse&quot;, &quot;identity&quot;, &quot;log&quot;
</p>
</li>
<li><p> tweedie: power link function specified through <code>link_power</code>. The default link power in the tweedie family is <code>1 - variance_power</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)

sc &lt;- spark_connect(master = "local")
mtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

# Specify the grid
family &lt;- c("gaussian", "gamma", "poisson")
link &lt;- c("identity", "log")
family_link &lt;- expand.grid(family = family, link = link, stringsAsFactors = FALSE)
family_link &lt;- data.frame(family_link, rmse = 0)

# Train the models
for (i in seq_len(nrow(family_link))) {
  glm_model &lt;- mtcars_training %&gt;%
    ml_generalized_linear_regression(mpg ~ .,
      family = family_link[i, 1],
      link = family_link[i, 2]
    )

  pred &lt;- ml_predict(glm_model, mtcars_test)
  family_link[i, 3] &lt;- ml_regression_evaluator(pred, label_col = "mpg")
}

family_link

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_glm_tidiers'>Tidying methods for Spark ML linear models</h2><span id='topic+ml_glm_tidiers'></span><span id='topic+tidy.ml_model_generalized_linear_regression'></span><span id='topic+tidy.ml_model_linear_regression'></span><span id='topic+augment.ml_model_generalized_linear_regression'></span><span id='topic+augment._ml_model_linear_regression'></span><span id='topic+augment.ml_model_linear_regression'></span><span id='topic+glance.ml_model_generalized_linear_regression'></span><span id='topic+glance.ml_model_linear_regression'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_generalized_linear_regression'
tidy(x, exponentiate = FALSE, ...)

## S3 method for class 'ml_model_linear_regression'
tidy(x, ...)

## S3 method for class 'ml_model_generalized_linear_regression'
augment(
  x,
  newdata = NULL,
  type.residuals = c("working", "deviance", "pearson", "response"),
  ...
)

## S3 method for class ''_ml_model_linear_regression''
augment(
  x,
  new_data = NULL,
  type.residuals = c("working", "deviance", "pearson", "response"),
  ...
)

## S3 method for class 'ml_model_linear_regression'
augment(
  x,
  newdata = NULL,
  type.residuals = c("working", "deviance", "pearson", "response"),
  ...
)

## S3 method for class 'ml_model_generalized_linear_regression'
glance(x, ...)

## S3 method for class 'ml_model_linear_regression'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_glm_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_glm_tidiers_+3A_exponentiate">exponentiate</code></td>
<td>
<p>For GLM, whether to exponentiate the coefficient estimates (typical for logistic regression.)</p>
</td></tr>
<tr><td><code id="ml_glm_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_glm_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
<tr><td><code id="ml_glm_tidiers_+3A_type.residuals">type.residuals</code></td>
<td>
<p>type of residuals, defaults to <code>"working"</code>. Must be set to
<code>"working"</code> when <code>newdata</code> is supplied.</p>
</td></tr>
<tr><td><code id="ml_glm_tidiers_+3A_new_data">new_data</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The residuals attached by <code>augment</code> are of type &quot;working&quot; by default,
which is different from the default of &quot;deviance&quot; for <code>residuals()</code> or <code>sdf_residuals()</code>.
</p>

<hr>
<h2 id='ml_isotonic_regression'>Spark ML &ndash; Isotonic Regression</h2><span id='topic+ml_isotonic_regression'></span>

<h3>Description</h3>

<p>Currently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_isotonic_regression(
  x,
  formula = NULL,
  feature_index = 0,
  isotonic = TRUE,
  weight_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("isotonic_regression_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_isotonic_regression_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_feature_index">feature_index</code></td>
<td>
<p>Index of the feature if <code>features_col</code> is a vector column (default: 0), no effect otherwise.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_isotonic">isotonic</code></td>
<td>
<p>Whether the output sequence should be isotonic/increasing (true) or antitonic/decreasing (false). Default: true</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_weight_col">weight_col</code></td>
<td>
<p>The name of the column to use as weights for the model fit.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

iso_res &lt;- iris_tbl %&gt;%
  ml_isotonic_regression(Petal_Length ~ Petal_Width)

pred &lt;- ml_predict(iso_res, iris_test)

pred

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_isotonic_regression_tidiers'>Tidying methods for Spark ML Isotonic Regression</h2><span id='topic+ml_isotonic_regression_tidiers'></span><span id='topic+tidy.ml_model_isotonic_regression'></span><span id='topic+augment.ml_model_isotonic_regression'></span><span id='topic+glance.ml_model_isotonic_regression'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_isotonic_regression'
tidy(x, ...)

## S3 method for class 'ml_model_isotonic_regression'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_isotonic_regression'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_isotonic_regression_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_isotonic_regression_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_kmeans'>Spark ML &ndash; K-Means Clustering</h2><span id='topic+ml_kmeans'></span><span id='topic+ml_compute_cost'></span><span id='topic+ml_compute_silhouette_measure'></span>

<h3>Description</h3>

<p>K-means clustering with support for k-means|| initialization proposed by Bahmani et al.
Using 'ml_kmeans()' with the formula interface requires Spark 2.0+.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_kmeans(
  x,
  formula = NULL,
  k = 2,
  max_iter = 20,
  tol = 1e-04,
  init_steps = 2,
  init_mode = "k-means||",
  seed = NULL,
  features_col = "features",
  prediction_col = "prediction",
  uid = random_string("kmeans_"),
  ...
)

ml_compute_cost(model, dataset)

ml_compute_silhouette_measure(
  model,
  dataset,
  distance_measure = c("squaredEuclidean", "cosine")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_kmeans_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_k">k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_init_steps">init_steps</code></td>
<td>
<p>Number of steps for the k-means|| initialization mode. This is an advanced setting &ndash; the default of 2 is almost always enough. Must be &gt; 0. Default: 2.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_init_mode">init_mode</code></td>
<td>
<p>Initialization algorithm. This can be either &quot;random&quot; to choose random points as initial cluster centers, or &quot;k-means||&quot; to use a parallel variant of k-means++ (Bahmani et al., Scalable K-Means++, VLDB 2012). Default: k-means||.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_...">...</code></td>
<td>
<p>Optional arguments, see Details.
#' @return The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_model">model</code></td>
<td>
<p>A fitted K-means model returned by <code>ml_kmeans()</code></p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_dataset">dataset</code></td>
<td>
<p>Dataset on which to calculate K-means cost</p>
</td></tr>
<tr><td><code id="ml_kmeans_+3A_distance_measure">distance_measure</code></td>
<td>
<p>Distance measure to apply when computing the Silhouette measure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ml_compute_cost()</code> returns the K-means cost (sum of
squared distances of points to their nearest center) for the model
on the given data.
</p>
<p><code>ml_compute_silhouette_measure()</code> returns the Silhouette measure
of the clustering on the given data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
ml_kmeans(iris_tbl, Species ~ .)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_kmeans_cluster_eval'>Evaluate a K-mean clustering</h2><span id='topic+ml_kmeans_cluster_eval'></span>

<h3>Description</h3>

<p>Evaluate a K-mean clustering
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_kmeans_cluster_eval_+3A_model">model</code></td>
<td>
<p>A fitted K-means model returned by <code>ml_kmeans()</code></p>
</td></tr>
<tr><td><code id="ml_kmeans_cluster_eval_+3A_dataset">dataset</code></td>
<td>
<p>Dataset on which to calculate K-means cost</p>
</td></tr>
</table>

<hr>
<h2 id='ml_lda'>Spark ML &ndash; Latent Dirichlet Allocation</h2><span id='topic+ml_lda'></span><span id='topic+ml_describe_topics'></span><span id='topic+ml_log_likelihood'></span><span id='topic+ml_log_perplexity'></span><span id='topic+ml_topics_matrix'></span>

<h3>Description</h3>

<p>Latent Dirichlet Allocation (LDA), a topic model designed for text documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_lda(
  x,
  formula = NULL,
  k = 10,
  max_iter = 20,
  doc_concentration = NULL,
  topic_concentration = NULL,
  subsampling_rate = 0.05,
  optimizer = "online",
  checkpoint_interval = 10,
  keep_last_checkpoint = TRUE,
  learning_decay = 0.51,
  learning_offset = 1024,
  optimize_doc_concentration = TRUE,
  seed = NULL,
  features_col = "features",
  topic_distribution_col = "topicDistribution",
  uid = random_string("lda_"),
  ...
)

ml_describe_topics(model, max_terms_per_topic = 10)

ml_log_likelihood(model, dataset)

ml_log_perplexity(model, dataset)

ml_topics_matrix(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_lda_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_k">k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_doc_concentration">doc_concentration</code></td>
<td>
<p>Concentration parameter (commonly named &quot;alpha&quot;)
for the prior placed on documents' distributions over topics (&quot;theta&quot;).
See details.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_topic_concentration">topic_concentration</code></td>
<td>
<p>Concentration parameter (commonly named &quot;beta&quot; or
&quot;eta&quot;) for the prior placed on topics' distributions over terms.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_subsampling_rate">subsampling_rate</code></td>
<td>
<p>(For Online optimizer only) Fraction of the corpus
to be sampled and used in each iteration of mini-batch gradient descent, in
range (0, 1]. Note that this should be adjusted in synch with <code>max_iter</code>
so the entire corpus is used. Specifically, set both so that maxIterations *
miniBatchFraction greater than or equal to 1.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_optimizer">optimizer</code></td>
<td>
<p>Optimizer or inference algorithm used to estimate the LDA
model. Supported: &quot;online&quot; for Online Variational Bayes (default) and &quot;em&quot;
for Expectation-Maximization.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_keep_last_checkpoint">keep_last_checkpoint</code></td>
<td>
<p>(Spark 2.0.0+) (For EM optimizer only) If using
checkpointing, this indicates whether to keep the last checkpoint.
If <code>FALSE</code>, then the checkpoint will be deleted. Deleting the checkpoint
can cause failures if a data partition is lost, so set this bit with care.
Note that checkpoints will be cleaned up via reference counting, regardless.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_learning_decay">learning_decay</code></td>
<td>
<p>(For Online optimizer only) Learning rate, set as an
exponential decay rate. This should be between (0.5, 1.0] to guarantee
asymptotic convergence. This is called &quot;kappa&quot; in the Online LDA paper
(Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_learning_offset">learning_offset</code></td>
<td>
<p>(For Online optimizer only) A (positive) learning
parameter that downweights early iterations. Larger values make early
iterations count less. This is called &quot;tau0&quot; in the Online LDA paper (Hoffman
et al., 2010) Default: 1024, following Hoffman et al.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_optimize_doc_concentration">optimize_doc_concentration</code></td>
<td>
<p>(For Online optimizer only) Indicates
whether the <code>doc_concentration</code> (Dirichlet parameter for document-topic
distribution) will be optimized during training. Setting this to true will
make the model more expressive and fit the training data better. Default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="ml_lda_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_topic_distribution_col">topic_distribution_col</code></td>
<td>
<p>Output column with estimates of the topic
mixture distribution for each document (often called &quot;theta&quot; in the
literature). Returns a vector of zeros for an empty document.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_...">...</code></td>
<td>
<p>Optional arguments, see Details.
#' @return The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_model">model</code></td>
<td>
<p>A fitted LDA model returned by <code>ml_lda()</code>.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_max_terms_per_topic">max_terms_per_topic</code></td>
<td>
<p>Maximum number of terms to collect for each topic. Default value of 10.</p>
</td></tr>
<tr><td><code id="ml_lda_+3A_dataset">dataset</code></td>
<td>
<p>test corpus to use for calculating log likelihood or log perplexity</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For 'ml_lda.tbl_spark' with the formula interface, you can specify named
arguments in '...' that will be passed 'ft_regex_tokenizer()',
'ft_stop_words_remover()', and 'ft_count_vectorizer()'. For example, to
increase the default 'min_token_length', you can use
'ml_lda(dataset, ~ text, min_token_length = 4)'.
</p>
<p>Terminology for LDA:
</p>

<ul>
<li><p> &quot;term&quot; = &quot;word&quot;: an element of the vocabulary
</p>
</li>
<li><p> &quot;token&quot;: instance of a term appearing in a document
</p>
</li>
<li><p> &quot;topic&quot;: multinomial distribution over terms representing some concept
</p>
</li>
<li><p> &quot;document&quot;: one piece of text, corresponding to one row in the input
data
</p>
</li></ul>

<p>Original LDA paper (journal version): Blei, Ng, and Jordan. &quot;Latent Dirichlet
Allocation.&quot; JMLR, 2003.
</p>
<p>Input data (<code>features_col</code>): LDA is given a collection of documents as
input data, via the <code>features_col</code> parameter. Each document is specified
as a Vector of length <code>vocab_size</code>, where each entry is the count for
the corresponding term (word) in the document. Feature transformers such as
<code><a href="#topic+ft_tokenizer">ft_tokenizer</a></code> and <code><a href="#topic+ft_count_vectorizer">ft_count_vectorizer</a></code> can be
useful for converting text to word count vectors
</p>


<h3>Value</h3>

<p><code>ml_describe_topics</code> returns a DataFrame with topics and their top-weighted terms.
</p>
<p><code>ml_log_likelihood</code> calculates a lower bound on the log likelihood of
the entire corpus
</p>


<h3>Parameter details</h3>



<h4><code>doc_concentration</code></h4>

<p>This is the parameter to a Dirichlet distribution, where larger values mean
more smoothing (more regularization). If not set by the user, then
<code>doc_concentration</code> is set automatically. If set to singleton vector
[alpha], then alpha is replicated to a vector of length k in fitting.
Otherwise, the <code>doc_concentration</code> vector must be length k.
(default = automatic)
</p>
<p>Optimizer-specific parameter settings:
</p>
<p>EM
</p>

<ul>
<li><p> Currently only supports symmetric distributions, so all values in
the vector should be the same.
</p>
</li>
<li><p> Values should be greater than 1.0
</p>
</li>
<li><p> default = uniformly (50 / k) + 1, where 50/k is common in LDA
libraries and +1 follows from Asuncion et al. (2009), who recommend a +1
adjustment for EM.
</p>
</li></ul>

<p>Online
</p>

<ul>
<li><p> Values should be greater than or equal to 0
</p>
</li>
<li><p> default = uniformly (1.0 / k), following the implementation from
<a href="https://github.com/Blei-Lab/onlineldavb">here</a>
</p>
</li></ul>




<h4><code>topic_concentration</code></h4>

<p>This is the parameter to a symmetric Dirichlet distribution.
</p>
<p>Note: The topics' distributions over terms are called &quot;beta&quot; in the original
LDA paper by Blei et al., but are called &quot;phi&quot; in many later papers such as
Asuncion et al., 2009.
</p>
<p>If not set by the user, then <code>topic_concentration</code> is set automatically.
(default = automatic)
</p>
<p>Optimizer-specific parameter settings:
</p>
<p>EM
</p>

<ul>
<li><p> Value should be greater than 1.0
</p>
</li>
<li><p> default = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1
follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.
</p>
</li></ul>

<p>Online
</p>

<ul>
<li><p> Value should be greater than or equal to 0
</p>
</li>
<li><p> default = (1.0 / k), following the implementation from
<a href="https://github.com/Blei-Lab/onlineldavb">here</a>.
</p>
</li></ul>




<h4><code>topic_distribution_col</code></h4>

<p>This uses a variational approximation following Hoffman et al. (2010),
where the approximate distribution is called &quot;gamma.&quot; Technically, this
method returns this approximation &quot;gamma&quot; for each document.
</p>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(janeaustenr)
library(dplyr)
sc &lt;- spark_connect(master = "local")

lines_tbl &lt;- sdf_copy_to(sc,
  austen_books()[c(1:30), ],
  name = "lines_tbl",
  overwrite = TRUE
)

# transform the data in a tidy form
lines_tbl_tidy &lt;- lines_tbl %&gt;%
  ft_tokenizer(
    input_col = "text",
    output_col = "word_list"
  ) %&gt;%
  ft_stop_words_remover(
    input_col = "word_list",
    output_col = "wo_stop_words"
  ) %&gt;%
  mutate(text = explode(wo_stop_words)) %&gt;%
  filter(text != "") %&gt;%
  select(text, book)

lda_model &lt;- lines_tbl_tidy %&gt;%
  ml_lda(~text, k = 4)

# vocabulary and topics
tidy(lda_model)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_lda_tidiers'>Tidying methods for Spark ML LDA models</h2><span id='topic+ml_lda_tidiers'></span><span id='topic+tidy.ml_model_lda'></span><span id='topic+augment.ml_model_lda'></span><span id='topic+glance.ml_model_lda'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_lda'
tidy(x, ...)

## S3 method for class 'ml_model_lda'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_lda'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_lda_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_lda_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_lda_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_linear_regression'>Spark ML &ndash; Linear Regression</h2><span id='topic+ml_linear_regression'></span>

<h3>Description</h3>

<p>Perform regression using linear regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_linear_regression(
  x,
  formula = NULL,
  fit_intercept = TRUE,
  elastic_net_param = 0,
  reg_param = 0,
  max_iter = 100,
  weight_col = NULL,
  loss = "squaredError",
  solver = "auto",
  standardization = TRUE,
  tol = 1e-06,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("linear_regression_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_linear_regression_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>Boolean; should the model be fit with an intercept term?</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_elastic_net_param">elastic_net_param</code></td>
<td>
<p>ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_reg_param">reg_param</code></td>
<td>
<p>Regularization parameter (aka lambda)</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_weight_col">weight_col</code></td>
<td>
<p>The name of the column to use as weights for the model fit.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_loss">loss</code></td>
<td>
<p>The loss function to be optimized. Supported options: &quot;squaredError&quot;
and &quot;huber&quot;. Default: &quot;squaredError&quot;</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_solver">solver</code></td>
<td>
<p>Solver algorithm for optimization.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_standardization">standardization</code></td>
<td>
<p>Whether to standardize the training features before fitting the model.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_linear_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
mtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

lm_model &lt;- mtcars_training %&gt;%
  ml_linear_regression(mpg ~ .)

pred &lt;- ml_predict(lm_model, mtcars_test)

ml_regression_evaluator(pred, label_col = "mpg")

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_linear_svc'>Spark ML &ndash; LinearSVC</h2><span id='topic+ml_linear_svc'></span>

<h3>Description</h3>

<p>Perform classification using linear support vector machines (SVM). This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. Only supports L2 regularization currently.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_linear_svc(
  x,
  formula = NULL,
  fit_intercept = TRUE,
  reg_param = 0,
  max_iter = 100,
  standardization = TRUE,
  weight_col = NULL,
  tol = 1e-06,
  threshold = 0,
  aggregation_depth = 2,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  raw_prediction_col = "rawPrediction",
  uid = random_string("linear_svc_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_linear_svc_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>Boolean; should the model be fit with an intercept term?</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_reg_param">reg_param</code></td>
<td>
<p>Regularization parameter (aka lambda)</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_standardization">standardization</code></td>
<td>
<p>Whether to standardize the training features before fitting the model.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_weight_col">weight_col</code></td>
<td>
<p>The name of the column to use as weights for the model fit.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_threshold">threshold</code></td>
<td>
<p>in binary classification prediction, in range [0, 1].</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_aggregation_depth">aggregation_depth</code></td>
<td>
<p>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  filter(Species != "setosa") %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

svc_model &lt;- iris_training %&gt;%
  ml_linear_svc(Species ~ .)

pred &lt;- ml_predict(svc_model, iris_test)

ml_binary_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_linear_svc_tidiers'>Tidying methods for Spark ML linear svc</h2><span id='topic+ml_linear_svc_tidiers'></span><span id='topic+tidy.ml_model_linear_svc'></span><span id='topic+augment.ml_model_linear_svc'></span><span id='topic+glance.ml_model_linear_svc'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_linear_svc'
tidy(x, ...)

## S3 method for class 'ml_model_linear_svc'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_linear_svc'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_linear_svc_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_linear_svc_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_linear_svc_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_logistic_regression'>Spark ML &ndash; Logistic Regression</h2><span id='topic+ml_logistic_regression'></span>

<h3>Description</h3>

<p>Perform classification using logistic regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_logistic_regression(
  x,
  formula = NULL,
  fit_intercept = TRUE,
  elastic_net_param = 0,
  reg_param = 0,
  max_iter = 100,
  threshold = 0.5,
  thresholds = NULL,
  tol = 1e-06,
  weight_col = NULL,
  aggregation_depth = 2,
  lower_bounds_on_coefficients = NULL,
  lower_bounds_on_intercepts = NULL,
  upper_bounds_on_coefficients = NULL,
  upper_bounds_on_intercepts = NULL,
  features_col = "features",
  label_col = "label",
  family = "auto",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("logistic_regression_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_logistic_regression_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>Boolean; should the model be fit with an intercept term?</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_elastic_net_param">elastic_net_param</code></td>
<td>
<p>ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_reg_param">reg_param</code></td>
<td>
<p>Regularization parameter (aka lambda)</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_threshold">threshold</code></td>
<td>
<p>in binary classification prediction, in range [0, 1].</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_weight_col">weight_col</code></td>
<td>
<p>The name of the column to use as weights for the model fit.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_aggregation_depth">aggregation_depth</code></td>
<td>
<p>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_lower_bounds_on_coefficients">lower_bounds_on_coefficients</code></td>
<td>
<p>(Spark 2.2.0+) Lower bounds on
coefficients if fitting under bound constrained optimization.
The bound matrix must be compatible with the shape (1, number of features)
for binomial regression, or (number of classes, number of features) for
multinomial regression.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_lower_bounds_on_intercepts">lower_bounds_on_intercepts</code></td>
<td>
<p>(Spark 2.2.0+) Lower bounds on intercepts
if fitting under bound constrained optimization.
The bounds vector size must be equal with 1 for binomial regression, or
the number of classes for multinomial regression.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_upper_bounds_on_coefficients">upper_bounds_on_coefficients</code></td>
<td>
<p>(Spark 2.2.0+) Upper bounds on
coefficients if fitting under bound constrained optimization.
The bound matrix must be compatible with the shape (1, number of features)
for binomial regression, or (number of classes, number of features) for
multinomial regression.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_upper_bounds_on_intercepts">upper_bounds_on_intercepts</code></td>
<td>
<p>(Spark 2.2.0+) Upper bounds on intercepts
if fitting under bound constrained optimization.
The bounds vector size must be equal with 1 for binomial regression, or
the number of classes for multinomial regression.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_family">family</code></td>
<td>
<p>(Spark 2.1.0+) Param for the name of family which is a
description of the label distribution to be used in the model. Supported
options: &quot;auto&quot;, &quot;binomial&quot;, and &quot;multinomial.&quot;</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
mtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

lr_model &lt;- mtcars_training %&gt;%
  ml_logistic_regression(am ~ gear + carb)

pred &lt;- ml_predict(lr_model, mtcars_test)

ml_binary_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_logistic_regression_tidiers'>Tidying methods for Spark ML Logistic Regression</h2><span id='topic+ml_logistic_regression_tidiers'></span><span id='topic+tidy.ml_model_logistic_regression'></span><span id='topic+augment.ml_model_logistic_regression'></span><span id='topic+augment._ml_model_logistic_regression'></span><span id='topic+glance.ml_model_logistic_regression'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_logistic_regression'
tidy(x, ...)

## S3 method for class 'ml_model_logistic_regression'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_logistic_regression''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_logistic_regression'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_logistic_regression_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
<tr><td><code id="ml_logistic_regression_tidiers_+3A_new_data">new_data</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_metrics_binary'>Extracts metrics from a fitted table</h2><span id='topic+ml_metrics_binary'></span>

<h3>Description</h3>

<p>The function works best when passed a 'tbl_spark' created by
'ml_predict()'. The output 'tbl_spark' will contain the correct variable
types and format that the given Spark model &quot;evaluator&quot; expects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_metrics_binary(
  x,
  truth = label,
  estimate = rawPrediction,
  metrics = c("roc_auc", "pr_auc"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_metrics_binary_+3A_x">x</code></td>
<td>
<p>A 'tbl_spark' containing the estimate (prediction) and the truth (value
of what actually happened)</p>
</td></tr>
<tr><td><code id="ml_metrics_binary_+3A_truth">truth</code></td>
<td>
<p>The name of the column from 'x' with an integer field
containing the binary response (0 or 1). The 'ml_predict()' function will
create a new field named 'label' which contains the expected type and values.
'truth' defaults to 'label'.</p>
</td></tr>
<tr><td><code id="ml_metrics_binary_+3A_estimate">estimate</code></td>
<td>
<p>The name of the column from 'x' that contains the prediction.
Defaults to 'rawPrediction', since its type and expected values will match 'truth'.</p>
</td></tr>
<tr><td><code id="ml_metrics_binary_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics to calculate. For binary models
the possible values are: 'roc_auc' (Area under the Receiver Operator curve),
'pr_auc' (Area under the Precesion Recall curve).
Defaults to: 'roc_auc', 'pr_auc'</p>
</td></tr>
<tr><td><code id="ml_metrics_binary_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &lsquo;ml_metrics' family of functions implement Spark&rsquo;s 'evaluate'
closer to how the 'yardstick' package works. The functions expect a table
containing the truth and estimate, and return a 'tibble' with the results. The
'tibble' has the same format and variable names as the output of the 'yardstick'
functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect("local")
tbl_iris &lt;- copy_to(sc, iris)
prep_iris &lt;- tbl_iris %&gt;%
  mutate(is_setosa = ifelse(Species == "setosa", 1, 0))
iris_split &lt;- sdf_random_split(prep_iris, training = 0.5, test = 0.5)
model &lt;- ml_logistic_regression(iris_split$training, "is_setosa ~ Sepal_Length")
tbl_predictions &lt;- ml_predict(model, iris_split$test)
ml_metrics_binary(tbl_predictions)

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_metrics_multiclass'>Extracts metrics from a fitted table</h2><span id='topic+ml_metrics_multiclass'></span>

<h3>Description</h3>

<p>The function works best when passed a 'tbl_spark' created by
'ml_predict()'. The output 'tbl_spark' will contain the correct variable
types and format that the given Spark model &quot;evaluator&quot; expects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_metrics_multiclass(
  x,
  truth = label,
  estimate = prediction,
  metrics = c("accuracy"),
  beta = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_metrics_multiclass_+3A_x">x</code></td>
<td>
<p>A 'tbl_spark' containing the estimate (prediction) and the truth (value
of what actually happened)</p>
</td></tr>
<tr><td><code id="ml_metrics_multiclass_+3A_truth">truth</code></td>
<td>
<p>The name of the column from 'x' with an integer field containing
an the indexed value for each outcome . The 'ml_predict()' function will
create a new field named 'label' which contains the expected type and values.
'truth' defaults to 'label'.</p>
</td></tr>
<tr><td><code id="ml_metrics_multiclass_+3A_estimate">estimate</code></td>
<td>
<p>The name of the column from 'x' that contains the prediction.
Defaults to 'prediction', since its type and indexed values will match 'truth'.</p>
</td></tr>
<tr><td><code id="ml_metrics_multiclass_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics to calculate. For multiclass
models the possible values are: 'acurracy', 'f_meas' (F-score), 'recall' and
'precision'. This function translates the argument into an acceptable Spark
parameter. If no translation is found, then the raw value of the argument is
passed to Spark. This makes it possible to request a metric that is not listed
here but, depending on version, it is available in Spark. Other metrics form
multi-class models are: 'weightedTruePositiveRate', 'weightedFalsePositiveRate',
'weightedFMeasure', 'truePositiveRateByLabel', 'falsePositiveRateByLabel',
'precisionByLabel', 'recallByLabel', 'fMeasureByLabel', 'logLoss', 'hammingLoss'</p>
</td></tr>
<tr><td><code id="ml_metrics_multiclass_+3A_beta">beta</code></td>
<td>
<p>Numerical value used for precision and recall. Defaults to NULL, but
if the Spark session's verion is 3.0 and above, then NULL is changed to 1,
unless something different is supplied in this argument.</p>
</td></tr>
<tr><td><code id="ml_metrics_multiclass_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &lsquo;ml_metrics' family of functions implement Spark&rsquo;s 'evaluate'
closer to how the 'yardstick' package works. The functions expect a table
containing the truth and estimate, and return a 'tibble' with the results. The
'tibble' has the same format and variable names as the output of the 'yardstick'
functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect("local")
tbl_iris &lt;- copy_to(sc, iris)
iris_split &lt;- sdf_random_split(tbl_iris, training = 0.5, test = 0.5)
model &lt;- ml_random_forest(iris_split$training, "Species ~ .")
tbl_predictions &lt;- ml_predict(model, iris_split$test)

ml_metrics_multiclass(tbl_predictions)

# Request different metrics
ml_metrics_multiclass(tbl_predictions, metrics = c("recall", "precision"))

# Request metrics not translated by the function, but valid in Spark
ml_metrics_multiclass(tbl_predictions, metrics = c("logLoss", "hammingLoss"))

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_metrics_regression'>Extracts metrics from a fitted table</h2><span id='topic+ml_metrics_regression'></span>

<h3>Description</h3>

<p>The function works best when passed a 'tbl_spark' created by
'ml_predict()'. The output 'tbl_spark' will contain the correct variable
types and format that the given Spark model &quot;evaluator&quot; expects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_metrics_regression(
  x,
  truth,
  estimate = prediction,
  metrics = c("rmse", "rsq", "mae"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_metrics_regression_+3A_x">x</code></td>
<td>
<p>A 'tbl_spark' containing the estimate (prediction) and the truth (value
of what actually happened)</p>
</td></tr>
<tr><td><code id="ml_metrics_regression_+3A_truth">truth</code></td>
<td>
<p>The name of the column from 'x' that contains the value of what
actually happened</p>
</td></tr>
<tr><td><code id="ml_metrics_regression_+3A_estimate">estimate</code></td>
<td>
<p>The name of the column from 'x' that contains the prediction.
Defaults to 'prediction', since it is the default that 'ml_predict()' uses.</p>
</td></tr>
<tr><td><code id="ml_metrics_regression_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics to calculate. For regression
models the possible values are: 'rmse' (Root mean squared error), 'mse' (Mean
squared error),'rsq' (R squared), 'mae' (Mean absolute error), and 'var'
(Explained variance). Defaults to: 'rmse', 'rsq', 'mae'</p>
</td></tr>
<tr><td><code id="ml_metrics_regression_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &lsquo;ml_metrics' family of functions implement Spark&rsquo;s 'evaluate'
closer to how the 'yardstick' package works. The functions expect a table
containing the truth and estimate, and return a 'tibble' with the results. The
'tibble' has the same format and variable names as the output of the 'yardstick'
functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect("local")
tbl_iris &lt;- copy_to(sc, iris)
iris_split &lt;- sdf_random_split(tbl_iris, training = 0.5, test = 0.5)
training &lt;- iris_split$training
reg_formula &lt;- "Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width"
model &lt;- ml_generalized_linear_regression(training, reg_formula)
tbl_predictions &lt;- ml_predict(model, iris_split$test)
tbl_predictions %&gt;%
  ml_metrics_regression(Sepal_Length)

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_model_data'>Extracts data associated with a Spark ML model</h2><span id='topic+ml_model_data'></span>

<h3>Description</h3>

<p>Extracts data associated with a Spark ML model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_model_data(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_model_data_+3A_object">object</code></td>
<td>
<p>a Spark ML model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tbl_spark
</p>

<hr>
<h2 id='ml_multilayer_perceptron_classifier'>Spark ML &ndash; Multilayer Perceptron</h2><span id='topic+ml_multilayer_perceptron_classifier'></span><span id='topic+ml_multilayer_perceptron'></span>

<h3>Description</h3>

<p>Classification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_multilayer_perceptron_classifier(
  x,
  formula = NULL,
  layers = NULL,
  max_iter = 100,
  step_size = 0.03,
  tol = 1e-06,
  block_size = 128,
  solver = "l-bfgs",
  seed = NULL,
  initial_weights = NULL,
  thresholds = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("multilayer_perceptron_classifier_"),
  ...
)

ml_multilayer_perceptron(
  x,
  formula = NULL,
  layers,
  max_iter = 100,
  step_size = 0.03,
  tol = 1e-06,
  block_size = 128,
  solver = "l-bfgs",
  seed = NULL,
  initial_weights = NULL,
  features_col = "features",
  label_col = "label",
  thresholds = NULL,
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("multilayer_perceptron_classifier_"),
  response = NULL,
  features = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_layers">layers</code></td>
<td>
<p>A numeric vector describing the layers &ndash; each element in the vector gives the size of a layer. For example, <code>c(4, 5, 2)</code> would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_step_size">step_size</code></td>
<td>
<p>Step size to be used for each iteration of optimization (&gt; 0).</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_tol">tol</code></td>
<td>
<p>Param for the convergence tolerance for iterative algorithms.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_block_size">block_size</code></td>
<td>
<p>Block size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_solver">solver</code></td>
<td>
<p>The solver algorithm for optimization. Supported options: &quot;gd&quot; (minibatch gradient descent) or &quot;l-bfgs&quot;. Default: &quot;l-bfgs&quot;</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_initial_weights">initial_weights</code></td>
<td>
<p>The initial weights of the model.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_response">response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_classifier_+3A_features">features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ml_multilayer_perceptron()</code> is an alias for <code>ml_multilayer_perceptron_classifier()</code> for backwards compatibility.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")

iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

mlp_model &lt;- iris_training %&gt;%
  ml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3))

pred &lt;- ml_predict(mlp_model, iris_test)

ml_multiclass_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_multilayer_perceptron_tidiers'>Tidying methods for Spark ML MLP</h2><span id='topic+ml_multilayer_perceptron_tidiers'></span><span id='topic+tidy.ml_model_multilayer_perceptron_classification'></span><span id='topic+augment.ml_model_multilayer_perceptron_classification'></span><span id='topic+glance.ml_model_multilayer_perceptron_classification'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_multilayer_perceptron_classification'
tidy(x, ...)

## S3 method for class 'ml_model_multilayer_perceptron_classification'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_multilayer_perceptron_classification'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_multilayer_perceptron_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_multilayer_perceptron_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_naive_bayes'>Spark ML &ndash; Naive-Bayes</h2><span id='topic+ml_naive_bayes'></span>

<h3>Description</h3>

<p>Naive Bayes Classifiers. It supports Multinomial NB (see <a href="https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">here</a>) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see <a href="http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">here</a>). The input feature values must be nonnegative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_naive_bayes(
  x,
  formula = NULL,
  model_type = "multinomial",
  smoothing = 1,
  thresholds = NULL,
  weight_col = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("naive_bayes_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_naive_bayes_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_model_type">model_type</code></td>
<td>
<p>The model type. Supported options: <code>"multinomial"</code>
and <code>"bernoulli"</code>. (default = <code>multinomial</code>)</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_smoothing">smoothing</code></td>
<td>
<p>The (Laplace) smoothing parameter. Defaults to 1.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_weight_col">weight_col</code></td>
<td>
<p>(Spark 2.1.0+) Weight column name. If this is not set or empty, we treat all instance weights as 1.0.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

nb_model &lt;- iris_training %&gt;%
  ml_naive_bayes(Species ~ .)

pred &lt;- ml_predict(nb_model, iris_test)

ml_multiclass_classification_evaluator(pred)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_naive_bayes_tidiers'>Tidying methods for Spark ML Naive Bayes</h2><span id='topic+ml_naive_bayes_tidiers'></span><span id='topic+tidy.ml_model_naive_bayes'></span><span id='topic+augment.ml_model_naive_bayes'></span><span id='topic+glance.ml_model_naive_bayes'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_naive_bayes'
tidy(x, ...)

## S3 method for class 'ml_model_naive_bayes'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_naive_bayes'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_naive_bayes_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_naive_bayes_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_one_vs_rest'>Spark ML &ndash; OneVsRest</h2><span id='topic+ml_one_vs_rest'></span>

<h3>Description</h3>

<p>Reduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_one_vs_rest(
  x,
  formula = NULL,
  classifier = NULL,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("one_vs_rest_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_one_vs_rest_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_classifier">classifier</code></td>
<td>
<p>Object of class <code>ml_estimator</code>. Base binary classifier that we reduce multiclass classification into.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_one_vs_rest_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_random_forest_classifier">ml_random_forest_classifier</a>()</code>
</p>

<hr>
<h2 id='ml_pca_tidiers'>Tidying methods for Spark ML Principal Component Analysis</h2><span id='topic+ml_pca_tidiers'></span><span id='topic+tidy.ml_model_pca'></span><span id='topic+augment.ml_model_pca'></span><span id='topic+glance.ml_model_pca'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_pca'
tidy(x, ...)

## S3 method for class 'ml_model_pca'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_pca'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_pca_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_pca_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_pca_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_pipeline'>Spark ML &ndash; Pipelines</h2><span id='topic+ml_pipeline'></span>

<h3>Description</h3>

<p>Create Spark ML Pipelines
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_pipeline(x, ..., uid = random_string("pipeline_"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_pipeline_+3A_x">x</code></td>
<td>
<p>Either a <code>spark_connection</code> or <code>ml_pipeline_stage</code> objects</p>
</td></tr>
<tr><td><code id="ml_pipeline_+3A_...">...</code></td>
<td>
<p><code>ml_pipeline_stage</code> objects.</p>
</td></tr>
<tr><td><code id="ml_pipeline_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When <code>x</code> is a <code>spark_connection</code>, <code>ml_pipeline()</code> returns an empty pipeline object. When <code>x</code> is a <code>ml_pipeline_stage</code>, <code>ml_pipeline()</code> returns an <code>ml_pipeline</code> with the stages set to <code>x</code> and any transformers or estimators given in <code>...</code>.
</p>

<hr>
<h2 id='ml_power_iteration'>Spark ML &ndash; Power Iteration Clustering</h2><span id='topic+ml_power_iteration'></span>

<h3>Description</h3>

<p>Power iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in the paper &quot;Power Iteration Clustering&quot; by Frank Lin and William W. Cohen. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_power_iteration(
  x,
  k = 4,
  max_iter = 20,
  init_mode = "random",
  src_col = "src",
  dst_col = "dst",
  weight_col = "weight",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_power_iteration_+3A_x">x</code></td>
<td>
<p>A 'spark_connection'  or a 'tbl_spark'.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_k">k</code></td>
<td>
<p>The number of clusters to create.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to run.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_init_mode">init_mode</code></td>
<td>
<p>This can be either &quot;random&quot;, which is the default, to use a random vector as vertex properties, or &quot;degree&quot; to use normalized sum similarities.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_src_col">src_col</code></td>
<td>
<p>Column in the input Spark dataframe containing 0-based indexes of all source vertices in the affinity matrix described in the PIC paper.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_dst_col">dst_col</code></td>
<td>
<p>Column in the input Spark dataframe containing 0-based indexes of all destination vertices in the affinity matrix described in the PIC paper.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_weight_col">weight_col</code></td>
<td>
<p>Column in the input Spark dataframe containing non-negative edge weights in the affinity matrix described in the PIC paper.</p>
</td></tr>
<tr><td><code id="ml_power_iteration_+3A_...">...</code></td>
<td>
<p>Optional arguments. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2-column R dataframe with columns named &quot;id&quot; and &quot;cluster&quot; describing the resulting cluster assignments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)

sc &lt;- spark_connect(master = "local")

r1 &lt;- 1
n1 &lt;- 80L
r2 &lt;- 4
n2 &lt;- 80L

gen_circle &lt;- function(radius, num_pts) {
  # generate evenly distributed points on a circle centered at the origin
  seq(0, num_pts - 1) %&gt;%
    lapply(
      function(pt) {
        theta &lt;- 2 * pi * pt / num_pts

        radius * c(cos(theta), sin(theta))
      }
    )
}

guassian_similarity &lt;- function(pt1, pt2) {
  dist2 &lt;- sum((pt2 - pt1)^2)

  exp(-dist2 / 2)
}

gen_pic_data &lt;- function() {
  # generate points on 2 concentric circle centered at the origin and then
  # compute pairwise Gaussian similarity values of all unordered pair of
  # points
  n &lt;- n1 + n2
  pts &lt;- append(gen_circle(r1, n1), gen_circle(r2, n2))
  num_unordered_pairs &lt;- n * (n - 1) / 2

  src &lt;- rep(0L, num_unordered_pairs)
  dst &lt;- rep(0L, num_unordered_pairs)
  sim &lt;- rep(0, num_unordered_pairs)

  idx &lt;- 1
  for (i in seq(2, n)) {
    for (j in seq(i - 1)) {
      src[[idx]] &lt;- i - 1L
      dst[[idx]] &lt;- j - 1L
      sim[[idx]] &lt;- guassian_similarity(pts[[i]], pts[[j]])
      idx &lt;- idx + 1
    }
  }

  dplyr::tibble(src = src, dst = dst, sim = sim)
}

pic_data &lt;- copy_to(sc, gen_pic_data())

clusters &lt;- ml_power_iteration(
  pic_data,
  src_col = "src", dst_col = "dst", weight_col = "sim", k = 2, max_iter = 40
)
print(clusters)

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_prefixspan'>Frequent Pattern Mining &ndash; PrefixSpan</h2><span id='topic+ml_prefixspan'></span><span id='topic+ml_freq_seq_patterns'></span>

<h3>Description</h3>

<p>PrefixSpan algorithm for mining frequent itemsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_prefixspan(
  x,
  seq_col = "sequence",
  min_support = 0.1,
  max_pattern_length = 10,
  max_local_proj_db_size = 3.2e+07,
  uid = random_string("prefixspan_"),
  ...
)

ml_freq_seq_patterns(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_prefixspan_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_seq_col">seq_col</code></td>
<td>
<p>The name of the sequence column in dataset (defaults to
&quot;sequence&quot;). Rows with nulls in this column are ignored.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_min_support">min_support</code></td>
<td>
<p>The minimum support required to be considered a frequent
sequential pattern.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_max_pattern_length">max_pattern_length</code></td>
<td>
<p>The maximum length of a frequent sequential
pattern. Any frequent pattern exceeding this length will not be included in
the results.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_max_local_proj_db_size">max_local_proj_db_size</code></td>
<td>
<p>The maximum number of items allowed in a
prefix-projected database before local iterative processing of the
projected database begins. This parameter should be tuned with respect to
the size of your executors.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml_prefixspan_+3A_model">model</code></td>
<td>
<p>A Prefix Span model.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "2.4.0")

items_df &lt;- dplyr::tibble(
  seq = list(
    list(list(1, 2), list(3)),
    list(list(1), list(3, 2), list(1, 2)),
    list(list(1, 2), list(5)),
    list(list(6))
  )
)
items_sdf &lt;- copy_to(sc, items_df, overwrite = TRUE)

prefix_span_model &lt;- ml_prefixspan(
  sc,
  seq_col = "seq",
  min_support = 0.5,
  max_pattern_length = 5,
  max_local_proj_db_size = 32000000
)

frequent_items &lt;- prefix_span_model$frequent_sequential_patterns(items_sdf) %&gt;% collect()

## End(Not run)

</code></pre>

<hr>
<h2 id='ml_random_forest_classifier'>Spark ML &ndash; Random Forest</h2><span id='topic+ml_random_forest_classifier'></span><span id='topic+ml_random_forest'></span><span id='topic+ml_random_forest_regressor'></span>

<h3>Description</h3>

<p>Perform classification and regression using random forests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_random_forest_classifier(
  x,
  formula = NULL,
  num_trees = 20,
  subsampling_rate = 1,
  max_depth = 5,
  min_instances_per_node = 1,
  feature_subset_strategy = "auto",
  impurity = "gini",
  min_info_gain = 0,
  max_bins = 32,
  seed = NULL,
  thresholds = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("random_forest_classifier_"),
  ...
)

ml_random_forest(
  x,
  formula = NULL,
  type = c("auto", "regression", "classification"),
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  feature_subset_strategy = "auto",
  impurity = "auto",
  checkpoint_interval = 10,
  max_bins = 32,
  max_depth = 5,
  num_trees = 20,
  min_info_gain = 0,
  min_instances_per_node = 1,
  subsampling_rate = 1,
  seed = NULL,
  thresholds = NULL,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  uid = random_string("random_forest_"),
  response = NULL,
  features = NULL,
  ...
)

ml_random_forest_regressor(
  x,
  formula = NULL,
  num_trees = 20,
  subsampling_rate = 1,
  max_depth = 5,
  min_instances_per_node = 1,
  feature_subset_strategy = "auto",
  impurity = "variance",
  min_info_gain = 0,
  max_bins = 32,
  seed = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("random_forest_regressor_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_random_forest_classifier_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="#topic+ft_r_formula">ft_r_formula</a> for details.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_num_trees">num_trees</code></td>
<td>
<p>Number of trees to train (&gt;= 1). If 1, then no bootstrapping is used. If &gt; 1, then bootstrapping is done.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_subsampling_rate">subsampling_rate</code></td>
<td>
<p>Fraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_min_instances_per_node">min_instances_per_node</code></td>
<td>
<p>Minimum number of instances each child must
have after split.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_feature_subset_strategy">feature_subset_strategy</code></td>
<td>
<p>The number of features to consider for splits at each tree node. See details for options.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_impurity">impurity</code></td>
<td>
<p>Criterion used for information gain calculation. Supported: &quot;entropy&quot;
and &quot;gini&quot; (default) for classification and &quot;variance&quot; (default) for regression. For
<code>ml_decision_tree</code>, setting <code>"auto"</code> will default to the appropriate
criterion based on model type.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_min_info_gain">min_info_gain</code></td>
<td>
<p>Minimum information gain for a split to be considered
at a tree node. Should be &gt;= 0, defaults to 0.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_max_bins">max_bins</code></td>
<td>
<p>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. More bins give higher granularity.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_seed">seed</code></td>
<td>
<p>Seed for random numbers.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_thresholds">thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_checkpoint_interval">checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10
iterations, defaults to 10.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_cache_node_ids">cache_node_ids</code></td>
<td>
<p>If <code>FALSE</code>, the algorithm will pass trees to
executors to match instances with nodes. If <code>TRUE</code>, the algorithm will
cache node IDs for each instance. Caching can speed up training of deeper
trees. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_max_memory_in_mb">max_memory_in_mb</code></td>
<td>
<p>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration, and its aggregates
may exceed this size. Defaults to 256.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_...">...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_type">type</code></td>
<td>
<p>The type of model to fit. <code>"regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. When <code>"auto"</code> is used, the model type is
inferred based on the response variable type &ndash; if it is a numeric type,
then regression is used; classification otherwise.</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_response">response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td></tr>
<tr><td><code id="ml_random_forest_classifier_+3A_features">features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported options for <code>feature_subset_strategy</code> are
</p>

<ul>
<li> <p><code>"auto"</code>: Choose automatically for task: If <code>num_trees == 1</code>, set to <code>"all"</code>. If <code>num_trees &gt; 1</code> (forest), set to <code>"sqrt"</code> for classification and to <code>"onethird"</code> for regression.
</p>
</li>
<li> <p><code>"all"</code>: use all features
</p>
</li>
<li> <p><code>"onethird"</code>: use 1/3 of the features
</p>
</li>
<li> <p><code>"sqrt"</code>: use use sqrt(number of features)
</p>
</li>
<li> <p><code>"log2"</code>: use log2(number of features)
</p>
</li>
<li> <p><code>"n"</code>: when <code>n</code> is in the range (0, 1.0], use n * number of features. When <code>n</code> is in the range (1, number of features), use <code>n</code> features. (default = <code>"auto"</code>)
</p>
</li></ul>

<p><code>ml_random_forest</code> is a wrapper around <code>ml_random_forest_regressor.tbl_spark</code> and <code>ml_random_forest_classifier.tbl_spark</code> and calls the appropriate method based on model type.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code><a href="#topic+ml_aft_survival_regression">ml_aft_survival_regression</a>()</code>,
<code><a href="#topic+ml_decision_tree_classifier">ml_decision_tree_classifier</a>()</code>,
<code><a href="#topic+ml_gbt_classifier">ml_gbt_classifier</a>()</code>,
<code><a href="#topic+ml_generalized_linear_regression">ml_generalized_linear_regression</a>()</code>,
<code><a href="#topic+ml_isotonic_regression">ml_isotonic_regression</a>()</code>,
<code><a href="#topic+ml_linear_regression">ml_linear_regression</a>()</code>,
<code><a href="#topic+ml_linear_svc">ml_linear_svc</a>()</code>,
<code><a href="#topic+ml_logistic_regression">ml_logistic_regression</a>()</code>,
<code><a href="#topic+ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier</a>()</code>,
<code><a href="#topic+ml_naive_bayes">ml_naive_bayes</a>()</code>,
<code><a href="#topic+ml_one_vs_rest">ml_one_vs_rest</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

rf_model &lt;- iris_training %&gt;%
  ml_random_forest(Species ~ ., type = "classification")

pred &lt;- ml_predict(rf_model, iris_test)

ml_multiclass_classification_evaluator(pred)

## End(Not run)
</code></pre>

<hr>
<h2 id='ml_stage'>Spark ML &ndash; Pipeline stage extraction</h2><span id='topic+ml_stage'></span><span id='topic+ml_stages'></span>

<h3>Description</h3>

<p>Extraction of stages from a Pipeline or PipelineModel object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_stage(x, stage)

ml_stages(x, stages = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_stage_+3A_x">x</code></td>
<td>
<p>A <code>ml_pipeline</code> or a <code>ml_pipeline_model</code> object</p>
</td></tr>
<tr><td><code id="ml_stage_+3A_stage">stage</code></td>
<td>
<p>The UID of a stage in the pipeline.</p>
</td></tr>
<tr><td><code id="ml_stage_+3A_stages">stages</code></td>
<td>
<p>The UIDs of stages in the pipeline as a character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>ml_stage()</code>: The stage specified.
</p>
<p>For <code>ml_stages()</code>: A list of stages. If <code>stages</code> is not set, the function returns all stages of the pipeline in a list.
</p>

<hr>
<h2 id='ml_standardize_formula'>Standardize Formula Input for 'ml_model'</h2><span id='topic+ml_standardize_formula'></span>

<h3>Description</h3>

<p>Generates a formula string from user inputs, to be used in 'ml_model' constructor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_standardize_formula(formula = NULL, response = NULL, features = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_standardize_formula_+3A_formula">formula</code></td>
<td>
<p>The 'formula' argument.</p>
</td></tr>
<tr><td><code id="ml_standardize_formula_+3A_response">response</code></td>
<td>
<p>The 'response' argument.</p>
</td></tr>
<tr><td><code id="ml_standardize_formula_+3A_features">features</code></td>
<td>
<p>The 'features' argument.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_summary'>Spark ML &ndash; Extraction of summary metrics</h2><span id='topic+ml_summary'></span>

<h3>Description</h3>

<p>Extracts a metric from the summary object of a Spark ML model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_summary(x, metric = NULL, allow_null = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_summary_+3A_x">x</code></td>
<td>
<p>A Spark ML model that has a summary.</p>
</td></tr>
<tr><td><code id="ml_summary_+3A_metric">metric</code></td>
<td>
<p>The name of the metric to extract. If not set, returns the summary object.</p>
</td></tr>
<tr><td><code id="ml_summary_+3A_allow_null">allow_null</code></td>
<td>
<p>Whether null results are allowed when the metric is not found in the summary.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_supervised_pipeline'>Constructors for 'ml_model' Objects</h2><span id='topic+ml_supervised_pipeline'></span><span id='topic+ml_clustering_pipeline'></span><span id='topic+ml_construct_model_supervised'></span><span id='topic+ml_construct_model_clustering'></span><span id='topic+ml-model-constructors'></span><span id='topic+new_ml_model_prediction'></span><span id='topic+new_ml_model'></span><span id='topic+new_ml_model_classification'></span><span id='topic+new_ml_model_regression'></span><span id='topic+new_ml_model_clustering'></span>

<h3>Description</h3>

<p>Functions for developers writing extensions for Spark ML. These functions are constructors
for 'ml_model' objects that are returned when using the formula interface.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_supervised_pipeline(predictor, dataset, formula, features_col, label_col)

ml_clustering_pipeline(predictor, dataset, formula, features_col)

ml_construct_model_supervised(
  constructor,
  predictor,
  formula,
  dataset,
  features_col,
  label_col,
  ...
)

ml_construct_model_clustering(
  constructor,
  predictor,
  formula,
  dataset,
  features_col,
  ...
)

new_ml_model_prediction(
  pipeline_model,
  formula,
  dataset,
  label_col,
  features_col,
  ...,
  class = character()
)

new_ml_model(pipeline_model, formula, dataset, ..., class = character())

new_ml_model_classification(
  pipeline_model,
  formula,
  dataset,
  label_col,
  features_col,
  predicted_label_col,
  ...,
  class = character()
)

new_ml_model_regression(
  pipeline_model,
  formula,
  dataset,
  label_col,
  features_col,
  ...,
  class = character()
)

new_ml_model_clustering(
  pipeline_model,
  formula,
  dataset,
  features_col,
  ...,
  class = character()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_supervised_pipeline_+3A_predictor">predictor</code></td>
<td>
<p>The pipeline stage corresponding to the ML algorithm.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_dataset">dataset</code></td>
<td>
<p>The training dataset.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_formula">formula</code></td>
<td>
<p>The formula used for data preprocessing</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_constructor">constructor</code></td>
<td>
<p>The constructor function for the 'ml_model'.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_pipeline_model">pipeline_model</code></td>
<td>
<p>The pipeline model object returned by 'ml_supervised_pipeline()'.</p>
</td></tr>
<tr><td><code id="ml_supervised_pipeline_+3A_class">class</code></td>
<td>
<p>Name of the subclass.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_survival_regression_tidiers'>Tidying methods for Spark ML Survival Regression</h2><span id='topic+ml_survival_regression_tidiers'></span><span id='topic+tidy.ml_model_aft_survival_regression'></span><span id='topic+augment.ml_model_aft_survival_regression'></span><span id='topic+glance.ml_model_aft_survival_regression'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_aft_survival_regression'
tidy(x, ...)

## S3 method for class 'ml_model_aft_survival_regression'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_aft_survival_regression'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_survival_regression_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_survival_regression_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_survival_regression_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_tree_tidiers'>Tidying methods for Spark ML tree models</h2><span id='topic+ml_tree_tidiers'></span><span id='topic+tidy.ml_model_decision_tree_classification'></span><span id='topic+tidy.ml_model_decision_tree_regression'></span><span id='topic+augment.ml_model_decision_tree_classification'></span><span id='topic+augment._ml_model_decision_tree_classification'></span><span id='topic+augment.ml_model_decision_tree_regression'></span><span id='topic+augment._ml_model_decision_tree_regression'></span><span id='topic+glance.ml_model_decision_tree_classification'></span><span id='topic+glance.ml_model_decision_tree_regression'></span><span id='topic+tidy.ml_model_random_forest_classification'></span><span id='topic+tidy.ml_model_random_forest_regression'></span><span id='topic+augment.ml_model_random_forest_classification'></span><span id='topic+augment._ml_model_random_forest_classification'></span><span id='topic+augment.ml_model_random_forest_regression'></span><span id='topic+augment._ml_model_random_forest_regression'></span><span id='topic+glance.ml_model_random_forest_classification'></span><span id='topic+glance.ml_model_random_forest_regression'></span><span id='topic+tidy.ml_model_gbt_classification'></span><span id='topic+tidy.ml_model_gbt_regression'></span><span id='topic+augment.ml_model_gbt_classification'></span><span id='topic+augment._ml_model_gbt_classification'></span><span id='topic+augment.ml_model_gbt_regression'></span><span id='topic+augment._ml_model_gbt_regression'></span><span id='topic+glance.ml_model_gbt_classification'></span><span id='topic+glance.ml_model_gbt_regression'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_decision_tree_classification'
tidy(x, ...)

## S3 method for class 'ml_model_decision_tree_regression'
tidy(x, ...)

## S3 method for class 'ml_model_decision_tree_classification'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_decision_tree_classification''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_decision_tree_regression'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_decision_tree_regression''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_decision_tree_classification'
glance(x, ...)

## S3 method for class 'ml_model_decision_tree_regression'
glance(x, ...)

## S3 method for class 'ml_model_random_forest_classification'
tidy(x, ...)

## S3 method for class 'ml_model_random_forest_regression'
tidy(x, ...)

## S3 method for class 'ml_model_random_forest_classification'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_random_forest_classification''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_random_forest_regression'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_random_forest_regression''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_random_forest_classification'
glance(x, ...)

## S3 method for class 'ml_model_random_forest_regression'
glance(x, ...)

## S3 method for class 'ml_model_gbt_classification'
tidy(x, ...)

## S3 method for class 'ml_model_gbt_regression'
tidy(x, ...)

## S3 method for class 'ml_model_gbt_classification'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_gbt_classification''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_gbt_regression'
augment(x, newdata = NULL, ...)

## S3 method for class ''_ml_model_gbt_regression''
augment(x, new_data = NULL, ...)

## S3 method for class 'ml_model_gbt_classification'
glance(x, ...)

## S3 method for class 'ml_model_gbt_regression'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_tree_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_tree_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_tree_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
<tr><td><code id="ml_tree_tidiers_+3A_new_data">new_data</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml_uid'>Spark ML &ndash; UID</h2><span id='topic+ml_uid'></span>

<h3>Description</h3>

<p>Extracts the UID of an ML object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_uid(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_uid_+3A_x">x</code></td>
<td>
<p>A Spark ML object</p>
</td></tr>
</table>

<hr>
<h2 id='ml_unsupervised_tidiers'>Tidying methods for Spark ML unsupervised models</h2><span id='topic+ml_unsupervised_tidiers'></span><span id='topic+tidy.ml_model_kmeans'></span><span id='topic+augment.ml_model_kmeans'></span><span id='topic+glance.ml_model_kmeans'></span><span id='topic+tidy.ml_model_bisecting_kmeans'></span><span id='topic+augment.ml_model_bisecting_kmeans'></span><span id='topic+glance.ml_model_bisecting_kmeans'></span><span id='topic+tidy.ml_model_gaussian_mixture'></span><span id='topic+augment.ml_model_gaussian_mixture'></span><span id='topic+glance.ml_model_gaussian_mixture'></span>

<h3>Description</h3>

<p>These methods summarize the results of Spark ML models into tidy forms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_kmeans'
tidy(x, ...)

## S3 method for class 'ml_model_kmeans'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_kmeans'
glance(x, ...)

## S3 method for class 'ml_model_bisecting_kmeans'
tidy(x, ...)

## S3 method for class 'ml_model_bisecting_kmeans'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_bisecting_kmeans'
glance(x, ...)

## S3 method for class 'ml_model_gaussian_mixture'
tidy(x, ...)

## S3 method for class 'ml_model_gaussian_mixture'
augment(x, newdata = NULL, ...)

## S3 method for class 'ml_model_gaussian_mixture'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_unsupervised_tidiers_+3A_x">x</code></td>
<td>
<p>a Spark ML model.</p>
</td></tr>
<tr><td><code id="ml_unsupervised_tidiers_+3A_...">...</code></td>
<td>
<p>extra arguments (not used.)</p>
</td></tr>
<tr><td><code id="ml_unsupervised_tidiers_+3A_newdata">newdata</code></td>
<td>
<p>a tbl_spark of new data to use for prediction.</p>
</td></tr>
</table>

<hr>
<h2 id='ml-constructors'>Constructors for Pipeline Stages</h2><span id='topic+ml-constructors'></span><span id='topic+new_ml_transformer'></span><span id='topic+new_ml_prediction_model'></span><span id='topic+new_ml_classification_model'></span><span id='topic+new_ml_probabilistic_classification_model'></span><span id='topic+new_ml_clustering_model'></span><span id='topic+new_ml_estimator'></span><span id='topic+new_ml_predictor'></span><span id='topic+new_ml_classifier'></span><span id='topic+new_ml_probabilistic_classifier'></span>

<h3>Description</h3>

<p>Functions for developers writing extensions for Spark ML.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_ml_transformer(jobj, ..., class = character())

new_ml_prediction_model(jobj, ..., class = character())

new_ml_classification_model(jobj, ..., class = character())

new_ml_probabilistic_classification_model(jobj, ..., class = character())

new_ml_clustering_model(jobj, ..., class = character())

new_ml_estimator(jobj, ..., class = character())

new_ml_predictor(jobj, ..., class = character())

new_ml_classifier(jobj, ..., class = character())

new_ml_probabilistic_classifier(jobj, ..., class = character())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml-constructors_+3A_jobj">jobj</code></td>
<td>
<p>Pointer to the pipeline stage object.</p>
</td></tr>
<tr><td><code id="ml-constructors_+3A_...">...</code></td>
<td>
<p>(Optional) additional attributes of the object.</p>
</td></tr>
<tr><td><code id="ml-constructors_+3A_class">class</code></td>
<td>
<p>Name of class.</p>
</td></tr>
</table>

<hr>
<h2 id='ml-params'>Spark ML &ndash; ML Params</h2><span id='topic+ml-params'></span><span id='topic+ml_is_set'></span><span id='topic+ml_param_map'></span><span id='topic+ml_param'></span><span id='topic+ml_params'></span>

<h3>Description</h3>

<p>Helper methods for working with parameters for ML objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_is_set(x, param, ...)

ml_param_map(x, ...)

ml_param(x, param, allow_null = FALSE, ...)

ml_params(x, params = NULL, allow_null = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml-params_+3A_x">x</code></td>
<td>
<p>A Spark ML object, either a pipeline stage or an evaluator.</p>
</td></tr>
<tr><td><code id="ml-params_+3A_param">param</code></td>
<td>
<p>The parameter to extract or set.</p>
</td></tr>
<tr><td><code id="ml-params_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml-params_+3A_allow_null">allow_null</code></td>
<td>
<p>Whether to allow <code>NULL</code> results when extracting parameters. If <code>FALSE</code>, an error will be thrown if the specified parameter is not found. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ml-params_+3A_params">params</code></td>
<td>
<p>A vector of parameters to extract.</p>
</td></tr>
</table>

<hr>
<h2 id='ml-persistence'>Spark ML &ndash; Model Persistence</h2><span id='topic+ml-persistence'></span><span id='topic+ml_save'></span><span id='topic+ml_save.ml_model'></span><span id='topic+ml_load'></span>

<h3>Description</h3>

<p>Save/load Spark ML objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_save(x, path, overwrite = FALSE, ...)

## S3 method for class 'ml_model'
ml_save(
  x,
  path,
  overwrite = FALSE,
  type = c("pipeline_model", "pipeline"),
  ...
)

ml_load(sc, path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml-persistence_+3A_x">x</code></td>
<td>
<p>A ML object, which could be a <code>ml_pipeline_stage</code> or a <code>ml_model</code></p>
</td></tr>
<tr><td><code id="ml-persistence_+3A_path">path</code></td>
<td>
<p>The path where the object is to be serialized/deserialized.</p>
</td></tr>
<tr><td><code id="ml-persistence_+3A_overwrite">overwrite</code></td>
<td>
<p>Whether to overwrite the existing path, defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ml-persistence_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml-persistence_+3A_type">type</code></td>
<td>
<p>Whether to save the pipeline model or the pipeline.</p>
</td></tr>
<tr><td><code id="ml-persistence_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ml_save()</code> serializes a Spark object into a format that can be read back into <code>sparklyr</code> or by the Scala or PySpark APIs. When called on <code>ml_model</code> objects, i.e. those that were created via the <code>tbl_spark - formula</code> signature, the associated pipeline model is serialized. In other words, the saved model contains both the data processing (<code>RFormulaModel</code>) stage and the machine learning stage.
</p>
<p><code>ml_load()</code> reads a saved Spark object into <code>sparklyr</code>. It calls the correct Scala <code>load</code> method based on parsing the saved metadata. Note that a <code>PipelineModel</code> object saved from a sparklyr <code>ml_model</code> via <code>ml_save()</code> will be read back in as an <code>ml_pipeline_model</code>, rather than the <code>ml_model</code> object.
</p>

<hr>
<h2 id='ml-transform-methods'>Spark ML &ndash; Transform, fit, and predict methods (ml_ interface)</h2><span id='topic+ml-transform-methods'></span><span id='topic+is_ml_transformer'></span><span id='topic+is_ml_estimator'></span><span id='topic+ml_fit'></span><span id='topic+ml_fit.default'></span><span id='topic+ml_transform'></span><span id='topic+ml_fit_and_transform'></span><span id='topic+ml_predict'></span><span id='topic+ml_predict.ml_model_classification'></span>

<h3>Description</h3>

<p>Methods for transformation, fit, and prediction. These are mirrors of the corresponding <a href="#topic+sdf-transform-methods">sdf-transform-methods</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_ml_transformer(x)

is_ml_estimator(x)

ml_fit(x, dataset, ...)

## Default S3 method:
ml_fit(x, dataset, ...)

ml_transform(x, dataset, ...)

ml_fit_and_transform(x, dataset, ...)

ml_predict(x, dataset, ...)

## S3 method for class 'ml_model_classification'
ml_predict(x, dataset, probability_prefix = "probability_", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml-transform-methods_+3A_x">x</code></td>
<td>
<p>A <code>ml_estimator</code>, <code>ml_transformer</code> (or a list thereof), or <code>ml_model</code> object.</p>
</td></tr>
<tr><td><code id="ml-transform-methods_+3A_dataset">dataset</code></td>
<td>
<p>A <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml-transform-methods_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml-transform-methods_+3A_probability_prefix">probability_prefix</code></td>
<td>
<p>String used to prepend the class probability output columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These methods are
</p>


<h3>Value</h3>

<p>When <code>x</code> is an estimator, <code>ml_fit()</code> returns a transformer whereas <code>ml_fit_and_transform()</code> returns a transformed dataset. When <code>x</code> is a transformer, <code>ml_transform()</code> and <code>ml_predict()</code> return a transformed dataset. When <code>ml_predict()</code> is called on a <code>ml_model</code> object, additional columns (e.g. probabilities in case of classification models) are appended to the transformed output for the user's convenience.
</p>

<hr>
<h2 id='ml-tuning'>Spark ML &ndash; Tuning</h2><span id='topic+ml-tuning'></span><span id='topic+ml_sub_models'></span><span id='topic+ml_validation_metrics'></span><span id='topic+ml_cross_validator'></span><span id='topic+ml_train_validation_split'></span>

<h3>Description</h3>

<p>Perform hyper-parameter tuning using either K-fold cross validation or train-validation split.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_sub_models(model)

ml_validation_metrics(model)

ml_cross_validator(
  x,
  estimator = NULL,
  estimator_param_maps = NULL,
  evaluator = NULL,
  num_folds = 3,
  collect_sub_models = FALSE,
  parallelism = 1,
  seed = NULL,
  uid = random_string("cross_validator_"),
  ...
)

ml_train_validation_split(
  x,
  estimator = NULL,
  estimator_param_maps = NULL,
  evaluator = NULL,
  train_ratio = 0.75,
  collect_sub_models = FALSE,
  parallelism = 1,
  seed = NULL,
  uid = random_string("train_validation_split_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml-tuning_+3A_model">model</code></td>
<td>
<p>A cross validation or train-validation-split model.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_estimator">estimator</code></td>
<td>
<p>A <code>ml_estimator</code> object.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_estimator_param_maps">estimator_param_maps</code></td>
<td>
<p>A named list of stages and hyper-parameter sets to tune. See details.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_evaluator">evaluator</code></td>
<td>
<p>A <code>ml_evaluator</code> object, see <a href="#topic+ml_evaluator">ml_evaluator</a>.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_num_folds">num_folds</code></td>
<td>
<p>Number of folds for cross validation. Must be &gt;= 2. Default: 3</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_collect_sub_models">collect_sub_models</code></td>
<td>
<p>Whether to collect a list of sub-models trained during tuning.
If set to <code>FALSE</code>, then only the single best sub-model will be available after fitting.
If set to true, then all sub-models will be available. Warning: For large models, collecting
all sub-models can cause OOMs on the Spark driver.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_parallelism">parallelism</code></td>
<td>
<p>The number of threads to use when running parallel algorithms. Default is 1 for serial execution.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="ml-tuning_+3A_train_ratio">train_ratio</code></td>
<td>
<p>Ratio between train and validation data. Must be between 0 and 1. Default: 0.75</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ml_cross_validator()</code> performs k-fold cross validation while <code>ml_train_validation_split()</code> performs tuning on one pair of train and validation datasets.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>.
</p>

<ul>
<li> <p><code>spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_cross_validator</code> or <code>ml_traing_validation_split</code> object.
</p>
</li>
<li> <p><code>ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
the tuning estimator appended to the pipeline.
</p>
</li>
<li> <p><code>tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a tuning estimator is constructed then
immediately fit with the input <code>tbl_spark</code>, returning a <code>ml_cross_validation_model</code> or a
<code>ml_train_validation_split_model</code> object.
</p>
</li></ul>

<p>For cross validation, <code>ml_sub_models()</code> returns a nested
list of models, where the first layer represents fold indices and the
second layer represents param maps. For train-validation split,
<code>ml_sub_models()</code> returns a list of models, corresponding to the
order of the estimator param maps.
</p>
<p><code>ml_validation_metrics()</code> returns a data frame of performance
metrics and hyperparameter combinations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

# Create a pipeline
pipeline &lt;- ml_pipeline(sc) %&gt;%
  ft_r_formula(Species ~ .) %&gt;%
  ml_random_forest_classifier()

# Specify hyperparameter grid
grid &lt;- list(
  random_forest = list(
    num_trees = c(5, 10),
    max_depth = c(5, 10),
    impurity = c("entropy", "gini")
  )
)

# Create the cross validator object
cv &lt;- ml_cross_validator(
  sc,
  estimator = pipeline, estimator_param_maps = grid,
  evaluator = ml_multiclass_classification_evaluator(sc),
  num_folds = 3,
  parallelism = 4
)

# Train the models
cv_model &lt;- ml_fit(cv, iris_tbl)

# Print the metrics
ml_validation_metrics(cv_model)

## End(Not run)

</code></pre>

<hr>
<h2 id='mutate'>Mutate</h2><span id='topic+mutate'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+mutate">mutate</a></code> for more details.
</p>

<hr>
<h2 id='na.replace'>Replace Missing Values in Objects</h2><span id='topic+na.replace'></span>

<h3>Description</h3>

<p>This S3 generic provides an interface for replacing
<code><a href="base.html#topic+NA">NA</a></code> values within an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.replace(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="na.replace_+3A_object">object</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="na.replace_+3A_...">...</code></td>
<td>
<p>Arguments passed along to implementing methods.</p>
</td></tr>
</table>

<hr>
<h2 id='nest'>Nest</h2><span id='topic+nest'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+nest">nest</a></code> for more details.
</p>

<hr>
<h2 id='pivot_longer'>Pivot longer</h2><span id='topic+pivot_longer'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+pivot_longer">pivot_longer</a></code> for more details.
</p>

<hr>
<h2 id='pivot_wider'>Pivot wider</h2><span id='topic+pivot_wider'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+pivot_wider">pivot_wider</a></code> for more details.
</p>

<hr>
<h2 id='print_jobj'>Generic method for print jobj for a connection type</h2><span id='topic+print_jobj'></span>

<h3>Description</h3>

<p>Generic method for print jobj for a connection type
</p>


<h3>Usage</h3>

<pre><code class='language-R'>print_jobj(sc, jobj, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print_jobj_+3A_sc">sc</code></td>
<td>
<p><code>spark_connection</code> (used for type dispatch)</p>
</td></tr>
<tr><td><code id="print_jobj_+3A_jobj">jobj</code></td>
<td>
<p>Object to print</p>
</td></tr>
</table>

<hr>
<h2 id='quote_sql_name'>Translate input character vector or symbol to a SQL identifier</h2><span id='topic+quote_sql_name'></span>

<h3>Description</h3>

<p>Calls dbplyr::translate_sql_ on the input character vector or symbol to obtain
the corresponding SQL identifier that is escaped and quoted properly
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quote_sql_name(x, con = NULL)
</code></pre>

<hr>
<h2 id='random_string'>Random string generation</h2><span id='topic+random_string'></span>

<h3>Description</h3>

<p>Generate a random string with a given prefix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_string(prefix = "table")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="random_string_+3A_prefix">prefix</code></td>
<td>
<p>A length-one character vector.</p>
</td></tr>
</table>

<hr>
<h2 id='reactiveSpark'>Reactive spark reader</h2><span id='topic+reactiveSpark'></span>

<h3>Description</h3>

<p>Given a spark object, returns a reactive data source for the contents
of the spark object. This function is most useful to read Spark streams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reactiveSpark(x, intervalMillis = 1000, session = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reactiveSpark_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="reactiveSpark_+3A_intervalmillis">intervalMillis</code></td>
<td>
<p>Approximate number of milliseconds to wait to retrieve
updated data frame. This can be a numeric value, or a function that returns
a numeric value.</p>
</td></tr>
<tr><td><code id="reactiveSpark_+3A_session">session</code></td>
<td>
<p>The user session to associate this file reader with, or NULL if
none. If non-null, the reader will automatically stop when the session ends.</p>
</td></tr>
</table>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tidy'></span><span id='topic+augment'></span><span id='topic+glance'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+glance">glance</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
</dl>

<hr>
<h2 id='register_extension'>Register a Package that Implements a Spark Extension</h2><span id='topic+register_extension'></span><span id='topic+registered_extensions'></span>

<h3>Description</h3>

<p>Registering an extension package will result in the package being
automatically scanned for spark dependencies when a connection to Spark is
created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_extension(package)

registered_extensions()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_extension_+3A_package">package</code></td>
<td>
<p>The package(s) to register.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Packages should typically register their extensions in their
<code>.onLoad</code> hook &ndash; this ensures that their extensions are registered
when their namespaces are loaded.
</p>

<hr>
<h2 id='registerDoSpark'>Register a Parallel Backend</h2><span id='topic+registerDoSpark'></span>

<h3>Description</h3>

<p>Registers a parallel backend using the <code>foreach</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>registerDoSpark(spark_conn, parallelism = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="registerDoSpark_+3A_spark_conn">spark_conn</code></td>
<td>
<p>Spark connection to use</p>
</td></tr>
<tr><td><code id="registerDoSpark_+3A_parallelism">parallelism</code></td>
<td>
<p>Level of parallelism to use for task execution
(if unspecified, then it will take the value of
'SparkContext.defaultParallelism()' which by default is the number
of cores available to the 'sparklyr' application)</p>
</td></tr>
<tr><td><code id="registerDoSpark_+3A_...">...</code></td>
<td>
<p>additional options for sparklyr parallel backend
(currently only the only valid option is 'nocompile')</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

sc &lt;- spark_connect(master = "local")
registerDoSpark(sc, nocompile = FALSE)

## End(Not run)

</code></pre>

<hr>
<h2 id='replace_na'>Replace NA</h2><span id='topic+replace_na'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+replace_na">replace_na</a></code> for more details.
</p>

<hr>
<h2 id='right_join'>Right join</h2><span id='topic+right_join'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+mutate-joins">right_join</a></code> for more details.
</p>

<hr>
<h2 id='sdf_along'>Create DataFrame for along Object</h2><span id='topic+sdf_along'></span>

<h3>Description</h3>

<p>Creates a DataFrame along the given object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_along(sc, along, repartition = NULL, type = c("integer", "integer64"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_along_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_along_+3A_along">along</code></td>
<td>
<p>Takes the length from the length of this argument.</p>
</td></tr>
<tr><td><code id="sdf_along_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
data across the Spark cluster.</p>
</td></tr>
<tr><td><code id="sdf_along_+3A_type">type</code></td>
<td>
<p>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_bind'>Bind multiple Spark DataFrames by row and column</h2><span id='topic+sdf_bind'></span><span id='topic+sdf_bind_rows'></span><span id='topic+sdf_bind_cols'></span>

<h3>Description</h3>

<p><code>sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> are implementation of the common pattern of
<code>do.call(rbind, sdfs)</code> or <code>do.call(cbind, sdfs)</code> for binding many
Spark DataFrames into one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_bind_rows(..., id = NULL)

sdf_bind_cols(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_bind_+3A_...">...</code></td>
<td>
<p>Spark tbls to combine.
</p>
<p>Each argument can either be a Spark DataFrame or a list of
Spark DataFrames
</p>
<p>When row-binding, columns are matched by name, and any missing
columns with be filled with NA.
</p>
<p>When column-binding, rows are matched by position, so all data
frames must have the same number of rows.</p>
</td></tr>
<tr><td><code id="sdf_bind_+3A_id">id</code></td>
<td>
<p>Data frame identifier.
</p>
<p>When <code>id</code> is supplied, a new column of identifiers is
created to link each row to its original Spark DataFrame. The labels
are taken from the named arguments to <code>sdf_bind_rows()</code>. When a
list of Spark DataFrames is supplied, the labels are taken from the
names of the list. If no names are found a numeric sequence is
used instead.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of <code>sdf_bind_rows()</code> will contain a column if that column
appears in any of the inputs.
</p>


<h3>Value</h3>

<p><code>sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> return <code>tbl_spark</code>
</p>

<hr>
<h2 id='sdf_broadcast'>Broadcast hint</h2><span id='topic+sdf_broadcast'></span>

<h3>Description</h3>

<p>Used to force broadcast hash joins.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_broadcast(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_broadcast_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_checkpoint'>Checkpoint a Spark DataFrame</h2><span id='topic+sdf_checkpoint'></span>

<h3>Description</h3>

<p>Checkpoint a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_checkpoint(x, eager = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_checkpoint_+3A_x">x</code></td>
<td>
<p>an object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_checkpoint_+3A_eager">eager</code></td>
<td>
<p>whether to truncate the lineage of the DataFrame</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_coalesce'>Coalesces a Spark DataFrame</h2><span id='topic+sdf_coalesce'></span>

<h3>Description</h3>

<p>Coalesces a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_coalesce(x, partitions)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_coalesce_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_coalesce_+3A_partitions">partitions</code></td>
<td>
<p>number of partitions</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_collect'>Collect a Spark DataFrame into R.</h2><span id='topic+sdf_collect'></span>

<h3>Description</h3>

<p>Collects a Spark dataframe into R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_collect(object, impl = c("row-wise", "row-wise-iter", "column-wise"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_collect_+3A_object">object</code></td>
<td>
<p>Spark dataframe to collect</p>
</td></tr>
<tr><td><code id="sdf_collect_+3A_impl">impl</code></td>
<td>
<p>Which implementation to use while collecting Spark dataframe
- row-wise: fetch the entire dataframe into memory and then process it row-by-row
- row-wise-iter: iterate through the dataframe using RDD local iterator, processing one row at
a time (hence reducing memory footprint)
- column-wise: fetch the entire dataframe into memory and then process it column-by-column
NOTE: (1) this will not apply to streaming or arrow use cases (2) this parameter will only affect
implementation detail, and will not affect result of 'sdf_collect', and should only be set if
performance profiling indicates any particular choice will be significantly better than the default
choice (&quot;row-wise&quot;)</p>
</td></tr>
<tr><td><code id="sdf_collect_+3A_...">...</code></td>
<td>
<p>Additional options.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_copy_to'>Copy an Object into Spark</h2><span id='topic+sdf_copy_to'></span><span id='topic+sdf_import'></span>

<h3>Description</h3>

<p>Copy an object into Spark, and return an <span class="rlang"><b>R</b></span> object wrapping the
copied object (typically, a Spark DataFrame).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_copy_to(sc, x, name, memory, repartition, overwrite, struct_columns, ...)

sdf_import(x, sc, name, memory, repartition, overwrite, struct_columns, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_copy_to_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object from which a Spark DataFrame can be generated.</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_name">name</code></td>
<td>
<p>The name to assign to the copied table in Spark.</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the table be cached into memory?</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
table across the Spark cluster. The default (0) can be used to avoid
partitioning.</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite a pre-existing table with the name <code>name</code>
if one already exists?</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_struct_columns">struct_columns</code></td>
<td>
<p>(only supported with Spark 2.4.0 or higher) A list of
columns from the source data frame that should be converted to Spark SQL
StructType columns.
The source columns can contain either json strings or nested lists.
All rows within each source column should have identical schemas (because
otherwise the conversion result will contain unexpected null values or
missing values as Spark currently does not support schema discovery on
individual rows within a struct column).</p>
</td></tr>
<tr><td><code id="sdf_copy_to_+3A_...">...</code></td>
<td>
<p>Optional arguments, passed to implementing methods.</p>
</td></tr>
</table>


<h3>Advanced Usage</h3>

<p><code>sdf_copy_to</code> is an S3 generic that, by default, dispatches to
<code>sdf_import</code>. Package authors that would like to implement
<code>sdf_copy_to</code> for a custom object type can accomplish this by
implementing the associated method on <code>sdf_import</code>.
</p>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
sc &lt;- spark_connect(master = "spark://HOST:PORT")
sdf_copy_to(sc, iris)

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_crosstab'>Cross Tabulation</h2><span id='topic+sdf_crosstab'></span>

<h3>Description</h3>

<p>Builds a contingency table at each combination of factor levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_crosstab(x, col1, col2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_crosstab_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_crosstab_+3A_col1">col1</code></td>
<td>
<p>The name of the first column. Distinct items will make the first item of each row.</p>
</td></tr>
<tr><td><code id="sdf_crosstab_+3A_col2">col2</code></td>
<td>
<p>The name of the second column. Distinct items will make the column names of the DataFrame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A DataFrame containing the contingency table.
</p>

<hr>
<h2 id='sdf_debug_string'>Debug Info for Spark DataFrame</h2><span id='topic+sdf_debug_string'></span>

<h3>Description</h3>

<p>Prints plan of execution to generate <code>x</code>. This plan will, among other things, show the
number of partitions in parenthesis at the far left and indicate stages using indentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_debug_string(x, print = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_debug_string_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object wrapping, or containing, a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_debug_string_+3A_print">print</code></td>
<td>
<p>Print debug information?</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_describe'>Compute summary statistics for columns of a data frame</h2><span id='topic+sdf_describe'></span>

<h3>Description</h3>

<p>Compute summary statistics for columns of a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_describe(x, cols = colnames(x))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_describe_+3A_x">x</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_describe_+3A_cols">cols</code></td>
<td>
<p>Columns to compute statistics for, given as a character vector</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_dim'>Support for Dimension Operations</h2><span id='topic+sdf_dim'></span><span id='topic+sdf_nrow'></span><span id='topic+sdf_ncol'></span>

<h3>Description</h3>

<p><code>sdf_dim()</code>,  <code>sdf_nrow()</code> and <code>sdf_ncol()</code> provide similar
functionality to <code>dim()</code>, <code>nrow()</code> and <code>ncol()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_dim(x)

sdf_nrow(x)

sdf_ncol(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_dim_+3A_x">x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>).</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_distinct'>Invoke distinct on a Spark DataFrame</h2><span id='topic+sdf_distinct'></span>

<h3>Description</h3>

<p>Invoke distinct on a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_distinct(x, ..., name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_distinct_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_distinct_+3A_...">...</code></td>
<td>
<p>Optional variables to use when determining uniqueness.
If there are multiple rows for a given combination of inputs,
only the first row will be preserved. If omitted, will use all
variables.</p>
</td></tr>
<tr><td><code id="sdf_distinct_+3A_name">name</code></td>
<td>
<p>A name to assign this table. Passed to [sdf_register()].</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>

<hr>
<h2 id='sdf_drop_duplicates'>Remove duplicates from a Spark DataFrame</h2><span id='topic+sdf_drop_duplicates'></span>

<h3>Description</h3>

<p>Remove duplicates from a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_drop_duplicates(x, cols = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_drop_duplicates_+3A_x">x</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_drop_duplicates_+3A_cols">cols</code></td>
<td>
<p>Subset of Columns to consider, given as a character vector</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_expand_grid'>Create a Spark dataframe containing all combinations of inputs</h2><span id='topic+sdf_expand_grid'></span>

<h3>Description</h3>

<p>Given one or more R vectors/factors or single-column Spark dataframes,
perform an expand.grid operation on all of them and store the result in
a Spark dataframe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_expand_grid(
  sc,
  ...,
  broadcast_vars = NULL,
  memory = TRUE,
  repartition = NULL,
  partition_by = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_expand_grid_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_expand_grid_+3A_...">...</code></td>
<td>
<p>Each input variable can be either a R vector/factor or a Spark
dataframe. Unnamed inputs will assume the default names of 'Var1', 'Var2',
etc in the result, similar to what 'expand.grid' does for unnamed inputs.</p>
</td></tr>
<tr><td><code id="sdf_expand_grid_+3A_broadcast_vars">broadcast_vars</code></td>
<td>
<p>Indicates which input(s) should be broadcasted to all
nodes of the Spark cluster during the join process (default: none).</p>
</td></tr>
<tr><td><code id="sdf_expand_grid_+3A_memory">memory</code></td>
<td>
<p>Boolean; whether the resulting Spark dataframe should be
cached into memory (default: TRUE)</p>
</td></tr>
<tr><td><code id="sdf_expand_grid_+3A_repartition">repartition</code></td>
<td>
<p>Number of partitions the resulting Spark dataframe should
have</p>
</td></tr>
<tr><td><code id="sdf_expand_grid_+3A_partition_by">partition_by</code></td>
<td>
<p>Vector of column names used for partitioning the
resulting Spark dataframe, only supported for Spark 2.0+</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
sc &lt;- spark_connect(master = "local")
grid_sdf &lt;- sdf_expand_grid(sc, seq(5), rnorm(10), letters)

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_fast_bind_cols'>Fast cbind for Spark DataFrames</h2><span id='topic+sdf_fast_bind_cols'></span>

<h3>Description</h3>

<p>This is a version of 'sdf_bind_cols' that works by zipping
RDDs. From the API docs: &quot;Assumes that the two RDDs have the
*same number of partitions* and the *same number of elements
in each partition* (e.g. one was made through a map on the
other).&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_fast_bind_cols(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_fast_bind_cols_+3A_...">...</code></td>
<td>
<p>Spark DataFrames to cbind</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_from_avro'>Convert column(s) from avro format</h2><span id='topic+sdf_from_avro'></span>

<h3>Description</h3>

<p>Convert column(s) from avro format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_from_avro(x, cols)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_from_avro_+3A_x">x</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_from_avro_+3A_cols">cols</code></td>
<td>
<p>Named list of columns to transform from Avro format plus a valid Avro
schema string for each column, where column names are keys and column schema strings
are values (e.g.,
<code>c(example_primitive_col = "string",
example_complex_col = "{\"type\":\"record\",\"name\":\"person\",\"fields\":[
{\"name\":\"person_name\",\"type\":\"string\"}, {\"name\":\"person_id\",\"type\":\"long\"}]}")</code></p>
</td></tr>
</table>

<hr>
<h2 id='sdf_is_streaming'>Spark DataFrame is Streaming</h2><span id='topic+sdf_is_streaming'></span>

<h3>Description</h3>

<p>Is the given Spark DataFrame a streaming data?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_is_streaming(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_is_streaming_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_last_index'>Returns the last index of a Spark DataFrame</h2><span id='topic+sdf_last_index'></span>

<h3>Description</h3>

<p>Returns the last index of a Spark DataFrame. The Spark
<code>mapPartitionsWithIndex</code> function is used to iterate
through the last nonempty partition of the RDD to find the last record.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_last_index(x, id = "id")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_last_index_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_last_index_+3A_id">id</code></td>
<td>
<p>The name of the index column.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_len'>Create DataFrame for Length</h2><span id='topic+sdf_len'></span>

<h3>Description</h3>

<p>Creates a DataFrame for the given length.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_len(sc, length, repartition = NULL, type = c("integer", "integer64"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_len_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_len_+3A_length">length</code></td>
<td>
<p>The desired length of the sequence.</p>
</td></tr>
<tr><td><code id="sdf_len_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
data across the Spark cluster.</p>
</td></tr>
<tr><td><code id="sdf_len_+3A_type">type</code></td>
<td>
<p>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_num_partitions'>Gets number of partitions of a Spark DataFrame</h2><span id='topic+sdf_num_partitions'></span>

<h3>Description</h3>

<p>Gets number of partitions of a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_num_partitions(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_num_partitions_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_partition_sizes'>Compute the number of records within each partition of a Spark DataFrame</h2><span id='topic+sdf_partition_sizes'></span>

<h3>Description</h3>

<p>Compute the number of records within each partition of a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_partition_sizes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_partition_sizes_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "spark://HOST:PORT")
example_sdf &lt;- sdf_len(sc, 100L, repartition = 10L)
example_sdf %&gt;%
  sdf_partition_sizes() %&gt;%
  print()

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_persist'>Persist a Spark DataFrame</h2><span id='topic+sdf_persist'></span>

<h3>Description</h3>

<p>Persist a Spark DataFrame, forcing any pending computations and (optionally)
serializing the results to disk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_persist(x, storage.level = "MEMORY_AND_DISK", name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_persist_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_persist_+3A_storage.level">storage.level</code></td>
<td>
<p>The storage level to be used. Please view the
<a href="https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">Spark Documentation</a>
for information on what storage levels are accepted.</p>
</td></tr>
<tr><td><code id="sdf_persist_+3A_name">name</code></td>
<td>
<p>A name to assign this table. Passed to [sdf_register()].</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Spark DataFrames invoke their operations lazily &ndash; pending operations are
deferred until their results are actually needed. Persisting a Spark
DataFrame effectively 'forces' any pending computations, and then persists
the generated Spark DataFrame as requested (to memory, to disk, or
otherwise).
</p>
<p>Users of Spark should be careful to persist the results of any computations
which are non-deterministic &ndash; otherwise, one might see that the values
within a column seem to 'change' as new operations are performed on that
data set.
</p>

<hr>
<h2 id='sdf_pivot'>Pivot a Spark DataFrame</h2><span id='topic+sdf_pivot'></span>

<h3>Description</h3>

<p>Construct a pivot table over a Spark Dataframe, using a syntax similar to
that from <code>reshape2::dcast</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_pivot(x, formula, fun.aggregate = "count")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_pivot_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_pivot_+3A_formula">formula</code></td>
<td>
<p>A two-sided <span class="rlang"><b>R</b></span> formula of the form <code>x_1 + x_2 + ... ~ y_1</code>.
The left-hand side of the formula indicates which variables are used for grouping,
and the right-hand side indicates which variable is used for pivoting. Currently,
only a single pivot column is supported.</p>
</td></tr>
<tr><td><code id="sdf_pivot_+3A_fun.aggregate">fun.aggregate</code></td>
<td>
<p>How should the grouped dataset be aggregated? Can be
a length-one character vector, giving the name of a Spark aggregation function
to be called; a named <span class="rlang"><b>R</b></span> list mapping column names to an aggregation method,
or an <span class="rlang"><b>R</b></span> function that is invoked on the grouped dataset.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
library(dplyr)

sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

# aggregating by mean
iris_tbl %&gt;%
  mutate(Petal_Width = ifelse(Petal_Width &gt; 1.5, "High", "Low")) %&gt;%
  sdf_pivot(Petal_Width ~ Species,
    fun.aggregate = list(Petal_Length = "mean")
  )

# aggregating all observations in a list
iris_tbl %&gt;%
  mutate(Petal_Width = ifelse(Petal_Width &gt; 1.5, "High", "Low")) %&gt;%
  sdf_pivot(Petal_Width ~ Species,
    fun.aggregate = list(Petal_Length = "collect_list")
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_project'>Project features onto principal components</h2><span id='topic+sdf_project'></span>

<h3>Description</h3>

<p>Project features onto principal components
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_project(
  object,
  newdata,
  features = dimnames(object$pc)[[1]],
  feature_prefix = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_project_+3A_object">object</code></td>
<td>
<p>A Spark PCA model object</p>
</td></tr>
<tr><td><code id="sdf_project_+3A_newdata">newdata</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_project_+3A_features">features</code></td>
<td>
<p>A vector of names of columns to be projected</p>
</td></tr>
<tr><td><code id="sdf_project_+3A_feature_prefix">feature_prefix</code></td>
<td>
<p>The prefix used in naming the output features</p>
</td></tr>
<tr><td><code id="sdf_project_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_quantile'>Compute (Approximate) Quantiles with a Spark DataFrame</h2><span id='topic+sdf_quantile'></span>

<h3>Description</h3>

<p>Given a numeric column within a Spark DataFrame, compute
approximate quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_quantile(
  x,
  column,
  probabilities = c(0, 0.25, 0.5, 0.75, 1),
  relative.error = 1e-05,
  weight.column = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_quantile_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_quantile_+3A_column">column</code></td>
<td>
<p>The column(s) for which quantiles should be computed.
Multiple columns are only supported in Spark 2.0+.</p>
</td></tr>
<tr><td><code id="sdf_quantile_+3A_probabilities">probabilities</code></td>
<td>
<p>A numeric vector of probabilities, for
which quantiles should be computed.</p>
</td></tr>
<tr><td><code id="sdf_quantile_+3A_relative.error">relative.error</code></td>
<td>
<p>The maximal possible difference between the actual
percentile of a result and its expected percentile (e.g., if
'relative.error' is 0.01 and 'probabilities' is 0.95, then any value
between the 94th and 96th percentile will be considered an acceptable
approximation).</p>
</td></tr>
<tr><td><code id="sdf_quantile_+3A_weight.column">weight.column</code></td>
<td>
<p>If not NULL, then a generalized version of the Greenwald-
Khanna algorithm will be run to compute weighted percentiles, with each
sample from 'column' having a relative weight specified by the corresponding
value in 'weight.column'. The weights can be considered as relative
frequencies of sample data points.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_random_split'>Partition a Spark Dataframe</h2><span id='topic+sdf_random_split'></span><span id='topic+sdf_partition'></span>

<h3>Description</h3>

<p>Partition a Spark DataFrame into multiple groups. This routine is useful
for splitting a DataFrame into, for example, training and test datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_random_split(
  x,
  ...,
  weights = NULL,
  seed = sample(.Machine$integer.max, 1)
)

sdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_random_split_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_random_split_+3A_...">...</code></td>
<td>
<p>Named parameters, mapping table names to weights. The weights
will be normalized such that they sum to 1.</p>
</td></tr>
<tr><td><code id="sdf_random_split_+3A_weights">weights</code></td>
<td>
<p>An alternate mechanism for supplying weights &ndash; when
specified, this takes precedence over the <code>...</code> arguments.</p>
</td></tr>
<tr><td><code id="sdf_random_split_+3A_seed">seed</code></td>
<td>
<p>Random seed to use for randomly partitioning the dataset. Set
this if you want your partitioning to be reproducible on repeated runs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling weights define the probability that a particular observation
will be assigned to a particular partition, not the resulting size of the
partition. This implies that partitioning a DataFrame with, for example,
</p>
<p><code>sdf_random_split(x, training = 0.5, test = 0.5)</code>
</p>
<p>is not guaranteed to produce <code>training</code> and <code>test</code> partitions
of equal size.
</p>


<h3>Value</h3>

<p>An <span class="rlang"><b>R</b></span> <code>list</code> of <code>tbl_spark</code>s.
</p>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# randomly partition data into a 'training' and 'test'
# dataset, with 60% of the observations assigned to the
# 'training' dataset, and 40% assigned to the 'test' dataset
data(diamonds, package = "ggplot2")
diamonds_tbl &lt;- copy_to(sc, diamonds, "diamonds")
partitions &lt;- diamonds_tbl %&gt;%
  sdf_random_split(training = 0.6, test = 0.4)
print(partitions)

# alternate way of specifying weights
weights &lt;- c(training = 0.6, test = 0.4)
diamonds_tbl %&gt;% sdf_random_split(weights = weights)

## End(Not run)
</code></pre>

<hr>
<h2 id='sdf_rbeta'>Generate random samples from a Beta distribution</h2><span id='topic+sdf_rbeta'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a Betal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rbeta(
  sc,
  n,
  shape1,
  shape2,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rbeta_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_shape1">shape1</code></td>
<td>
<p>Non-negative parameter (alpha) of the Beta distribution.</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_shape2">shape2</code></td>
<td>
<p>Non-negative parameter (beta) of the Beta distribution.</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rbeta_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rbinom'>Generate random samples from a binomial distribution</h2><span id='topic+sdf_rbinom'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a binomial distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rbinom(
  sc,
  n,
  size,
  prob,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rbinom_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_size">size</code></td>
<td>
<p>Number of trials (zero or more).</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_prob">prob</code></td>
<td>
<p>Probability of success on each trial.</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rbinom_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rcauchy'>Generate random samples from a Cauchy distribution</h2><span id='topic+sdf_rcauchy'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a Cauchy distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rcauchy(
  sc,
  n,
  location = 0,
  scale = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rcauchy_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_location">location</code></td>
<td>
<p>Location parameter of the distribution.</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_scale">scale</code></td>
<td>
<p>Scale parameter of the distribution.</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rcauchy_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rchisq'>Generate random samples from a chi-squared distribution</h2><span id='topic+sdf_rchisq'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a chi-squared distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rchisq(sc, n, df, num_partitions = NULL, seed = NULL, output_col = "x")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rchisq_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rchisq_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rchisq_+3A_df">df</code></td>
<td>
<p>Degrees of freedom (non-negative, but can be non-integer).</p>
</td></tr>
<tr><td><code id="sdf_rchisq_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rchisq_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rchisq_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_read_column'>Read a Column from a Spark DataFrame</h2><span id='topic+sdf_read_column'></span>

<h3>Description</h3>

<p>Read a single column from a Spark DataFrame, and return
the contents of that column back to <span class="rlang"><b>R</b></span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_read_column(x, column)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_read_column_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_read_column_+3A_column">column</code></td>
<td>
<p>The name of a column within <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is expected for this operation to preserve row order.
</p>

<hr>
<h2 id='sdf_register'>Register a Spark DataFrame</h2><span id='topic+sdf_register'></span>

<h3>Description</h3>

<p>Registers a Spark DataFrame (giving it a table name for the
Spark SQL context), and returns a <code>tbl_spark</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_register(x, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_register_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_register_+3A_name">name</code></td>
<td>
<p>A name to assign this table.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>

<hr>
<h2 id='sdf_repartition'>Repartition a Spark DataFrame</h2><span id='topic+sdf_repartition'></span>

<h3>Description</h3>

<p>Repartition a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_repartition(x, partitions = NULL, partition_by = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_repartition_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_repartition_+3A_partitions">partitions</code></td>
<td>
<p>number of partitions</p>
</td></tr>
<tr><td><code id="sdf_repartition_+3A_partition_by">partition_by</code></td>
<td>
<p>vector of column names used for partitioning, only supported for Spark 2.0+</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_residuals.ml_model_generalized_linear_regression'>Model Residuals</h2><span id='topic+sdf_residuals.ml_model_generalized_linear_regression'></span><span id='topic+sdf_residuals.ml_model_linear_regression'></span><span id='topic+sdf_residuals'></span>

<h3>Description</h3>

<p>This generic method returns a Spark DataFrame with model
residuals added as a column to the model training data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ml_model_generalized_linear_regression'
sdf_residuals(
  object,
  type = c("deviance", "pearson", "working", "response"),
  ...
)

## S3 method for class 'ml_model_linear_regression'
sdf_residuals(object, ...)

sdf_residuals(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_residuals.ml_model_generalized_linear_regression_+3A_object">object</code></td>
<td>
<p>Spark ML model object.</p>
</td></tr>
<tr><td><code id="sdf_residuals.ml_model_generalized_linear_regression_+3A_type">type</code></td>
<td>
<p>type of residuals which should be returned.</p>
</td></tr>
<tr><td><code id="sdf_residuals.ml_model_generalized_linear_regression_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_rexp'>Generate random samples from an exponential distribution</h2><span id='topic+sdf_rexp'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from an exponential distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rexp(sc, n, rate = 1, num_partitions = NULL, seed = NULL, output_col = "x")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rexp_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rexp_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rexp_+3A_rate">rate</code></td>
<td>
<p>Rate of the exponential distribution (default: 1). The exponential
distribution with rate lambda has mean 1 / lambda and density f(x) = lambda e ^ - lambda x.</p>
</td></tr>
<tr><td><code id="sdf_rexp_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rexp_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rexp_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rgamma'>Generate random samples from a Gamma distribution</h2><span id='topic+sdf_rgamma'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a Gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rgamma(
  sc,
  n,
  shape,
  rate = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rgamma_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_shape">shape</code></td>
<td>
<p>Shape parameter (greater than 0) for the Gamma distribution.</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_rate">rate</code></td>
<td>
<p>Rate parameter (greater than 0) for the Gamma distribution (scale is 1/rate).</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rgamma_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rgeom'>Generate random samples from a geometric distribution</h2><span id='topic+sdf_rgeom'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a geometric distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rgeom(sc, n, prob, num_partitions = NULL, seed = NULL, output_col = "x")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rgeom_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rgeom_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rgeom_+3A_prob">prob</code></td>
<td>
<p>Probability of success in each trial.</p>
</td></tr>
<tr><td><code id="sdf_rgeom_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rgeom_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rgeom_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rhyper'>Generate random samples from a hypergeometric distribution</h2><span id='topic+sdf_rhyper'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a hypergeometric distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rhyper(
  sc,
  nn,
  m,
  n,
  k,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rhyper_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_nn">nn</code></td>
<td>
<p>Sample Size.</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_m">m</code></td>
<td>
<p>The number of successes among the population.</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_n">n</code></td>
<td>
<p>The number of failures among the population.</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_k">k</code></td>
<td>
<p>The number of draws.</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rhyper_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rlnorm'>Generate random samples from a log normal distribution</h2><span id='topic+sdf_rlnorm'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a log normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rlnorm(
  sc,
  n,
  meanlog = 0,
  sdlog = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rlnorm_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_meanlog">meanlog</code></td>
<td>
<p>The mean of the normally distributed natural logarithm of this distribution.</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_sdlog">sdlog</code></td>
<td>
<p>The Standard deviation of the normally distributed natural logarithm of this distribution.</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rlnorm_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rnorm'>Generate random samples from the standard normal distribution</h2><span id='topic+sdf_rnorm'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from the standard normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rnorm(
  sc,
  n,
  mean = 0,
  sd = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rnorm_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_mean">mean</code></td>
<td>
<p>The mean value of the normal distribution.</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_sd">sd</code></td>
<td>
<p>The standard deviation of the normal distribution.</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rnorm_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rpois'>Generate random samples from a Poisson distribution</h2><span id='topic+sdf_rpois'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a Poisson distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rpois(sc, n, lambda, num_partitions = NULL, seed = NULL, output_col = "x")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rpois_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rpois_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rpois_+3A_lambda">lambda</code></td>
<td>
<p>Mean, or lambda, of the Poisson distribution.</p>
</td></tr>
<tr><td><code id="sdf_rpois_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rpois_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rpois_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rt'>Generate random samples from a t-distribution</h2><span id='topic+sdf_rt'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a t-distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rt(sc, n, df, num_partitions = NULL, seed = NULL, output_col = "x")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rt_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rt_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rt_+3A_df">df</code></td>
<td>
<p>Degrees of freedom (&gt; 0, maybe non-integer).</p>
</td></tr>
<tr><td><code id="sdf_rt_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rt_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rt_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_runif'>Generate random samples from the uniform distribution U(0, 1).</h2><span id='topic+sdf_runif'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from the uniform distribution U(0, 1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_runif(
  sc,
  n,
  min = 0,
  max = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_runif_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_min">min</code></td>
<td>
<p>The lower limit of the distribution.</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_max">max</code></td>
<td>
<p>The upper limit of the distribution.</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_runif_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_rweibull">sdf_rweibull</a>()</code>
</p>

<hr>
<h2 id='sdf_rweibull'>Generate random samples from a Weibull distribution.</h2><span id='topic+sdf_rweibull'></span>

<h3>Description</h3>

<p>Generator method for creating a single-column Spark dataframes comprised of
i.i.d. samples from a Weibull distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_rweibull(
  sc,
  n,
  shape,
  scale = 1,
  num_partitions = NULL,
  seed = NULL,
  output_col = "x"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_rweibull_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_shape">shape</code></td>
<td>
<p>The shape of the Weibull distribution.</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_scale">scale</code></td>
<td>
<p>The scale of the Weibull distribution (default: 1).</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="sdf_rweibull_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark statistical routines: 
<code><a href="#topic+sdf_rbeta">sdf_rbeta</a>()</code>,
<code><a href="#topic+sdf_rbinom">sdf_rbinom</a>()</code>,
<code><a href="#topic+sdf_rcauchy">sdf_rcauchy</a>()</code>,
<code><a href="#topic+sdf_rchisq">sdf_rchisq</a>()</code>,
<code><a href="#topic+sdf_rexp">sdf_rexp</a>()</code>,
<code><a href="#topic+sdf_rgamma">sdf_rgamma</a>()</code>,
<code><a href="#topic+sdf_rgeom">sdf_rgeom</a>()</code>,
<code><a href="#topic+sdf_rhyper">sdf_rhyper</a>()</code>,
<code><a href="#topic+sdf_rlnorm">sdf_rlnorm</a>()</code>,
<code><a href="#topic+sdf_rnorm">sdf_rnorm</a>()</code>,
<code><a href="#topic+sdf_rpois">sdf_rpois</a>()</code>,
<code><a href="#topic+sdf_rt">sdf_rt</a>()</code>,
<code><a href="#topic+sdf_runif">sdf_runif</a>()</code>
</p>

<hr>
<h2 id='sdf_sample'>Randomly Sample Rows from a Spark DataFrame</h2><span id='topic+sdf_sample'></span>

<h3>Description</h3>

<p>Draw a random sample of rows (with or without replacement)
from a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_sample_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_sample_+3A_fraction">fraction</code></td>
<td>
<p>The fraction to sample.</p>
</td></tr>
<tr><td><code id="sdf_sample_+3A_replacement">replacement</code></td>
<td>
<p>Boolean; sample with replacement?</p>
</td></tr>
<tr><td><code id="sdf_sample_+3A_seed">seed</code></td>
<td>
<p>An (optional) integer seed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>

<hr>
<h2 id='sdf_schema'>Read the Schema of a Spark DataFrame</h2><span id='topic+sdf_schema'></span>

<h3>Description</h3>

<p>Read the schema of a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_schema_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_schema_+3A_expand_nested_cols">expand_nested_cols</code></td>
<td>
<p>Whether to expand columns containing nested array
of structs (which are usually created by tidyr::nest on a Spark data frame)</p>
</td></tr>
<tr><td><code id="sdf_schema_+3A_expand_struct_cols">expand_struct_cols</code></td>
<td>
<p>Whether to expand columns containing structs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>type</code> column returned gives the string representation of the
underlying Spark  type for that column; for example, a vector of numeric
values would be returned with the type <code>"DoubleType"</code>. Please see the
<a href="https://spark.apache.org/docs/latest/api/scala/index.html">Spark Scala API Documentation</a>
for information on what types are available and exposed by Spark.
</p>


<h3>Value</h3>

<p>An <span class="rlang"><b>R</b></span> <code>list</code>, with each <code>list</code> element describing the
<code>name</code> and <code>type</code> of a column.
</p>

<hr>
<h2 id='sdf_separate_column'>Separate a Vector Column into Scalar Columns</h2><span id='topic+sdf_separate_column'></span>

<h3>Description</h3>

<p>Given a vector column in a Spark DataFrame, split that
into <code>n</code> separate columns, each column made up of
the different elements in the column <code>column</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_separate_column(x, column, into = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_separate_column_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_separate_column_+3A_column">column</code></td>
<td>
<p>The name of a (vector-typed) column.</p>
</td></tr>
<tr><td><code id="sdf_separate_column_+3A_into">into</code></td>
<td>
<p>A specification of the columns that should be
generated from <code>column</code>. This can either be a
vector of column names, or an <span class="rlang"><b>R</b></span> list mapping column
names to the (1-based) index at which a particular
vector element should be extracted.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_seq'>Create DataFrame for Range</h2><span id='topic+sdf_seq'></span>

<h3>Description</h3>

<p>Creates a DataFrame for the given range
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_seq(
  sc,
  from = 1L,
  to = 1L,
  by = 1L,
  repartition = NULL,
  type = c("integer", "integer64")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_seq_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="sdf_seq_+3A_from">from</code>, <code id="sdf_seq_+3A_to">to</code></td>
<td>
<p>The start and end to use as a range</p>
</td></tr>
<tr><td><code id="sdf_seq_+3A_by">by</code></td>
<td>
<p>The increment of the sequence.</p>
</td></tr>
<tr><td><code id="sdf_seq_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
data across the Spark cluster. Defaults to the minimum number of partitions.</p>
</td></tr>
<tr><td><code id="sdf_seq_+3A_type">type</code></td>
<td>
<p>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_sort'>Sort a Spark DataFrame</h2><span id='topic+sdf_sort'></span>

<h3>Description</h3>

<p>Sort a Spark DataFrame by one or more columns, with each column
sorted in ascending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_sort(x, columns)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_sort_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_sort_+3A_columns">columns</code></td>
<td>
<p>The column(s) to sort by.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_weighted_sample">sdf_weighted_sample</a>()</code>
</p>

<hr>
<h2 id='sdf_sql'>Spark DataFrame from SQL</h2><span id='topic+sdf_sql'></span>

<h3>Description</h3>

<p>Defines a Spark DataFrame from a SQL query, useful to create Spark DataFrames
without collecting the results immediately.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_sql(sc, sql)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_sql_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="sdf_sql_+3A_sql">sql</code></td>
<td>
<p>a 'SQL' query used to generate a Spark DataFrame.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_to_avro'>Convert column(s) to avro format</h2><span id='topic+sdf_to_avro'></span>

<h3>Description</h3>

<p>Convert column(s) to avro format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_to_avro(x, cols = colnames(x))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_to_avro_+3A_x">x</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="sdf_to_avro_+3A_cols">cols</code></td>
<td>
<p>Subset of Columns to convert into avro format</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_unnest_longer'>Unnest longer</h2><span id='topic+sdf_unnest_longer'></span>

<h3>Description</h3>

<p>Expand a struct column or an array column within a Spark dataframe into one
or more rows, similar what to tidyr::unnest_longer does to an R dataframe.
An index column, if included, will be 1-based if 'col' is an array column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_unnest_longer(
  data,
  col,
  values_to = NULL,
  indices_to = NULL,
  include_indices = NULL,
  names_repair = "check_unique",
  ptype = list(),
  transform = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_unnest_longer_+3A_data">data</code></td>
<td>
<p>The Spark dataframe to be unnested</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_col">col</code></td>
<td>
<p>The struct column to extract components from</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_values_to">values_to</code></td>
<td>
<p>Name of column to store vector values. Defaults to 'col'.</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_indices_to">indices_to</code></td>
<td>
<p>A string giving the name of column which will contain the
inner names or position (if not named) of the values. Defaults to 'col'
with '_id' suffix</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_include_indices">include_indices</code></td>
<td>
<p>Whether to include an index column. An index column
will be included by default if 'col' is a struct column. It will also be
included if 'indices_to' is not 'NULL'.</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_names_repair">names_repair</code></td>
<td>
<p>Strategy for fixing duplicate column names (the semantic
will be exactly identical to that of '.name_repair' option in
<code><a href="tibble.html#topic+tibble">tibble</a></code>)</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_ptype">ptype</code></td>
<td>
<p>Optionally, supply an R data frame prototype for the output.
Each column of the unnested result will be casted based on the Spark
equivalent of the type of the column with the same name within 'ptype',
e.g., if 'ptype' has a column 'x' of type 'character', then column 'x'
of the unnested result will be casted from its original SQL type to
StringType.</p>
</td></tr>
<tr><td><code id="sdf_unnest_longer_+3A_transform">transform</code></td>
<td>
<p>Optionally, a named list of transformation functions applied</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "2.4.0")

# unnesting a struct column
sdf &lt;- copy_to(
  sc,
  dplyr::tibble(
    x = 1:3,
    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))
  )
)

unnested &lt;- sdf %&gt;% sdf_unnest_longer(y, indices_to = "attr")

# unnesting an array column
sdf &lt;- copy_to(
  sc,
  dplyr::tibble(
    x = 1:3,
    y = list(1:10, 1:5, 1:2)
  )
)

unnested &lt;- sdf %&gt;% sdf_unnest_longer(y, indices_to = "array_idx")

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_unnest_wider'>Unnest wider</h2><span id='topic+sdf_unnest_wider'></span>

<h3>Description</h3>

<p>Flatten a struct column within a Spark dataframe into one or more columns,
similar what to tidyr::unnest_wider does to an R dataframe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_unnest_wider(
  data,
  col,
  names_sep = NULL,
  names_repair = "check_unique",
  ptype = list(),
  transform = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_unnest_wider_+3A_data">data</code></td>
<td>
<p>The Spark dataframe to be unnested</p>
</td></tr>
<tr><td><code id="sdf_unnest_wider_+3A_col">col</code></td>
<td>
<p>The struct column to extract components from</p>
</td></tr>
<tr><td><code id="sdf_unnest_wider_+3A_names_sep">names_sep</code></td>
<td>
<p>If 'NULL', the default, the names will be left as is.
If a string, the inner and outer names will be pasted together using
'names_sep' as the delimiter.</p>
</td></tr>
<tr><td><code id="sdf_unnest_wider_+3A_names_repair">names_repair</code></td>
<td>
<p>Strategy for fixing duplicate column names (the semantic
will be exactly identical to that of '.name_repair' option in
<code><a href="tibble.html#topic+tibble">tibble</a></code>)</p>
</td></tr>
<tr><td><code id="sdf_unnest_wider_+3A_ptype">ptype</code></td>
<td>
<p>Optionally, supply an R data frame prototype for the output.
Each column of the unnested result will be casted based on the Spark
equivalent of the type of the column with the same name within 'ptype',
e.g., if 'ptype' has a column 'x' of type 'character', then column 'x'
of the unnested result will be casted from its original SQL type to
StringType.</p>
</td></tr>
<tr><td><code id="sdf_unnest_wider_+3A_transform">transform</code></td>
<td>
<p>Optionally, a named list of transformation functions applied
to each component (e.g., list('x = as.character') to cast column 'x' to
String).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "local", version = "2.4.0")

sdf &lt;- copy_to(
  sc,
  dplyr::tibble(
    x = 1:3,
    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))
  )
)

# flatten struct column 'y' into two separate columns 'y_a' and 'y_b'
unnested &lt;- sdf %&gt;% sdf_unnest_wider(y, names_sep = "_")

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_weighted_sample'>Perform Weighted Random Sampling on a Spark DataFrame</h2><span id='topic+sdf_weighted_sample'></span>

<h3>Description</h3>

<p>Draw a random sample of rows (with or without replacement) from a Spark
DataFrame
If the sampling is done without replacement, then it will be conceptually
equivalent to an iterative process such that in each step the probability of
adding a row to the sample set is equal to its weight divided by summation of
weights of all rows that are not in the sample set yet in that step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_weighted_sample(x, weight_col, k, replacement = TRUE, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_weighted_sample_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_weighted_sample_+3A_weight_col">weight_col</code></td>
<td>
<p>Name of the weight column</p>
</td></tr>
<tr><td><code id="sdf_weighted_sample_+3A_k">k</code></td>
<td>
<p>Sample set size</p>
</td></tr>
<tr><td><code id="sdf_weighted_sample_+3A_replacement">replacement</code></td>
<td>
<p>Whether to sample with replacement</p>
</td></tr>
<tr><td><code id="sdf_weighted_sample_+3A_seed">seed</code></td>
<td>
<p>An (optional) integer seed</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code><a href="#topic+sdf_copy_to">sdf_copy_to</a>()</code>,
<code><a href="#topic+sdf_distinct">sdf_distinct</a>()</code>,
<code><a href="#topic+sdf_random_split">sdf_random_split</a>()</code>,
<code><a href="#topic+sdf_register">sdf_register</a>()</code>,
<code><a href="#topic+sdf_sample">sdf_sample</a>()</code>,
<code><a href="#topic+sdf_sort">sdf_sort</a>()</code>
</p>

<hr>
<h2 id='sdf_with_sequential_id'>Add a Sequential ID Column to a Spark DataFrame</h2><span id='topic+sdf_with_sequential_id'></span>

<h3>Description</h3>

<p>Add a sequential ID column to a Spark DataFrame. The Spark
<code>zipWithIndex</code> function is used to produce these. This differs from
<code>sdf_with_unique_id</code> in that the IDs generated are independent of
partitioning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_with_sequential_id(x, id = "id", from = 1L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_with_sequential_id_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_with_sequential_id_+3A_id">id</code></td>
<td>
<p>The name of the column to host the generated IDs.</p>
</td></tr>
<tr><td><code id="sdf_with_sequential_id_+3A_from">from</code></td>
<td>
<p>The starting value of the id column</p>
</td></tr>
</table>

<hr>
<h2 id='sdf_with_unique_id'>Add a Unique ID Column to a Spark DataFrame</h2><span id='topic+sdf_with_unique_id'></span>

<h3>Description</h3>

<p>Add a unique ID column to a Spark DataFrame. The Spark
<code>monotonicallyIncreasingId</code> function is used to produce these and is
guaranteed to produce unique, monotonically increasing ids; however, there
is no guarantee that these IDs will be sequential. The table is persisted
immediately after the column is generated, to ensure that the column is
stable &ndash; otherwise, it can differ across new computations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_with_unique_id(x, id = "id")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf_with_unique_id_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf_with_unique_id_+3A_id">id</code></td>
<td>
<p>The name of the column to host the generated IDs.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf-saveload'>Save / Load a Spark DataFrame</h2><span id='topic+sdf-saveload'></span><span id='topic+sdf_save_table'></span><span id='topic+sdf_load_table'></span><span id='topic+sdf_save_parquet'></span><span id='topic+sdf_load_parquet'></span>

<h3>Description</h3>

<p>Routines for saving and loading Spark DataFrames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_save_table(x, name, overwrite = FALSE, append = FALSE)

sdf_load_table(sc, name)

sdf_save_parquet(x, path, overwrite = FALSE, append = FALSE)

sdf_load_parquet(sc, path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf-saveload_+3A_x">x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf-saveload_+3A_name">name</code></td>
<td>
<p>The table name to assign to the saved Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf-saveload_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite a pre-existing table of the same name?</p>
</td></tr>
<tr><td><code id="sdf-saveload_+3A_append">append</code></td>
<td>
<p>Boolean; append to a pre-existing table of the same name?</p>
</td></tr>
<tr><td><code id="sdf-saveload_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code> object.</p>
</td></tr>
<tr><td><code id="sdf-saveload_+3A_path">path</code></td>
<td>
<p>The path where the Spark DataFrame should be saved.</p>
</td></tr>
</table>

<hr>
<h2 id='sdf-transform-methods'>Spark ML &ndash; Transform, fit, and predict methods (sdf_ interface)</h2><span id='topic+sdf-transform-methods'></span><span id='topic+sdf_predict'></span><span id='topic+sdf_transform'></span><span id='topic+sdf_fit'></span><span id='topic+sdf_fit_and_transform'></span>

<h3>Description</h3>

<p>Deprecated methods for transformation, fit, and prediction. These are mirrors of the corresponding <a href="#topic+ml-transform-methods">ml-transform-methods</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_predict(x, model, ...)

sdf_transform(x, transformer, ...)

sdf_fit(x, estimator, ...)

sdf_fit_and_transform(x, estimator, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sdf-transform-methods_+3A_x">x</code></td>
<td>
<p>A <code>tbl_spark</code>.</p>
</td></tr>
<tr><td><code id="sdf-transform-methods_+3A_model">model</code></td>
<td>
<p>A <code>ml_transformer</code> or a <code>ml_model</code> object.</p>
</td></tr>
<tr><td><code id="sdf-transform-methods_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to the corresponding <code>ml_</code> methods.</p>
</td></tr>
<tr><td><code id="sdf-transform-methods_+3A_transformer">transformer</code></td>
<td>
<p>A <code>ml_transformer</code> object.</p>
</td></tr>
<tr><td><code id="sdf-transform-methods_+3A_estimator">estimator</code></td>
<td>
<p>A <code>ml_estimator</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sdf_predict()</code>, <code>sdf_transform()</code>, and <code>sdf_fit_and_transform()</code> return a transformed dataframe whereas <code>sdf_fit()</code> returns a <code>ml_transformer</code>.
</p>

<hr>
<h2 id='select'>Select</h2><span id='topic+select'></span>

<h3>Description</h3>

<p>See <code><a href="dplyr.html#topic+select">select</a></code> for more details.
</p>

<hr>
<h2 id='separate'>Separate</h2><span id='topic+separate'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+separate">separate</a></code> for more details.
</p>

<hr>
<h2 id='spark_adaptive_query_execution'>Retrieves or sets status of Spark AQE</h2><span id='topic+spark_adaptive_query_execution'></span>

<h3>Description</h3>

<p>Retrieves or sets whether Spark adaptive query execution is enabled
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_adaptive_query_execution(sc, enable = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_adaptive_query_execution_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_adaptive_query_execution_+3A_enable">enable</code></td>
<td>
<p>Whether to enable Spark adaptive query execution. Defaults to
<code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_advisory_shuffle_partition_size'>Retrieves or sets advisory size of the shuffle partition</h2><span id='topic+spark_advisory_shuffle_partition_size'></span>

<h3>Description</h3>

<p>Retrieves or sets advisory size in bytes of the shuffle partition during adaptive optimization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_advisory_shuffle_partition_size(sc, size = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_advisory_shuffle_partition_size_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_advisory_shuffle_partition_size_+3A_size">size</code></td>
<td>
<p>Advisory size in bytes of the shuffle partition.
Defaults to <code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_apply'>Apply an R Function in Spark</h2><span id='topic+spark_apply'></span>

<h3>Description</h3>

<p>Applies an R function to a Spark object (typically, a Spark DataFrame).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_apply(
  x,
  f,
  columns = NULL,
  memory = TRUE,
  group_by = NULL,
  packages = NULL,
  context = NULL,
  name = NULL,
  barrier = NULL,
  fetch_result_as_sdf = TRUE,
  partition_index_param = "",
  arrow_max_records_per_batch = NULL,
  auto_deps = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_apply_+3A_x">x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>) coercable to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_f">f</code></td>
<td>
<p>A function that transforms a data frame partition into a data frame.
The function <code>f</code> has signature <code>f(df, context, group1, group2, ...)</code> where
<code>df</code> is a data frame with the data to be processed, <code>context</code>
is an optional object passed as the <code>context</code> parameter and <code>group1</code> to
<code>groupN</code> contain the values of the <code>group_by</code> values. When
<code>group_by</code> is not specified, <code>f</code> takes only one argument.
</p>
<p>Can also be an <code>rlang</code> anonymous function. For example, as <code>~ .x + 1</code>
to define an expression that adds one to the given <code>.x</code> data frame.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types for
the transformed object. When not specified, a sample of 10 rows is taken to
infer out the output columns automatically, to avoid this performance penalty,
specify the column types. The sample size is configurable using the
<code>sparklyr.apply.schema.infer</code> configuration option.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the table be cached into memory?</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_group_by">group_by</code></td>
<td>
<p>Column name used to group by data frame partitions.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_packages">packages</code></td>
<td>
<p>Boolean to distribute <code>.libPaths()</code> packages to each node,
a list of packages to distribute, or a package bundle created with
<code>spark_apply_bundle()</code>.
</p>
<p>Defaults to <code>TRUE</code> or the <code>sparklyr.apply.packages</code> value set in
<code>spark_config()</code>.
</p>
<p>For clusters using Yarn cluster mode, <code>packages</code> can point to a package
bundle created using <code>spark_apply_bundle()</code> and made available as a Spark
file using <code>config$sparklyr.shell.files</code>. For clusters using Livy, packages
can be manually installed on the driver node.
</p>
<p>For offline clusters where <code>available.packages()</code> is not available,
manually download the packages database from
https://cran.r-project.org/web/packages/packages.rds and set
<code>Sys.setenv(sparklyr.apply.packagesdb = "&lt;pathl-to-rds&gt;")</code>. Otherwise,
all packages will be used by default.
</p>
<p>For clusters where R packages already installed in every worker node,
the <code>spark.r.libpaths</code> config entry can be set in <code>spark_config()</code>
to the local packages library. To specify multiple paths collapse them
(without spaces) with a comma delimiter (e.g., <code>"/lib/path/one,/lib/path/two"</code>).</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_context">context</code></td>
<td>
<p>Optional object to be serialized and passed back to <code>f()</code>.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_name">name</code></td>
<td>
<p>Optional table name while registering the resulting data frame.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_barrier">barrier</code></td>
<td>
<p>Optional to support Barrier Execution Mode in the scheduler.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_fetch_result_as_sdf">fetch_result_as_sdf</code></td>
<td>
<p>Whether to return the transformed results in a Spark
Dataframe (defaults to <code>TRUE</code>). When set to <code>FALSE</code>, results will be
returned as a list of R objects instead.
</p>
<p>NOTE: <code>fetch_result_as_sdf</code> must be set to <code>FALSE</code> when the transformation
function being applied is returning R objects that cannot be stored in a Spark
Dataframe (e.g., complex numbers or any other R data type that does not have an
equivalent representation among Spark SQL data types).</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_partition_index_param">partition_index_param</code></td>
<td>
<p>Optional if non-empty, then <code>f</code> also receives
the index of the partition being processed as a named argument with this name, in
addition to all positional argument(s) it will receive
</p>
<p>NOTE: when <code>fetch_result_as_sdf</code> is set to <code>FALSE</code>, object returned from the
transformation function also must be serializable by the <code>base::serialize</code>
function in R.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_arrow_max_records_per_batch">arrow_max_records_per_batch</code></td>
<td>
<p>Maximum size of each Arrow record batch,
ignored if Arrow serialization is not enabled.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_auto_deps">auto_deps</code></td>
<td>
<p>[Experimental] Whether to infer all required R packages by
examining the closure <code>f()</code> and only distribute required R and their
transitive dependencies to Spark worker nodes (default: FALSE).
NOTE: this option will only take effect if <code>packages</code> is set to
<code>TRUE</code> or is a character vector of R package names. If <code>packages</code>
is a character vector of R package names, then both the set of packages
specified by <code>packages</code> and the set of inferred packages will be
distributed to Spark workers.</p>
</td></tr>
<tr><td><code id="spark_apply_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Configuration</h3>

<p><code>spark_config()</code> settings can be specified to change the workers
environment.
</p>
<p>For instance, to set additional environment variables to each
worker node use the <code>sparklyr.apply.env.*</code> config, to launch workers
without <code>--vanilla</code> use <code>sparklyr.apply.options.vanilla</code> set to
<code>FALSE</code>, to run a custom script before launching Rscript use
<code>sparklyr.apply.options.rscript.before</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local[3]")

# creates an Spark data frame with 10 elements then multiply times 10 in R
sdf_len(sc, 10) %&gt;% spark_apply(function(df) df * 10)

# using barrier mode
sdf_len(sc, 3, repartition = 3) %&gt;%
  spark_apply(nrow, barrier = TRUE, columns = c(id = "integer")) %&gt;%
  collect()

## End(Not run)

</code></pre>

<hr>
<h2 id='spark_apply_bundle'>Create Bundle for Spark Apply</h2><span id='topic+spark_apply_bundle'></span>

<h3>Description</h3>

<p>Creates a bundle of packages for <code>spark_apply()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_apply_bundle(packages = TRUE, base_path = getwd(), session_id = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_apply_bundle_+3A_packages">packages</code></td>
<td>
<p>List of packages to pack or <code>TRUE</code> to pack all.</p>
</td></tr>
<tr><td><code id="spark_apply_bundle_+3A_base_path">base_path</code></td>
<td>
<p>Base path used to store the resulting bundle.</p>
</td></tr>
<tr><td><code id="spark_apply_bundle_+3A_session_id">session_id</code></td>
<td>
<p>An optional ID string to include in the bundle file name to allow the bundle to be session-specific</p>
</td></tr>
</table>

<hr>
<h2 id='spark_apply_log'>Log Writer for Spark Apply</h2><span id='topic+spark_apply_log'></span>

<h3>Description</h3>

<p>Writes data to log under <code>spark_apply()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_apply_log(..., level = "INFO")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_apply_log_+3A_...">...</code></td>
<td>
<p>Arguments to write to log.</p>
</td></tr>
<tr><td><code id="spark_apply_log_+3A_level">level</code></td>
<td>
<p>Severity level for this entry; recommended values: <code>INFO</code>,
<code>ERROR</code> or <code>WARN</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_auto_broadcast_join_threshold'>Retrieves or sets the auto broadcast join threshold</h2><span id='topic+spark_auto_broadcast_join_threshold'></span>

<h3>Description</h3>

<p>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes
when performing a join. By setting this value to -1 broadcasting can be disabled. Note that
currently statistics are only supported for Hive Metastore tables where the command
'ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan' has been run, and file-based data source
tables where the statistics are computed directly on the files of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_auto_broadcast_join_threshold(sc, threshold = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_auto_broadcast_join_threshold_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_auto_broadcast_join_threshold_+3A_threshold">threshold</code></td>
<td>
<p>Maximum size in bytes for a table that will be broadcast to all worker nodes
when performing a join. Defaults to <code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_coalesce_initial_num_partitions'>Retrieves or sets initial number of shuffle partitions before coalescing</h2><span id='topic+spark_coalesce_initial_num_partitions'></span>

<h3>Description</h3>

<p>Retrieves or sets initial number of shuffle partitions before coalescing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_coalesce_initial_num_partitions(sc, num_partitions = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_coalesce_initial_num_partitions_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_coalesce_initial_num_partitions_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Initial number of shuffle partitions before coalescing.
Defaults to <code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_coalesce_min_num_partitions'>Retrieves or sets the minimum number of shuffle partitions after coalescing</h2><span id='topic+spark_coalesce_min_num_partitions'></span>

<h3>Description</h3>

<p>Retrieves or sets the minimum number of shuffle partitions after coalescing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_coalesce_min_num_partitions(sc, num_partitions = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_coalesce_min_num_partitions_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_coalesce_min_num_partitions_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Minimum number of shuffle partitions after coalescing.
Defaults to <code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_coalesce_shuffle_partitions'>Retrieves or sets whether coalescing contiguous shuffle partitions is enabled</h2><span id='topic+spark_coalesce_shuffle_partitions'></span>

<h3>Description</h3>

<p>Retrieves or sets whether coalescing contiguous shuffle partitions is enabled
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_coalesce_shuffle_partitions(sc, enable = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_coalesce_shuffle_partitions_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_coalesce_shuffle_partitions_+3A_enable">enable</code></td>
<td>
<p>Whether to enable coalescing of contiguous shuffle partitions.
Defaults to <code>NULL</code> to retrieve configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_session_config">spark_session_config</a>()</code>
</p>

<hr>
<h2 id='spark_compilation_spec'>Define a Spark Compilation Specification</h2><span id='topic+spark_compilation_spec'></span>

<h3>Description</h3>

<p>For use with <code><a href="#topic+compile_package_jars">compile_package_jars</a></code>. The Spark compilation
specification is used when compiling Spark extension Java Archives, and
defines which versions of Spark, as well as which versions of Scala, should
be used for compilation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_compilation_spec(
  spark_version = NULL,
  spark_home = NULL,
  scalac_path = NULL,
  scala_filter = NULL,
  jar_name = NULL,
  jar_path = NULL,
  jar_dep = NULL,
  embedded_srcs = "embedded_sources.R"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_compilation_spec_+3A_spark_version">spark_version</code></td>
<td>
<p>The Spark version to build against. This can
be left unset if the path to a suitable Spark home is supplied.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to a Spark home installation. This can
be left unset if <code>spark_version</code> is supplied; in such a case,
<code>sparklyr</code> will attempt to discover the associated Spark
installation using <code><a href="#topic+spark_home_dir">spark_home_dir</a></code>.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_scalac_path">scalac_path</code></td>
<td>
<p>The path to the <code>scalac</code> compiler to be used
during compilation of your Spark extension. Note that you should
ensure the version of <code>scalac</code> selected matches the version of
<code>scalac</code> used with the version of Spark you are compiling against.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_scala_filter">scala_filter</code></td>
<td>
<p>An optional <span class="rlang"><b>R</b></span> function that can be used to filter
which <code>scala</code> files are used during compilation. This can be
useful if you have auxiliary files that should only be included with
certain versions of Spark.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_jar_name">jar_name</code></td>
<td>
<p>The name to be assigned to the generated <code>jar</code>.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_jar_path">jar_path</code></td>
<td>
<p>The path to the <code>jar</code> tool to be used
during compilation of your Spark extension.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_jar_dep">jar_dep</code></td>
<td>
<p>An optional list of additional <code>jar</code> dependencies.</p>
</td></tr>
<tr><td><code id="spark_compilation_spec_+3A_embedded_srcs">embedded_srcs</code></td>
<td>
<p>Embedded source file(s) under <code>&lt;R package root&gt;/java</code> to
be included in the root of the resulting jar file as resources</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most Spark extensions won't need to define their own compilation specification,
and can instead rely on the default behavior of <code>compile_package_jars</code>.
</p>

<hr>
<h2 id='spark_compile'>Compile Scala sources into a Java Archive</h2><span id='topic+spark_compile'></span>

<h3>Description</h3>

<p>Given a set of <code>scala</code> source files, compile them
into a Java Archive (<code>jar</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_compile(
  jar_name,
  spark_home = NULL,
  filter = NULL,
  scalac = NULL,
  jar = NULL,
  jar_dep = NULL,
  embedded_srcs = "embedded_sources.R"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_compile_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to the Spark sources to be used
alongside compilation.</p>
</td></tr>
<tr><td><code id="spark_compile_+3A_filter">filter</code></td>
<td>
<p>An optional function, used to filter out discovered <code>scala</code>
files during compilation. This can be used to ensure that e.g. certain files
are only compiled with certain versions of Spark, and so on.</p>
</td></tr>
<tr><td><code id="spark_compile_+3A_scalac">scalac</code></td>
<td>
<p>The path to the <code>scalac</code> program to be used, for
compilation of <code>scala</code> files.</p>
</td></tr>
<tr><td><code id="spark_compile_+3A_jar">jar</code></td>
<td>
<p>The path to the <code>jar</code> program to be used, for
generating of the resulting <code>jar</code>.</p>
</td></tr>
<tr><td><code id="spark_compile_+3A_jar_dep">jar_dep</code></td>
<td>
<p>An optional list of additional <code>jar</code> dependencies.</p>
</td></tr>
<tr><td><code id="spark_compile_+3A_embedded_srcs">embedded_srcs</code></td>
<td>
<p>Embedded source file(s) under <code>&lt;R package root&gt;/java</code> to
be included in the root of the resulting jar file as resources</p>
</td></tr>
</table>

<hr>
<h2 id='spark_config'>Read Spark Configuration</h2><span id='topic+spark_config'></span>

<h3>Description</h3>

<p>Read Spark Configuration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config(file = "config.yml", use_default = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_config_+3A_file">file</code></td>
<td>
<p>Name of the configuration file</p>
</td></tr>
<tr><td><code id="spark_config_+3A_use_default">use_default</code></td>
<td>
<p>TRUE to use the built-in defaults provided in this package</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Read Spark configuration using the <span class="pkg"><a href="config.html#topic+config">config</a></span> package.
</p>


<h3>Value</h3>

<p>Named list with configuration data
</p>

<hr>
<h2 id='spark_config_exists'>A helper function to check value exist under <code>spark_config()</code></h2><span id='topic+spark_config_exists'></span>

<h3>Description</h3>

<p>A helper function to check value exist under <code>spark_config()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config_exists(config, name, default = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_config_exists_+3A_config">config</code></td>
<td>
<p>The configuration list from <code>spark_config()</code></p>
</td></tr>
<tr><td><code id="spark_config_exists_+3A_name">name</code></td>
<td>
<p>The name of the configuration entry</p>
</td></tr>
<tr><td><code id="spark_config_exists_+3A_default">default</code></td>
<td>
<p>The default value to use when entry is not present</p>
</td></tr>
</table>

<hr>
<h2 id='spark_config_kubernetes'>Kubernetes Configuration</h2><span id='topic+spark_config_kubernetes'></span>

<h3>Description</h3>

<p>Convenience function to initialize a Kubernetes configuration instead
of <code>spark_config()</code>, exposes common properties to set in Kubernetes
clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config_kubernetes(
  master,
  version = "3.2.3",
  image = "spark:sparklyr",
  driver = random_string("sparklyr-"),
  account = "spark",
  jars = "local:///opt/sparklyr",
  forward = TRUE,
  executors = NULL,
  conf = NULL,
  timeout = 120,
  ports = c(8880, 8881, 4040),
  fix_config = identical(.Platform$OS.type, "windows"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_config_kubernetes_+3A_master">master</code></td>
<td>
<p>Kubernetes url to connect to, found by running <code>kubectl cluster-info</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_version">version</code></td>
<td>
<p>The version of Spark being used.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_image">image</code></td>
<td>
<p>Container image to use to launch Spark and sparklyr. Also known
as <code>spark.kubernetes.container.image</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_driver">driver</code></td>
<td>
<p>Name of the driver pod. If not set, the driver pod name is set
to &quot;sparklyr&quot; suffixed by id to avoid name conflicts. Also known as
<code>spark.kubernetes.driver.pod.name</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_account">account</code></td>
<td>
<p>Service account that is used when running the driver pod. The driver
pod uses this service account when requesting executor pods from the API
server. Also known as <code>spark.kubernetes.authenticate.driver.serviceAccountName</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_jars">jars</code></td>
<td>
<p>Path to the sparklyr jars; either, a local path inside the container
image with the sparklyr jars copied when the image was created or, a path
accesible by the container where the sparklyr jars were copied. You can find
a path to the sparklyr jars by running <code>system.file("java/", package = "sparklyr")</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_forward">forward</code></td>
<td>
<p>Should ports used in sparklyr be forwarded automatically through Kubernetes?
Default to <code>TRUE</code> which runs <code>kubectl port-forward</code> and <code>pkill kubectl</code>
on disconnection.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_executors">executors</code></td>
<td>
<p>Number of executors to request while connecting.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_conf">conf</code></td>
<td>
<p>A named list of additional entries to add to <code>sparklyr.shell.conf</code>.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_timeout">timeout</code></td>
<td>
<p>Total seconds to wait before giving up on connection.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_ports">ports</code></td>
<td>
<p>Ports to forward using kubectl.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_fix_config">fix_config</code></td>
<td>
<p>Should the spark-defaults.conf get fixed? <code>TRUE</code> for Windows.</p>
</td></tr>
<tr><td><code id="spark_config_kubernetes_+3A_...">...</code></td>
<td>
<p>Additional parameters, currently not in use.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_config_packages'>Creates Spark Configuration</h2><span id='topic+spark_config_packages'></span>

<h3>Description</h3>

<p>Creates Spark Configuration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config_packages(config, packages, version, scala_version = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_config_packages_+3A_config">config</code></td>
<td>
<p>The Spark configuration object.</p>
</td></tr>
<tr><td><code id="spark_config_packages_+3A_packages">packages</code></td>
<td>
<p>A list of named packages or versioned packagese to add.</p>
</td></tr>
<tr><td><code id="spark_config_packages_+3A_version">version</code></td>
<td>
<p>The version of Spark being used.</p>
</td></tr>
<tr><td><code id="spark_config_packages_+3A_scala_version">scala_version</code></td>
<td>
<p>Acceptable Scala version of packages to be loaded</p>
</td></tr>
<tr><td><code id="spark_config_packages_+3A_...">...</code></td>
<td>
<p>Additional configurations</p>
</td></tr>
</table>

<hr>
<h2 id='spark_config_settings'>Retrieve Available Settings</h2><span id='topic+spark_config_settings'></span>

<h3>Description</h3>

<p>Retrieves available sparklyr settings that can be used in configuration files or <code>spark_config()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config_settings()
</code></pre>

<hr>
<h2 id='spark_config_value'>A helper function to retrieve values from <code>spark_config()</code></h2><span id='topic+spark_config_value'></span>

<h3>Description</h3>

<p>A helper function to retrieve values from <code>spark_config()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_config_value(config, name, default = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_config_value_+3A_config">config</code></td>
<td>
<p>The configuration list from <code>spark_config()</code></p>
</td></tr>
<tr><td><code id="spark_config_value_+3A_name">name</code></td>
<td>
<p>The name of the configuration entry</p>
</td></tr>
<tr><td><code id="spark_config_value_+3A_default">default</code></td>
<td>
<p>The default value to use when entry is not present</p>
</td></tr>
</table>

<hr>
<h2 id='spark_connect_method'>Function that negotiates the connection with the Spark back-end</h2><span id='topic+spark_connect_method'></span>

<h3>Description</h3>

<p>Function that negotiates the connection with the Spark back-end
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_connect_method(
  x,
  method,
  master,
  spark_home,
  config,
  app_name,
  version,
  hadoop_version,
  extensions,
  scala_version,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_connect_method_+3A_x">x</code></td>
<td>
<p>A dummy method object to determine which code to use to connect</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_method">method</code></td>
<td>
<p>The method used to connect to Spark. Default connection method
is <code>"shell"</code> to connect using spark-submit, use <code>"livy"</code> to
perform remote connections using HTTP, or <code>"databricks"</code> when using a
Databricks clusters.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_master">master</code></td>
<td>
<p>Spark cluster url to connect to. Use <code>"local"</code> to
connect to a local instance of Spark installed via
<code><a href="#topic+spark_install">spark_install</a></code>.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to a Spark installation. Defaults to the path
provided by the <code>SPARK_HOME</code> environment variable. If
<code>SPARK_HOME</code> is defined, it will always be used unless the
<code>version</code> parameter is specified to force the use of a locally
installed version.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_config">config</code></td>
<td>
<p>Custom configuration for the generated Spark connection. See
<code><a href="#topic+spark_config">spark_config</a></code> for details.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_app_name">app_name</code></td>
<td>
<p>The application name to be used while running in the Spark
cluster.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_version">version</code></td>
<td>
<p>The version of Spark to use. Required for <code>"local"</code> Spark
connections, optional otherwise.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_hadoop_version">hadoop_version</code></td>
<td>
<p>Version of Hadoop to use</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_extensions">extensions</code></td>
<td>
<p>Extension R packages to enable for this connection. By
default, all packages enabled through the use of
<code><a href="#topic+register_extension">sparklyr::register_extension</a></code> will be passed here.</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_scala_version">scala_version</code></td>
<td>
<p>Load the sparklyr jar file that is built with the version of
Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will
by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore
&lsquo;scala_version = &rsquo;2.12'' is needed if sparklyr is connecting to Spark 2.4 built with
Scala 2.12)</p>
</td></tr>
<tr><td><code id="spark_connect_method_+3A_...">...</code></td>
<td>
<p>Additional params to be passed to each 'spark_disconnect()' call
(e.g., 'terminate = TRUE')</p>
</td></tr>
</table>

<hr>
<h2 id='spark_connection'>Retrieve the Spark Connection Associated with an R Object</h2><span id='topic+spark_connection'></span>

<h3>Description</h3>

<p>Retrieve the <code>spark_connection</code> associated with an <span class="rlang"><b>R</b></span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_connection(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_connection_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object from which a <code>spark_connection</code> can be obtained.</p>
</td></tr>
<tr><td><code id="spark_connection_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_connection_find'>Find Spark Connection</h2><span id='topic+spark_connection_find'></span>

<h3>Description</h3>

<p>Finds an active spark connection in the environment given the
connection parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_connection_find(master = NULL, app_name = NULL, method = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_connection_find_+3A_master">master</code></td>
<td>
<p>The Spark master parameter.</p>
</td></tr>
<tr><td><code id="spark_connection_find_+3A_app_name">app_name</code></td>
<td>
<p>The Spark application name.</p>
</td></tr>
<tr><td><code id="spark_connection_find_+3A_method">method</code></td>
<td>
<p>The method used to connect to Spark.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_connection-class'>spark_connection class</h2><span id='topic+spark_connection-class'></span>

<h3>Description</h3>

<p>spark_connection class
</p>

<hr>
<h2 id='spark_context_config'>Runtime configuration interface for the Spark Context.</h2><span id='topic+spark_context_config'></span>

<h3>Description</h3>

<p>Retrieves the runtime configuration interface for the Spark Context.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_context_config(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_context_config_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_dataframe'>Retrieve a Spark DataFrame</h2><span id='topic+spark_dataframe'></span>

<h3>Description</h3>

<p>This S3 generic is used to access a Spark DataFrame object (as a Java
object reference) from an <span class="rlang"><b>R</b></span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_dataframe(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_dataframe_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object wrapping, or containing, a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="spark_dataframe_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+spark_jobj">spark_jobj</a></code> representing a Java object reference
to a Spark DataFrame.
</p>

<hr>
<h2 id='spark_default_compilation_spec'>Default Compilation Specification for Spark Extensions</h2><span id='topic+spark_default_compilation_spec'></span>

<h3>Description</h3>

<p>This is the default compilation specification used for
Spark extensions, when used with <code><a href="#topic+compile_package_jars">compile_package_jars</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_default_compilation_spec(
  pkg = infer_active_package_name(),
  locations = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_default_compilation_spec_+3A_pkg">pkg</code></td>
<td>
<p>The package containing Spark extensions to be compiled.</p>
</td></tr>
<tr><td><code id="spark_default_compilation_spec_+3A_locations">locations</code></td>
<td>
<p>Additional locations to scan. By default, the
directories <code>/opt/scala</code> and <code>/usr/local/scala</code> will
be scanned.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_default_version'>determine the version that will be used by default if version is NULL</h2><span id='topic+spark_default_version'></span>

<h3>Description</h3>

<p>determine the version that will be used by default if version is NULL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_default_version()
</code></pre>

<hr>
<h2 id='spark_dependency'>Define a Spark dependency</h2><span id='topic+spark_dependency'></span>

<h3>Description</h3>

<p>Define a Spark dependency consisting of a set of custom JARs, Spark packages,
and customized dbplyr SQL translation env.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_dependency(
  jars = NULL,
  packages = NULL,
  initializer = NULL,
  catalog = NULL,
  repositories = NULL,
  dbplyr_sql_variant = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_dependency_+3A_jars">jars</code></td>
<td>
<p>Character vector of full paths to JAR files.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_packages">packages</code></td>
<td>
<p>Character vector of Spark packages names.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_initializer">initializer</code></td>
<td>
<p>Optional callback function called when initializing a connection.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_catalog">catalog</code></td>
<td>
<p>Optional location where extension JAR files can be downloaded for Livy.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_repositories">repositories</code></td>
<td>
<p>Character vector of Spark package repositories.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_dbplyr_sql_variant">dbplyr_sql_variant</code></td>
<td>
<p>Customization of dbplyr SQL translation env. Must be a
named list of the following form:
<code>
  list(
    scalar = list(scalar_fn1 = ..., scalar_fn2 = ..., &lt;etc&gt;),
    aggregate = list(agg_fn1 = ..., agg_fn2 = ..., &lt;etc&gt;),
    window = list(wnd_fn1 = ..., wnd_fn2 = ..., &lt;etc&gt;)
  )
  </code>
See <a href="dbplyr.html#topic+sql_substr">sql_variant</a> for details.</p>
</td></tr>
<tr><td><code id="spark_dependency_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type 'spark_dependency'
</p>

<hr>
<h2 id='spark_dependency_fallback'>Fallback to Spark Dependency</h2><span id='topic+spark_dependency_fallback'></span>

<h3>Description</h3>

<p>Helper function to assist falling back to previous Spark versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_dependency_fallback(spark_version, supported_versions)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_dependency_fallback_+3A_spark_version">spark_version</code></td>
<td>
<p>The Spark version being requested in <code>spark_dependencies</code>.</p>
</td></tr>
<tr><td><code id="spark_dependency_fallback_+3A_supported_versions">supported_versions</code></td>
<td>
<p>The Spark versions that are supported by this extension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Spark version to use.
</p>

<hr>
<h2 id='spark_extension'>Create Spark Extension</h2><span id='topic+spark_extension'></span>

<h3>Description</h3>

<p>Creates an R package ready to be used as an Spark extension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_extension(path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_extension_+3A_path">path</code></td>
<td>
<p>Location where the extension will be created.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_get_java'>Find path to Java</h2><span id='topic+spark_get_java'></span>

<h3>Description</h3>

<p>Finds the path to <code>JAVA_HOME</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_get_java(throws = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_get_java_+3A_throws">throws</code></td>
<td>
<p>Throw an error when path not found?</p>
</td></tr>
</table>

<hr>
<h2 id='spark_home_dir'>Find the SPARK_HOME directory for a version of Spark</h2><span id='topic+spark_home_dir'></span>

<h3>Description</h3>

<p>Find the SPARK_HOME directory for a given version of Spark that
was previously installed using <code><a href="#topic+spark_install">spark_install</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_home_dir(version = NULL, hadoop_version = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_home_dir_+3A_version">version</code></td>
<td>
<p>Version of Spark</p>
</td></tr>
<tr><td><code id="spark_home_dir_+3A_hadoop_version">hadoop_version</code></td>
<td>
<p>Version of Hadoop</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Path to SPARK_HOME (or <code>NULL</code> if the specified version
was not found).
</p>

<hr>
<h2 id='spark_home_set'>Set the SPARK_HOME environment variable</h2><span id='topic+spark_home_set'></span>

<h3>Description</h3>

<p>Set the <code>SPARK_HOME</code> environment variable. This slightly speeds up some
operations, including the connection time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_home_set(path = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_home_set_+3A_path">path</code></td>
<td>
<p>A string containing the path to the installation location of
Spark. If <code>NULL</code>, the path to the most latest Spark/Hadoop versions is
used.</p>
</td></tr>
<tr><td><code id="spark_home_set_+3A_...">...</code></td>
<td>
<p>Additional parameters not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function is mostly invoked for the side-effect of setting the
<code>SPARK_HOME</code> environment variable. It also returns <code>TRUE</code> if the
environment was successfully set, and <code>FALSE</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Not run due to side-effects
spark_home_set()

## End(Not run)
</code></pre>

<hr>
<h2 id='spark_ide_connection_open'>Set of functions to provide integration with the RStudio IDE</h2><span id='topic+spark_ide_connection_open'></span><span id='topic+spark_ide_connection_closed'></span><span id='topic+spark_ide_connection_updated'></span><span id='topic+spark_ide_connection_actions'></span><span id='topic+spark_ide_objects'></span><span id='topic+spark_ide_columns'></span><span id='topic+spark_ide_preview'></span>

<h3>Description</h3>

<p>Set of functions to provide integration with the RStudio IDE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_ide_connection_open(con, env, connect_call)

spark_ide_connection_closed(con)

spark_ide_connection_updated(con, hint)

spark_ide_connection_actions(con)

spark_ide_objects(con, catalog, schema, name, type)

spark_ide_columns(
  con,
  table = NULL,
  view = NULL,
  catalog = NULL,
  schema = NULL
)

spark_ide_preview(
  con,
  rowLimit,
  table = NULL,
  view = NULL,
  catalog = NULL,
  schema = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_ide_connection_open_+3A_con">con</code></td>
<td>
<p>Valid Spark connection</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_env">env</code></td>
<td>
<p>R environment of the interactive R session</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_connect_call">connect_call</code></td>
<td>
<p>R code that can be used to re-connect to the Spark connection</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_hint">hint</code></td>
<td>
<p>Name of the Spark connection that the RStudio IDE can use as reference.</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_catalog">catalog</code></td>
<td>
<p>Name of the top level of the requested table or view</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_schema">schema</code></td>
<td>
<p>Name of the second most top level of the requested level or view</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_name">name</code></td>
<td>
<p>The new of the view or table being requested</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_type">type</code></td>
<td>
<p>Type of the object being requested, 'view' or 'table'</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_table">table</code></td>
<td>
<p>Name of the requested table</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_view">view</code></td>
<td>
<p>Name of the requested view</p>
</td></tr>
<tr><td><code id="spark_ide_connection_open_+3A_rowlimit">rowLimit</code></td>
<td>
<p>The number of rows to show in the 'Preview' pane of the RStudio
IDE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These function are meant for downstream packages, that provide additional
backends to 'sparklyr', to override the opening, closing, update, and preview
functionality. The arguments are driven by what the RStudio IDE API expects them
to be, so this is the reason why some use 'type' to designated views or tables,
and others have one argument for 'table', and another for 'view'.
</p>

<hr>
<h2 id='spark_insert_table'>Inserts a Spark DataFrame into a Spark table</h2><span id='topic+spark_insert_table'></span>

<h3>Description</h3>

<p>Inserts a Spark DataFrame into a Spark table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_insert_table(
  x,
  name,
  mode = NULL,
  overwrite = FALSE,
  options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_insert_table_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_insert_table_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_insert_table_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_insert_table_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_insert_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_insert_table_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_install'>Download and install various versions of Spark</h2><span id='topic+spark_install'></span><span id='topic+spark_uninstall'></span><span id='topic+spark_install_dir'></span><span id='topic+spark_install_tar'></span><span id='topic+spark_installed_versions'></span><span id='topic+spark_available_versions'></span>

<h3>Description</h3>

<p>Install versions of Spark for use with local Spark connections
(i.e. <code>spark_connect(master = "local"</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_install(
  version = NULL,
  hadoop_version = NULL,
  reset = TRUE,
  logging = "INFO",
  verbose = interactive()
)

spark_uninstall(version, hadoop_version)

spark_install_dir()

spark_install_tar(tarfile)

spark_installed_versions()

spark_available_versions(
  show_hadoop = FALSE,
  show_minor = FALSE,
  show_future = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_install_+3A_version">version</code></td>
<td>
<p>Version of Spark to install. See <code>spark_available_versions</code> for a list of supported versions</p>
</td></tr>
<tr><td><code id="spark_install_+3A_hadoop_version">hadoop_version</code></td>
<td>
<p>Version of Hadoop to install. See <code>spark_available_versions</code> for a list of supported versions</p>
</td></tr>
<tr><td><code id="spark_install_+3A_reset">reset</code></td>
<td>
<p>Attempts to reset settings to defaults.</p>
</td></tr>
<tr><td><code id="spark_install_+3A_logging">logging</code></td>
<td>
<p>Logging level to configure install. Supported options: &quot;WARN&quot;, &quot;INFO&quot;</p>
</td></tr>
<tr><td><code id="spark_install_+3A_verbose">verbose</code></td>
<td>
<p>Report information as Spark is downloaded / installed</p>
</td></tr>
<tr><td><code id="spark_install_+3A_tarfile">tarfile</code></td>
<td>
<p>Path to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ###
reference spark and hadoop versions respectively.</p>
</td></tr>
<tr><td><code id="spark_install_+3A_show_hadoop">show_hadoop</code></td>
<td>
<p>Show Hadoop distributions?</p>
</td></tr>
<tr><td><code id="spark_install_+3A_show_minor">show_minor</code></td>
<td>
<p>Show minor Spark versions?</p>
</td></tr>
<tr><td><code id="spark_install_+3A_show_future">show_future</code></td>
<td>
<p>Should future versions which have not been released be shown?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with information about the installed version.
</p>

<hr>
<h2 id='spark_install_find'>Find a given Spark installation by version.</h2><span id='topic+spark_install_find'></span>

<h3>Description</h3>

<p>Find a given Spark installation by version.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_install_find(
  version = NULL,
  hadoop_version = NULL,
  installed_only = TRUE,
  latest = FALSE,
  hint = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_install_find_+3A_version">version</code></td>
<td>
<p>Version of Spark to install. See <code>spark_available_versions</code> for a list of supported versions</p>
</td></tr>
<tr><td><code id="spark_install_find_+3A_hadoop_version">hadoop_version</code></td>
<td>
<p>Version of Hadoop to install. See <code>spark_available_versions</code> for a list of supported versions</p>
</td></tr>
<tr><td><code id="spark_install_find_+3A_installed_only">installed_only</code></td>
<td>
<p>Search only the locally installed versions?</p>
</td></tr>
<tr><td><code id="spark_install_find_+3A_latest">latest</code></td>
<td>
<p>Check for latest version?</p>
</td></tr>
<tr><td><code id="spark_install_find_+3A_hint">hint</code></td>
<td>
<p>On failure should the installation code be provided?</p>
</td></tr>
</table>

<hr>
<h2 id='spark_install_sync'>helper function to sync sparkinstall project to sparklyr</h2><span id='topic+spark_install_sync'></span>

<h3>Description</h3>

<p>See: https://github.com/rstudio/spark-install
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_install_sync(project_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_install_sync_+3A_project_path">project_path</code></td>
<td>
<p>The path to the sparkinstall project</p>
</td></tr>
</table>

<hr>
<h2 id='spark_integ_test_skip'>It lets the package know if it should test a particular functionality or not</h2><span id='topic+spark_integ_test_skip'></span>

<h3>Description</h3>

<p>It lets the package know if it should test a particular functionality or not
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_integ_test_skip(sc, test_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_integ_test_skip_+3A_sc">sc</code></td>
<td>
<p>Spark connection</p>
</td></tr>
<tr><td><code id="spark_integ_test_skip_+3A_test_name">test_name</code></td>
<td>
<p>The name of the test</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It expects a boolean to be returned. If TRUE, the corresponding test will be
skipped. If FALSE the test will be conducted.
</p>

<hr>
<h2 id='spark_jobj'>Retrieve a Spark JVM Object Reference</h2><span id='topic+spark_jobj'></span>

<h3>Description</h3>

<p>This S3 generic is used for accessing the underlying Java Virtual Machine
(JVM) Spark objects associated with <span class="rlang"><b>R</b></span> objects. These objects act as
references to Spark objects living in the JVM. Methods on these objects
can be called with the <code><a href="#topic+invoke">invoke</a></code> family of functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_jobj(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_jobj_+3A_x">x</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object containing, or wrapping, a <code>spark_jobj</code>.</p>
</td></tr>
<tr><td><code id="spark_jobj_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+invoke">invoke</a></code>, for calling methods on Java object references.
</p>

<hr>
<h2 id='spark_jobj-class'>spark_jobj class</h2><span id='topic+spark_jobj-class'></span>

<h3>Description</h3>

<p>spark_jobj class
</p>

<hr>
<h2 id='spark_last_error'>Surfaces the last error from Spark captured by internal 'spark_error' function</h2><span id='topic+spark_last_error'></span>

<h3>Description</h3>

<p>Surfaces the last error from Spark captured by internal 'spark_error' function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_last_error()
</code></pre>

<hr>
<h2 id='spark_load_table'>Reads from a Spark Table into a Spark DataFrame.</h2><span id='topic+spark_load_table'></span>

<h3>Description</h3>

<p>Reads from a Spark Table into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_load_table(
  sc,
  name,
  path,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_load_table_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options. See
<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_load_table_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_log'>View Entries in the Spark Log</h2><span id='topic+spark_log'></span>

<h3>Description</h3>

<p>View the most recent entries in the Spark log. This can be useful when
inspecting output / errors produced by Spark during the invocation of
various commands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_log(sc, n = 100, filter = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_log_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_log_+3A_n">n</code></td>
<td>
<p>The max number of log entries to retrieve. Use <code>NULL</code> to
retrieve all entries within the log.</p>
</td></tr>
<tr><td><code id="spark_log_+3A_filter">filter</code></td>
<td>
<p>Character string to filter log entries.</p>
</td></tr>
<tr><td><code id="spark_log_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_pipeline_stage'>Create a Pipeline Stage Object</h2><span id='topic+spark_pipeline_stage'></span>

<h3>Description</h3>

<p>Helper function to create pipeline stage objects with common parameter setters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_pipeline_stage(
  sc,
  class,
  uid,
  features_col = NULL,
  label_col = NULL,
  prediction_col = NULL,
  probability_col = NULL,
  raw_prediction_col = NULL,
  k = NULL,
  max_iter = NULL,
  seed = NULL,
  input_col = NULL,
  input_cols = NULL,
  output_col = NULL,
  output_cols = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_pipeline_stage_+3A_sc">sc</code></td>
<td>
<p>A 'spark_connection' object.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_class">class</code></td>
<td>
<p>Class name for the pipeline stage.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_uid">uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_label_col">label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code><a href="#topic+ft_r_formula">ft_r_formula</a></code>.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_probability_col">probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_raw_prediction_col">raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_k">k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_seed">seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_input_col">input_col</code></td>
<td>
<p>The name of the input column.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_input_cols">input_cols</code></td>
<td>
<p>Names of output columns.</p>
</td></tr>
<tr><td><code id="spark_pipeline_stage_+3A_output_col">output_col</code></td>
<td>
<p>The name of the output column.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_read'>Read file(s) into a Spark DataFrame using a custom reader</h2><span id='topic+spark_read'></span>

<h3>Description</h3>

<p>Run a custom R function on Spark workers to ingest data from one or more files
into a Spark DataFrame, assuming all files follow the same schema.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read(sc, paths, reader, columns, packages = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_+3A_paths">paths</code></td>
<td>
<p>A character vector of one or more file URIs (e.g.,
c(&quot;hdfs://localhost:9000/file.txt&quot;, &quot;hdfs://localhost:9000/file2.txt&quot;))</p>
</td></tr>
<tr><td><code id="spark_read_+3A_reader">reader</code></td>
<td>
<p>A self-contained R function that takes a single file URI as
argument and returns the data read from that file as a data frame.</p>
</td></tr>
<tr><td><code id="spark_read_+3A_columns">columns</code></td>
<td>
<p>a named list of column names and column types of the resulting
data frame (e.g., list(column_1 = &quot;integer&quot;, column_2 = &quot;character&quot;)), or a
list of column names only if column types should be inferred from the data
(e.g., list(&quot;column_1&quot;, &quot;column_2&quot;), or NULL if column types should be
inferred and resulting data frame can have arbitrary column names</p>
</td></tr>
<tr><td><code id="spark_read_+3A_packages">packages</code></td>
<td>
<p>A list of R packages to distribute to Spark workers</p>
</td></tr>
<tr><td><code id="spark_read_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)
sc &lt;- spark_connect(
  master = "yarn",
  spark_home = "~/spark/spark-2.4.5-bin-hadoop2.7"
)

# This is a contrived example to show reader tasks will be distributed across
# all Spark worker nodes
spark_read(
  sc,
  rep("/dev/null", 10),
  reader = function(path) system("hostname", intern = TRUE),
  columns = c(hostname = "string")
) %&gt;% sdf_collect()

## End(Not run)

</code></pre>

<hr>
<h2 id='spark_read_avro'>Read Apache Avro data into a Spark DataFrame.</h2><span id='topic+spark_read_avro'></span>

<h3>Description</h3>

<p>Notice this functionality requires the Spark connection <code>sc</code> to be instantiated with either
an explicitly specified Spark version (i.e.,
<code>spark_connect(..., version = &lt;version&gt;, packages = c("avro", &lt;other package(s)&gt;), ...)</code>)
or a specific version of Spark avro package to use (e.g.,
<code>spark_connect(..., packages = c("org.apache.spark:spark-avro_2.12:3.0.0", &lt;other package(s)&gt;), ...)</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_avro(
  sc,
  name = NULL,
  path = name,
  avro_schema = NULL,
  ignore_extension = TRUE,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_avro_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_avro_schema">avro_schema</code></td>
<td>
<p>Optional Avro schema in JSON format</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_ignore_extension">ignore_extension</code></td>
<td>
<p>If enabled, all files with and without .avro extension
are loaded (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_avro_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_binary'>Read binary data into a Spark DataFrame.</h2><span id='topic+spark_read_binary'></span>

<h3>Description</h3>

<p>Read binary files within a directory and convert each file into a record
within the resulting Spark dataframe. The output will be a Spark dataframe
with the following columns and possibly partition columns:
</p>

<ul>
<li><p> path: StringType
</p>
</li>
<li><p> modificationTime: TimestampType
</p>
</li>
<li><p> length: LongType
</p>
</li>
<li><p> content: BinaryType
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>spark_read_binary(
  sc,
  name = NULL,
  dir = name,
  path_glob_filter = "*",
  recursive_file_lookup = FALSE,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_binary_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_dir">dir</code></td>
<td>
<p>Directory to read binary files from.</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_path_glob_filter">path_glob_filter</code></td>
<td>
<p>Glob pattern of binary files to be loaded
(e.g., &quot;*.jpg&quot;).</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_recursive_file_lookup">recursive_file_lookup</code></td>
<td>
<p>If FALSE (default), then partition discovery
will be enabled (i.e., if a partition naming scheme is present, then
partitions specified by subdirectory names such as &quot;date=2019-07-01&quot; will
be created and files outside subdirectories following a partition naming
scheme will be ignored). If TRUE, then all nested directories will be
searched even if their names do not follow a partition naming scheme.</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_binary_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_csv'>Read a CSV file into a Spark DataFrame</h2><span id='topic+spark_read_csv'></span>

<h3>Description</h3>

<p>Read a tabular data file into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_csv(
  sc,
  name = NULL,
  path = name,
  header = TRUE,
  columns = NULL,
  infer_schema = is.null(columns),
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_csv_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_header">header</code></td>
<td>
<p>Boolean; should the first row of data be used as a header?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_infer_schema">infer_schema</code></td>
<td>
<p>Boolean; should column types be automatically inferred?
Requires one extra pass over the data. Defaults to <code>is.null(columns)</code>.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_delimiter">delimiter</code></td>
<td>
<p>The character used to delimit each column. Defaults to &lsquo;<span class="samp">&#8288;','&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_quote">quote</code></td>
<td>
<p>The character used as a quote. Defaults to &lsquo;<span class="samp">&#8288;'"'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_escape">escape</code></td>
<td>
<p>The character used to escape other characters. Defaults to &lsquo;<span class="samp">&#8288;'\'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_charset">charset</code></td>
<td>
<p>The character set. Defaults to &lsquo;<span class="samp">&#8288;"UTF-8"&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_null_value">null_value</code></td>
<td>
<p>The character to use for null, or missing, values. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_csv_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>),
as well as the local file system (<code>file://</code>).
</p>
<p>When <code>header</code> is <code>FALSE</code>, the column names are generated with a
<code>V</code> prefix; e.g. <code>V1, V2, ...</code>.
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_delta'>Read from Delta Lake into a Spark DataFrame.</h2><span id='topic+spark_read_delta'></span>

<h3>Description</h3>

<p>Read from Delta Lake into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_delta(
  sc,
  path,
  name = NULL,
  version = NULL,
  timestamp = NULL,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_delta_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_version">version</code></td>
<td>
<p>The version of the delta table to read.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_timestamp">timestamp</code></td>
<td>
<p>The timestamp of the delta table to read. For example,
<code>"2019-01-01"</code> or <code>"2019-01-01'T'00:00:00.000Z"</code>.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_delta_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_image'>Read image data into a Spark DataFrame.</h2><span id='topic+spark_read_image'></span>

<h3>Description</h3>

<p>Read image files within a directory and convert each file into a record
within the resulting Spark dataframe. The output will be a Spark dataframe
consisting of struct types containing the following attributes:
</p>

<ul>
<li><p> origin: StringType
</p>
</li>
<li><p> height: IntegerType
</p>
</li>
<li><p> width: IntegerType
</p>
</li>
<li><p> nChannels: IntegerType
</p>
</li>
<li><p> mode: IntegerType
</p>
</li>
<li><p> data: BinaryType
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>spark_read_image(
  sc,
  name = NULL,
  dir = name,
  drop_invalid = TRUE,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_image_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_dir">dir</code></td>
<td>
<p>Directory to read binary files from.</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_drop_invalid">drop_invalid</code></td>
<td>
<p>Whether to drop files that are not valid images from the
result (default: TRUE).</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_image_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_jdbc'>Read from JDBC connection into a Spark DataFrame.</h2><span id='topic+spark_read_jdbc'></span>

<h3>Description</h3>

<p>Read from JDBC connection into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_jdbc(
  sc,
  name,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  columns = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_jdbc_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.
See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_jdbc_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(
  master = "local",
  config = list(
    `sparklyr.shell.driver-class-path` = "/usr/share/java/mysql-connector-java-8.0.25.jar"
  )
)
spark_read_jdbc(
  sc,
  name = "my_sql_table",
  options = list(
    url = "jdbc:mysql://localhost:3306/my_sql_schema",
    driver = "com.mysql.jdbc.Driver",
    user = "me",
    password = "******",
    dbtable = "my_sql_table"
  )
)

## End(Not run)

</code></pre>

<hr>
<h2 id='spark_read_json'>Read a JSON file into a Spark DataFrame</h2><span id='topic+spark_read_json'></span>

<h3>Description</h3>

<p>Read a table serialized in the <a href="https://www.json.org/">JavaScript
Object Notation</a> format into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_json(
  sc,
  name = NULL,
  path = name,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  columns = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_json_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_json_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
the local file system (<code>file://</code>).
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_libsvm'>Read libsvm file into a Spark DataFrame.</h2><span id='topic+spark_read_libsvm'></span>

<h3>Description</h3>

<p>Read libsvm file into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_libsvm(
  sc,
  name = NULL,
  path = name,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_libsvm_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_read_libsvm_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_orc'>Read a ORC file into a Spark DataFrame</h2><span id='topic+spark_read_orc'></span>

<h3>Description</h3>

<p>Read a <a href="https://orc.apache.org/">ORC</a> file into a Spark
DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_orc(
  sc,
  name = NULL,
  path = name,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  columns = NULL,
  schema = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_orc_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.
See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_schema">schema</code></td>
<td>
<p>A (java) read schema. Useful for optimizing read operation on nested data.</p>
</td></tr>
<tr><td><code id="spark_read_orc_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
the local file system (<code>file://</code>).
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_parquet'>Read a Parquet file into a Spark DataFrame</h2><span id='topic+spark_read_parquet'></span>

<h3>Description</h3>

<p>Read a <a href="https://parquet.apache.org/">Parquet</a> file into a Spark
DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_parquet(
  sc,
  name = NULL,
  path = name,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  columns = NULL,
  schema = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_parquet_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_schema">schema</code></td>
<td>
<p>A (java) read schema. Useful for optimizing read operation on nested data.</p>
</td></tr>
<tr><td><code id="spark_read_parquet_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
the local file system (<code>file://</code>).
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_source'>Read from a generic source into a Spark DataFrame.</h2><span id='topic+spark_read_source'></span>

<h3>Description</h3>

<p>Read from a generic source into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_source(
  sc,
  name = NULL,
  path = name,
  source,
  options = list(),
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  columns = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_source_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_source">source</code></td>
<td>
<p>A data source capable of reading data.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.
See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_source_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_table'>Reads from a Spark Table into a Spark DataFrame.</h2><span id='topic+spark_read_table'></span>

<h3>Description</h3>

<p>Reads from a Spark Table into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_table(
  sc,
  name,
  options = list(),
  repartition = 0,
  memory = TRUE,
  columns = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_table_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.
See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="spark_read_table_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_read_text'>Read a Text file into a Spark DataFrame</h2><span id='topic+spark_read_text'></span>

<h3>Description</h3>

<p>Read a Text file into a Spark DataFrame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_text(
  sc,
  name = NULL,
  path = name,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE,
  options = list(),
  whole = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_text_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_repartition">repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_memory">memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_whole">whole</code></td>
<td>
<p>Read the entire text file as a single entry? Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="spark_read_text_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
the local file system (<code>file://</code>).
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_save_table'>Saves a Spark DataFrame as a Spark table</h2><span id='topic+spark_save_table'></span>

<h3>Description</h3>

<p>Saves a Spark DataFrame and as a Spark table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_save_table(x, path, mode = NULL, options = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_save_table_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_save_table_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_save_table_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_save_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_session_config'>Runtime configuration interface for the Spark Session</h2><span id='topic+spark_session_config'></span>

<h3>Description</h3>

<p>Retrieves or sets runtime configuration entries for the Spark Session
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_session_config(sc, config = TRUE, value = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_session_config_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_session_config_+3A_config">config</code></td>
<td>
<p>The configuration entry name(s) (e.g., <code>"spark.sql.shuffle.partitions"</code>).
Defaults to <code>NULL</code> to retrieve all configuration entries.</p>
</td></tr>
<tr><td><code id="spark_session_config_+3A_value">value</code></td>
<td>
<p>The configuration value to be set. Defaults to <code>NULL</code> to retrieve
configuration entries.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark runtime configuration: 
<code><a href="#topic+spark_adaptive_query_execution">spark_adaptive_query_execution</a>()</code>,
<code><a href="#topic+spark_advisory_shuffle_partition_size">spark_advisory_shuffle_partition_size</a>()</code>,
<code><a href="#topic+spark_auto_broadcast_join_threshold">spark_auto_broadcast_join_threshold</a>()</code>,
<code><a href="#topic+spark_coalesce_initial_num_partitions">spark_coalesce_initial_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_min_num_partitions">spark_coalesce_min_num_partitions</a>()</code>,
<code><a href="#topic+spark_coalesce_shuffle_partitions">spark_coalesce_shuffle_partitions</a>()</code>
</p>

<hr>
<h2 id='spark_statistical_routines'>Generate random samples from some distribution</h2><span id='topic+spark_statistical_routines'></span>

<h3>Description</h3>

<p>Generator methods for creating single-column Spark dataframes comprised of
i.i.d. samples from some distribution.
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_statistical_routines_+3A_sc">sc</code></td>
<td>
<p>A Spark connection.</p>
</td></tr>
<tr><td><code id="spark_statistical_routines_+3A_n">n</code></td>
<td>
<p>Sample Size (default: 1000).</p>
</td></tr>
<tr><td><code id="spark_statistical_routines_+3A_num_partitions">num_partitions</code></td>
<td>
<p>Number of partitions in the resulting Spark dataframe
(default: default parallelism of the Spark cluster).</p>
</td></tr>
<tr><td><code id="spark_statistical_routines_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: a random long integer).</p>
</td></tr>
<tr><td><code id="spark_statistical_routines_+3A_output_col">output_col</code></td>
<td>
<p>Name of the output column containing sample values (default: &quot;x&quot;).</p>
</td></tr>
</table>

<hr>
<h2 id='spark_table_name'>Generate a Table Name from Expression</h2><span id='topic+spark_table_name'></span>

<h3>Description</h3>

<p>Attempts to generate a table name from an expression; otherwise,
assigns an auto-generated generic name with &quot;sparklyr_&quot; prefix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_table_name(expr)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_table_name_+3A_expr">expr</code></td>
<td>
<p>The expression to attempt to use as name</p>
</td></tr>
</table>

<hr>
<h2 id='spark_version'>Get the Spark Version Associated with a Spark Connection</h2><span id='topic+spark_version'></span>

<h3>Description</h3>

<p>Retrieve the version of Spark associated with a Spark connection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_version(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_version_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suffixes for e.g. preview versions, or snapshotted versions,
are trimmed &ndash; if you require the full Spark version, you can
retrieve it with <code>invoke(spark_context(sc), "version")</code>.
</p>


<h3>Value</h3>

<p>The Spark version as a <code><a href="base.html#topic+numeric_version">numeric_version</a></code>.
</p>

<hr>
<h2 id='spark_version_from_home'>Get the Spark Version Associated with a Spark Installation</h2><span id='topic+spark_version_from_home'></span>

<h3>Description</h3>

<p>Retrieve the version of Spark associated with a Spark installation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_version_from_home(spark_home, default = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_version_from_home_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to a Spark installation.</p>
</td></tr>
<tr><td><code id="spark_version_from_home_+3A_default">default</code></td>
<td>
<p>The default version to be inferred, in case
version lookup failed, e.g. no Spark installation was found
at <code>spark_home</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_versions'>Returns a data frame of available Spark versions that can be installed.</h2><span id='topic+spark_versions'></span>

<h3>Description</h3>

<p>Returns a data frame of available Spark versions that can be installed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_versions(latest = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_versions_+3A_latest">latest</code></td>
<td>
<p>Check for latest version?</p>
</td></tr>
</table>

<hr>
<h2 id='spark_web'>Open the Spark web interface</h2><span id='topic+spark_web'></span>

<h3>Description</h3>

<p>Open the Spark web interface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_web(sc, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_web_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark_web_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='spark_write'>Write Spark DataFrame to file using a custom writer</h2><span id='topic+spark_write'></span>

<h3>Description</h3>

<p>Run a custom R function on Spark worker to write a Spark DataFrame
into file(s). If Spark's speculative execution feature is enabled (i.e.,
'spark.speculation' is true), then each write task may be executed more than
once and the user-defined writer function will need to ensure no concurrent
writes happen to the same file path (e.g., by appending UUID to each file name).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write(x, writer, paths, packages = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_+3A_x">x</code></td>
<td>
<p>A Spark Dataframe to be saved into file(s)</p>
</td></tr>
<tr><td><code id="spark_write_+3A_writer">writer</code></td>
<td>
<p>A writer function with the signature function(partition, path)
where <code>partition</code> is a R dataframe containing all rows from one partition
of the original Spark Dataframe <code>x</code> and path is a string specifying the
file to write <code>partition</code> to</p>
</td></tr>
<tr><td><code id="spark_write_+3A_paths">paths</code></td>
<td>
<p>A single destination path or a list of destination paths, each one
specifying a location for a partition from <code>x</code> to be written to. If
number of partition(s) in <code>x</code> is not equal to <code>length(paths)</code> then
<code>x</code> will be re-partitioned to contain <code>length(paths)</code> partition(s)</p>
</td></tr>
<tr><td><code id="spark_write_+3A_packages">packages</code></td>
<td>
<p>Boolean to distribute <code>.libPaths()</code> packages to each node,
a list of packages to distribute, or a package bundle created with</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)

sc &lt;- spark_connect(master = "local[3]")

# copy some test data into a Spark Dataframe
sdf &lt;- sdf_copy_to(sc, iris, overwrite = TRUE)

# create a writer function
writer &lt;- function(df, path) {
  write.csv(df, path)
}

spark_write(
  sdf,
  writer,
  # re-partition sdf into 3 partitions and write them to 3 separate files
  paths = list("file:///tmp/file1", "file:///tmp/file2", "file:///tmp/file3"),
)

spark_write(
  sdf,
  writer,
  # save all rows into a single file
  paths = list("file:///tmp/all_rows")
)

## End(Not run)

</code></pre>

<hr>
<h2 id='spark_write_avro'>Serialize a Spark DataFrame into Apache Avro format</h2><span id='topic+spark_write_avro'></span>

<h3>Description</h3>

<p>Notice this functionality requires the Spark connection <code>sc</code> to be
instantiated with either
an explicitly specified Spark version (i.e.,
<code>spark_connect(..., version = &lt;version&gt;, packages = c("avro", &lt;other package(s)&gt;), ...)</code>)
or a specific version of Spark avro package to use (e.g.,
<code>spark_connect(..., packages =
c("org.apache.spark:spark-avro_2.12:3.0.0", &lt;other package(s)&gt;), ...)</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_avro(
  x,
  path,
  avro_schema = NULL,
  record_name = "topLevelRecord",
  record_namespace = "",
  compression = "snappy",
  partition_by = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_avro_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_avro_schema">avro_schema</code></td>
<td>
<p>Optional Avro schema in JSON format</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_record_name">record_name</code></td>
<td>
<p>Optional top level record name in write result
(default: &quot;topLevelRecord&quot;)</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_record_namespace">record_namespace</code></td>
<td>
<p>Record namespace in write result (default: &quot;&quot;)</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_compression">compression</code></td>
<td>
<p>Compression codec to use (default: &quot;snappy&quot;)</p>
</td></tr>
<tr><td><code id="spark_write_avro_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_csv'>Write a Spark DataFrame to a CSV</h2><span id='topic+spark_write_csv'></span>

<h3>Description</h3>

<p>Write a Spark DataFrame to a tabular (typically, comma-separated) file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_csv(
  x,
  path,
  header = TRUE,
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  mode = NULL,
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_csv_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_header">header</code></td>
<td>
<p>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_delimiter">delimiter</code></td>
<td>
<p>The character used to delimit each column, defaults to <code>,</code>.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_quote">quote</code></td>
<td>
<p>The character used as a quote. Defaults to &lsquo;<span class="samp">&#8288;'"'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_escape">escape</code></td>
<td>
<p>The character used to escape other characters, defaults to <code>\</code>.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_charset">charset</code></td>
<td>
<p>The character set, defaults to <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_null_value">null_value</code></td>
<td>
<p>The character to use for default values, defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_csv_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_delta'>Writes a Spark DataFrame into Delta Lake</h2><span id='topic+spark_write_delta'></span>

<h3>Description</h3>

<p>Writes a Spark DataFrame into Delta Lake.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_delta(
  x,
  path,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_delta_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_delta_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_delta_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_delta_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_delta_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_delta_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_jdbc'>Writes a Spark DataFrame into a JDBC table</h2><span id='topic+spark_write_jdbc'></span>

<h3>Description</h3>

<p>Writes a Spark DataFrame into a JDBC table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_jdbc(
  x,
  name,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_jdbc_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_jdbc_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_write_jdbc_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_jdbc_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_jdbc_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_jdbc_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(
  master = "local",
  config = list(
    `sparklyr.shell.driver-class-path` = "/usr/share/java/mysql-connector-java-8.0.25.jar"
  )
)
spark_write_jdbc(
  sdf_len(sc, 10),
  name = "my_sql_table",
  options = list(
    url = "jdbc:mysql://localhost:3306/my_sql_schema",
    driver = "com.mysql.jdbc.Driver",
    user = "me",
    password = "******",
    dbtable = "my_sql_table"
  )
)

## End(Not run)
</code></pre>

<hr>
<h2 id='spark_write_json'>Write a Spark DataFrame to a JSON file</h2><span id='topic+spark_write_json'></span>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the <a href="https://www.json.org/">JavaScript
Object Notation</a> format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_json(
  x,
  path,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_json_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_json_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_json_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_json_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_json_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_json_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_orc'>Write a Spark DataFrame to a ORC file</h2><span id='topic+spark_write_orc'></span>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the
<a href="https://orc.apache.org/">ORC</a> format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_orc(
  x,
  path,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_orc_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_orc_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_orc_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_orc_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.
See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_write_orc_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_orc_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_parquet'>Write a Spark DataFrame to a Parquet file</h2><span id='topic+spark_write_parquet'></span>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the
<a href="https://parquet.apache.org/">Parquet</a> format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_parquet(
  x,
  path,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_parquet_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_parquet_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_parquet_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_parquet_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</p>
</td></tr>
<tr><td><code id="spark_write_parquet_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_parquet_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_rds'>Write Spark DataFrame to RDS files</h2><span id='topic+spark_write_rds'></span>

<h3>Description</h3>

<p>Write Spark dataframe to RDS files. Each partition of the dataframe will be
exported to a separate RDS file so that all partitions can be processed in
parallel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_rds(x, dest_uri)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_rds_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame to be exported</p>
</td></tr>
<tr><td><code id="spark_write_rds_+3A_dest_uri">dest_uri</code></td>
<td>
<p>Can be a URI template containing 'partitionId' (e.g.,
<code>"hdfs://my_data_part_{partitionId}.rds"</code>) where 'partitionId' will be
substituted with ID of each partition using 'glue', or a list of URIs
to be assigned to RDS output from all partitions (e.g.,
<code>"hdfs://my_data_part_0.rds"</code>, <code>"hdfs://my_data_part_1.rds"</code>, and so on)
If working with a Spark instance running locally, then all URIs should be
in <code>"file://&lt;local file path&gt;"</code> form. Otherwise the scheme of the URI should
reflect the underlying file system the Spark instance is working with
(e.g., &quot;hdfs://&quot;). If the resulting list of URI(s) does not contain unique
values, then it will be post-processed with 'make.unique()' to ensure
uniqueness.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing partition ID and RDS file location for each
partition of the input Spark dataframe.
</p>

<hr>
<h2 id='spark_write_source'>Writes a Spark DataFrame into a generic source</h2><span id='topic+spark_write_source'></span>

<h3>Description</h3>

<p>Writes a Spark DataFrame into a generic source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_source(
  x,
  source,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_source_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_source_+3A_source">source</code></td>
<td>
<p>A data source capable of reading data.</p>
</td></tr>
<tr><td><code id="spark_write_source_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_source_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_source_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_source_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_table'>Writes a Spark DataFrame into a Spark table</h2><span id='topic+spark_write_table'></span>

<h3>Description</h3>

<p>Writes a Spark DataFrame into a Spark table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_table(
  x,
  name,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_table_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_table_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr><td><code id="spark_write_table_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_table_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_table_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_text">spark_write_text</a>()</code>
</p>

<hr>
<h2 id='spark_write_text'>Write a Spark DataFrame to a Text file</h2><span id='topic+spark_write_text'></span>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the plain text format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_text(
  x,
  path,
  mode = NULL,
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_text_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="spark_write_text_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="spark_write_text_+3A_mode">mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td></tr>
<tr><td><code id="spark_write_text_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="spark_write_text_+3A_partition_by">partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td></tr>
<tr><td><code id="spark_write_text_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code><a href="#topic+collect_from_rds">collect_from_rds</a>()</code>,
<code><a href="#topic+spark_insert_table">spark_insert_table</a>()</code>,
<code><a href="#topic+spark_load_table">spark_load_table</a>()</code>,
<code><a href="#topic+spark_read">spark_read</a>()</code>,
<code><a href="#topic+spark_read_avro">spark_read_avro</a>()</code>,
<code><a href="#topic+spark_read_binary">spark_read_binary</a>()</code>,
<code><a href="#topic+spark_read_csv">spark_read_csv</a>()</code>,
<code><a href="#topic+spark_read_delta">spark_read_delta</a>()</code>,
<code><a href="#topic+spark_read_image">spark_read_image</a>()</code>,
<code><a href="#topic+spark_read_jdbc">spark_read_jdbc</a>()</code>,
<code><a href="#topic+spark_read_json">spark_read_json</a>()</code>,
<code><a href="#topic+spark_read_libsvm">spark_read_libsvm</a>()</code>,
<code><a href="#topic+spark_read_orc">spark_read_orc</a>()</code>,
<code><a href="#topic+spark_read_parquet">spark_read_parquet</a>()</code>,
<code><a href="#topic+spark_read_source">spark_read_source</a>()</code>,
<code><a href="#topic+spark_read_table">spark_read_table</a>()</code>,
<code><a href="#topic+spark_read_text">spark_read_text</a>()</code>,
<code><a href="#topic+spark_save_table">spark_save_table</a>()</code>,
<code><a href="#topic+spark_write_avro">spark_write_avro</a>()</code>,
<code><a href="#topic+spark_write_csv">spark_write_csv</a>()</code>,
<code><a href="#topic+spark_write_delta">spark_write_delta</a>()</code>,
<code><a href="#topic+spark_write_jdbc">spark_write_jdbc</a>()</code>,
<code><a href="#topic+spark_write_json">spark_write_json</a>()</code>,
<code><a href="#topic+spark_write_orc">spark_write_orc</a>()</code>,
<code><a href="#topic+spark_write_parquet">spark_write_parquet</a>()</code>,
<code><a href="#topic+spark_write_source">spark_write_source</a>()</code>,
<code><a href="#topic+spark_write_table">spark_write_table</a>()</code>
</p>

<hr>
<h2 id='spark-api'>Access the Spark API</h2><span id='topic+spark-api'></span><span id='topic+spark_context'></span><span id='topic+java_context'></span><span id='topic+hive_context'></span><span id='topic+spark_session'></span>

<h3>Description</h3>

<p>Access the commonly-used Spark objects associated with a Spark instance.
These objects provide access to different facets of the Spark API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_context(sc)

java_context(sc)

hive_context(sc)

spark_session(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark-api_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="https://spark.apache.org/docs/latest/api/scala/">Scala API documentation</a>
is useful for discovering what methods are available for each of these
objects. Use <code><a href="#topic+invoke">invoke</a></code> to call methods on these objects.
</p>


<h3>Spark Context</h3>

<p>The main entry point for Spark functionality. The <strong>Spark Context</strong>
represents the connection to a Spark cluster, and can be used to create
<code>RDD</code>s, accumulators and broadcast variables on that cluster.
</p>


<h3>Java Spark Context</h3>

<p>A Java-friendly version of the aforementioned <strong>Spark Context</strong>.
</p>


<h3>Hive Context</h3>

<p>An instance of the Spark SQL execution engine that integrates with data
stored in Hive. Configuration for Hive is read from <code>hive-site.xml</code> on
the classpath.
</p>
<p>Starting with Spark &gt;= 2.0.0, the <strong>Hive Context</strong> class has been
deprecated &ndash; it is superceded by the <strong>Spark Session</strong> class, and
<code>hive_context</code> will return a <strong>Spark Session</strong> object instead.
Note that both classes share a SQL interface, and therefore one can invoke
SQL through these objects.
</p>


<h3>Spark Session</h3>

<p>Available since Spark 2.0.0, the <strong>Spark Session</strong> unifies the
<strong>Spark Context</strong> and <strong>Hive Context</strong> classes into a single
interface. Its use is recommended over the older APIs for code
targeting Spark 2.0.0 and above.
</p>

<hr>
<h2 id='spark-connections'>Manage Spark Connections</h2><span id='topic+spark-connections'></span><span id='topic+spark_connect'></span><span id='topic+spark_connection_is_open'></span><span id='topic+spark_disconnect'></span><span id='topic+spark_disconnect_all'></span><span id='topic+spark_submit'></span>

<h3>Description</h3>

<p>These routines allow you to manage your connections to Spark.
</p>
<p>Call 'spark_disconnect()' on each open Spark connection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_connect(
  master,
  spark_home = Sys.getenv("SPARK_HOME"),
  method = c("shell", "livy", "databricks", "test", "qubole", "synapse"),
  app_name = "sparklyr",
  version = NULL,
  config = spark_config(),
  extensions = sparklyr::registered_extensions(),
  packages = NULL,
  scala_version = NULL,
  ...
)

spark_connection_is_open(sc)

spark_disconnect(sc, ...)

spark_disconnect_all(...)

spark_submit(
  master,
  file,
  spark_home = Sys.getenv("SPARK_HOME"),
  app_name = "sparklyr",
  version = NULL,
  config = spark_config(),
  extensions = sparklyr::registered_extensions(),
  scala_version = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark-connections_+3A_master">master</code></td>
<td>
<p>Spark cluster url to connect to. Use <code>"local"</code> to
connect to a local instance of Spark installed via
<code><a href="#topic+spark_install">spark_install</a></code>.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_spark_home">spark_home</code></td>
<td>
<p>The path to a Spark installation. Defaults to the path
provided by the <code>SPARK_HOME</code> environment variable. If
<code>SPARK_HOME</code> is defined, it will always be used unless the
<code>version</code> parameter is specified to force the use of a locally
installed version.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_method">method</code></td>
<td>
<p>The method used to connect to Spark. Default connection method
is <code>"shell"</code> to connect using spark-submit, use <code>"livy"</code> to
perform remote connections using HTTP, or <code>"databricks"</code> when using a
Databricks clusters.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_app_name">app_name</code></td>
<td>
<p>The application name to be used while running in the Spark
cluster.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_version">version</code></td>
<td>
<p>The version of Spark to use. Required for <code>"local"</code> Spark
connections, optional otherwise.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_config">config</code></td>
<td>
<p>Custom configuration for the generated Spark connection. See
<code><a href="#topic+spark_config">spark_config</a></code> for details.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_extensions">extensions</code></td>
<td>
<p>Extension R packages to enable for this connection. By
default, all packages enabled through the use of
<code><a href="#topic+register_extension">sparklyr::register_extension</a></code> will be passed here.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_packages">packages</code></td>
<td>
<p>A list of Spark packages to load. For example, <code>"delta"</code> or
<code>"kafka"</code> to enable Delta Lake or Kafka. Also supports full versions like
<code>"io.delta:delta-core_2.11:0.4.0"</code>. This is similar to adding packages into the
<code>sparklyr.shell.packages</code> configuration option. Notice that the <code>version</code>
parameter is used to choose the correct package, otherwise assumes the latest version
is being used.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_scala_version">scala_version</code></td>
<td>
<p>Load the sparklyr jar file that is built with the version of
Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will
by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore
&lsquo;scala_version = &rsquo;2.12'' is needed if sparklyr is connecting to Spark 2.4 built with
Scala 2.12)</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_...">...</code></td>
<td>
<p>Additional params to be passed to each 'spark_disconnect()' call
(e.g., 'terminate = TRUE')</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="spark-connections_+3A_file">file</code></td>
<td>
<p>Path to R source file to submit for batch execution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, when using <code>method = "livy"</code>, jars are downloaded from GitHub. But
an alternative path (local to Livy server or on HDFS or HTTP(s)) to <code>sparklyr</code>
JAR can also be specified through the <code>sparklyr.livy.jar</code> setting.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>conf &lt;- spark_config()
conf$`sparklyr.shell.conf` &lt;- c(
  "spark.executor.extraJavaOptions=-Duser.timezone='UTC'",
  "spark.driver.extraJavaOptions=-Duser.timezone='UTC'",
  "spark.sql.session.timeZone='UTC'"
)

sc &lt;- spark_connect(
  master = "spark://HOST:PORT", config = conf
)
connection_is_open(sc)

spark_disconnect(sc)
</code></pre>

<hr>
<h2 id='sparklyr_get_backend_port'>Return the port number of a 'sparklyr' backend.</h2><span id='topic+sparklyr_get_backend_port'></span>

<h3>Description</h3>

<p>Retrieve the port number of the 'sparklyr' backend associated with a Spark
connection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparklyr_get_backend_port(sc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sparklyr_get_backend_port_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The port number of the 'sparklyr' backend associated with <code>sc</code>.
</p>

<hr>
<h2 id='src_databases'>Show database list</h2><span id='topic+src_databases'></span>

<h3>Description</h3>

<p>Show database list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>src_databases(sc, col = "databaseName", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="src_databases_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="src_databases_+3A_col">col</code></td>
<td>
<p>The column name of the table that lists all databases
may be referred to as <code>namespace</code> in some versions of the system.</p>
</td></tr>
<tr><td><code id="src_databases_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>

<hr>
<h2 id='stream_find'>Find Stream</h2><span id='topic+stream_find'></span>

<h3>Description</h3>

<p>Finds and returns a stream based on the stream's identifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_find(sc, id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_find_+3A_sc">sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td></tr>
<tr><td><code id="stream_find_+3A_id">id</code></td>
<td>
<p>The stream identifier to find.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
sdf_len(sc, 10) %&gt;%
  spark_write_parquet(path = "parquet-in")

stream &lt;- stream_read_parquet(sc, "parquet-in") %&gt;%
  stream_write_parquet("parquet-out")

stream_id &lt;- stream_id(stream)
stream_find(sc, stream_id)

## End(Not run)

</code></pre>

<hr>
<h2 id='stream_generate_test'>Generate Test Stream</h2><span id='topic+stream_generate_test'></span>

<h3>Description</h3>

<p>Generates a local test stream, useful when testing streams locally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_generate_test(
  df = rep(1:1000),
  path = "source",
  distribution = floor(10 + 1e+05 * stats::dbinom(1:20, 20, 0.5)),
  iterations = 50,
  interval = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_generate_test_+3A_df">df</code></td>
<td>
<p>The data frame used as a source of rows to the stream, will
be cast to data frame if needed. Defaults to a sequence of one thousand
entries.</p>
</td></tr>
<tr><td><code id="stream_generate_test_+3A_path">path</code></td>
<td>
<p>Path to save stream of files to, defaults to <code>"source"</code>.</p>
</td></tr>
<tr><td><code id="stream_generate_test_+3A_distribution">distribution</code></td>
<td>
<p>The distribution of rows to use over each iteration,
defaults to a binomial distribution. The stream will cycle through the
distribution if needed.</p>
</td></tr>
<tr><td><code id="stream_generate_test_+3A_iterations">iterations</code></td>
<td>
<p>Number of iterations to execute before stopping, defaults
to fifty.</p>
</td></tr>
<tr><td><code id="stream_generate_test_+3A_interval">interval</code></td>
<td>
<p>The inverval in seconds use to write the stream, defaults
to one second.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function requires the <code>callr</code> package to be installed.
</p>

<hr>
<h2 id='stream_id'>Spark Stream's Identifier</h2><span id='topic+stream_id'></span>

<h3>Description</h3>

<p>Retrieves the identifier of the Spark stream.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_id(stream)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_id_+3A_stream">stream</code></td>
<td>
<p>The spark stream object.</p>
</td></tr>
</table>

<hr>
<h2 id='stream_lag'>Apply lag function to columns of a Spark Streaming DataFrame</h2><span id='topic+stream_lag'></span>

<h3>Description</h3>

<p>Given a streaming Spark dataframe as input, this function will return another
streaming dataframe that contains all columns in the input and column(s) that
are shifted behind by the offset(s) specified in '...' (see example)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_lag(x, cols, thresholds = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_lag_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark Streaming DataFrame.</p>
</td></tr>
<tr><td><code id="stream_lag_+3A_cols">cols</code></td>
<td>
<p>A list of expressions for a single or multiple variables to create
that will contain the value of a previous entry.</p>
</td></tr>
<tr><td><code id="stream_lag_+3A_thresholds">thresholds</code></td>
<td>
<p>Optional named list of timestamp column(s) and
corresponding time duration(s) for deterimining whether a previous record
is sufficiently recent relative to the current record.
If the any of the time difference(s) between the current and a previous
record is greater than the maximal duration allowed, then the previous
record is discarded and will not be part of the query result.
The durations can be specified with numeric types (which will be
interpreted as max difference allowed in number of milliseconds between 2
UNIX timestamps) or time duration strings such as &quot;5s&quot;, &quot;5sec&quot;, &quot;5min&quot;,
&quot;5hour&quot;, etc.
Any timestamp column in 'x' that is not of timestamp of date Spark SQL
types will be interepreted as number of milliseconds since the UNIX epoch.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(sparklyr)

sc &lt;- spark_connect(master = "local", version = "2.2.0")

streaming_path &lt;- tempfile("days_df_")
days_df &lt;- dplyr::tibble(
  today = weekdays(as.Date(seq(7), origin = "1970-01-01"))
)
num_iters &lt;- 7
stream_generate_test(
  df = days_df,
  path = streaming_path,
  distribution = rep(nrow(days_df), num_iters),
  iterations = num_iters
)

stream_read_csv(sc, streaming_path) %&gt;%
  stream_lag(cols = c(yesterday = today ~ 1, two_days_ago = today ~ 2)) %&gt;%
  collect() %&gt;%
  print(n = 10L)

## End(Not run)

</code></pre>

<hr>
<h2 id='stream_name'>Spark Stream's Name</h2><span id='topic+stream_name'></span>

<h3>Description</h3>

<p>Retrieves the name of the Spark stream if available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_name(stream)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_name_+3A_stream">stream</code></td>
<td>
<p>The spark stream object.</p>
</td></tr>
</table>

<hr>
<h2 id='stream_read_csv'>Read files created by the stream</h2><span id='topic+stream_read_csv'></span><span id='topic+stream_read_text'></span><span id='topic+stream_read_json'></span><span id='topic+stream_read_parquet'></span><span id='topic+stream_read_orc'></span><span id='topic+stream_read_kafka'></span><span id='topic+stream_read_socket'></span><span id='topic+stream_read_delta'></span><span id='topic+stream_read_cloudfiles'></span><span id='topic+stream_read_table'></span>

<h3>Description</h3>

<p>Read files created by the stream
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_read_csv(
  sc,
  path,
  name = NULL,
  header = TRUE,
  columns = NULL,
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  ...
)

stream_read_text(sc, path, name = NULL, options = list(), ...)

stream_read_json(sc, path, name = NULL, columns = NULL, options = list(), ...)

stream_read_parquet(
  sc,
  path,
  name = NULL,
  columns = NULL,
  options = list(),
  ...
)

stream_read_orc(sc, path, name = NULL, columns = NULL, options = list(), ...)

stream_read_kafka(sc, name = NULL, options = list(), ...)

stream_read_socket(sc, name = NULL, columns = NULL, options = list(), ...)

stream_read_delta(sc, path, name = NULL, options = list(), ...)

stream_read_cloudfiles(sc, path, name = NULL, options = list(), ...)

stream_read_table(sc, path, name = NULL, options = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_read_csv_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated stream.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_header">header</code></td>
<td>
<p>Boolean; should the first row of data be used as a header?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_columns">columns</code></td>
<td>
<p>A vector of column names or a named vector of column types.
If specified, the elements can be <code>"binary"</code> for <code>BinaryType</code>,
<code>"boolean"</code> for <code>BooleanType</code>, <code>"byte"</code> for <code>ByteType</code>,
<code>"integer"</code> for <code>IntegerType</code>, <code>"integer64"</code> for <code>LongType</code>,
<code>"double"</code> for <code>DoubleType</code>, <code>"character"</code> for <code>StringType</code>,
<code>"timestamp"</code> for <code>TimestampType</code> and <code>"date"</code> for <code>DateType</code>.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_delimiter">delimiter</code></td>
<td>
<p>The character used to delimit each column. Defaults to &lsquo;<span class="samp">&#8288;','&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_quote">quote</code></td>
<td>
<p>The character used as a quote. Defaults to &lsquo;<span class="samp">&#8288;'"'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_escape">escape</code></td>
<td>
<p>The character used to escape other characters. Defaults to &lsquo;<span class="samp">&#8288;'\'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_charset">charset</code></td>
<td>
<p>The character set. Defaults to &lsquo;<span class="samp">&#8288;"UTF-8"&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_null_value">null_value</code></td>
<td>
<p>The character to use for null, or missing, values. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="stream_read_csv_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

sc &lt;- spark_connect(master = "local")

dir.create("csv-in")
write.csv(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- file.path("file://", getwd(), "csv-in")

stream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_csv("csv-out")

stream_stop(stream)

## End(Not run)

</code></pre>

<hr>
<h2 id='stream_render'>Render Stream</h2><span id='topic+stream_render'></span>

<h3>Description</h3>

<p>Collects streaming statistics to render the stream as an 'htmlwidget'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_render(stream = NULL, collect = 10, stats = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_render_+3A_stream">stream</code></td>
<td>
<p>The stream to render</p>
</td></tr>
<tr><td><code id="stream_render_+3A_collect">collect</code></td>
<td>
<p>The interval in seconds to collect data before rendering the
'htmlwidget'.</p>
</td></tr>
<tr><td><code id="stream_render_+3A_stats">stats</code></td>
<td>
<p>Optional stream statistics collected using <code>stream_stats()</code>,
when specified, <code>stream</code> should be omitted.</p>
</td></tr>
<tr><td><code id="stream_render_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "local")

dir.create("iris-in")
write.csv(iris, "iris-in/iris.csv", row.names = FALSE)

stream &lt;- stream_read_csv(sc, "iris-in/") %&gt;%
  stream_write_csv("iris-out/")

stream_render(stream)
stream_stop(stream)

## End(Not run)
</code></pre>

<hr>
<h2 id='stream_stats'>Stream Statistics</h2><span id='topic+stream_stats'></span>

<h3>Description</h3>

<p>Collects streaming statistics, usually, to be used with <code>stream_render()</code>
to render streaming statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_stats(stream, stats = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_stats_+3A_stream">stream</code></td>
<td>
<p>The stream to collect statistics from.</p>
</td></tr>
<tr><td><code id="stream_stats_+3A_stats">stats</code></td>
<td>
<p>An optional stats object generated using <code>stream_stats()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A stats object containing streaming statistics that can be passed
back to the <code>stats</code> parameter to continue aggregating streaming stats.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sc &lt;- spark_connect(master = "local")
sdf_len(sc, 10) %&gt;%
  spark_write_parquet(path = "parquet-in")

stream &lt;- stream_read_parquet(sc, "parquet-in") %&gt;%
  stream_write_parquet("parquet-out")

stream_stats(stream)

## End(Not run)

</code></pre>

<hr>
<h2 id='stream_stop'>Stops a Spark Stream</h2><span id='topic+stream_stop'></span>

<h3>Description</h3>

<p>Stops processing data from a Spark stream.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_stop(stream)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_stop_+3A_stream">stream</code></td>
<td>
<p>The spark stream object to be stopped.</p>
</td></tr>
</table>

<hr>
<h2 id='stream_trigger_continuous'>Spark Stream Continuous Trigger</h2><span id='topic+stream_trigger_continuous'></span>

<h3>Description</h3>

<p>Creates a Spark structured streaming trigger to execute
continuously. This mode is the most performant but not all operations
are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_trigger_continuous(checkpoint = 5000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_trigger_continuous_+3A_checkpoint">checkpoint</code></td>
<td>
<p>The checkpoint interval specified in milliseconds.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+stream_trigger_interval">stream_trigger_interval</a></code>
</p>

<hr>
<h2 id='stream_trigger_interval'>Spark Stream Interval Trigger</h2><span id='topic+stream_trigger_interval'></span>

<h3>Description</h3>

<p>Creates a Spark structured streaming trigger to execute
over the specified interval.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_trigger_interval(interval = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_trigger_interval_+3A_interval">interval</code></td>
<td>
<p>The execution interval specified in milliseconds.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+stream_trigger_continuous">stream_trigger_continuous</a></code>
</p>

<hr>
<h2 id='stream_view'>View Stream</h2><span id='topic+stream_view'></span>

<h3>Description</h3>

<p>Opens a Shiny gadget to visualize the given stream.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_view(stream, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_view_+3A_stream">stream</code></td>
<td>
<p>The stream to visualize.</p>
</td></tr>
<tr><td><code id="stream_view_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(sparklyr)
sc &lt;- spark_connect(master = "local")

dir.create("iris-in")
write.csv(iris, "iris-in/iris.csv", row.names = FALSE)

stream_read_csv(sc, "iris-in/") %&gt;%
  stream_write_csv("iris-out/") %&gt;%
  stream_view() %&gt;%
  stream_stop()

## End(Not run)
</code></pre>

<hr>
<h2 id='stream_watermark'>Watermark Stream</h2><span id='topic+stream_watermark'></span>

<h3>Description</h3>

<p>Ensures a stream has a watermark defined, which is required for some
operations over streams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_watermark(x, column = "timestamp", threshold = "10 minutes")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_watermark_+3A_x">x</code></td>
<td>
<p>An object coercable to a Spark Streaming DataFrame.</p>
</td></tr>
<tr><td><code id="stream_watermark_+3A_column">column</code></td>
<td>
<p>The name of the column that contains the event time of the row,
if the column is missing, a column with the current time will be added.</p>
</td></tr>
<tr><td><code id="stream_watermark_+3A_threshold">threshold</code></td>
<td>
<p>The minimum delay to wait to data to arrive late, defaults
to ten minutes.</p>
</td></tr>
</table>

<hr>
<h2 id='stream_write_csv'>Write files to the stream</h2><span id='topic+stream_write_csv'></span><span id='topic+stream_write_text'></span><span id='topic+stream_write_json'></span><span id='topic+stream_write_parquet'></span><span id='topic+stream_write_orc'></span><span id='topic+stream_write_kafka'></span><span id='topic+stream_write_console'></span><span id='topic+stream_write_delta'></span>

<h3>Description</h3>

<p>Write files to the stream
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_write_csv(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoint"),
  header = TRUE,
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_text(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_json(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_parquet(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_orc(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_kafka(
  x,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path("checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_console(
  x,
  mode = c("append", "complete", "update"),
  options = list(),
  trigger = stream_trigger_interval(),
  partition_by = NULL,
  ...
)

stream_write_delta(
  x,
  path,
  mode = c("append", "complete", "update"),
  checkpoint = file.path("checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_write_csv_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_mode">mode</code></td>
<td>
<p>Specifies how data is written to a streaming sink. Valid values are
<code>"append"</code>, <code>"complete"</code> or <code>"update"</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_trigger">trigger</code></td>
<td>
<p>The trigger for the stream query, defaults to micro-batches
running every 5 seconds. See <code><a href="#topic+stream_trigger_interval">stream_trigger_interval</a></code> and
<code><a href="#topic+stream_trigger_continuous">stream_trigger_continuous</a></code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_checkpoint">checkpoint</code></td>
<td>
<p>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_header">header</code></td>
<td>
<p>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_delimiter">delimiter</code></td>
<td>
<p>The character used to delimit each column, defaults to <code>,</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_quote">quote</code></td>
<td>
<p>The character used as a quote. Defaults to &lsquo;<span class="samp">&#8288;'"'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_escape">escape</code></td>
<td>
<p>The character used to escape other characters, defaults to <code>\</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_charset">charset</code></td>
<td>
<p>The character set, defaults to <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_null_value">null_value</code></td>
<td>
<p>The character to use for default values, defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_partition_by">partition_by</code></td>
<td>
<p>Partitions the output by the given list of columns.</p>
</td></tr>
<tr><td><code id="stream_write_csv_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark stream serialization: 
<code><a href="#topic+stream_write_memory">stream_write_memory</a>()</code>,
<code><a href="#topic+stream_write_table">stream_write_table</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

sc &lt;- spark_connect(master = "local")

dir.create("csv-in")
write.csv(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- file.path("file://", getwd(), "csv-in")

stream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_csv("csv-out")

stream_stop(stream)

## End(Not run)

</code></pre>

<hr>
<h2 id='stream_write_memory'>Write Memory Stream</h2><span id='topic+stream_write_memory'></span>

<h3>Description</h3>

<p>Writes a Spark dataframe stream into a memory stream.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_write_memory(
  x,
  name = random_string("sparklyr_tmp_"),
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path("checkpoints", name, random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_write_memory_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated stream.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_mode">mode</code></td>
<td>
<p>Specifies how data is written to a streaming sink. Valid values are
<code>"append"</code>, <code>"complete"</code> or <code>"update"</code>.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_trigger">trigger</code></td>
<td>
<p>The trigger for the stream query, defaults to micro-batches
running every 5 seconds. See <code><a href="#topic+stream_trigger_interval">stream_trigger_interval</a></code> and
<code><a href="#topic+stream_trigger_continuous">stream_trigger_continuous</a></code>.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_checkpoint">checkpoint</code></td>
<td>
<p>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_partition_by">partition_by</code></td>
<td>
<p>Partitions the output by the given list of columns.</p>
</td></tr>
<tr><td><code id="stream_write_memory_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark stream serialization: 
<code><a href="#topic+stream_write_csv">stream_write_csv</a>()</code>,
<code><a href="#topic+stream_write_table">stream_write_table</a>()</code>
</p>

<hr>
<h2 id='stream_write_table'>Write Stream to Table</h2><span id='topic+stream_write_table'></span>

<h3>Description</h3>

<p>Writes a Spark dataframe stream into a table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stream_write_table(
  x,
  path,
  format = NULL,
  mode = c("append", "complete", "update"),
  checkpoint = file.path("checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stream_write_table_+3A_x">x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_path">path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the &lsquo;<span class="samp">&#8288;"hdfs://"&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;"s3a://"&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;"file://"&#8288;</span>&rsquo; protocols.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_format">format</code></td>
<td>
<p>Specifies format of data written to table E.g.
<code>"delta"</code>, <code>"parquet"</code>. Defaults to <code>NULL</code> which will use
system default format.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_mode">mode</code></td>
<td>
<p>Specifies how data is written to a streaming sink. Valid values are
<code>"append"</code>, <code>"complete"</code> or <code>"update"</code>.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_checkpoint">checkpoint</code></td>
<td>
<p>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_options">options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_partition_by">partition_by</code></td>
<td>
<p>Partitions the output by the given list of columns.</p>
</td></tr>
<tr><td><code id="stream_write_table_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Spark stream serialization: 
<code><a href="#topic+stream_write_csv">stream_write_csv</a>()</code>,
<code><a href="#topic+stream_write_memory">stream_write_memory</a>()</code>
</p>

<hr>
<h2 id='tbl_cache'>Cache a Spark Table</h2><span id='topic+tbl_cache'></span>

<h3>Description</h3>

<p>Force a Spark table with name <code>name</code> to be loaded into memory.
Operations on cached tables should normally (although not always)
be more performant than the same operation performed on an uncached
table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tbl_cache(sc, name, force = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tbl_cache_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="tbl_cache_+3A_name">name</code></td>
<td>
<p>The table name.</p>
</td></tr>
<tr><td><code id="tbl_cache_+3A_force">force</code></td>
<td>
<p>Force the data to be loaded into memory? This is accomplished
by calling the <code>count</code> API on the associated Spark DataFrame.</p>
</td></tr>
</table>

<hr>
<h2 id='tbl_change_db'>Use specific database</h2><span id='topic+tbl_change_db'></span>

<h3>Description</h3>

<p>Use specific database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tbl_change_db(sc, name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tbl_change_db_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="tbl_change_db_+3A_name">name</code></td>
<td>
<p>The database name.</p>
</td></tr>
</table>

<hr>
<h2 id='tbl_uncache'>Uncache a Spark Table</h2><span id='topic+tbl_uncache'></span>

<h3>Description</h3>

<p>Force a Spark table with name <code>name</code> to be unloaded from memory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tbl_uncache(sc, name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tbl_uncache_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr><td><code id="tbl_uncache_+3A_name">name</code></td>
<td>
<p>The table name.</p>
</td></tr>
</table>

<hr>
<h2 id='transform_sdf'>transform a subset of column(s) in a Spark Dataframe</h2><span id='topic+transform_sdf'></span>

<h3>Description</h3>

<p>transform a subset of column(s) in a Spark Dataframe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform_sdf(x, cols, fn)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="transform_sdf_+3A_x">x</code></td>
<td>
<p>An object coercible to a Spark DataFrame</p>
</td></tr>
<tr><td><code id="transform_sdf_+3A_cols">cols</code></td>
<td>
<p>Subset of columns to apply transformation to</p>
</td></tr>
<tr><td><code id="transform_sdf_+3A_fn">fn</code></td>
<td>
<p>Transformation function taking column name as the 1st parameter, the
corresponding <code>org.apache.spark.sql.Column</code> object as the 2nd parameter,
and returning a transformed <code>org.apache.spark.sql.Column</code> object</p>
</td></tr>
</table>

<hr>
<h2 id='unite'>Unite</h2><span id='topic+unite'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+unite">unite</a></code> for more details.
</p>

<hr>
<h2 id='unnest'>Unnest</h2><span id='topic+unnest'></span>

<h3>Description</h3>

<p>See <code><a href="tidyr.html#topic+nest">unnest</a></code> for more details.
</p>

<hr>
<h2 id='worker_spark_apply_unbundle'>Extracts a bundle of dependencies required by <code>spark_apply()</code></h2><span id='topic+worker_spark_apply_unbundle'></span>

<h3>Description</h3>

<p>Extracts a bundle of dependencies required by <code>spark_apply()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>worker_spark_apply_unbundle(bundle_path, base_path, bundle_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="worker_spark_apply_unbundle_+3A_bundle_path">bundle_path</code></td>
<td>
<p>Path to the bundle created using <code>spark_apply_bundle()</code></p>
</td></tr>
<tr><td><code id="worker_spark_apply_unbundle_+3A_base_path">base_path</code></td>
<td>
<p>Base path to use while extracting bundles</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
