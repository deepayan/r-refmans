<!DOCTYPE html><html lang="en"><head><title>Help for package HDclassif</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {HDclassif}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#HDclassif-package'>
<p>High Dimensional Discriminant Analysis and Data Clustering</p></a></li>
<li><a href='#Crabs'>
<p>Morphological Measurements on Leptograpsus Crabs.</p></a></li>
<li><a href='#demo_hddc'>
<p>Demonstration of the clustering process of HDDC.</p></a></li>
<li><a href='#hdda'><p>High Dimensional Discriminant Analysis</p></a></li>
<li><a href='#hddc'><p>High Dimensional Data Clustering</p></a></li>
<li><a href='#hdmda'><p>Mixture Discriminant Analysis with HD Gaussians</p></a></li>
<li><a href='#plot.hdc'>
<p>Cattell's Scree-Test for 'hdc' class objects.</p></a></li>
<li><a href='#predict.hdc'><p>Prediction method for &lsquo;hdc&rsquo; class objects.</p></a></li>
<li><a href='#predict.hdmda'>
<p>Prediction method for &lsquo;hdmda&rsquo; class objects.</p></a></li>
<li><a href='#setHDclassif.show'><p>Sets/gets the default 'show' argument in HDDC and HDDA</p></a></li>
<li><a href='#simuldata'>
<p>Gaussian Data Generation</p></a></li>
<li><a href='#slopeHeuristic'><p>Slope Heuristic for HDDC objects</p></a></li>
<li><a href='#wine'>
<p>Wine dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High Dimensional Supervised Classification and Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>grDevices, utils, graphics, stats, MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>rARPACK</td>
</tr>
<tr>
<td>Author:</td>
<td>Laurent Berge, Charles Bouveyron and Stephane Girard</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Laurent Berge &lt;laurent.berge@u-bordeaux.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Discriminant analysis and data clustering methods for high
    dimensional data, based on the assumption that high-dimensional data live in
    different subspaces with low dimensionality proposing a new parametrization of
    the Gaussian mixture model which combines the ideas of dimension reduction and
    constraints on the model.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>ZipData:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-22 14:05:28 UTC; lrberge</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-23 07:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='HDclassif-package'>
High Dimensional Discriminant Analysis and Data Clustering
</h2><span id='topic+HDclassif-package'></span><span id='topic+HDclassif'></span>

<h3>Description</h3>

<p>Disciminant analysis and data clustering methods for high dimensional data, based on the asumption that high-dimensional data live in different subspaces with low dimensionality, proposing a new parametrization of the Gaussian mixture model which combines the ideas of dimension reduction and constraints on the model.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> HDclassif</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2018-05-11</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package is used to make efficient supervised and unsupervised classification with high dimensional data.
The supervised method uses the <var>hdda</var> function to get the data parameters and the <var>predict</var> function to realise the class prediction of a dataset.
The unsupervised method is implemented in the <var>hddc</var> function, and once the parameters are estimated, the <var>predict</var> gives the class prediction of other datasets. The method used in the hddc is based on the Expectation - Maximisation algorithm.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>
<p>Maintainer: Laurent Berge &lt;laurent.berge at uni.lu&gt; 
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High Dimensional Discriminant Analysis&rdquo;, <em>Communications in Statistics: Theory and Methods</em>, vol. <b>36</b> (14), pp. 2607&ndash;2623
</p>
<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High-Dimensional Data Clustering&rdquo;, <em>Computational Statistics and Data Analysis</em>, vol. <b>52</b> (1), pp. 502&ndash;519
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012) &ldquo;HDclassif: An R Package for Model-Based 
Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, <em>Journal of Statistical Software</em>, 
<b>46</b>(6), 1&ndash;29, url: <a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>
</p>

<hr>
<h2 id='Crabs'>
Morphological Measurements on Leptograpsus Crabs.
</h2><span id='topic+Crabs'></span>

<h3>Description</h3>

<p>The Crabs data frame has 200 rows and 6 columns, describing 5 morphological measurements on 50 crabs each of two colour forms and both sexes, of the species Leptograspus Variegatus collected at Fremantle, W. Australia.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Crabs)</code></pre>


<h3>Format</h3>

<p>A data frame with 200 observations on the following 6 variables.
</p>

<dl>
<dt><code>class</code></dt><dd><p>Type of the crabs: the first character represents the species - &quot;B&quot; or &quot;O&quot; for blue or orange-, the second represents the sex -&quot;M&quot; or &quot;F&quot; for male or female-.</p>
</dd>
<dt><code>FL</code></dt><dd><p>Frontal lob size (mm).</p>
</dd>
<dt><code>RW</code></dt><dd><p>Rear width (mm).</p>
</dd>
<dt><code>CL</code></dt><dd><p>Carapace length (mm).</p>
</dd>
<dt><code>CW</code></dt><dd><p>Carapace width (mm).</p>
</dd>
<dt><code>BD</code></dt><dd><p>Body depth (mm).</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset can also be found in the MASS package, the unique difference is the class vector which is easier to use here.
</p>


<h3>Source</h3>

<p>Campbell, N. A. and Mahon, R. J. (1974) &ldquo;A multivariate study of variation on two species of rock crab of genus Leptograspus&rdquo;, <em>Australian Journal of Zoology</em>, <b>22</b>, 417&ndash;425.
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) &ldquo;Modern Applied Statistics with S&rdquo;. Fourth edition. Springer.
</p>

<hr>
<h2 id='demo_hddc'>
Demonstration of the clustering process of HDDC.
</h2><span id='topic+demo_hddc'></span>

<h3>Description</h3>

<p>This demonstration uses a PCA on the first two principal axis of the Crabs dataset -that can be found in the package- to show the clustering process of HDDC. At each step of the clustering, the means and directions are shown by, respectively, points and lines.
This function should only be used in demo(hddc).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>demo_hddc()
</code></pre>


<h3>Value</h3>

<p> The plots of the clustering process.</p>


<h3>Note</h3>

<p>The algorithm and the initialization are interactively chosen.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hddc">hddc</a></code>.
</p>

<hr>
<h2 id='hdda'>High Dimensional Discriminant Analysis</h2><span id='topic+hdda'></span>

<h3>Description</h3>

<p>HDDA is a model-based discriminant analysis method assuming each class of the dataset live in a proper Gaussian subspace which is much smaller than the original one, the hdda.learn function calculates the parameters of each subspace in order to predict  the class of new observation of this kind.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdda(
  data,
  cls,
  model = "AkjBkQkDk",
  graph = FALSE,
  d_select = "Cattell",
  threshold = 0.2,
  com_dim = NULL,
  show = getHDclassif.show(),
  scaling = FALSE,
  cv.dim = 1:10,
  cv.threshold = c(0.001, 0.005, 0.05, 1:9 * 0.1),
  cv.vfold = 10,
  LOO = FALSE,
  noise.ctrl = 1e-08,
  d
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hdda_+3A_data">data</code></td>
<td>
<p>A matrix or a data frame of observations, assuming the rows are the observations and the columns the variables. Note that NAs are not allowed.</p>
</td></tr>
<tr><td><code id="hdda_+3A_cls">cls</code></td>
<td>
<p>The vector of the class of each observations, its type can be numeric or string.</p>
</td></tr>
<tr><td><code id="hdda_+3A_model">model</code></td>
<td>
<p>A character string vector, or an integer vector indicating the models to be used. The available models are: &quot;AkjBkQkDk&quot; (default), &quot;AkBkQkDk&quot;, &quot;ABkQkDk&quot;, &quot;AkjBQkDk&quot;, &quot;AkBQkDk&quot;, &quot;ABQkDk&quot;, &quot;AkjBkQkD&quot;, &quot;AkBkQkD&quot;, &quot;ABkQkD&quot;, &quot;AkjBQkD&quot;, &quot;AkBQkD&quot;, &quot;ABQkD&quot;, &quot;AjBQD&quot;, &quot;ABQD&quot;. It is not case sensitive and integers can be used instead of names, see details for more information. Several models can be used, if it is, only the results of the one which maximizes the BIC criterion is kept. To run all models, use model=&quot;ALL&quot;.</p>
</td></tr>
<tr><td><code id="hdda_+3A_graph">graph</code></td>
<td>
<p>It is for comparison sake only, when several estimations are run at the same time (either when using several models, or when using cross-validation to select the best dimension/threshold). If graph = TRUE, the plot of the results of all estimations is displayed. Default is FALSE.</p>
</td></tr>
<tr><td><code id="hdda_+3A_d_select">d_select</code></td>
<td>
<p>Either &ldquo;Cattell&rdquo; (default), &ldquo;BIC&rdquo; or &ldquo;CV&rdquo;. See details for more information. This parameter selects which method to use to select the intrinsic dimensions.</p>
</td></tr>
<tr><td><code id="hdda_+3A_threshold">threshold</code></td>
<td>
<p>A float stricly within 0 and 1. It is the threshold used in the Cattell's Scree-Test.</p>
</td></tr>
<tr><td><code id="hdda_+3A_com_dim">com_dim</code></td>
<td>
<p>It is used only for common dimensions models. The user can give the common dimension s/he wants. If used, it must be an integer. Its default is set to NULL.</p>
</td></tr>
<tr><td><code id="hdda_+3A_show">show</code></td>
<td>
<p>Single logical. To diplay summary information on the results after the algorithm is done: set it to <code>TRUE</code>. By default it takes the value of <code><a href="#topic+getHDclassif.show">getHDclassif.show</a></code> which is FALSE at the loading of the package. To permanently have <code>show=TRUE</code>, use <code>setHDclassif.show(TRUE)</code>.</p>
</td></tr>
<tr><td><code id="hdda_+3A_scaling">scaling</code></td>
<td>
<p>Logical: whether to scale the dataset (mean=0 and standard-error=1 for each variable) or not. By default the data is not scaled.</p>
</td></tr>
<tr><td><code id="hdda_+3A_cv.dim">cv.dim</code></td>
<td>
<p>A vector of integers. Only when d_select=&ldquo;CV&rdquo;. Gives the dimensions for which the CV is to be done. Note that if some dimensions are greater than what it is possible to have, those are taken off.</p>
</td></tr>
<tr><td><code id="hdda_+3A_cv.threshold">cv.threshold</code></td>
<td>
<p>A vector of floats strictly within 0 and 1. Only when d_select=&ldquo;CV&rdquo;. Gives the thresholds for which the CV is to be done.</p>
</td></tr>
<tr><td><code id="hdda_+3A_cv.vfold">cv.vfold</code></td>
<td>
<p>An integer. Only when d_select=&ldquo;CV&rdquo;. It gives the number of different subsamples in which the dataset is split. If &ldquo;cv.vfold&rdquo; is greater than the number of observations, then the program equalize them.</p>
</td></tr>
<tr><td><code id="hdda_+3A_loo">LOO</code></td>
<td>
<p>If TRUE, it returns the results (classes and posterior probabilities) for leave-one-out cross-validation.</p>
</td></tr>
<tr><td><code id="hdda_+3A_noise.ctrl">noise.ctrl</code></td>
<td>
<p>This parameter avoids to have a too low value of the 'noise' parameter b. It garantees that the dimension selection process do not select too many dimensions (which leads to a potential too low value of the noise parameter b). When selecting the intrinsic dimensions using Cattell's scree-test or BIC, the function doesn't use the eigenvalues inferior to noise.ctrl, so that the intrinsic dimensions selected can't be higher or equal to the order of these eigenvalues.</p>
</td></tr>
<tr><td><code id="hdda_+3A_d">d</code></td>
<td>
<p>DEPRECATED. This parameter is kept for retro compatibility. Now please use the parameter d_select.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some information on the signification of the model names:
</p>

<dl>
<dt>Akj are the parameters of the classes subspaces:</dt><dd>

<ul>
<li><p>if Akj: each class has its parameters and there is one parameter for each dimension
</p>
</li>
<li><p>if Ak: the classes have different parameters but there is only one per class
</p>
</li>
<li><p>if Aj: all the classes have the same parameters for each dimension (it's a particular case with a common orientation matrix)
</p>
</li>
<li><p>if A: all classes have the same one parameter
</p>
</li></ul>

</dd>
<dt>Bk are the noises of the classes subspaces:</dt><dd>

<ul>
<li><p>If Bk: each class has its proper noise
</p>
</li>
<li><p>if B:  all classes have the same noise
</p>
</li></ul>

</dd>
<dt>Qk is the orientation matrix of each class:</dt><dd> 

<ul>
<li><p>if Qk: all classes have its proper orientation matrix
</p>
</li>
<li><p>if Q: all classes have the same orientation matrix
</p>
</li></ul>

</dd>
<dt>Dk is the intrinsic dimension of each class:</dt><dd> 

<ul>
<li><p>if Dk: the dimensions are free and proper to each class
</p>
</li>
<li><p>if D: the dimension is common to all classes
</p>
</li></ul>

</dd>
</dl>

<p>The model &ldquo;all&rdquo; will compute all the models, give their BIC and keep the model with the highest BIC value.
Instead of writing the model names, they can also be specified using an integer.  1 represents the most general model (&ldquo;AkjBkQkDk&rdquo;) while 14 is the most constrained (&ldquo;ABQD&rdquo;), the others  number/name matching are given below. Note also that several models can be run at once, by using a vector of models (e.g. model = c(&quot;AKBKQKD&quot;,&quot;AKJBQKDK&quot;,&quot;AJBQD&quot;) is equivalent to model = c(8,4,13); to run the 6 first models, use model=1:6). If all the models are to be run, model=&quot;all&quot; is faster than model=1:14. 
</p>

<table>
<tr>
 <td style="text-align: left;">
AkjBkQkDk </td><td style="text-align: center;">   1   </td><td style="text-align: center;">   </td><td style="text-align: left;">  AkjBkQkD </td><td style="text-align: center;">   7   </td>
</tr>
<tr>
 <td style="text-align: left;"> 
AkBkQkDk </td><td style="text-align: center;">   2   </td><td style="text-align: center;"> </td><td style="text-align: left;">  AkBkQkD </td><td style="text-align: center;">   8   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABkQkDk </td><td style="text-align: center;">   3   </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABkQkD </td><td style="text-align: center;">   9   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkjBQkDk </td><td style="text-align: center;">   4   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkjBQkD </td><td style="text-align: center;">   10   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkBQkDk </td><td style="text-align: center;">   5   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkBQkD </td><td style="text-align: center;">   11   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABQkDk </td><td style="text-align: center;">   6   </td><td style="text-align: center;">  </td><td style="text-align: left;">  ABQkD </td><td style="text-align: center;">   12  </td>
</tr>
<tr>
 <td style="text-align: left;">
AjBQD </td><td style="text-align: center;"> 13 </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABQD </td><td style="text-align: center;"> 14
</td>
</tr>

</table>

<p>The parameter d, is used to select the intrinsic dimensions of the subclasses. Here are his definictions:
</p>

<ul>
<li><p>&ldquo;Cattell&rdquo;:
The Cattell's scree-test is used to gather the intrinsic dimension of each class. If the model is of common dimension (models 7 to 14), the scree-test is done on the covariance matrix of the whole dataset.

</p>
</li>
<li><p>&ldquo;BIC&rdquo;:
The intrinsic dimensions are selected with the BIC criterion. See Bouveyron <em>et al.</em> (2010) for a discussion of this topic.
For common dimension models, the procedure is done on the covariance matrix of the whole dataset.

</p>
</li>
<li><p>&ldquo;CV&rdquo;:
A V-fold cross-validation (CV) can be done in order to select the best threshold (for all models) or the best common dimensions (models 7 to 14).  
The V-fold cross-validation is done for each dimension (respectively threshold) in the argument &ldquo;cv.dim&rdquo; (resp. &ldquo;cv.threshold&rdquo;), then the dimension (resp. threshold) that gives the best good classification rate is kept.  
The dataset is split in &ldquo;cv.vfold&rdquo; (default is 10) <em>random</em> subsamples, then CV is done for each sample: each of them is used as validation data while the remaining data is used as training data. For sure, if &ldquo;cv.vfold&rdquo; equals the number of observations, then this CV is equivalent to a leave-one-out.

</p>
</li></ul>



<h3>Value</h3>

<p>hdda returns an 'hdc' object; it's a list containing:
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>The name of the model.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The number of classes.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>The dimensions of each class.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>The parameters of each class subspace.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The noise of each class subspace.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>The mean of each variable for each class.</p>
</td></tr>
<tr><td><code>prop</code></td>
<td>
<p>The proportion of each class.</p>
</td></tr>
<tr><td><code>ev</code></td>
<td>
<p>The eigen values of the var/covar matrix.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>The orthogonal matrix of orientation of each class.</p>
</td></tr>
<tr><td><code>kname</code></td>
<td>
<p>The name of each class.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>The BIC value of the model used.</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>The centers and the standard deviation of the original dataset.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High Dimensional Discriminant Analysis&rdquo;, <em>Communications in Statistics: Theory and Methods</em>, vol. <b>36</b> (14), pp. 2607&ndash;2623
</p>
<p>Bouveyron, C. Celeux, G. and Girard, S. (2010) &ldquo;Intrinsic dimension estimation by maximum likelihood in probabilistic PCA&rdquo;, Technical Report 440372, Universite Paris 1 Pantheon-Sorbonne
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012) &ldquo;HDclassif: An R Package 
for Model-Based Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, 
<em>Journal of Statistical Software</em>, <b>46</b>(6), 1&ndash;29, url: 
<a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hddc">hddc</a></code>, <code><a href="#topic+predict.hdc">predict.hdc</a></code>, <code><a href="#topic+plot.hdc">plot.hdc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1:
data&lt;-simuldata(1000, 1000, 50, K=5)
X &lt;- data$X
clx &lt;- data$clx
Y &lt;- data$Y
cly &lt;- data$cly
# we get the HDDA parameters:
prms1 &lt;- hdda(X, clx)         

cl1 &lt;- predict(prms1, Y, cly)
# the class vector of Y estimated with HDDA:
cl1$class                     

# another model is used:
prms1 &lt;- hdda(X, clx, model=12)
#model=12 is equivalent to model="ABQkD"     
cl1 &lt;- predict(prms1, Y, cly) 

# Example 2:
data(wine)
a &lt;- wine[,-1]
z &lt;- wine[,1]
prms2 &lt;- hdda(a, z, model='all', scaling=TRUE, d_select="bic", graph=TRUE)
cl2 &lt;- predict(prms2, a, z)

# getting the best dimension
# using a common dimension model
# we do LOO-CV using cv.vfold=nrow(a)
prms3 &lt;- hdda(a, z, model="akjbkqkd", d_select="CV", cv.vfold=nrow(a), scaling=TRUE, graph=TRUE)

cl3 &lt;- predict(prms3, a, z)

# Example 3:
# Validation with LOO
prms4 = hdda(a, z, LOO=TRUE, scaling=TRUE)
sum(prms4$class==z) / length(z)

</code></pre>

<hr>
<h2 id='hddc'>High Dimensional Data Clustering</h2><span id='topic+hddc'></span>

<h3>Description</h3>

<p>HDDC is a model-based clustering method. It is based on the Gaussian Mixture Model and on the idea that the data lives in subspaces with a lower dimension than the dimension of the original space. It uses the Expectation - Maximisation algorithm to estimate the parameters of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hddc(
  data,
  K = 1:10,
  model = c("AkjBkQkDk"),
  threshold = 0.2,
  criterion = "bic",
  com_dim = NULL,
  itermax = 200,
  eps = 0.001,
  algo = "EM",
  d_select = "Cattell",
  init = "kmeans",
  init.vector,
  show = getHDclassif.show(),
  mini.nb = c(5, 10),
  scaling = FALSE,
  min.individuals = 2,
  noise.ctrl = 1e-08,
  mc.cores = 1,
  nb.rep = 1,
  keepAllRes = TRUE,
  kmeans.control = list(),
  d_max = 100,
  subset = Inf,
  d
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hddc_+3A_data">data</code></td>
<td>
<p>A matrix or a data frame of observations, assuming the rows are the observations and the columns the variables. Note that NAs are not allowed.</p>
</td></tr>
<tr><td><code id="hddc_+3A_k">K</code></td>
<td>
<p>A vector of integers specifying the number of clusters for which the BIC and the parameters are to be calculated; the function keeps the parameters which maximises the <code>criterion</code>. Default is 1:10.</p>
</td></tr>
<tr><td><code id="hddc_+3A_model">model</code></td>
<td>
<p>A character string vector, or an integer vector indicating the models to be used. The available models are: &quot;AkjBkQkDk&quot; (default), &quot;AkBkQkDk&quot;, &quot;ABkQkDk&quot;, &quot;AkjBQkDk&quot;, &quot;AkBQkDk&quot;, &quot;ABQkDk&quot;, &quot;AkjBkQkD&quot;, &quot;AkBkQkD&quot;, &quot;ABkQkD&quot;, &quot;AkjBQkD&quot;, &quot;AkBQkD&quot;, &quot;ABQkD&quot;, &quot;AjBQD&quot;, &quot;ABQD&quot;. It is not case sensitive and integers can be used instead of names, see details for more information. Several models can be used, if it is, only the results of the one which maximizes the BIC criterion is kept. To run all models, use model=&quot;ALL&quot;.</p>
</td></tr>
<tr><td><code id="hddc_+3A_threshold">threshold</code></td>
<td>
<p>A float stricly within 0 and 1. It is the threshold used in the Cattell's Scree-Test.</p>
</td></tr>
<tr><td><code id="hddc_+3A_criterion">criterion</code></td>
<td>
<p>Either &ldquo;BIC&rdquo; or &ldquo;ICL&rdquo;. If several models are run, the best model is selected using the criterion defined by <code>criterion</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_com_dim">com_dim</code></td>
<td>
<p>It is used only for common dimensions models. The user can give the common dimension s/he wants. If used, it must be an integer. Its default is set to NULL.</p>
</td></tr>
<tr><td><code id="hddc_+3A_itermax">itermax</code></td>
<td>
<p>The maximum number of iterations allowed. The default is 200.</p>
</td></tr>
<tr><td><code id="hddc_+3A_eps">eps</code></td>
<td>
<p>A positive double, default is 0.001. It is the stopping criterion: the algorithm stops when the difference between two successive log-likelihoods is lower than <code>eps</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_algo">algo</code></td>
<td>
<p>A character string indicating the algorithm to be used. The available algorithms are the Expectation-Maximisation (&quot;EM&quot;), the Classification E-M (&quot;CEM&quot;) and the Stochastic E-M (&quot;SEM&quot;). The default algorithm is the &quot;EM&quot;.</p>
</td></tr>
<tr><td><code id="hddc_+3A_d_select">d_select</code></td>
<td>
<p>Either &ldquo;Cattell&rdquo; (default) or &ldquo;BIC&rdquo;. See details for more information. This parameter selects which method to use to select the intrinsic dimensions.</p>
</td></tr>
<tr><td><code id="hddc_+3A_init">init</code></td>
<td>
<p>A character string or a vector of clusters. It is the way to initialize the E-M algorithm. There are five possible initialization: &ldquo;kmeans&rdquo; (default), &ldquo;param&rdquo;, &ldquo;random&rdquo;, &ldquo;mini-em&rdquo; or &ldquo;vector&rdquo;. See details for more information. It can also be directly initialized with a vector containing the prior classes of the observations. If <code>init = "vector"</code>, then you should add the argument <code>init.vector</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_init.vector">init.vector</code></td>
<td>
<p>A vector of integers or factors. It is a user-given initialization. It should be of the same length as of the data. Only used when <code>init = "vector"</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_show">show</code></td>
<td>
<p>Single logical. To diplay summary information on the results after the algorithm is done: set it to <code>TRUE</code>. By default it takes the value of <code><a href="#topic+getHDclassif.show">getHDclassif.show</a></code> which is FALSE at the loading of the package. To permanently have <code>show=TRUE</code>, use <code>setHDclassif.show(TRUE)</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_mini.nb">mini.nb</code></td>
<td>
<p>A vector of integers of length two. This parameter is used in the &ldquo;mini-em&rdquo; initialization. The first integer sets how many times the algorithm is repeated; the second sets the maximum number of iterations the algorithm will do each time. For example, if <code>init="mini-em"</code> and <code>mini.nb=c(5,10)</code>, the algorithm wil be lauched 5 times, doing each time 10 iterations; finally the algorithm will begin with the initialization that maximizes the log-likelihood.</p>
</td></tr>
<tr><td><code id="hddc_+3A_scaling">scaling</code></td>
<td>
<p>Logical: whether to scale the dataset (mean=0 and standard-error=1 for each variable) or not. By default the data is not scaled.</p>
</td></tr>
<tr><td><code id="hddc_+3A_min.individuals">min.individuals</code></td>
<td>
<p>Positive integer greater than 2 (default). This parameter is used to control for the minimum population of a class. If the population of a class becomes stricly inferior to 'min.individuals' then the algorithm stops and gives the message: 'pop&lt;min.indiv.'. Here the meaning of &quot;population of a class&quot; is the sum of its posterior probabilities. The value of 'min.individuals' cannot be lower than 2.</p>
</td></tr>
<tr><td><code id="hddc_+3A_noise.ctrl">noise.ctrl</code></td>
<td>
<p>This parameter avoids to have a too low value of the 'noise' parameter b. It garantees that the dimension selection process do not select too many dimensions (which leads to a potential too low value of the noise parameter b). When selecting the intrinsic dimensions using Cattell's scree-test or BIC, the function doesn't use the eigenvalues inferior to noise.ctrl, so that the intrinsic dimensions selected can't be higher or equal to the order of these eigenvalues.</p>
</td></tr>
<tr><td><code id="hddc_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Positive integer, default is 1. If <code>mc.cores&gt;1</code>, then parallel computing is used, using <code>mc.cores</code> cores. Warning for Windows users only: the parallel computing can sometimes be slower than using one single core (due to how <code><a href="parallel.html#topic+parLapply">parLapply</a></code> works).</p>
</td></tr>
<tr><td><code id="hddc_+3A_nb.rep">nb.rep</code></td>
<td>
<p>A positive integer (default is 1). Each estimation (i.e. combination of (model, K, threshold)) is repeated <code>nb.rep</code> times and only the estimation with the highest log-likelihood is kept.</p>
</td></tr>
<tr><td><code id="hddc_+3A_keepallres">keepAllRes</code></td>
<td>
<p>Logical. Should the results of all runs be kept? If so, an argument <code>all_results</code> is created in the results. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="hddc_+3A_kmeans.control">kmeans.control</code></td>
<td>
<p>A list. The elements of this list should match the parameters of the kmeans initialization (see <code><a href="stats.html#topic+kmeans">kmeans</a></code> help for details). The parameters are &ldquo;iter.max&rdquo;, &ldquo;nstart&rdquo; and &ldquo;algorithm&rdquo;.</p>
</td></tr>
<tr><td><code id="hddc_+3A_d_max">d_max</code></td>
<td>
<p>A positive integer. The maximum number of dimensions to be computed. Default is 100. It means that the instrinsic dimension of any cluster cannot be larger than <code>d_max</code>. It quickens a lot the algorithm for datasets with a large number of variables (e.g. thousands).</p>
</td></tr>
<tr><td><code id="hddc_+3A_subset">subset</code></td>
<td>
<p>An positive integer, default is <code>Inf</code>. In case of large data sets it might be useful to perform HDDC on a subsample of the data: this is the use of this argument. If <code>subset</code> is to a value smaller than the number of observations of the dataset then: HDDC is performed on a random subsample of size <code>subset</code> and once a clustering is obtained on this subsample, the posterior of the clustering is computed on the full sample.</p>
</td></tr>
<tr><td><code id="hddc_+3A_d">d</code></td>
<td>
<p>DEPRECATED. This parameter is kept for retro compatibility. Now please use the parameter d_select.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some information on the signification of the model names:
</p>

<dl>
<dt>Akj are the parameters of the classes subspaces:</dt><dd>

<ul>
<li><p>if Akj: each class has its parameters and there is one parameter for each dimension
</p>
</li>
<li><p>if Ak: the classes have different parameters but there is only one per class
</p>
</li>
<li><p>if Aj: all the classes have the same parameters for each dimension (it's a particular case with a common orientation matrix)
</p>
</li>
<li><p>if A: all classes have the same one parameter
</p>
</li></ul>

</dd>
<dt>Bk are the noises of the classes subspaces:</dt><dd>

<ul>
<li><p>If Bk: each class has its proper noise
</p>
</li>
<li><p>if B:  all classes have the same noise
</p>
</li></ul>

</dd>
<dt>Qk is the orientation matrix of each class:</dt><dd> 

<ul>
<li><p>if Qk: all classes have its proper orientation matrix
</p>
</li>
<li><p>if Q: all classes have the same orientation matrix
</p>
</li></ul>

</dd>
<dt>Dk is the intrinsic dimension of each class:</dt><dd> 

<ul>
<li><p>if Dk: the dimensions are free and proper to each class
</p>
</li>
<li><p>if D: the dimension is common to all classes
</p>
</li></ul>

</dd>
</dl>

<p>The model &ldquo;ALL&rdquo; will compute all the models, give their BIC and keep the model with the highest BIC value.
Instead of writing the model names, they can also be specified using an integer.  1 represents the most general model (&ldquo;AkjBkQkDk&rdquo;) while 14 is the most constrained (&ldquo;ABQD&rdquo;), the others  number/name matching are given below. Note also that several models can be run at once, by using a vector of models (e.g. model = c(&quot;AKBKQKD&quot;,&quot;AKJBQKDK&quot;,&quot;AJBQD&quot;) is equivalent to model = c(8,4,13); to run the 6 first models, use model=1:6). If all the models are to be run, model=&quot;all&quot; is faster than model=1:14. 
</p>

<table>
<tr>
 <td style="text-align: left;">
AkjBkQkDk </td><td style="text-align: center;">   1   </td><td style="text-align: center;">   </td><td style="text-align: left;">  AkjBkQkD </td><td style="text-align: center;">   7   </td>
</tr>
<tr>
 <td style="text-align: left;"> 
AkBkQkDk </td><td style="text-align: center;">   2   </td><td style="text-align: center;"> </td><td style="text-align: left;">  AkBkQkD </td><td style="text-align: center;">   8   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABkQkDk </td><td style="text-align: center;">   3   </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABkQkD </td><td style="text-align: center;">   9   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkjBQkDk </td><td style="text-align: center;">   4   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkjBQkD </td><td style="text-align: center;">   10   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkBQkDk </td><td style="text-align: center;">   5   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkBQkD </td><td style="text-align: center;">   11   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABQkDk </td><td style="text-align: center;">   6   </td><td style="text-align: center;">  </td><td style="text-align: left;">  ABQkD </td><td style="text-align: center;">   12  </td>
</tr>
<tr>
 <td style="text-align: left;">
AjBQD </td><td style="text-align: center;"> 13 </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABQD </td><td style="text-align: center;"> 14
</td>
</tr>

</table>

<p>The parameter <code>d_select</code>, is used to select the intrinsic dimensions of the subclasses. Here are its definitions:
</p>

<ul>
<li><p>&ldquo;Cattell&rdquo;:
The Cattell's scree-test is used to gather the intrinsic dimension of each class. If the model is of common dimension (models 7 to 14), the scree-test is done on the covariance matrix of the whole dataset.

</p>
</li>
<li><p>&ldquo;BIC&rdquo;:
The intrinsic dimensions are selected with the BIC criterion. See Bouveyron <em>et al.</em> (2010) for a discussion of this topic.
For common dimension models, the procedure is done on the covariance matrix of the whole dataset.

</p>
</li>
<li><p>Note that &quot;Cattell&quot; (resp. &quot;BIC&quot;) can be abreviated to &quot;C&quot; (resp. &quot;B&quot;) and that this argument is not case sensitive.
</p>
</li></ul>

<p>The different initializations are:
</p>

<dl>
<dt>&ldquo;param&rdquo;:</dt><dd><p>it is initialized with the parameters, the means being generated by a multivariate normal distribution and the covariance matrix being common to the whole sample</p>
</dd>
<dt>&ldquo;mini-em&rdquo;:</dt><dd><p>it is an initialization strategy, the classes are randomly initialized and the E-M algorithm makes several iterations, this action is repetead a few times (the default is 5 iterations and 10 times), at the end, the initialization choosen is the one which maximise the log-likelihood (see mini.nb for more information about its parametrization)</p>
</dd>
<dt>&ldquo;random&rdquo;:</dt><dd><p>the classes are randomly given using a multinomial distribution</p>
</dd>
<dt>&ldquo;kmeans&rdquo;:</dt><dd><p>the classes are initialized using the kmeans function (with: algorithm=&quot;Hartigan-Wong&quot;; nstart=4; iter.max=50); note that the user can use his own arguments for kmeans using the dot-dot-dot argument </p>
</dd> 
<dt>A prior class vector:</dt><dd><p>It can also be directly initialized with a vector containing the prior classes of the observations. To do so use <code>init="vector"</code> and provide the vector in the argument <code>init.vector</code>.</p>
</dd>
</dl>

<p>The BIC criterion used in this function is to be maximized and is defined as 2*LL-k*log(n) where LL is the log-likelihood, k is the number of parameters and n is the number of observations.
</p>


<h3>Value</h3>

<p>hddc returns an 'hdc' object; it's a list containing:
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>The name of the model.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>The number of classes.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>The dimensions of each class.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>The parameters of each class subspace.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The noise of each class subspace.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>The mean of each variable for each class.</p>
</td></tr>
<tr><td><code>prop</code></td>
<td>
<p>The proportion of each class.</p>
</td></tr>
<tr><td><code>ev</code></td>
<td>
<p>The eigen values of the var/covar matrix.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>The orthogonal matrix of orientation of each class.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The log-likelihood.</p>
</td></tr>
<tr><td><code>loglik_all</code></td>
<td>
<p>The log-likelihood of all iterations. Note that if <code>subset</code> was used, then this vector represents the likelihoods evaluations for the subsample on which HDDC was performed (i.e. not the likelihood for the full dataset &ndash; so these values are smaller than the on given in &lsquo;loglik&rsquo; which concerns the whole sample after the estimation).</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>The matrix of the probabilities to belong to a class for each observation and each class.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>The class vector obtained by the clustering.</p>
</td></tr>
<tr><td><code>com_ev</code></td>
<td>
<p>Only if this is a common dimension model. The eigenvalues of the var/covar matrix of the whole dataset.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>The number of parameters of the model.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>The threshold used for the Cattell scree-test.</p>
</td></tr>
<tr><td><code>d_select</code></td>
<td>
<p>The way the dimensions were selected.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>The BIC of the model.</p>
</td></tr>
<tr><td><code>ICL</code></td>
<td>
<p>The ICL of the model.</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>The criterion used to select the model.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call.</p>
</td></tr>
<tr><td><code>allCriteria</code></td>
<td>
<p>The data.frame with the combination (model, K, threshold) and the associated values of the likelihood (LL), BIC and ICL, as well as the rank of each of the models with respect to the selection criterion. It also reports the original order in which were estimated the models as well as each model complexity</p>
</td></tr>
<tr><td><code>all_results</code></td>
<td>
<p>Only if <code>keepAllRes=TRUE</code>. The parameters of all estimations that were run.</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>Only if <code>scaling=TRUE</code>. The centers and the standard deviation of the original dataset.</p>
</td></tr>
<tr><td><code>id_subset</code></td>
<td>
<p>Only if <code>subset</code> is used. The observation IDs of the subsample on which the HDDC parameters were estimated.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High-Dimensional Data Clustering&rdquo;, <em>Computational Statistics and Data Analysis</em>, vol. <b>52</b> (1), pp. 502&ndash;519
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012) &ldquo;HDclassif: An R Package for 
Model-Based Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, 
<em>Journal of Statistical Software</em>, <b>46</b>(6), 1&ndash;29, url: 
<a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hdda">hdda</a></code>, <code><a href="#topic+predict.hdc">predict.hdc</a></code>, <code><a href="#topic+plot.hdc">plot.hdc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1:
data &lt;- simuldata(1000, 1000, 50)
X &lt;- data$X
clx &lt;- data$clx
Y &lt;- data$Y
cly &lt;- data$cly

#clustering of the simulated dataset:
prms1 &lt;- hddc(X, K=3, algo="CEM", init='param')                

#class vector obtained by the clustering:
prms1$class                

#We can look at the adjusted rand index to assess the goodness of fit
res1 &lt;- predict(prms1, X, clx)
res2 &lt;- predict(prms1, Y)       
#the class predicted using hddc parameters on the test dataset:  
res2$class                                                           


# Example 2:
data(Crabs)

# clustering of the Crabs dataset:
prms3 &lt;- hddc(Crabs[,-1], K=4, algo="EM", init='mini-em')        
res3 &lt;- predict(prms3, Crabs[,-1], Crabs[,1])

# another example using the Crabs dataset
prms4 &lt;- hddc(Crabs[,-1], K=1:8, model=c(1,2,7,9))

# model=c(1,2,7,9) is equivalent to:
# model=c("AKJBKQKDK","AKBKQKDK","AKJBKQKD"#' ,"ABKQKD") 
res4 &lt;- predict(prms4, Crabs[,-1], Crabs[,1])

# PARALLEL COMPUTING
## Not run: 
# Same example but with Parallel Computing =&gt; platform specific
# (slower for Windows users)
# To enable it, just use the argument 'mc.cores'
prms5 &lt;- hddc(Crabs[,-1], K=1:8, model=c(1,2,7,9), mc.cores=2)

## End(Not run)

# LARGE DATASETS
# Assume you have a very large data set 
# =&gt; you can use the argument 'subset' to obtain quick results:
## Not run: 
# we take a subset of 10000 observations and run hddc
# once the classification is done, the posterior is computed 
# on the full data
prms = hddc(bigData, subset = 10000)
# You obtain a much faster (although less precise) 
# classification of the full dataset:
table(prms$class)

## End(Not run)


</code></pre>

<hr>
<h2 id='hdmda'>Mixture Discriminant Analysis with HD Gaussians</h2><span id='topic+hdmda'></span>

<h3>Description</h3>

<p>HD-MDA implements mixture discriminant analysis (MDA, Hastie &amp; Tibshirani, 1996) with HD Gaussians instead of full Gaussians. Each class is assumed to be made of several class-specific groups in which the data live in low-dimensional subspaces. From a technical point of view, a clustering is done using <code><a href="#topic+hddc">hddc</a></code> in each class.</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdmda(X,cls,K=1:10,model='AkjBkQkDk',show=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hdmda_+3A_x">X</code></td>
<td>
<p>A matrix or a data frame of observations, assuming the rows are the observations and the columns the variables. Note that NAs are not allowed.</p>
</td></tr>
<tr><td><code id="hdmda_+3A_cls">cls</code></td>
<td>
<p>The vector of the class of each observations, its type can be numeric or string.</p>
</td></tr>
<tr><td><code id="hdmda_+3A_k">K</code></td>
<td>
<p>A vector of integers specifying the number of clusters for which the BIC and the parameters are to be calculated; the function keeps the parameters which maximises the BIC. Note that the length of the vector K can't be larger than 20. Default is 1:10.</p>
</td></tr>
<tr><td><code id="hdmda_+3A_model">model</code></td>
<td>
<p>A character string vector, or an integer vector indicating the models to be used. The available models are: &quot;AkjBkQkDk&quot; (default), &quot;AkBkQkDk&quot;, &quot;ABkQkDk&quot;, &quot;AkjBQkDk&quot;, &quot;AkBQkDk&quot;, &quot;ABQkDk&quot;, &quot;AkjBkQkD&quot;, &quot;AkBkQkD&quot;, &quot;ABkQkD&quot;, &quot;AkjBQkD&quot;, &quot;AkBQkD&quot;, &quot;ABQkD&quot;, &quot;AjBQD&quot;, &quot;ABQD&quot;. It is not case sensitive and integers can be used instead of names, see details for more information. Several models can be used, if it is, only the results of the one which maximizes the BIC criterion is kept. To run all models, use model=&quot;ALL&quot;.</p>
</td></tr>
<tr><td><code id="hdmda_+3A_show">show</code></td>
<td>
<p>Use show = TRUE to display some information related to the clustering.</p>
</td></tr>
<tr><td><code id="hdmda_+3A_...">...</code></td>
<td>
<p>Any argument that can be used by the function <code><a href="#topic+hddc">hddc</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some information on the signification of the model names:
</p>

<dl>
<dt>Akj are the parameters of the classes subspaces:</dt><dd>

<ul>
<li><p>if Akj: each class has its parameters and there is one parameter for each dimension
</p>
</li>
<li><p>if Ak: the classes have different parameters but there is only one per class
</p>
</li>
<li><p>if Aj: all the classes have the same parameters for each dimension (it's a particular case with a common orientation matrix)
</p>
</li>
<li><p>if A: all classes have the same one parameter
</p>
</li></ul>

</dd>
<dt>Bk are the noises of the classes subspaces:</dt><dd>

<ul>
<li><p>If Bk: each class has its proper noise
</p>
</li>
<li><p>if B:  all classes have the same noise
</p>
</li></ul>

</dd>
<dt>Qk is the orientation matrix of each class:</dt><dd> 

<ul>
<li><p>if Qk: all classes have its proper orientation matrix
</p>
</li>
<li><p>if Q: all classes have the same orientation matrix
</p>
</li></ul>

</dd>
<dt>Dk is the intrinsic dimension of each class:</dt><dd> 

<ul>
<li><p>if Dk: the dimensions are free and proper to each class
</p>
</li>
<li><p>if D: the dimension is common to all classes
</p>
</li></ul>

</dd>
</dl>

<p>The model &ldquo;all&rdquo; will compute all the models, give their BIC and keep the model with the highest BIC value.
Instead of writing the model names, they can also be specified using an integer.  1 represents the most general model (&ldquo;AkjBkQkDk&rdquo;) while 14 is the most constrained (&ldquo;ABQD&rdquo;), the others  number/name matching are given below:
</p>

<table>
<tr>
 <td style="text-align: left;">
AkjBkQkDk </td><td style="text-align: center;">   1   </td><td style="text-align: center;">   </td><td style="text-align: left;">  AkjBkQkD </td><td style="text-align: center;">   7   </td>
</tr>
<tr>
 <td style="text-align: left;"> 
AkBkQkDk </td><td style="text-align: center;">   2   </td><td style="text-align: center;"> </td><td style="text-align: left;">  AkBkQkD </td><td style="text-align: center;">   8   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABkQkDk </td><td style="text-align: center;">   3   </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABkQkD </td><td style="text-align: center;">   9   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkjBQkDk </td><td style="text-align: center;">   4   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkjBQkD </td><td style="text-align: center;">   10   </td>
</tr>
<tr>
 <td style="text-align: left;">   
AkBQkDk </td><td style="text-align: center;">   5   </td><td style="text-align: center;">  </td><td style="text-align: left;">  AkBQkD </td><td style="text-align: center;">   11   </td>
</tr>
<tr>
 <td style="text-align: left;">   
ABQkDk </td><td style="text-align: center;">   6   </td><td style="text-align: center;">  </td><td style="text-align: left;">  ABQkD </td><td style="text-align: center;">   12  </td>
</tr>
<tr>
 <td style="text-align: left;">
AjBQD </td><td style="text-align: center;"> 13 </td><td style="text-align: center;">  </td><td style="text-align: left;"> ABQD </td><td style="text-align: center;"> 14
</td>
</tr>

</table>



<h3>Value</h3>

<p><code>hdmda</code> returns an 'hdmda' object which is a list containing:
</p>
<table role = "presentation">
<tr><td><code>alpha</code></td>
<td>
<p>Estimated prior probabilities for the classes.</p>
</td></tr>
<tr><td><code>prms</code></td>
<td>
<p>Estimated mixture parameters for each class.</p>
</td></tr>
<tr><td><code>kname</code></td>
<td>
<p>The name (level) of each class.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>C. Bouveyron and C. Brunet (2014), &ldquo;Model-based clustering of high-dimensional data: A review&rdquo;, Computational Statistics and Data Analysis, vol. 71, pp. 52-78.
</p>
<p>Bouveyron, C. Girard, S. and Schmid, C. (2007), &ldquo;High Dimensional Discriminant Analysis&rdquo;, Communications in Statistics: Theory and Methods, vol. 36 (14), pp. 2607-2623.
</p>
<p>Bouveyron, C. Celeux, G. and Girard, S. (2011), &ldquo;Intrinsic dimension estimation by maximum likelihood in probabilistic PCA&rdquo;, Pattern Recognition Letters, vol. 32 (14), pp. 1706-1713.
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012), &ldquo;HDclassif: An R Package for Model-Based Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, Journal of Statistical Software, 46(6), pp. 1-29, url: <a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>.
</p>
<p>Hastie, T., &amp; Tibshirani, R. (1996), &ldquo;Discriminant analysis by Gaussian mixtures&rdquo;, Journal of the Royal Statistical Society, Series B (Methodological), pp. 155-176.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hdda">hdda</a></code>, <code><a href="#topic+hddc">hddc</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the Wine data set
data(wine)
cls = wine[,1]; X = scale(wine[,-1])

# A simple use...
out = hdmda(X[1:100,],cls[1:100])
res = predict(out,X[101:nrow(X),])

# Comparison between hdmda and hdda in a CV setup
set.seed(123); nb = 10; Err = matrix(NA,2,nb)
for (i in 1:nb){
  cat('.')
  test = sample(nrow(X),50)
  out0 = lda(X[-test,],cls[-test])
  res0 = predict(out0,X[test,])
  Err[1,i] = sum(res0$class != cls[test]) / length(test)
  out = hdmda(X[-test,],cls[-test],K=1:3,model="AKJBQKDK")
  res = predict(out,X[test,])
  Err[2,i] = sum(res$class != cls[test]) / length(test)
}
cat('\n')
boxplot(t(Err),names=c('LDA','HD-MDA'),col=2:3,ylab="CV classifciation error",
  main='CV classifciation error on Wine data')
</code></pre>

<hr>
<h2 id='plot.hdc'>
Cattell's Scree-Test for 'hdc' class objects.
</h2><span id='topic+plot.hdc'></span>

<h3>Description</h3>

<p>This function plots Cattell's scree-test or the BIC selection, using parameters coming from <var>hdda</var> or <var>hddc</var> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdc'
plot(x, method = NULL, threshold = NULL, noise.ctrl=1e-8, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.hdc_+3A_x">x</code></td>
<td>

<p>A 'hdc' class object obtained using <var>hdda</var> or <var>hddc</var> methods.
</p>
</td></tr>
<tr><td><code id="plot.hdc_+3A_method">method</code></td>
<td>

<p>The method used to select the intrinsic dimension. It can be &quot;BIC&quot; or &quot;Cattell&quot;. By default it takes the method used when obtaining the parameters using <var>hdda</var> or <var>hddc</var>.
Note that &quot;Cattell&quot; (resp. &quot;BIC&quot;) can be abreviated to &quot;C&quot; (resp. &quot;B&quot;) and that this argument is not case sensitive.
</p>
</td></tr>
<tr><td><code id="plot.hdc_+3A_threshold">threshold</code></td>
<td>

<p>The threshold used in Cattell's Scree-Test. By default it takes the threshold in the argument x, if none, the default value of the threshold is 0.2.
</p>
</td></tr>
<tr><td><code id="plot.hdc_+3A_noise.ctrl">noise.ctrl</code></td>
<td>

<p>This parameter avoids to have a too low value of the 'noise' parameter b. It garantees that the dimension selection process do not select too many dimensions (which leads to a potential too low value of the noise parameter b). When selecting the intrinsic dimensions using Cattell's scree-test or BIC, the function doesn't use the eigenvalues inferior to noise.ctrl, so that the intrinsic dimensions selected can't be higher or equal to the order of these eigenvalues.
</p>
</td></tr>
<tr><td><code id="plot.hdc_+3A_...">...</code></td>
<td>

<p>Arguments based from or to other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>If method = &quot;Cattell&quot;</dt><dd><p> The plot of the eigen values and of the sequential differences of the eigen values. The dimension to retain is the one before the last fall of the eigenvalues' differences below the threshold.</p>
</dd>
<dt>If method = &quot;BIC&quot;</dt><dd><p>The BIC related to the dimension for each class. It stops after the first fall of the BIC.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High Dimensional Discriminant Analysis&rdquo;, <em>Communications in Statistics: Theory and Methods</em>, vol. <b>36</b> (14), pp. 2607&ndash;2623
</p>
<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High-Dimensional Data Clustering&rdquo;, <em>Computational Statistics and Data Analysis</em>, vol. <b>52</b> (1), pp. 502&ndash;519
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012) &ldquo;HDclassif: An R Package for Model-Based 
Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, <em>Journal of Statistical Software</em>, 
<b>46</b>(6), 1&ndash;29, url: <a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hdda">hdda</a></code>, <code><a href="#topic+hddc">hddc</a></code>, <code><a href="#topic+predict.hdc">predict.hdc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 :
data(wine)
a &lt;- wine[,-1]
z &lt;- wine[,1]

prms1 &lt;- hdda(a, z, model="AkBkQkDk", scaling=TRUE, d_select="bic")

#the plot related to the selection that has been done: BIC
plot(prms1)     

#it shows the plot of Cattell's scree-test, with a threshold of .3
plot(prms1,"Cattell",0.3)                         


prms2 &lt;- hdda(a, z, model="AkBkQkD", scaling=TRUE, d_select="cattell")
#the plot related to the selection that has been done: Cattell's scree-test
plot(prms2) 
#the plot of the BIC
plot(prms2,"b") 

</code></pre>

<hr>
<h2 id='predict.hdc'>Prediction method for &lsquo;hdc&rsquo; class objects.</h2><span id='topic+predict.hdc'></span>

<h3>Description</h3>

<p>This function computes the class prediction of a dataset with respect to the model-based supervised and unsupervised classification methods <code><a href="#topic+hdda">hdda</a></code> and <code><a href="#topic+hddc">hddc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdc'
predict(object, data, cls = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.hdc_+3A_object">object</code></td>
<td>
<p>An &lsquo;hdc&rsquo; class object obtained by using <code><a href="#topic+hdda">hdda</a></code> or <code><a href="#topic+hddc">hddc</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.hdc_+3A_data">data</code></td>
<td>
<p>A matrix or a data frame of observations, assuming the rows are the observations and the columns the variables. The data should be in the exact same format as the one that trained the model. Note that NAs are not allowed.</p>
</td></tr>
<tr><td><code id="predict.hdc_+3A_cls">cls</code></td>
<td>
<p>A vector of the thue classes of each observation. It is optional and used to be compared to the predicted classes, default is NULL.</p>
</td></tr>
<tr><td><code id="predict.hdc_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>vector of the predicted class.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>The matrix of the probabilities to belong to a class for each observation and each class.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The likelihood of the classification on the new data.</p>
</td></tr>
</table>
<p>If the initial class vector is given to the argument &lsquo;cls&rsquo; then the adjusted rand index (ARI) is also returned. Also the following object is returned:
</p>
<table role = "presentation">
<tr><td><code>ARI</code></td>
<td>
<p>The confusion matrix of the classification.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High Dimensional Discriminant Analysis&rdquo;, <em>Communications in Statistics: Theory and Methods</em>, vol. <b>36</b> (14), pp. 2607&ndash;2623
</p>
<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High-Dimensional Data Clustering&rdquo;, <em>Computational Statistics and Data Analysis</em>, vol. <b>52</b> (1), pp. 502&ndash;519
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012) &ldquo;HDclassif: An R Package for Model-Based 
Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, <em>Journal of Statistical Software</em>, 
<b>46</b>(6), 1&ndash;29, url: <a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>
</p>


<h3>See Also</h3>

<p>The functions to do high dimensional classification <code><a href="#topic+hdda">hdda</a></code> or clustering <code><a href="#topic+hddc">hddc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1:
data &lt;- simuldata(1000, 1000, 50)
X &lt;- data$X
clx &lt;- data$clx
Y &lt;- data$Y
cly &lt;- data$cly

#clustering of the gaussian dataset:
prms1 &lt;- hddc(X, K=3, algo="CEM", init='param')      
          
#class vector obtained by the clustering:
prms1$class                   

# only to see the good classification rate and 
# the Adjusted Rand Index:                     
res1 &lt;- predict(prms1, X, clx)                                            
res2 &lt;- predict(prms1, Y)       

#the class predicted using hddc parameters on the test dataset:  
res2$class                                                           


# Example 2:
data(Crabs)
#clustering of the Crabs dataset:
prms3 &lt;- hddc(Crabs[,-1], K=4, algo="EM", init='kmeans')        
res3 &lt;- predict(prms3, Crabs[,-1], Crabs[,1])




</code></pre>

<hr>
<h2 id='predict.hdmda'>
Prediction method for &lsquo;hdmda&rsquo; class objects.
</h2><span id='topic+predict.hdmda'></span>

<h3>Description</h3>

<p>This function computes the class prediction of a dataset with respect to the model-based supervised classification method <code><a href="#topic+hdmda">hdmda</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdmda'
predict(object, X, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.hdmda_+3A_object">object</code></td>
<td>

<p>An object of class &lsquo;hdmda&rsquo;.
</p>
</td></tr>
<tr><td><code id="predict.hdmda_+3A_x">X</code></td>
<td>

<p>A matrix or a data frame of observations, assuming the rows are the observations and the columns the variables. Note that NAs are not allowed.
</p>
</td></tr>
<tr><td><code id="predict.hdmda_+3A_...">...</code></td>
<td>

<p>Arguments based from or to other methods. Not currently used.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>vector of the predicted class.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>The matrix of the probabilities to belong to a class for each observation and each class.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard
</p>


<h3>References</h3>

<p>C. Bouveyron and C. Brunet (2014), &ldquo;Model-based clustering of high-dimensional data: A review&rdquo;, Computational Statistics and Data Analysis, vol. 71, pp. 52-78.
</p>
<p>Bouveyron, C. Girard, S. and Schmid, C. (2007), &ldquo;High Dimensional Discriminant Analysis&rdquo;, Communications in Statistics: Theory and Methods, vol. 36 (14), pp. 2607-2623.
</p>
<p>Bouveyron, C. Celeux, G. and Girard, S. (2011), &ldquo;Intrinsic dimension estimation by maximum likelihood in probabilistic PCA&rdquo;, Pattern Recognition Letters, vol. 32 (14), pp. 1706-1713.
</p>
<p>Berge, L. Bouveyron, C. and Girard, S. (2012), &ldquo;HDclassif: An R Package 
for Model-Based Clustering and Discriminant Analysis of High-Dimensional Data&rdquo;, 
Journal of Statistical Software, 46(6), pp. 1-29, url: <a href="https://doi.org/10.18637/jss.v046.i06">doi:10.18637/jss.v046.i06</a>.
</p>
<p>Hastie, T., &amp; Tibshirani, R. (1996), &ldquo;Discriminant analysis by Gaussian mixtures&rdquo;, Journal of the Royal Statistical Society, Series B (Methodological), pp. 155-176.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hdmda">hdmda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the Wine data set
data(wine)
cls = wine[,1]; X = scale(wine[,-1])

# A simple use...
out = hdmda(X[1:100,],cls[1:100])
res = predict(out,X[101:nrow(X),])
</code></pre>

<hr>
<h2 id='setHDclassif.show'>Sets/gets the default 'show' argument in HDDC and HDDA</h2><span id='topic+setHDclassif.show'></span><span id='topic+getHDclassif.show'></span>

<h3>Description</h3>

<p>Sets/gets the default value for 'show' argument in HDDC and HDDC. When <code>TRUE</code> then clustering information is returned at the end of the process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setHDclassif.show(show)

getHDclassif.show
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setHDclassif.show_+3A_show">show</code></td>
<td>
<p>Single logical with default. Will specify the default value of the <code>show</code> argument in HDDA and HDDC.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>An object of class <code>function</code> of length 1.
</p>


<h3>Value</h3>

<p><code>getHDclassif.show</code> returns the default value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Crabs)

# clustering of the Crabs dataset:
prms &lt;- hddc(Crabs[,-1], K=4)  
# By default no information is displayed

# To show information:
prms &lt;- hddc(Crabs[,-1], K=4, show = TRUE)  

# To set it permanently:
setHDclassif.show(TRUE)
prms &lt;- hddc(Crabs[,-1], K=4)  

# to disable it permanently:
setHDclassif.show(FALSE)




</code></pre>

<hr>
<h2 id='simuldata'>
Gaussian Data Generation
</h2><span id='topic+simuldata'></span>

<h3>Description</h3>

<p>This function generates two datasets according to the model [AkBkQkDk] of the HDDA gaussian mixture model paramatrisation (see ref.).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simuldata(nlearn, ntest, p, K = 3, prop = NULL, d = NULL, a = NULL, b = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simuldata_+3A_nlearn">nlearn</code></td>
<td>

<p>The size of the learning dataset to be generated.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_ntest">ntest</code></td>
<td>

<p>The size of the testing dataset to be generated.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_p">p</code></td>
<td>

<p>The number of variables.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_k">K</code></td>
<td>

<p>The number of classes.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_prop">prop</code></td>
<td>

<p>The proportion of each class.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_d">d</code></td>
<td>

<p>The dimension of the intrinsic subspace of each class.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_a">a</code></td>
<td>

<p>The value of the main parameter of each class.
</p>
</td></tr>
<tr><td><code id="simuldata_+3A_b">b</code></td>
<td>

<p>The noise of each class.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p> The learning dataset.</p>
</td></tr>
<tr><td><code>clx</code></td>
<td>
<p> The class vector of the learning dataset. </p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p> The test dataset. </p>
</td></tr>
<tr><td><code>cly</code></td>
<td>
<p> The class vector of the test dataset. </p>
</td></tr>
<tr><td><code>prms</code></td>
<td>
<p> The principal parameters used to generate the datasets. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Laurent Berge, Charles Bouveyron and Stephane Girard 
</p>


<h3>References</h3>

<p>Bouveyron, C. Girard, S. and Schmid, C. (2007) &ldquo;High Dimensional Discriminant Analysis&rdquo;, <em>Communications in Statistics : Theory and Methods</em>, vol. <b>36</b>(14), pp. 2607&ndash;2623
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hddc">hddc</a></code>, <code><a href="#topic+hdda">hdda</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- simuldata(500, 1000, 50, K=5, prop=c(0.2,0.25,0.25,0.15,0.15))
X &lt;- data$X
clx &lt;- data$clx
f &lt;- hdda(X, clx)
Y &lt;- data$Y
cly &lt;- data$cly
e &lt;- predict(f, Y, cly)

</code></pre>

<hr>
<h2 id='slopeHeuristic'>Slope Heuristic for HDDC objects</h2><span id='topic+slopeHeuristic'></span>

<h3>Description</h3>

<p>This function computes the slope heuristic for a set of objects obtained by the function <code><a href="#topic+hddc">hddc</a></code>. The slope heuristic is a criterion in which the likelihood is penalized according to the result of the fit of the likelihoods on the complexities of the models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slopeHeuristic(x, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="slopeHeuristic_+3A_x">x</code></td>
<td>
<p>An <code>hdc</code> object, obtained from the function <code><a href="#topic+hddc">hddc</a></code>.</p>
</td></tr>
<tr><td><code id="slopeHeuristic_+3A_plot">plot</code></td>
<td>
<p>Logical, default is <code>FALSE</code>. If <code>TRUE</code>, then a graph representing: 1) the likelihoods, the complexity, the fit (i.e. the slope) and 2) the value of the slope heuristic (in blue squares).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is only useful if there are many models (at least 3, better if more) that were estimated by the function <code><a href="#topic+hddc">hddc</a></code>. If there are less than 2 models, the function wil l return an error.
</p>


<h3>Value</h3>

<p>A list of two elements:
</p>
<table role = "presentation">
<tr><td><code>best_model_index</code></td>
<td>
<p>The index of the best model, among all estimated models.</p>
</td></tr>
<tr><td><code>allCriteria</code></td>
<td>
<p>The data.frame containing all the criteria, with the new slope heuristic.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Clustering of the Crabs data set
data(Crabs)
prms = hddc(Crabs[,-1], K = 1:10) # we estimate ten models
slope = slopeHeuristic(prms, plot = TRUE)
plot(slope$allCriteria) # The best model is indeed for 4 clusters
prms$all_results[[slope$best_model_index]] # we extract the best model


</code></pre>

<hr>
<h2 id='wine'>
Wine dataset
</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. 
The analysis determined the quantities of 13 constituents found in each of the three types of wines. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wine)</code></pre>


<h3>Format</h3>

<p>A data frame with 178 observations on the following 14 variables :
</p>

<dl>
<dt><code>class</code></dt><dd><p>The class vector, the three different cultivars of wine are reprensented by the three integers : 1 to 3.</p>
</dd>
<dt><code>V1</code></dt><dd><p>Alcohol</p>
</dd>
<dt><code>V2</code></dt><dd><p>Malic acid</p>
</dd>
<dt><code>V3</code></dt><dd><p>Ash</p>
</dd>
<dt><code>V4</code></dt><dd><p>Alcalinity of ash</p>
</dd>
<dt><code>V5</code></dt><dd><p>Magnesium</p>
</dd>
<dt><code>V6</code></dt><dd><p>Total phenols</p>
</dd>
<dt><code>V7</code></dt><dd><p>Flavanoids</p>
</dd>
<dt><code>V8</code></dt><dd><p>Nonflavanoid phenols</p>
</dd>
<dt><code>V9</code></dt><dd><p>Proanthocyanins</p>
</dd>
<dt><code>V10</code></dt><dd><p>Color intensity</p>
</dd>
<dt><code>V11</code></dt><dd><p>Hue</p>
</dd>
<dt><code>V12</code></dt><dd><p>OD280/OD315 of diluted wines</p>
</dd>
<dt><code>V13</code></dt><dd><p>Proline</p>
</dd>
</dl>



<h3>Source</h3>

<p>This dataset is from the UCI machine learning repository, provided here : <a href="http://archive.ics.uci.edu/ml/datasets/Wine">http://archive.ics.uci.edu/ml/datasets/Wine</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
