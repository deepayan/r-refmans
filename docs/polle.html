<!DOCTYPE html><html><head><title>Help for package polle</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {polle}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#polle-package'><p>Policy Learning</p></a></li>
<li><a href='#conditional'><p>Conditional Policy Evaluation</p></a></li>
<li><a href='#control_blip'><p>Control arguments for doubly robust blip-learning</p></a></li>
<li><a href='#control_drql'><p>Control arguments for doubly robust Q-learning</p></a></li>
<li><a href='#control_earl'><p>Control arguments for Efficient Augmentation and Relaxation Learning</p></a></li>
<li><a href='#control_owl'><p>Control arguments for Outcome Weighted Learning</p></a></li>
<li><a href='#control_ptl'><p>Control arguments for Policy Tree Learning</p></a></li>
<li><a href='#control_rwl'><p>Control arguments for Residual Weighted Learning</p></a></li>
<li><a href='#copy_policy_data'><p>Copy Policy Data Object</p></a></li>
<li><a href='#fit_g_functions'><p>Fit g-functions</p></a></li>
<li><a href='#g_model'><p>g_model class object</p></a></li>
<li><a href='#get_action_set'><p>Get Action Set</p></a></li>
<li><a href='#get_actions'><p>Get Actions</p></a></li>
<li><a href='#get_g_functions'><p>Get g-functions</p></a></li>
<li><a href='#get_history_names'><p>Get history variable names</p></a></li>
<li><a href='#get_id'><p>Get IDs</p></a></li>
<li><a href='#get_id_stage'><p>Get IDs and Stages</p></a></li>
<li><a href='#get_K'><p>Get Maximal Stages</p></a></li>
<li><a href='#get_n'><p>Get Number of Observations</p></a></li>
<li><a href='#get_policy'><p>Get Policy</p></a></li>
<li><a href='#get_policy_actions'><p>Get Policy Actions</p></a></li>
<li><a href='#get_policy_functions.blip'><p>Get Policy Functions</p></a></li>
<li><a href='#get_policy_object'><p>Get Policy Object</p></a></li>
<li><a href='#get_q_functions'><p>Get Q-functions</p></a></li>
<li><a href='#get_stage_action_sets'><p>Get Stage Action Sets</p></a></li>
<li><a href='#get_utility'><p>Get the Utility</p></a></li>
<li><a href='#history'><p>Get History Object</p></a></li>
<li><a href='#nuisance_functions'><p>Nuisance Functions</p></a></li>
<li><a href='#partial'><p>Trim Number of Stages</p></a></li>
<li><a href='#plot.policy_data'><p>Plot policy data for given policies</p></a></li>
<li><a href='#plot.policy_eval'><p>Plot histogram of the influence curve for a <code>policy_eval</code> object</p></a></li>
<li><a href='#policy'><p>Policy-class</p></a></li>
<li><a href='#policy_data'><p>Create Policy Data Object</p></a></li>
<li><a href='#policy_def'><p>Define Policy</p></a></li>
<li><a href='#policy_eval'><p>Policy Evaluation</p></a></li>
<li><a href='#policy_learn'><p>Create Policy Learner</p></a></li>
<li><a href='#predict.nuisance_functions'><p>Predict g-functions and Q-functions</p></a></li>
<li><a href='#q_model'><p>q_model class object</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sim_multi_stage'><p>Simulate Multi-Stage Data</p></a></li>
<li><a href='#sim_single_stage'><p>Simulate Single-Stage Data</p></a></li>
<li><a href='#sim_single_stage_multi_actions'><p>Simulate Single-Stage Multi-Action Data</p></a></li>
<li><a href='#sim_two_stage'><p>Simulate Two-Stage Data</p></a></li>
<li><a href='#sim_two_stage_multi_actions'><p>Simulate Two-Stage Multi-Action Data</p></a></li>
<li><a href='#subset_id'><p>Subset Policy Data on ID</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Policy Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Framework for evaluating user-specified finite stage policies and learning realistic policies via doubly robust loss functions. Policy learning methods include doubly robust restricted Q-learning, sequential policy tree learning and outcome weighted learning. See Nordland and Holst (2022) &lt;<a href="https://doi.org/10.48550/arXiv.2212.02335">doi:10.48550/arXiv.2212.02335</a>&gt; for documentation and references.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0), SuperLearner</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.14.5), lava (&ge; 1.7.0), future.apply,
progressr, methods, policytree (&ge; 1.2.0), survival, targeted,
DynTxRegime</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DTRlearn2, glmnet (&ge; 4.1-6), mgcv, xgboost, knitr, ranger,
rmarkdown, testthat (&ge; 3.0), ggplot2</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/AndreasNordland/polle/issues">https://github.com/AndreasNordland/polle/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-06 16:38:38 UTC; andreasnordland</td>
</tr>
<tr>
<td>Author:</td>
<td>Andreas Nordland [aut, cre],
  Klaus Holst <a href="https://orcid.org/0000-0002-1364-6789"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andreas Nordland &lt;andreasnordland@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-06 17:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='polle-package'>Policy Learning</h2><span id='topic+polle-package'></span><span id='topic+polle'></span>

<h3>Description</h3>

<p>Framework for evaluating user-specified finite stage policies
and learning realistic policies via doubly robust loss functions. Policy
learning methods include doubly robust restricted Q-learning,
sequential policy tree learning and outcome weighted learning.
See Nordland and Holst (20222) <a href="https://arxiv.org/abs/2212.02335">https://arxiv.org/abs/2212.02335</a> for documentation and references.
</p>


<h3>Author(s)</h3>

<p>Andreas Nordland (Maintainer) <a href="mailto:andreas.nordland@gmail.com">andreas.nordland@gmail.com</a>, Klaus Holst.
</p>

<hr>
<h2 id='conditional'>Conditional Policy Evaluation</h2><span id='topic+conditional'></span>

<h3>Description</h3>

<p><code>conditional()</code> is used to calculate the
policy value for each group defined by a given baseline variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conditional(object, policy_data, baseline)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conditional_+3A_object">object</code></td>
<td>
<p>Policy evaluation object created by <code><a href="#topic+policy_eval">policy_eval()</a></code>.</p>
</td></tr>
<tr><td><code id="conditional_+3A_policy_data">policy_data</code></td>
<td>
<p>Policy data object created by <code><a href="#topic+policy_data">policy_data()</a></code>.</p>
</td></tr>
<tr><td><code id="conditional_+3A_baseline">baseline</code></td>
<td>
<p>Character string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of inherited class 'estimate', see <a href="lava.html#topic+estimate.default">lava::estimate.default</a>.
The object is a list with elements 'coef' (policy value estimate for each
group) and 'IC' (influence curve estimate matrix).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
library("data.table")
setDTthreads(1)
d &lt;- sim_single_stage(n=2e3)
pd &lt;- policy_data(d,
                  action = "A",
                  baseline = c("B"),
                  covariates = c("Z","L"),
                  utility = "U")

# static policy:
p &lt;- policy_def(1)

pe &lt;- policy_eval(pd,
                  policy = p)

# conditional value for each group defined by B
conditional(pe, pd, "B")
</code></pre>

<hr>
<h2 id='control_blip'>Control arguments for doubly robust blip-learning</h2><span id='topic+control_blip'></span>

<h3>Description</h3>

<p><code>control_blip</code> sets the default control arguments
for doubly robust blip-learning, <code>type = "blip"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_blip(blip_models = q_glm(~.))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_blip_+3A_blip_models">blip_models</code></td>
<td>
<p>Single element or list of V-restricted blip-models created
by <code><a href="#topic+q_glm">q_glm()</a></code>, <code><a href="#topic+q_rf">q_rf()</a></code>, <code><a href="#topic+q_sl">q_sl()</a></code> or similar functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='control_drql'>Control arguments for doubly robust Q-learning</h2><span id='topic+control_drql'></span>

<h3>Description</h3>

<p><code>control_drql</code> sets the default control arguments
for doubly robust Q-learning, <code>type = "drql"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_drql(qv_models = q_glm(~.))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_drql_+3A_qv_models">qv_models</code></td>
<td>
<p>Single element or list of V-restricted Q-models created
by <code><a href="#topic+q_glm">q_glm()</a></code>, <code><a href="#topic+q_rf">q_rf()</a></code>, <code><a href="#topic+q_sl">q_sl()</a></code> or similar functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='control_earl'>Control arguments for Efficient Augmentation and Relaxation Learning</h2><span id='topic+control_earl'></span>

<h3>Description</h3>

<p><code>control_earl</code> sets the default control arguments
for efficient augmentation and relaxation learning , <code>type = "earl"</code>.
The arguments are passed directly to <code><a href="DynTxRegime.html#topic+earl">DynTxRegime::earl()</a></code> if not
specified otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_earl(
  moPropen,
  moMain,
  moCont,
  regime,
  iter = 0L,
  fSet = NULL,
  lambdas = 0.5,
  cvFolds = 0L,
  surrogate = "hinge",
  kernel = "linear",
  kparam = NULL,
  verbose = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_earl_+3A_mopropen">moPropen</code></td>
<td>
<p>Propensity model of class &quot;ModelObj&quot;, see <a href="modelObj.html#topic+modelObj">modelObj::modelObj</a>.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_momain">moMain</code></td>
<td>
<p>Main effects outcome model of class &quot;ModelObj&quot;.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_mocont">moCont</code></td>
<td>
<p>Contrast outcome model of class &quot;ModelObj&quot;.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_regime">regime</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> specifying the design of the policy/regime.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_iter">iter</code></td>
<td>
<p>Maximum number of iterations for outcome regression.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_fset">fSet</code></td>
<td>
<p>A function or NULL defining subset structure.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_lambdas">lambdas</code></td>
<td>
<p>Numeric or numeric vector. Penalty parameter.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_cvfolds">cvFolds</code></td>
<td>
<p>Integer. Number of folds for cross-validation of the parameters.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_surrogate">surrogate</code></td>
<td>
<p>The surrogate 0-1 loss function. The options are
<code>"logit"</code>, <code>"exp"</code>, <code>"hinge"</code>, <code>"sqhinge"</code>, <code>"huber"</code>.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_kernel">kernel</code></td>
<td>
<p>The options are <code>"linear"</code>, <code>"poly"</code>, <code>"radial"</code>.</p>
</td></tr>
<tr><td><code id="control_earl_+3A_kparam">kparam</code></td>
<td>
<p>Numeric. Kernel parameter</p>
</td></tr>
<tr><td><code id="control_earl_+3A_verbose">verbose</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='control_owl'>Control arguments for Outcome Weighted Learning</h2><span id='topic+control_owl'></span>

<h3>Description</h3>

<p><code>control_owl()</code> sets the default control arguments
for backwards outcome weighted learning, <code>type = "owl"</code>.
The arguments are passed directly to <code><a href="DTRlearn2.html#topic+owl">DTRlearn2::owl()</a></code> if not
specified otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_owl(
  policy_vars = NULL,
  reuse_scales = TRUE,
  res.lasso = TRUE,
  loss = "hinge",
  kernel = "linear",
  augment = FALSE,
  c = 2^(-2:2),
  sigma = c(0.03, 0.05, 0.07),
  s = 2^(-2:2),
  m = 4
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_owl_+3A_policy_vars">policy_vars</code></td>
<td>
<p>Character vector/string or list of character
vectors/strings. Variable names used to restrict the policy.
The names must be a subset of the history names, see get_history_names().
Not passed to <code>owl()</code>.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_reuse_scales">reuse_scales</code></td>
<td>
<p>The history matrix passed to <code>owl()</code> is scaled
using <code><a href="base.html#topic+scale">scale()</a></code> as advised. If <code>TRUE</code>, the scales of the history matrix
will be saved and reused when applied to (new) test data.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_res.lasso">res.lasso</code></td>
<td>
<p>If <code>TRUE</code> a lasso penalty is applied.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_loss">loss</code></td>
<td>
<p>Loss function. The options are <code>"hinge"</code>, <code>"ramp"</code>,
<code>"logit"</code>, <code>"logit.lasso"</code>, <code>"l2"</code>, <code>"l2.lasso"</code>.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_kernel">kernel</code></td>
<td>
<p>Type of kernel used by the support vector machine. The
options are <code>"linear"</code>, <code>"rbf"</code>.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_augment">augment</code></td>
<td>
<p>If <code>TRUE</code> the outcomes are augmented.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_c">c</code></td>
<td>
<p>Regularization parameter.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_sigma">sigma</code></td>
<td>
<p>Tuning parameter.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_s">s</code></td>
<td>
<p>Slope parameter.</p>
</td></tr>
<tr><td><code id="control_owl_+3A_m">m</code></td>
<td>
<p>Number of folds for cross-validation of the parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='control_ptl'>Control arguments for Policy Tree Learning</h2><span id='topic+control_ptl'></span>

<h3>Description</h3>

<p><code>control_ptl</code> sets the default control arguments
for doubly robust policy tree learning, <code>type = "ptl"</code>.
The arguments are passed directly to <code><a href="policytree.html#topic+policy_tree">policytree::policy_tree()</a></code> (or
<code><a href="policytree.html#topic+hybrid_policy_tree">policytree::hybrid_policy_tree()</a></code>) if not specified otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_ptl(
  policy_vars = NULL,
  hybrid = FALSE,
  depth = 2,
  search.depth = 2,
  split.step = 1,
  min.node.size = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_ptl_+3A_policy_vars">policy_vars</code></td>
<td>
<p>Character vector/string or list of character
vectors/strings. Variable names used to construct the V-restricted policy tree.
The names must be a subset of the history names, see get_history_names().
Not passed to <code>policy_tree()</code>.</p>
</td></tr>
<tr><td><code id="control_ptl_+3A_hybrid">hybrid</code></td>
<td>
<p>If <code>TRUE</code>, <code><a href="policytree.html#topic+hybrid_policy_tree">policytree::hybrid_policy_tree()</a></code> is used to
fit a policy tree. Not passed to <code>policy_tree()</code>.</p>
</td></tr>
<tr><td><code id="control_ptl_+3A_depth">depth</code></td>
<td>
<p>Integer or integer vector. The depth of the fitted policy
tree for each stage.</p>
</td></tr>
<tr><td><code id="control_ptl_+3A_search.depth">search.depth</code></td>
<td>
<p>(only used if <code>hybrid = TRUE</code>) Integer or integer
vector. Depth to look ahead when splitting at each stage.</p>
</td></tr>
<tr><td><code id="control_ptl_+3A_split.step">split.step</code></td>
<td>
<p>Integer or integer vector. The number of possible splits
to consider when performing policy tree search at each stage.</p>
</td></tr>
<tr><td><code id="control_ptl_+3A_min.node.size">min.node.size</code></td>
<td>
<p>Integer or integer vector. The smallest terminal node
size permitted at each stage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='control_rwl'>Control arguments for Residual Weighted Learning</h2><span id='topic+control_rwl'></span>

<h3>Description</h3>

<p><code>control_rwl</code> sets the default control arguments
for residual learning , <code>type = "rwl"</code>.
The arguments are passed directly to <code><a href="DynTxRegime.html#topic+rwl">DynTxRegime::rwl()</a></code> if not
specified otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_rwl(
  moPropen,
  moMain,
  regime,
  fSet = NULL,
  lambdas = 2,
  cvFolds = 0L,
  kernel = "linear",
  kparam = NULL,
  responseType = "continuous",
  verbose = 2L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_rwl_+3A_mopropen">moPropen</code></td>
<td>
<p>Propensity model of class &quot;ModelObj&quot;, see <a href="modelObj.html#topic+modelObj">modelObj::modelObj</a>.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_momain">moMain</code></td>
<td>
<p>Main effects outcome model of class &quot;ModelObj&quot;.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_regime">regime</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> specifying the design of the policy/regime.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_fset">fSet</code></td>
<td>
<p>A function or NULL defining subset structure.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_lambdas">lambdas</code></td>
<td>
<p>Numeric or numeric vector. Penalty parameter.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_cvfolds">cvFolds</code></td>
<td>
<p>Integer. Number of folds for cross-validation of the parameters.
<code>"logit"</code>, <code>"exp"</code>, <code>"hinge"</code>, <code>"sqhinge"</code>, <code>"huber"</code>.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_kernel">kernel</code></td>
<td>
<p>The options are <code>"linear"</code>, <code>"poly"</code>, <code>"radial"</code>.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_kparam">kparam</code></td>
<td>
<p>Numeric. Kernel parameter</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_responsetype">responseType</code></td>
<td>
<p>Character string. Options are <code>"continuous"</code>,
<code>"binary"</code>, <code>"count"</code>.</p>
</td></tr>
<tr><td><code id="control_rwl_+3A_verbose">verbose</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of (default) control arguments.
</p>

<hr>
<h2 id='copy_policy_data'>Copy Policy Data Object</h2><span id='topic+copy_policy_data'></span>

<h3>Description</h3>

<p>Objects of class <a href="#topic+policy_data">policy_data</a> contains elements of class <a href="data.table.html#topic+data.table">data.table</a>.
<code>data.table</code> provide functions that operate on objects by reference.
Thus, the <code>policy_data</code> object is not copied when modified by reference,
see examples. An explicit copy can be made by <code>copy_policy_data</code>. The
function is a wrapper of <code><a href="data.table.html#topic+copy">data.table::copy()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>copy_policy_data(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="copy_policy_data_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="#topic+policy_data">policy_data</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage case: Wide data
d1 &lt;- sim_single_stage(5e2, seed=1)
head(d1, 5)
# constructing policy_data object:
pd1 &lt;- policy_data(d1,
                   action="A",
                   covariates=c("Z", "B", "L"),
                   utility="U")
pd1

# True copy
pd2 &lt;- copy_policy_data(pd1)
# manipulating the data.table by reference:
pd2$baseline_data[, id := id + 1]
head(pd2$baseline_data$id - pd1$baseline_data$id)

# False copy
pd2 &lt;- pd1
# manipulating the data.table by reference:
pd2$baseline_data[, id := id + 1]
head(pd2$baseline_data$id - pd1$baseline_data$id)
</code></pre>

<hr>
<h2 id='fit_g_functions'>Fit g-functions</h2><span id='topic+fit_g_functions'></span>

<h3>Description</h3>

<p><code>fit_g_functions</code> is used to fit a list of g-models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_g_functions(policy_data, g_models, full_history = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_g_functions_+3A_policy_data">policy_data</code></td>
<td>
<p>Policy data object created by <code><a href="#topic+policy_data">policy_data()</a></code>.</p>
</td></tr>
<tr><td><code id="fit_g_functions_+3A_g_models">g_models</code></td>
<td>
<p>List of action probability models/g-models for each stage
created by <code><a href="#topic+g_empir">g_empir()</a></code>, <code><a href="#topic+g_glm">g_glm()</a></code>, <code><a href="#topic+g_rf">g_rf()</a></code>, <code><a href="#topic+g_sl">g_sl()</a></code> or similar functions.</p>
</td></tr>
<tr><td><code id="fit_g_functions_+3A_full_history">full_history</code></td>
<td>
<p>If TRUE, the full history is used to fit each g-model.
If FALSE, the single stage/&quot;Markov type&quot; history is used to fit each g-model.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Simulating two-stage policy data
d &lt;- sim_two_stage(2e3, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# fitting a single g-model across all stages:
g_functions &lt;- fit_g_functions(policy_data = pd,
                               g_models = g_glm(),
                               full_history = FALSE)
g_functions

# fitting a g-model for each stage:
g_functions &lt;- fit_g_functions(policy_data = pd,
                               g_models = list(g_glm(), g_glm()),
                               full_history = TRUE)
g_functions
</code></pre>

<hr>
<h2 id='g_model'>g_model class object</h2><span id='topic+g_model'></span><span id='topic+g_empir'></span><span id='topic+g_glm'></span><span id='topic+g_glmnet'></span><span id='topic+g_rf'></span><span id='topic+g_sl'></span><span id='topic+g_xgboost'></span>

<h3>Description</h3>

<p>Use <code>g_glm()</code>, <code>g_empir()</code>,
<code>g_glmnet()</code>, <code>g_rf()</code>, <code>g_sl()</code>, <code>g_xgboost</code> to construct
an action probability model/g-model object.
The constructors are used as input for <code><a href="#topic+policy_eval">policy_eval()</a></code> and <code><a href="#topic+policy_learn">policy_learn()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g_empir(formula = ~1, ...)

g_glm(
  formula = ~.,
  family = "binomial",
  model = FALSE,
  na.action = na.pass,
  ...
)

g_glmnet(formula = ~., family = "binomial", alpha = 1, s = "lambda.min", ...)

g_rf(
  formula = ~.,
  num.trees = c(500),
  mtry = NULL,
  cv_args = list(K = 5, rep = 1),
  ...
)

g_sl(
  formula = ~.,
  SL.library = c("SL.mean", "SL.glm"),
  family = binomial(),
  env = as.environment("package:SuperLearner"),
  onlySL = TRUE,
  ...
)

g_xgboost(
  formula = ~.,
  objective = "binary:logistic",
  params = list(),
  nrounds,
  max_depth = 6,
  eta = 0.3,
  nthread = 1,
  cv_args = list(K = 3, rep = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="g_model_+3A_formula">formula</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> specifying the design matrix for
the propensity model/g-model. Use <code><a href="#topic+get_history_names">get_history_names()</a></code> to view the available
variable names.</p>
</td></tr>
<tr><td><code id="g_model_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="stats.html#topic+glm">glm()</a></code>, <a href="glmnet.html#topic+glmnet">glmnet::glmnet</a>,
<a href="ranger.html#topic+ranger">ranger::ranger</a> or <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a>.</p>
</td></tr>
<tr><td><code id="g_model_+3A_family">family</code></td>
<td>
<p>A description of the error distribution and link function to
be used in the model.</p>
</td></tr>
<tr><td><code id="g_model_+3A_model">model</code></td>
<td>
<p>(Only used by <code>g_glm</code>) If <code>FALSE</code> model frame will
not be saved.</p>
</td></tr>
<tr><td><code id="g_model_+3A_na.action">na.action</code></td>
<td>
<p>(Only used by <code>g_glm</code>) A function which indicates what
should happen when the data contain NAs, see <a href="stats.html#topic+na.pass">na.pass</a>.</p>
</td></tr>
<tr><td><code id="g_model_+3A_alpha">alpha</code></td>
<td>
<p>(Only used by <code>g_glmnet</code>) The elastic net mixing parameter
between 0 and 1. alpha equal to 1 is the lasso penalty, and alpha equal
to 0 the ridge penalty.</p>
</td></tr>
<tr><td><code id="g_model_+3A_s">s</code></td>
<td>
<p>(Only used by <code>g_glmnet</code>) Value(s) of the penalty parameter
lambda at which predictions are required, see <code><a href="glmnet.html#topic+predict.glmnet">glmnet::predict.glmnet()</a></code>.</p>
</td></tr>
<tr><td><code id="g_model_+3A_num.trees">num.trees</code></td>
<td>
<p>(Only used by <code>g_rf</code>) Number of trees.</p>
</td></tr>
<tr><td><code id="g_model_+3A_mtry">mtry</code></td>
<td>
<p>(Only used by <code>g_rf</code>) Number of variables to possibly split
at in each node.</p>
</td></tr>
<tr><td><code id="g_model_+3A_cv_args">cv_args</code></td>
<td>
<p>(Only used by <code>g_rf</code> and <code>g_xgboost</code>) Cross-validation parameters.
Only used if multiple hyper-parameters are given. <code>K</code> is the number
of folds and
<code>rep</code> is the number of replications.</p>
</td></tr>
<tr><td><code id="g_model_+3A_sl.library">SL.library</code></td>
<td>
<p>(Only used by <code>g_sl</code>) Either a character vector of prediction algorithms or
a list containing character vectors, see <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a>.</p>
</td></tr>
<tr><td><code id="g_model_+3A_env">env</code></td>
<td>
<p>(Only used by <code>g_sl</code>) Environment containing the learner functions. Defaults to the calling environment.</p>
</td></tr>
<tr><td><code id="g_model_+3A_onlysl">onlySL</code></td>
<td>
<p>(Only used by <code>g_sl</code>) Logical. If TRUE, only saves and computes predictions
for algorithms with non-zero coefficients in the super learner object.</p>
</td></tr>
<tr><td><code id="g_model_+3A_objective">objective</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) specify the learning
task and the corresponding learning objective, see <a href="xgboost.html#topic+xgb.train">xgboost::xgboost</a>.</p>
</td></tr>
<tr><td><code id="g_model_+3A_params">params</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) list of parameters.</p>
</td></tr>
<tr><td><code id="g_model_+3A_nrounds">nrounds</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="g_model_+3A_max_depth">max_depth</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) maximum depth of a tree.</p>
</td></tr>
<tr><td><code id="g_model_+3A_eta">eta</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) learning rate.</p>
</td></tr>
<tr><td><code id="g_model_+3A_nthread">nthread</code></td>
<td>
<p>(Only used by <code>g_xgboost</code>) number of threads.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>g_glm()</code> is a wrapper of <code><a href="stats.html#topic+glm">glm()</a></code> (generalized linear model).<br />
<code>g_empir()</code> calculates the empirical probabilities within the groups
defined by the formula.<br />
<code>g_glmnet()</code> is a wrapper of <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> (generalized linear model via
penalized maximum likelihood).<br />
<code>g_rf()</code> is a wrapper of <code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code> (random forest).
When multiple hyper-parameters are given, the
model with the lowest cross-validation error is selected.<br />
<code>g_sl()</code> is a wrapper of <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a> (ensemble model).<br />
<code>g_xgboost()</code> is a wrapper of <a href="xgboost.html#topic+xgb.train">xgboost::xgboost</a>.
</p>


<h3>Value</h3>

<p>g-model object: function with arguments 'A'
(action vector), 'H' (history matrix) and 'action_set'.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_history_names">get_history_names()</a></code>, <code><a href="#topic+get_g_functions">get_g_functions()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Two stages:
d &lt;- sim_two_stage(2e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# available state history variable names:
get_history_names(pd)
# defining a g-model:
g_model &lt;- g_glm(formula = ~B+C)

# evaluating the static policy (A=1) using inverse propensity weighting
# based on a state glm model across all stages:
pe &lt;- policy_eval(type = "ipw",
                  policy_data = pd,
                  policy = policy_def(1, reuse = TRUE),
                 g_models = g_model)
# inspecting the fitted g-model:
get_g_functions(pe)

# available full history variable names at each stage:
get_history_names(pd, stage = 1)
get_history_names(pd, stage = 2)

# evaluating the same policy based on a full history
# glm model for each stage:
pe &lt;- policy_eval(type = "ipw",
                   policy_data = pd,
                   policy = policy_def(1, reuse = TRUE),
                   g_models = list(g_glm(~ L_1 + B),
                                   g_glm(~ A_1 + L_2 + B)),
                   g_full_history = TRUE)
# inspecting the fitted g-models:
get_g_functions(pe)
</code></pre>

<hr>
<h2 id='get_action_set'>Get Action Set</h2><span id='topic+get_action_set'></span>

<h3>Description</h3>

<p><code>get_action_set</code> returns the action set, i.e., the possible
actions at each stage for the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_action_set(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_action_set_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the actions set:
get_action_set(pd)
</code></pre>

<hr>
<h2 id='get_actions'>Get Actions</h2><span id='topic+get_actions'></span>

<h3>Description</h3>

<p><code>get_actions</code> returns the actions at every stage for every observation
in the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_actions(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_actions_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with keys id and stage and character variable A.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the actions:
head(get_actions(pd))
</code></pre>

<hr>
<h2 id='get_g_functions'>Get g-functions</h2><span id='topic+get_g_functions'></span>

<h3>Description</h3>

<p><code>get_g_functions()</code> returns a list of (fitted) g-functions
associated with each stage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_g_functions(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_g_functions_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_eval">policy_eval</a> or <a href="#topic+policy_object">policy_object</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of class <a href="#topic+nuisance_functions">nuisance_functions</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+predict.nuisance_functions">predict.nuisance_functions</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# evaluating the static policy a=1 using inverse propensity weighting
# based on a GLM model at each stage
pe &lt;- policy_eval(type = "ipw",
                  policy_data = pd,
                  policy = policy_def(1, reuse = TRUE, name = "A=1"),
                  g_models = list(g_glm(), g_glm()))
pe

# getting the g-functions
g_functions &lt;- get_g_functions(pe)
g_functions

# getting the fitted g-function values
head(predict(g_functions, pd))
</code></pre>

<hr>
<h2 id='get_history_names'>Get history variable names</h2><span id='topic+get_history_names'></span>

<h3>Description</h3>

<p><code>get_history_names()</code> returns the state covariate names of the history data
table for a given stage. The function is useful when specifying
the design matrix for <a href="#topic+g_model">g_model</a> and <a href="#topic+q_model">q_model</a> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_history_names(object, stage)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_history_names_+3A_object">object</code></td>
<td>
<p>Policy data object created by <code><a href="#topic+policy_data">policy_data()</a></code>.</p>
</td></tr>
<tr><td><code id="get_history_names_+3A_stage">stage</code></td>
<td>
<p>Stage number. If NULL, the state/Markov-type history variable
names are returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Multiple stages:
d3 &lt;- sim_multi_stage(5e2, seed = 1)
pd3 &lt;- policy_data(data = d3$stage_data,
                   baseline_data = d3$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")
pd3
# state/Markov type history variable names (H):
get_history_names(pd3)
# full history variable names (H_k) at stage 2:
get_history_names(pd3, stage = 2)
</code></pre>

<hr>
<h2 id='get_id'>Get IDs</h2><span id='topic+get_id'></span>

<h3>Description</h3>

<p><code>get_id</code> returns the ID for every observation in the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_id(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_id_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a> or <a href="#topic+history">history</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the IDs:
head(get_id(pd))
</code></pre>

<hr>
<h2 id='get_id_stage'>Get IDs and Stages</h2><span id='topic+get_id_stage'></span>

<h3>Description</h3>

<p><code>get_id</code> returns the stages for every ID for every observation in the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_id_stage(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_id_stage_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a> or <a href="#topic+history">history</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with keys id and stage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the IDs and stages:
head(get_id_stage(pd))
</code></pre>

<hr>
<h2 id='get_K'>Get Maximal Stages</h2><span id='topic+get_K'></span>

<h3>Description</h3>

<p><code>get_K</code> returns the maximal number of stages for the observations in
the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_K(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_K_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- sim_multi_stage(5e2, seed = 1)
pd &lt;- policy_data(data = d$stage_data,
                   baseline_data = d$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")
pd
# getting the maximal number of stages:
get_K(pd)
</code></pre>

<hr>
<h2 id='get_n'>Get Number of Observations</h2><span id='topic+get_n'></span>

<h3>Description</h3>

<p><code>get_n</code> returns the number of observations in
the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_n(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_n_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the number of observations:
get_n(pd)
</code></pre>

<hr>
<h2 id='get_policy'>Get Policy</h2><span id='topic+get_policy'></span>

<h3>Description</h3>

<p><code>get_policy</code> extracts the policy from a policy object
or a policy evaluation object The policy is a function which take a
policy data object as input and returns the policy actions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_policy(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_policy_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_object">policy_object</a> or <a href="#topic+policy_eval">policy_eval</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>function of class <a href="#topic+policy">policy</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("BB"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### V-restricted (Doubly Robust) Q-learning

# specifying the learner:
pl &lt;- policy_learn(type = "drql",
                   control = control_drql(qv_models = q_glm(formula = ~ C)))

# fitting the policy (object):
po &lt;- pl(policy_data = pd,
         q_models = q_glm(),
         g_models = g_glm())

# getting and applying the policy:
head(get_policy(po)(pd))

# the policy learner can also be evaluated directly:
pe &lt;- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())

# getting and applying the policy again:
head(get_policy(pe)(pd))
</code></pre>

<hr>
<h2 id='get_policy_actions'>Get Policy Actions</h2><span id='topic+get_policy_actions'></span>

<h3>Description</h3>

<p><code>get_policy_actions()</code> extract the actions dictated by the
(learned and possibly cross-fitted) policy a every stage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_policy_actions(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_policy_actions_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_eval">policy_eval</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with keys <code>id</code> and <code>stage</code> and action variable
<code>d</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# defining a policy learner based on cross-fitted doubly robust Q-learning:
pl &lt;- policy_learn(type = "drql",
                   control = control_drql(qv_models = list(q_glm(~C_1), q_glm(~C_1+C_2))),
                   full_history = TRUE,
                   L = 2) # number of folds for cross-fitting

# evaluating the policy learner using 2-fold cross fitting:
pe &lt;- policy_eval(type = "dr",
                   policy_data = pd,
                   policy_learn = pl,
                   q_models = q_glm(),
                   g_models = g_glm(),
                   M = 2) # number of folds for cross-fitting

# Getting the cross-fitted actions dictated by the fitted policy:
head(get_policy_actions(pe))
</code></pre>

<hr>
<h2 id='get_policy_functions.blip'>Get Policy Functions</h2><span id='topic+get_policy_functions.blip'></span><span id='topic+get_policy_functions.drql'></span><span id='topic+get_policy_functions'></span><span id='topic+get_policy_functions.ptl'></span><span id='topic+get_policy_functions.ql'></span>

<h3>Description</h3>

<p><code>get_policy_functions()</code> returns a function defining the policy at
the given stage. <code>get_policy_functions()</code> is useful when implementing
the learned policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blip'
get_policy_functions(object, stage, include_g_values = FALSE, ...)

## S3 method for class 'drql'
get_policy_functions(object, stage, include_g_values = FALSE, ...)

get_policy_functions(object, stage, ...)

## S3 method for class 'ptl'
get_policy_functions(object, stage, ...)

## S3 method for class 'ql'
get_policy_functions(object, stage, include_g_values = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_policy_functions.blip_+3A_object">object</code></td>
<td>
<p>Object of class &quot;policy_object&quot; or &quot;policy_eval&quot;,
see <a href="#topic+policy_learn">policy_learn</a> and <a href="#topic+policy_eval">policy_eval</a>.</p>
</td></tr>
<tr><td><code id="get_policy_functions.blip_+3A_stage">stage</code></td>
<td>
<p>Integer. Stage number.</p>
</td></tr>
<tr><td><code id="get_policy_functions.blip_+3A_include_g_values">include_g_values</code></td>
<td>
<p>If TRUE, the g-values are included as an attribute.</p>
</td></tr>
<tr><td><code id="get_policy_functions.blip_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Functions with arguments:
</p>
<table>
<tr><td><code>H</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> containing the variables needed to evaluate the policy (and g-function).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = "BB",
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### Realistic V-restricted Policy Tree Learning
# specifying the learner:
pl &lt;- policy_learn(type = "ptl",
                   control = control_ptl(policy_vars = list(c("C_1", "BB"),
                                                            c("L_1", "BB"))),
                   full_history = TRUE,
                   alpha = 0.05)

# evaluating the learner:
pe &lt;- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())

# getting the policy function at stage 2:
pf2 &lt;- get_policy_functions(pe, stage = 2)
args(pf2)

# applying the policy function to new data:
set.seed(1)
L_1 &lt;- rnorm(n = 10)
new_H &lt;- data.frame(C = rnorm(n = 10),
                    L = L_1,
                    L_1 = L_1,
                    BB = "group1")
d2 &lt;- pf2(H = new_H)
head(d2)
</code></pre>

<hr>
<h2 id='get_policy_object'>Get Policy Object</h2><span id='topic+get_policy_object'></span>

<h3>Description</h3>

<p>Extract the fitted policy object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_policy_object(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_policy_object_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_eval">policy_eval</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="#topic+policy_object">policy_object</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage:
d1 &lt;- sim_single_stage(5e2, seed=1)
pd1 &lt;- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1


# evaluating the policy:
pe1 &lt;- policy_eval(policy_data = pd1,
                   policy_learn = policy_learn(type = "drql",
                                               control = control_drql(qv_models = q_glm(~.))),
                   g_models = g_glm(),
                   q_models = q_glm())

# extracting the policy object:
get_policy_object(pe1)
</code></pre>

<hr>
<h2 id='get_q_functions'>Get Q-functions</h2><span id='topic+get_q_functions'></span>

<h3>Description</h3>

<p><code>get_q_functions()</code> returns a list of (fitted) Q-functions
associated with each stage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_q_functions(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_q_functions_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_eval">policy_eval</a> or <a href="#topic+policy_object">policy_object</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of class <a href="#topic+nuisance_functions">nuisance_functions</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+predict.nuisance_functions">predict.nuisance_functions</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# evaluating the static policy a=1 using outcome regression
# based on a GLM model at each stage.
pe &lt;- policy_eval(type = "or",
                  policy_data = pd,
                  policy = policy_def(1, reuse = TRUE, name = "A=1"),
                  q_models = list(q_glm(), q_glm()))
pe

# getting the Q-functions
q_functions &lt;- get_q_functions(pe)

# getting the fitted g-function values
head(predict(q_functions, pd))
</code></pre>

<hr>
<h2 id='get_stage_action_sets'>Get Stage Action Sets</h2><span id='topic+get_stage_action_sets'></span>

<h3>Description</h3>

<p><code>get_stage_action_sets</code> returns the action sets at each stage, i.e.,
the possible actions at each stage for the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stage_action_sets(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stage_action_sets_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of character vectors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage_multi_actions(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the stage actions set:
get_stage_action_sets(pd)
</code></pre>

<hr>
<h2 id='get_utility'>Get the Utility</h2><span id='topic+get_utility'></span>

<h3>Description</h3>

<p><code>get_utility()</code> returns the utility, i.e., the sum of the rewards,
for every observation in the policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_utility(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_utility_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with key id and numeric variable U.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# getting the utility:
head(get_utility(pd))
</code></pre>

<hr>
<h2 id='history'>Get History Object</h2><span id='topic+history'></span><span id='topic+get_history'></span>

<h3>Description</h3>

<p><code>get_history</code> summarizes the history and action at a given stage from a
<a href="#topic+policy_data">policy_data</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_history(object, stage = NULL, full_history = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="history_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
<tr><td><code id="history_+3A_stage">stage</code></td>
<td>
<p>Stage number. If NULL, the state/Markov-type history across
all stages is returned.</p>
</td></tr>
<tr><td><code id="history_+3A_full_history">full_history</code></td>
<td>
<p>Logical. If TRUE, the full history is returned
If FALSE, only the state/Markov-type history is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each observation has the sequential form
</p>
<p style="text-align: center;"><code class="reqn">O= {B, U_1, X_1, A_1, ..., U_K, X_K, A_K, U_{K+1}},</code>
</p>

<p>for a possibly stochastic number of stages K.
</p>

<ul>
<li> <p><code class="reqn">B</code> is a vector of baseline covariates.
</p>
</li>
<li> <p><code class="reqn">U_k</code> is the reward at stage k (not influenced by the action <code class="reqn">A_k</code>).
</p>
</li>
<li> <p><code class="reqn">X_k</code> is a vector of state covariates summarizing the state at stage k.
</p>
</li>
<li> <p><code class="reqn">A_k</code> is the categorical action at stage k.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class <a href="#topic+history">history</a>. The object is a list
containing the following elements:
</p>
<table>
<tr><td><code>H</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> with keys id and stage and with variables
{<code class="reqn">B</code>, <code class="reqn">X_k</code>} (state history) or
{<code class="reqn">B</code>, <code class="reqn">X_1</code>, <code class="reqn">A_1</code>, ..., <code class="reqn">X_k</code>}
(full history), see details.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> with keys id and stage and variable <code class="reqn">A_k</code>, see
details.</p>
</td></tr>
<tr><td><code>action_name</code></td>
<td>
<p>Name of the action variable in <code>A</code>.</p>
</td></tr>
<tr><td><code>action_set</code></td>
<td>
<p>Sorted character vector defining the action set.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>(If <code>stage</code> is not NULL) <a href="data.table.html#topic+data.table">data.table</a> with keys id and stage
and with variables U_bar and U_Aa for every a in the actions set.
U_bar is the accumulated rewards up till and including the given
stage, i.e., <code class="reqn">\sum_{j=1}^k U_j</code>. U_Aa is the deterministic
reward of action a.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage:
d1 &lt;- sim_single_stage(5e2, seed=1)
# constructing policy_data object:
pd1 &lt;- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1

# In the single stage case, set stage = NULL
h1 &lt;- get_history(pd1)
head(h1$H)
head(h1$A)

### Two stages:
d2 &lt;- sim_two_stage(5e2, seed=1)
# constructing policy_data object:
pd2 &lt;- policy_data(d2,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2
# getting the state/Markov-type history across all stages:
h2 &lt;- get_history(pd2)
head(h2$H)
head(h2$A)

# getting the full history at stage 2:
h2 &lt;- get_history(pd2, stage = 2, full_history = TRUE)
head(h2$H)
head(h2$A)
head(h2$U)

# getting the state/Markov-type history at stage 2:
h2 &lt;- get_history(pd2, stage = 2, full_history = FALSE)
head(h2$H)
head(h2$A)

### Multiple stages
d3 &lt;- sim_multi_stage(5e2, seed = 1)
# constructing policy_data object:
pd3 &lt;- policy_data(data = d3$stage_data,
                   baseline_data = d3$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")
pd3

# getting the full history at stage 2:
h3 &lt;- get_history(pd3, stage = 2, full_history = TRUE)
head(h3$H)
# note that not all observations have two stages:
nrow(h3$H) # number of observations with two stages.
get_n(pd3) # number of observations in total.
</code></pre>

<hr>
<h2 id='nuisance_functions'>Nuisance Functions</h2><span id='topic+nuisance_functions'></span>

<h3>Description</h3>

<p>The fitted g-functions and Q-functions are stored in an object
of class &quot;nuisance_functions&quot;. The object is a list with a fitted model
object for every stage. Information on whether the full history or the
state/Markov-type history is stored as an attribute (&quot;full_history&quot;).
</p>


<h3>S3 generics</h3>

<p>The following S3 generic functions are available for an object of class
<code>nuisance_functions</code>:
</p>

<ul>
<li><p><code>predict</code> Predict the values of the g- or Q-functions based
on a <a href="#topic+policy_data">policy_data</a> object.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

# evaluating the static policy a=1:
pe &lt;- policy_eval(policy_data = pd,
                  policy = policy_def(1, reuse = TRUE),
                  g_models = g_glm(),
                  q_models = q_glm())

# getting the fitted g-functions:
(g_functions &lt;- get_g_functions(pe))

# getting the fitted Q-functions:
(q_functions &lt;- get_q_functions(pe))

# getting the fitted values:
head(predict(g_functions, pd))
head(predict(q_functions, pd))
</code></pre>

<hr>
<h2 id='partial'>Trim Number of Stages</h2><span id='topic+partial'></span>

<h3>Description</h3>

<p><code>partial</code> creates a partial policy data object by trimming
the maximum number of stages in the policy data object to a fixed
given number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial(object, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
<tr><td><code id="partial_+3A_k">K</code></td>
<td>
<p>Maximum number of stages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="#topic+policy_data">policy_data</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Multiple stage case
d &lt;- sim_multi_stage(5e2, seed = 1)
# constructing policy_data object:
pd &lt;- policy_data(data = d$stage_data,
                   baseline_data = d$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")
pd
# Creating a partial policy data object with 3 stages
pd3 &lt;- partial(pd, K = 3)
pd3
</code></pre>

<hr>
<h2 id='plot.policy_data'>Plot policy data for given policies</h2><span id='topic+plot.policy_data'></span>

<h3>Description</h3>

<p>Plot policy data for given policies
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'policy_data'
plot(
  x,
  policy = NULL,
  which = c(1),
  stage = 1,
  history_variables = NULL,
  jitter = 0.05,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.policy_data_+3A_x">x</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a></p>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_policy">policy</code></td>
<td>
<p>An object or list of objects of class <a href="#topic+policy">policy</a></p>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_which">which</code></td>
<td>
<p>A subset of the numbers 1:2
</p>

<ul>
<li><p>1 Spaghetti plot of the cumulative rewards
</p>
</li>
<li><p>2 Plot of the policy actions for a given stage
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_stage">stage</code></td>
<td>
<p>Stage number for plot 2</p>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_history_variables">history_variables</code></td>
<td>
<p>character vector of length 2 for plot 2</p>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_jitter">jitter</code></td>
<td>
<p>numeric</p>
</td></tr>
<tr><td><code id="plot.policy_data_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
library("data.table")
setDTthreads(1)
d3 &lt;- sim_multi_stage(2e2, seed = 1)
pd3 &lt;- policy_data(data = d3$stage_data,
                   baseline_data = d3$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")

# specifying two static policies:
p0 &lt;- policy_def(c(1,1,0,0), name = "p0")
p1 &lt;- policy_def(c(1,0,0,0), name = "p1")

plot(pd3)
plot(pd3, policy = list(p0, p1))

# learning and plotting a policy:
 pe3 &lt;- policy_eval(pd3,
                    policy_learn = policy_learn(),
                    q_models = q_glm(formula = ~t + X + X_lead))
plot(pd3, list(get_policy(pe3), p0))

# plotting the recommended actions at a specific stage:
plot(pd3, get_policy(pe3),
     which = 2,
     stage = 2,
     history_variables = c("t","X"))
</code></pre>

<hr>
<h2 id='plot.policy_eval'>Plot histogram of the influence curve for a <code>policy_eval</code> object</h2><span id='topic+plot.policy_eval'></span>

<h3>Description</h3>

<p>Plot histogram of the influence curve for a <code>policy_eval</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'policy_eval'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.policy_eval_+3A_x">x</code></td>
<td>
<p>Object of class <a href="#topic+policy_eval">policy_eval</a></p>
</td></tr>
<tr><td><code id="plot.policy_eval_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- sim_two_stage(2e3, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = "BB",
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))

pe &lt;- policy_eval(pd,
                  policy_learn = policy_learn())

plot(pe)
</code></pre>

<hr>
<h2 id='policy'>Policy-class</h2><span id='topic+policy'></span>

<h3>Description</h3>

<p>A function of inherited class &quot;policy&quot; takes a policy
data object as input and returns the policy actions for every observation
for every (observed) stage.
</p>


<h3>Details</h3>

<p>A policy can either be defined directly by the user using
<a href="#topic+policy_def">policy_def</a> or a policy can be fitted using <a href="#topic+policy_learn">policy_learn</a>
(or <a href="#topic+policy_eval">policy_eval</a>). <a href="#topic+policy_learn">policy_learn</a> returns a <a href="#topic+policy_object">policy_object</a> from which
the policy can be extracted using <a href="#topic+get_policy">get_policy</a>.
</p>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with keys <code>id</code> and <code>stage</code> and
action variable <code>d</code>.
</p>


<h3>S3 generics</h3>

<p>The following S3 generic functions are available for an object of class
<code>policy</code>:
</p>

<ul>
<li><p><code>print</code>Baisc print function
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))

# defining a dynamic policy:
p &lt;- policy_def(
  function(L) (L&gt;0)*1,
  reuse = TRUE
)
p
head(p(pd), 5)

# V-restricted (Doubly Robust) Q-learning:
# specifying the learner:
pl &lt;- policy_learn(type = "drql",
                   control = control_drql(qv_models = q_glm(formula = ~ C)))

# fitting the policy (object):
po &lt;- pl(policy_data = pd,
         q_models = q_glm(),
         g_models = g_glm())

p &lt;- get_policy(po)
p

head(p(pd))
</code></pre>

<hr>
<h2 id='policy_data'>Create Policy Data Object</h2><span id='topic+policy_data'></span><span id='topic+print.policy_data'></span><span id='topic+summary.policy_data'></span>

<h3>Description</h3>

<p><code>policy_data()</code> creates a policy data object which
is used as input to <code><a href="#topic+policy_eval">policy_eval()</a></code> and <code><a href="#topic+policy_learn">policy_learn()</a></code> for policy
evaluation and data adaptive policy learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_data(
  data,
  baseline_data,
  type = "wide",
  action,
  covariates,
  utility,
  baseline = NULL,
  deterministic_rewards = NULL,
  id = NULL,
  stage = NULL,
  event = NULL,
  action_set = NULL,
  verbose = FALSE
)

## S3 method for class 'policy_data'
print(x, digits = 2, ...)

## S3 method for class 'policy_data'
summary(object, probs = seq(0, 1, 0.25), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_data_+3A_data">data</code></td>
<td>
<p><a href="base.html#topic+data.frame">data.frame</a> or <a href="data.table.html#topic+data.table">data.table</a>; see Examples.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_baseline_data">baseline_data</code></td>
<td>
<p><a href="base.html#topic+data.frame">data.frame</a> or <a href="data.table.html#topic+data.table">data.table</a>; see Examples.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_type">type</code></td>
<td>
<p>Character string. If &quot;wide&quot;, <code>data</code> is considered to be on wide format.
If &quot;long&quot;, <code>data</code> is considered to be on long format; see Examples.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_action">action</code></td>
<td>
<p>Action variable name(s). Character vector or character string.
</p>

<ul>
<li><p> A vector is valid for wide data. The length of the vector determines the number of stages (K).
</p>
</li>
<li><p> A string is valid for single stage wide data or long data.
</p>
</li></ul>
</td></tr>
<tr><td><code id="policy_data_+3A_covariates">covariates</code></td>
<td>
<p>Stage specific covariate name(s). Character vector or named list of character vectors.
</p>

<ul>
<li><p> A vector is valid for single stage wide data or long data.
</p>
</li>
<li><p> A named list is valid for multiple stages wide data. Each element
must be a character vector with length K. Each vector can contain NA
elements, if a covariate is not available for the given stage(s).
</p>
</li></ul>
</td></tr>
<tr><td><code id="policy_data_+3A_utility">utility</code></td>
<td>
<p>Utility/Reward variable name(s). Character string or vector.
</p>

<ul>
<li><p> A string is valid for long data and wide data with a single final utility.
</p>
</li>
<li><p> A vector is valid for wide data with incremental rewards. Must have length K+1; see Examples.
</p>
</li></ul>
</td></tr>
<tr><td><code id="policy_data_+3A_baseline">baseline</code></td>
<td>
<p>Baseline covariate name(s). Character vector.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_deterministic_rewards">deterministic_rewards</code></td>
<td>
<p>Deterministic reward variable name(s). Named list of character vectors of length K.
The name of each element must be on the form &quot;U_Aa&quot; where &quot;a&quot; corresponds to an action in the action set.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_id">id</code></td>
<td>
<p>ID variable name. Character string.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_stage">stage</code></td>
<td>
<p>Stage number variable name.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_event">event</code></td>
<td>
<p>Event indicator name.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_action_set">action_set</code></td>
<td>
<p>Character string. Action set across all stages.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If TRUE, formatting comments are printed to the console.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_x">x</code></td>
<td>
<p>Object to be printed.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_digits">digits</code></td>
<td>
<p>Minimum number of digits to be printed.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to print.</p>
</td></tr>
<tr><td><code id="policy_data_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a></p>
</td></tr>
<tr><td><code id="policy_data_+3A_probs">probs</code></td>
<td>
<p>numeric vector (probabilities)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each observation has the sequential form
</p>
<p style="text-align: center;"><code class="reqn">O= {B, U_1, X_1, A_1, ..., U_K, X_K, A_K, U_{K+1}},</code>
</p>

<p>for a possibly stochastic number of stages K.
</p>

<ul>
<li> <p><code class="reqn">B</code> is a vector of baseline covariates.
</p>
</li>
<li> <p><code class="reqn">U_k</code> is the reward at stage k (not influenced by the action <code class="reqn">A_k</code>).
</p>
</li>
<li> <p><code class="reqn">X_k</code> is a vector of state covariates summarizing the state at stage k.
</p>
</li>
<li> <p><code class="reqn">A_k</code> is the categorical action at stage k.
</p>
</li></ul>

<p>The utility is given by the sum of the rewards, i.e.,
<code class="reqn">U = \sum_{k = 1}^{K+1} U_k</code>.
</p>


<h3>Value</h3>

<p><code>policy_data()</code> returns an object of class &quot;policy_data&quot;.
The object is a list containing the following elements:
</p>
<table>
<tr><td><code>stage_data</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> containing the id, stage number, event
indicator, action (<code class="reqn">A_k</code>), state covariates
(<code class="reqn">X_k</code>), reward (<code class="reqn">U_k</code>), and the
deterministic rewards.</p>
</td></tr>
<tr><td><code>baseline_data</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> containing the id and baseline
covariates (<code class="reqn">B</code>).</p>
</td></tr>
<tr><td><code>colnames</code></td>
<td>
<p>List containing the state covariate names, baseline
covariate names, and the deterministic reward variable
names.</p>
</td></tr>
<tr><td><code>action_set</code></td>
<td>
<p>Sorted character vector describing the action set, i.e.,
the possible actions at all stages.</p>
</td></tr>
<tr><td><code>stage_action_sets</code></td>
<td>
<p>List of sorted character vectors describing
the observed actions at each stage.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>List containing the number of observations (n) and the
number of stages (K).</p>
</td></tr>
</table>


<h3>S3 generics</h3>

<p>The following S3 generic functions are available for an object of
class <code>policy_data</code>:
</p>

<ul>
<li><p><code><a href="#topic+partial">partial()</a></code> Trim the maximum number
of stages in a <code>policy_data</code> object.
</p>
</li>
<li><p><code><a href="#topic+subset_id">subset_id()</a></code> Subset a a <code>policy_data</code> object on ID.
</p>
</li>
<li><p><code><a href="#topic+get_history">get_history()</a></code> Summarize the history and action at
a given stage.
</p>
</li>
<li><p><code><a href="#topic+get_history_names">get_history_names()</a></code> Get history variable names.
</p>
</li>
<li><p><code><a href="#topic+get_actions">get_actions()</a></code> Get the action at every stage.
</p>
</li>
<li><p><code><a href="#topic+get_utility">get_utility()</a></code>Get the utility.
</p>
</li>
<li><p><code><a href="base.html#topic+plot">plot()</a></code>Plot method.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+policy_eval">policy_eval()</a></code>, <code><a href="#topic+policy_learn">policy_learn()</a></code>, <code><a href="#topic+copy_policy_data">copy_policy_data()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage: Wide data
d1 &lt;- sim_single_stage(n = 5e2, seed=1)
head(d1, 5)
# constructing policy_data object:
pd1 &lt;- policy_data(d1,
                   action="A",
                   covariates=c("Z", "B", "L"),
                   utility="U")
pd1
# associated S3 methods:
methods(class = "policy_data")
head(get_actions(pd1), 5)
head(get_utility(pd1), 5)
head(get_history(pd1)$H, 5)

### Two stage: Wide data
d2 &lt;- sim_two_stage(5e2, seed=1)
head(d2, 5)
# constructing policy_data object:
pd2 &lt;- policy_data(d2,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2
head(get_history(pd2, stage = 2)$H, 5) # state/Markov type history and action, (H_k,A_k).
head(get_history(pd2, stage = 2, full_history = TRUE)$H, 5) # Full history and action, (H_k,A_k).

### Multiple stages: Long data
d3 &lt;- sim_multi_stage(5e2, seed = 1)
head(d3$stage_data, 10)
# constructing policy_data object:
pd3 &lt;- policy_data(data = d3$stage_data,
                   baseline_data = d3$baseline_data,
                   type = "long",
                   id = "id",
                   stage = "stage",
                   event = "event",
                   action = "A",
                   utility = "U")
pd3
head(get_history(pd3, stage = 3)$H, 5) # state/Markov type history and action, (H_k,A_k).
head(get_history(pd3, stage = 2, full_history = TRUE)$H, 5) # Full history and action, (H_k,A_k).
</code></pre>

<hr>
<h2 id='policy_def'>Define Policy</h2><span id='topic+policy_def'></span>

<h3>Description</h3>

<p><code>policy_def</code> returns a function of class <a href="#topic+policy">policy</a>.
The function input is a <a href="#topic+policy_data">policy_data</a> object and it returns a <a href="data.table.html#topic+data.table">data.table</a>
with keys <code>id</code> and <code>stage</code> and action variable <code>d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_def(policy_functions, full_history = FALSE, reuse = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_def_+3A_policy_functions">policy_functions</code></td>
<td>
<p>A single function/character string or a list of
functions/character strings. The list must have the same length as the number
of stages.</p>
</td></tr>
<tr><td><code id="policy_def_+3A_full_history">full_history</code></td>
<td>
<p>If TRUE, the full history at each stage is used as
input to the policy functions.</p>
</td></tr>
<tr><td><code id="policy_def_+3A_reuse">reuse</code></td>
<td>
<p>If TRUE, the policy function is reused at every stage.</p>
</td></tr>
<tr><td><code id="policy_def_+3A_name">name</code></td>
<td>
<p>Character string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function of class <code>"policy"</code>. The function takes a
<a href="#topic+policy_data">policy_data</a> object as input and returns a <a href="data.table.html#topic+data.table">data.table</a>
with keys <code>id</code> and <code>stage</code> and action variable <code>d</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_history_names">get_history_names()</a></code>, <code><a href="#topic+get_history">get_history()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage"
d1 &lt;- sim_single_stage(5e2, seed=1)
pd1 &lt;- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1

# defining a static policy (A=1):
p1_static &lt;- policy_def(1)

# applying the policy:
p1_static(pd1)

# defining a dynamic policy:
p1_dynamic &lt;- policy_def(
  function(Z, L) ((3*Z + 1*L -2.5)&gt;0)*1
)
p1_dynamic(pd1)

### Two stages:
d2 &lt;- sim_two_stage(5e2, seed = 1)
pd2 &lt;- policy_data(d2,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))

# defining a static policy (A=0):
p2_static &lt;- policy_def(0,
                        reuse = TRUE)
p2_static(pd2)

# defining a reused dynamic policy:
p2_dynamic_reuse &lt;- policy_def(
  function(L) (L &gt; 0)*1,
  reuse = TRUE
)
p2_dynamic_reuse(pd2)

# defining a dynamic policy for each stage based on the full history:
# available variable names at each stage:
get_history_names(pd2, stage = 1)
get_history_names(pd2, stage = 2)

p2_dynamic &lt;- policy_def(
  policy_functions = list(
    function(L_1) (L_1 &gt; 0)*1,
    function(L_1, L_2) (L_1 + L_2 &gt; 0)*1
  ),
  full_history = TRUE
)
p2_dynamic(pd2)
</code></pre>

<hr>
<h2 id='policy_eval'>Policy Evaluation</h2><span id='topic+policy_eval'></span><span id='topic+coef.policy_eval'></span><span id='topic+IC.policy_eval'></span><span id='topic+vcov.policy_eval'></span><span id='topic+print.policy_eval'></span><span id='topic+summary.policy_eval'></span><span id='topic+estimate.policy_eval'></span><span id='topic+merge.policy_eval'></span><span id='topic++2B.policy_eval'></span>

<h3>Description</h3>

<p><code>policy_eval()</code> is used to estimate the value of a given fixed policy
or a data adaptive policy (e.g. a policy learned from the data).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_eval(
  policy_data,
  policy = NULL,
  policy_learn = NULL,
  g_functions = NULL,
  g_models = g_glm(),
  g_full_history = FALSE,
  save_g_functions = TRUE,
  q_functions = NULL,
  q_models = q_glm(),
  q_full_history = FALSE,
  save_q_functions = TRUE,
  type = "dr",
  M = 1,
  future_args = list(future.seed = TRUE),
  name = NULL
)

## S3 method for class 'policy_eval'
coef(object, ...)

## S3 method for class 'policy_eval'
IC(x, ...)

## S3 method for class 'policy_eval'
vcov(object, ...)

## S3 method for class 'policy_eval'
print(x, ...)

## S3 method for class 'policy_eval'
summary(object, ...)

## S3 method for class 'policy_eval'
estimate(x, ..., labels = x$name)

## S3 method for class 'policy_eval'
merge(x, y, ..., paired = TRUE)

## S3 method for class 'policy_eval'
x + ...
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_eval_+3A_policy_data">policy_data</code></td>
<td>
<p>Policy data object created by <code><a href="#topic+policy_data">policy_data()</a></code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_policy">policy</code></td>
<td>
<p>Policy object created by <code><a href="#topic+policy_def">policy_def()</a></code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_policy_learn">policy_learn</code></td>
<td>
<p>Policy learner object created by <code><a href="#topic+policy_learn">policy_learn()</a></code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_g_functions">g_functions</code></td>
<td>
<p>Fitted g-model objects, see <a href="#topic+nuisance_functions">nuisance_functions</a>.
Preferably, use <code>g_models</code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_g_models">g_models</code></td>
<td>
<p>List of action probability models/g-models for each stage
created by <code><a href="#topic+g_empir">g_empir()</a></code>, <code><a href="#topic+g_glm">g_glm()</a></code>, <code><a href="#topic+g_rf">g_rf()</a></code>, <code><a href="#topic+g_sl">g_sl()</a></code> or similar functions.
Only used for evaluation if <code>g_functions</code> is <code>NULL</code>.
If a single model is provided and <code>g_full_history</code> is <code>FALSE</code>,
a single g-model is fitted across all stages. If <code>g_full_history</code> is
<code>TRUE</code> the model is reused at every stage.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_g_full_history">g_full_history</code></td>
<td>
<p>If TRUE, the full history is used to fit each g-model.
If FALSE, the state/Markov type history is used to fit each g-model.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_save_g_functions">save_g_functions</code></td>
<td>
<p>If TRUE, the fitted g-functions are saved.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_q_functions">q_functions</code></td>
<td>
<p>Fitted Q-model objects, see <a href="#topic+nuisance_functions">nuisance_functions</a>.
Only valid if the Q-functions are fitted using the same policy.
Preferably, use <code>q_models</code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_q_models">q_models</code></td>
<td>
<p>Outcome regression models/Q-models created by
<code><a href="#topic+q_glm">q_glm()</a></code>, <code><a href="#topic+q_rf">q_rf()</a></code>, <code><a href="#topic+q_sl">q_sl()</a></code> or similar functions.
Only used for evaluation if <code>q_functions</code> is <code>NULL</code>.
If a single model is provided, the model is reused at every stage.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_q_full_history">q_full_history</code></td>
<td>
<p>Similar to g_full_history.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_save_q_functions">save_q_functions</code></td>
<td>
<p>Similar to save_g_functions.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_type">type</code></td>
<td>
<p>Type of evaluation (dr/doubly robust, ipw/inverse propensity
weighting, or/outcome regression).</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_m">M</code></td>
<td>
<p>Number of folds for cross-fitting.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_future_args">future_args</code></td>
<td>
<p>Arguments passed to <code><a href="future.apply.html#topic+future_apply">future.apply::future_apply()</a></code>.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_name">name</code></td>
<td>
<p>Character string.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_object">object</code>, <code id="policy_eval_+3A_x">x</code>, <code id="policy_eval_+3A_y">y</code></td>
<td>
<p>Objects of class &quot;policy_eval&quot;.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_labels">labels</code></td>
<td>
<p>Name(s) of the estimate(s).</p>
</td></tr>
<tr><td><code id="policy_eval_+3A_paired">paired</code></td>
<td>
<p><code>TRUE</code> indicates that the estimates are based on
the same data sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each observation has the sequential form
</p>
<p style="text-align: center;"><code class="reqn">O= {B, U_1, X_1, A_1, ..., U_K, X_K, A_K, U_{K+1}},</code>
</p>

<p>for a possibly stochastic number of stages K.
</p>

<ul>
<li> <p><code class="reqn">B</code> is a vector of baseline covariates.
</p>
</li>
<li> <p><code class="reqn">U_k</code> is the reward at stage k (not influenced by the action <code class="reqn">A_k</code>).
</p>
</li>
<li> <p><code class="reqn">X_k</code> is a vector of state covariates summarizing the state at stage k.
</p>
</li>
<li> <p><code class="reqn">A_k</code> is the categorical action within the action set <code class="reqn">\mathcal{A}</code> at stage k.
</p>
</li></ul>

<p>The utility is given by the sum of the rewards, i.e.,
<code class="reqn">U = \sum_{k = 1}^{K+1} U_k</code>.
</p>
<p>A policy is a set of functions
</p>
<p style="text-align: center;"><code class="reqn">d = \{d_1, ..., d_K\},</code>
</p>

<p>where <code class="reqn">d_k</code> for <code class="reqn">k\in \{1, ..., K\}</code> maps <code class="reqn">\{B, X_1, A_1, ..., A_{k-1}, X_k\}</code> into the
action set.
</p>
<p>Recursively define the Q-models (<code>q_models</code>):
</p>
<p style="text-align: center;"><code class="reqn">Q^d_K(h_K, a_K) = E[U|H_K = h_K, A_K = a_K]</code>
</p>

<p style="text-align: center;"><code class="reqn">Q^d_k(h_k, a_k) = E[Q_{k+1}(H_{k+1}, d_{k+1}(B,X_1, A_1,...,X_{k+1}))|H_k = h_k, A_k = a_k].</code>
</p>

<p>If <code>q_full_history = TRUE</code>,
<code class="reqn">H_k = \{B, X_1, A_1, ..., A_{k-1}, X_k\}</code>, and if
<code>q_full_history = FALSE</code>, <code class="reqn">H_k = \{B, X_k\}</code>.
</p>
<p>The g-models (<code>g_models</code>) are defined as
</p>
<p style="text-align: center;"><code class="reqn">g_k(h_k, a_k) = P(A_k = a_k|H_k = h_k).</code>
</p>

<p>If <code>g_full_history = TRUE</code>,
<code class="reqn">H_k = \{B, X_1, A_1, ..., A_{k-1}, X_k\}</code>, and if
<code>g_full_history = FALSE</code>, <code class="reqn">H_k = \{B, X_k\}</code>.
Furthermore, if <code>g_full_history = FALSE</code> and <code>g_models</code> is a
single model, it is assumed that <code class="reqn">g_1(h_1, a_1) = ... = g_K(h_K, a_K)</code>.
</p>
<p>If <code>type = "or"</code> <code>policy_eval</code> returns the empirical estimates of
the value (<code>value_estimate</code>):
</p>
<p style="text-align: center;"><code class="reqn">E[Q^d_1(H_1, d_1(...))]</code>
</p>

<p>for an appropriate input <code class="reqn">...</code> to the policy.
</p>
<p>If <code>type = "ipw"</code> <code>policy_eval</code> returns the empirical estimates of
the value (<code>value_estimate</code>) and score (<code>IC</code>):
</p>
<p style="text-align: center;"><code class="reqn">E[(\prod_{k=1}^K I\{A_k = d_k(...)\} g_k(H_k, A_k)^{-1}) U].</code>
</p>

<p style="text-align: center;"><code class="reqn">(\prod_{k=1}^K I\{A_k = d_k(...)\} g_k(H_k, A_k)^{-1}) U - E[(\prod_{k=1}^K I\{A_k = d_k(...)\} g_k(H_k, A_k)^{-1}) U].</code>
</p>

<p>If <code>type = "dr"</code> <code>policy_eval</code> returns the empirical estimates of
the value (<code>value_estimate</code>) and influence curve (<code>IC</code>):
</p>
<p style="text-align: center;"><code class="reqn">E[Z^d_1],</code>
</p>

<p style="text-align: center;"><code class="reqn">Z^d_1 - E[Z^d_1],</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">
Z^d_1 = Q^d_1(H_1 , d_1(...)) + \sum_{r = 1}^K \prod_{j = 1}^{r}
\frac{I\{A_j = d_j(...)\}}{g_{j}(H_j, A_j)}
\{Q_{r+1}^d(H_{r+1} , d_{r+1}(...)) - Q_{r}^d(H_r , d_r(...))\}.
</code>
</p>



<h3>Value</h3>

<p><code>policy_eval()</code> returns an object of class &quot;policy_eval&quot;.
The object is a list containing the following elements:
</p>
<table>
<tr><td><code>value_estimate</code></td>
<td>
<p>Numeric. The estimated value of the policy.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Character string. The type of evaluation (&quot;dr&quot;, &quot;ipw&quot;,
&quot;or&quot;).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Numeric vector. Estimated influence curve associated with
the value estimate.</p>
</td></tr>
<tr><td><code>value_estimate_ipw</code></td>
<td>
<p>(only if <code>type = "dr"</code>) Numeric.
The estimated value of the policy based on inverse probability weighting.</p>
</td></tr>
<tr><td><code>value_estimate_or</code></td>
<td>
<p>(only if <code>type = "dr"</code>) Numeric.
The estimated value of the policy based on outcome regression.</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>Character vector. The IDs of the observations.</p>
</td></tr>
<tr><td><code>policy_actions</code></td>
<td>
<p><a href="data.table.html#topic+data.table">data.table</a> with keys id and stage. Actions
associated with the policy for every observation and stage.</p>
</td></tr>
<tr><td><code>policy_object</code></td>
<td>
<p>(only if <code>policy = NULL</code> and <code>M = 1</code>)
The policy object returned by <code>policy_learn</code>, see <a href="#topic+policy_learn">policy_learn</a>.</p>
</td></tr>
<tr><td><code>g_functions</code></td>
<td>
<p>(only if <code>M = 1</code>) The
fitted g-functions. Object of class &quot;nuisance_functions&quot;.</p>
</td></tr>
<tr><td><code>g_values</code></td>
<td>
<p>The fitted g-function values.</p>
</td></tr>
<tr><td><code>q_functions</code></td>
<td>
<p>(only if <code>M = 1</code>) The
fitted Q-functions. Object of class &quot;nuisance_functions&quot;.</p>
</td></tr>
<tr><td><code>q_values</code></td>
<td>
<p>The fitted Q-function values.</p>
</td></tr>
<tr><td><code>cross_fits</code></td>
<td>
<p>(only if <code>M &gt; 1</code>) List containing the
&quot;policy_eval&quot; object for every (validation) fold.</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>
<p>(only if <code>M &gt; 1</code>) The (validation) folds used
for cross-fitting.</p>
</td></tr>
</table>


<h3>S3 generics</h3>

<p>The following S3 generic functions are available for an object of
class <code>policy_eval</code>:
</p>

<ul>
<li><p><code><a href="#topic+get_g_functions">get_g_functions()</a></code> Extract the fitted g-functions.
</p>
</li>
<li><p><code><a href="#topic+get_q_functions">get_q_functions()</a></code> Extract the fitted Q-functions.
</p>
</li>
<li><p><code><a href="#topic+get_policy">get_policy()</a></code> Extract the fitted policy object.
</p>
</li>
<li><p><code><a href="#topic+get_policy_functions">get_policy_functions()</a></code> Extract the fitted policy function for
a given stage.
</p>
</li>
<li><p><code><a href="#topic+get_policy_actions">get_policy_actions()</a></code> Extract the (fitted) policy actions.
</p>
</li>
<li><p><code><a href="#topic+plot.policy_eval">plot.policy_eval()</a></code>Plot diagnostics.
</p>
</li></ul>



<h3>References</h3>

<p>van der Laan, Mark J., and Alexander R. Luedtke. &quot;Targeted learning of the
mean outcome under an optimal dynamic treatment rule.&quot; Journal of causal
inference 3.1 (2015): 61-95. <a href="https://doi.org/10.1515/jci-2013-0022">doi:10.1515/jci-2013-0022</a><br />
<br />
Tsiatis, Anastasios A., et al. Dynamic treatment regimes: Statistical methods
for precision medicine. Chapman and Hall/CRC, 2019. <a href="https://doi.org/10.1201/9780429192692">doi:10.1201/9780429192692</a>.
</p>


<h3>See Also</h3>

<p><a href="lava.html#topic+IC">lava::IC</a>, <a href="lava.html#topic+estimate.default">lava::estimate.default</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage:
d1 &lt;- sim_single_stage(5e2, seed=1)
pd1 &lt;- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1

# defining a static policy (A=1):
pl1 &lt;- policy_def(1)

# evaluating the policy:
pe1 &lt;- policy_eval(policy_data = pd1,
                   policy = pl1,
                   g_models = g_glm(),
                   q_models = q_glm(),
                   name = "A=1 (glm)")

# summarizing the estimated value of the policy:
# (equivalent to summary(pe1)):
pe1
coef(pe1) # value coefficient
sqrt(vcov(pe1)) # value standard error

# getting the g-function and Q-function values:
head(predict(get_g_functions(pe1), pd1))
head(predict(get_q_functions(pe1), pd1))

# getting the fitted influence curve (IC) for the value:
head(IC(pe1))

# evaluating the policy using random forest nuisance models:
set.seed(1)
pe1_rf &lt;- policy_eval(policy_data = pd1,
                      policy = pl1,
                      g_models = g_rf(),
                      q_models = q_rf(),
                      name = "A=1 (rf)")

# merging the two estimates (equivalent to pe1 + pe1_rf):
(est1 &lt;- merge(pe1, pe1_rf))
coef(est1)
head(IC(est1))

### Two stages:
d2 &lt;- sim_two_stage(5e2, seed=1)
pd2 &lt;- policy_data(d2,
                   action = c("A_1", "A_2"),
                   covariates = list(L = c("L_1", "L_2"),
                                     C = c("C_1", "C_2")),
                   utility = c("U_1", "U_2", "U_3"))
pd2

# defining a policy learner based on cross-fitted doubly robust Q-learning:
pl2 &lt;- policy_learn(type = "drql",
                    control = control_drql(qv_models = list(q_glm(~C_1),
                                                            q_glm(~C_1+C_2))),
                    full_history = TRUE,
                    L = 2) # number of folds for cross-fitting

# evaluating the policy learner using 2-fold cross fitting:
pe2 &lt;- policy_eval(type = "dr",
                   policy_data = pd2,
                   policy_learn = pl2,
                   q_models = q_glm(),
                   g_models = g_glm(),
                   M = 2, # number of folds for cross-fitting
                   name = "drql")
# summarizing the estimated value of the policy:
pe2

# getting the cross-fitted policy actions:
head(get_policy_actions(pe2))
</code></pre>

<hr>
<h2 id='policy_learn'>Create Policy Learner</h2><span id='topic+policy_learn'></span><span id='topic+policy_object'></span><span id='topic+print.policy_learn'></span><span id='topic+print.policy_object'></span>

<h3>Description</h3>

<p><code>policy_learn()</code> is used to specify a policy learning method (Q-learning,
doubly robust Q-learning, policy tree
learning and outcome weighted learning). Evaluating the policy learner returns a policy object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_learn(
  type = "ql",
  control = list(),
  alpha = 0,
  full_history = FALSE,
  L = 1,
  cross_fit_g_models = TRUE,
  save_cross_fit_models = FALSE,
  future_args = list(future.seed = TRUE),
  name = type
)

## S3 method for class 'policy_learn'
print(x, ...)

## S3 method for class 'policy_object'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_learn_+3A_type">type</code></td>
<td>
<p>Type of policy learner method:
</p>

<ul>
<li> <p><code>"ql"</code>: Quality/Q-learning.
</p>
</li>
<li> <p><code>"drql"</code>: Doubly Robust Q-learning.
</p>
</li>
<li> <p><code>"blip"</code>: Doubly Robust blip-learning (only for dichotomous actions).
</p>
</li>
<li> <p><code>"ptl"</code>: Policy Tree Learning.
</p>
</li>
<li> <p><code>"owl"</code>: Outcome Weighted Learning.
</p>
</li>
<li> <p><code>"earl"</code>: Efficient Augmentation and Relaxation Learning (only single stage).
</p>
</li>
<li> <p><code>"rwl"</code>: Residual Weighted Learning (only single stage).
</p>
</li></ul>
</td></tr>
<tr><td><code id="policy_learn_+3A_control">control</code></td>
<td>
<p>List of control arguments. Values (and default values) are set using
<code>control_{type}()</code>. Key arguments include:<br />
<code><a href="#topic+control_drql">control_drql()</a></code>:<br />
</p>

<ul>
<li> <p><code>qv_models</code>: Single element or list of V-restricted Q-models created
by <code><a href="#topic+q_glm">q_glm()</a></code>, <code><a href="#topic+q_rf">q_rf()</a></code>, <code><a href="#topic+q_sl">q_sl()</a></code> or similar functions.
</p>
</li></ul>

<p><code><a href="#topic+control_blip">control_blip()</a></code>:<br />
</p>

<ul>
<li> <p><code>blip_models</code>: Single element or list of V-restricted blip-models created
by <code><a href="#topic+q_glm">q_glm()</a></code>, <code><a href="#topic+q_rf">q_rf()</a></code>, <code><a href="#topic+q_sl">q_sl()</a></code> or similar functions.
</p>
</li></ul>

<p><code><a href="#topic+control_ptl">control_ptl()</a></code>: <br />
</p>

<ul>
<li> <p><code>policy_vars</code>: Character vector/string or list of character
vectors/strings. Variable names used to construct the V-restricted policy tree.
The names must be a subset of the history names, see get_history_names().
</p>
</li>
<li> <p><code>hybrid</code>: If <code>TRUE</code>, <code><a href="policytree.html#topic+hybrid_policy_tree">policytree::hybrid_policy_tree()</a></code> is used to
fit a policy tree.
</p>
</li>
<li> <p><code>depth</code>: Integer or integer vector. The depth of the fitted policy
tree for each stage.
</p>
</li></ul>

<p><code><a href="#topic+control_owl">control_owl()</a></code>: <br />
</p>

<ul>
<li> <p><code>policy_vars</code>: As in <code>control_ptl()</code>.
</p>
</li>
<li> <p><code>loss</code>: Loss function. The options are <code>"hinge"</code>, <code>"ramp"</code>,
<code>"logit"</code>, <code>"logit.lasso"</code>, <code>"l2"</code>, <code>"l2.lasso"</code>.
</p>
</li>
<li> <p><code>kernel</code>: Type of kernel used by the support vector machine. The
options are <code>"linear"</code>, <code>"rbf"</code>.
</p>
</li>
<li> <p><code>augment</code>:  If <code>TRUE</code> the outcomes are augmented.
</p>
</li></ul>

<p><code><a href="#topic+control_earl">control_earl()</a></code>/<code><a href="#topic+control_rwl">control_rwl()</a></code>: <br />
</p>

<ul>
<li> <p><code>moPropen</code>: Propensity model of class &quot;ModelObj&quot;, see <a href="modelObj.html#topic+modelObj">modelObj::modelObj</a>.
</p>
</li>
<li> <p><code>moMain</code>: Main effects outcome model of class &quot;ModelObj&quot;.
</p>
</li>
<li> <p><code>moCont</code> Contrast outcome model of class &quot;ModelObj&quot;.
</p>
</li>
<li> <p><code>regime</code>: An object of class <a href="stats.html#topic+formula">formula</a> specifying the design of the policy.
</p>
</li>
<li> <p><code>surrogate</code>: The surrogate 0-1 loss function. The options are
<code>"logit"</code>, <code>"exp"</code>, <code>"hinge"</code>, <code>"sqhinge"</code>, <code>"huber"</code>.
</p>
</li>
<li> <p><code>kernel</code>: The options are <code>"linear"</code>, <code>"poly"</code>, <code>"radial"</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="policy_learn_+3A_alpha">alpha</code></td>
<td>
<p>Probability threshold for determining realistic actions.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_full_history">full_history</code></td>
<td>
<p>If <code>TRUE</code>, the full
history is used to fit each policy function (e.g. QV-model, policy tree). If FALSE, the single stage/
&quot;Markov type&quot; history is used to fit each policy function.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_l">L</code></td>
<td>
<p>Number of folds for cross-fitting nuisance models.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_cross_fit_g_models">cross_fit_g_models</code></td>
<td>
<p>If <code>TRUE</code>, the g-models will not be
cross-fitted even if L &gt; 1.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_save_cross_fit_models">save_cross_fit_models</code></td>
<td>
<p>If <code>TRUE</code>, the cross-fitted models will be saved.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_future_args">future_args</code></td>
<td>
<p>Arguments passed to <code><a href="future.apply.html#topic+future_apply">future.apply::future_apply()</a></code>.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_name">name</code></td>
<td>
<p>Character string.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_x">x</code></td>
<td>
<p>Object of class &quot;policy_object&quot; or &quot;policy_learn&quot;.</p>
</td></tr>
<tr><td><code id="policy_learn_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to print.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function of inherited class <code>"policy_learn"</code>.
Evaluating the function on a <a href="#topic+policy_data">policy_data</a> object returns an object of
class <a href="#topic+policy_object">policy_object</a>. A policy object is a list containing all or
some of the following elements:
</p>
<table>
<tr><td><code>q_functions</code></td>
<td>
<p>Fitted Q-functions. Object of class &quot;nuisance_functions&quot;.</p>
</td></tr>
<tr><td><code>g_functions</code></td>
<td>
<p>Fitted g-functions. Object of class &quot;nuisance_functions&quot;.</p>
</td></tr>
<tr><td><code>action_set</code></td>
<td>
<p>Sorted character vector describing the action set, i.e.,
the possible actions at each stage.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Numeric. Probability threshold to determine realistic actions.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Integer. Maximal number of stages.</p>
</td></tr>
<tr><td><code>qv_functions</code></td>
<td>
<p>(only if <code>type = "drql"</code>) Fitted V-restricted
Q-functions. Contains a fitted model for each stage and action.</p>
</td></tr>
<tr><td><code>ptl_objects</code></td>
<td>
<p>(only if <code>type = "ptl"</code>) Fitted V-restricted
policy trees. Contains a <a href="policytree.html#topic+policy_tree">policy_tree</a> for each stage.</p>
</td></tr>
<tr><td><code>ptl_designs</code></td>
<td>
<p>(only if <code>type = "ptl"</code>) Specification of the
V-restricted design matrix for each stage</p>
</td></tr>
</table>


<h3>S3 generics</h3>

<p>The following S3 generic functions are available for an object of
class &quot;policy_object&quot;:
</p>

<ul>
<li><p><code><a href="#topic+get_g_functions">get_g_functions()</a></code> Extract the fitted g-functions.
</p>
</li>
<li><p><code><a href="#topic+get_q_functions">get_q_functions()</a></code> Extract the fitted Q-functions.
</p>
</li>
<li><p><code><a href="#topic+get_policy">get_policy()</a></code> Extract the fitted policy object.
</p>
</li>
<li><p><code><a href="#topic+get_policy_functions">get_policy_functions()</a></code> Extract the fitted policy function for
a given stage.
</p>
</li>
<li><p><code><a href="#topic+get_policy_actions">get_policy_actions()</a></code> Extract the (fitted) policy actions.
</p>
</li></ul>



<h3>References</h3>

<p>Doubly Robust Q-learning (<code>type = "drql"</code>): Luedtke, Alexander R., and
Mark J. van der Laan. &quot;Super-learning of an optimal dynamic treatment rule.&quot;
The international journal of biostatistics 12.1 (2016): 305-332.
<a href="https://doi.org/10.1515/ijb-2015-0052">doi:10.1515/ijb-2015-0052</a>.<br />
<br />
Policy Tree Learning (<code>type = "ptl"</code>): Zhou, Zhengyuan, Susan Athey,
and Stefan Wager. &quot;Offline multi-action policy learning: Generalization and
optimization.&quot; Operations Research (2022). <a href="https://doi.org/10.1287/opre.2022.2271">doi:10.1287/opre.2022.2271</a>.<br />
<br />
(Augmented) Outcome Weighted Learning: Liu, Ying, et al. &quot;Augmented
outcomeweighted learning for estimating optimal dynamic treatment regimens.&quot;
Statistics in medicine 37.26 (2018): 3776-3788. <a href="https://doi.org/10.1002/sim.7844">doi:10.1002/sim.7844</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+policy_eval">policy_eval()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Two stages:
d &lt;- sim_two_stage(5e2, seed=1)
pd &lt;- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("BB"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### V-restricted (Doubly Robust) Q-learning

# specifying the learner:
pl &lt;- policy_learn(
  type = "drql",
  control = control_drql(qv_models = list(q_glm(formula = ~ C_1 + BB),
                                          q_glm(formula = ~ L_1 + BB))),
  full_history = TRUE
)

# evaluating the learned policy
pe &lt;- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())
pe
# getting the policy object:
po &lt;- get_policy_object(pe)
# inspecting the fitted QV-model for each action strata at stage 1:
po$qv_functions$stage_1
head(get_policy(pe)(pd))
</code></pre>

<hr>
<h2 id='predict.nuisance_functions'>Predict g-functions and Q-functions</h2><span id='topic+predict.nuisance_functions'></span>

<h3>Description</h3>

<p><code>predict()</code> returns the fitted values of the g-functions and
Q-functions when applied to a (new) policy data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nuisance_functions'
predict(object, new_policy_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nuisance_functions_+3A_object">object</code></td>
<td>
<p>Object of class &quot;nuisance_functions&quot;. Either <code>g_functions</code>
or <code>q_functions</code> as returned by <code><a href="#topic+policy_eval">policy_eval()</a></code> or <code><a href="#topic+policy_learn">policy_learn()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.nuisance_functions_+3A_new_policy_data">new_policy_data</code></td>
<td>
<p>Policy data object created by <code><a href="#topic+policy_data">policy_data()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.nuisance_functions_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with keys <code>id</code> and <code>stage</code> and variables <code>g_a</code> or <code>Q_a</code> for
each action a in the actions set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage:
d &lt;- sim_single_stage(5e2, seed=1)
pd &lt;- policy_data(d, action="A", covariates=list("Z", "B", "L"), utility="U")
pd
# defining a static policy (A=1):
pl &lt;- policy_def(1, name = "A=1")

# doubly robust evaluation of the policy:
pe &lt;- policy_eval(policy_data = pd,
                  policy = pl,
                  g_models = g_glm(),
                  q_models = q_glm())
# summarizing the estimated value of the policy:
pe

# getting the fitted g-function values:
head(predict(get_g_functions(pe), pd))

# getting the fitted Q-function values:
head(predict(get_q_functions(pe), pd))
</code></pre>

<hr>
<h2 id='q_model'>q_model class object</h2><span id='topic+q_model'></span><span id='topic+q_glm'></span><span id='topic+q_glmnet'></span><span id='topic+q_rf'></span><span id='topic+q_sl'></span><span id='topic+q_xgboost'></span>

<h3>Description</h3>

<p>Use <code>q_glm()</code>, <code>q_glmnet()</code>, <code>q_rf()</code>, and <code>q_sl()</code> to construct
an outcome regression model/Q-model object.
The constructors are used as input for <code><a href="#topic+policy_eval">policy_eval()</a></code> and <code><a href="#topic+policy_learn">policy_learn()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q_glm(
  formula = ~A * .,
  family = gaussian(),
  model = FALSE,
  na.action = na.pass,
  ...
)

q_glmnet(
  formula = ~A * .,
  family = "gaussian",
  alpha = 1,
  s = "lambda.min",
  ...
)

q_rf(
  formula = ~.,
  num.trees = c(250, 500, 750),
  mtry = NULL,
  cv_args = list(K = 3, rep = 1),
  ...
)

q_sl(
  formula = ~.,
  SL.library = c("SL.mean", "SL.glm"),
  env = as.environment("package:SuperLearner"),
  onlySL = TRUE,
  discreteSL = FALSE,
  ...
)

q_xgboost(
  formula = ~.,
  objective = "reg:squarederror",
  params = list(),
  nrounds,
  max_depth = 6,
  eta = 0.3,
  nthread = 1,
  cv_args = list(K = 3, rep = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="q_model_+3A_formula">formula</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> specifying the design matrix for
the outcome regression model/Q-model at the given stage. The action at the
given stage is always denoted 'A', see examples. Use
<code><a href="#topic+get_history_names">get_history_names()</a></code> to see the additional
available variable names.</p>
</td></tr>
<tr><td><code id="q_model_+3A_family">family</code></td>
<td>
<p>A description of the error distribution and link function to
be used in the model.</p>
</td></tr>
<tr><td><code id="q_model_+3A_model">model</code></td>
<td>
<p>(Only used by <code>q_glm</code>) If <code>FALSE</code> model frame will
not be saved.</p>
</td></tr>
<tr><td><code id="q_model_+3A_na.action">na.action</code></td>
<td>
<p>(Only used by <code>q_glm</code>) A function which indicates what
should happen when the data contain NAs, see <a href="stats.html#topic+na.pass">na.pass</a>.</p>
</td></tr>
<tr><td><code id="q_model_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="stats.html#topic+glm">glm()</a></code>, <a href="glmnet.html#topic+glmnet">glmnet::glmnet</a>,
<a href="ranger.html#topic+ranger">ranger::ranger</a> or <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a>.</p>
</td></tr>
<tr><td><code id="q_model_+3A_alpha">alpha</code></td>
<td>
<p>(Only used by <code>q_glmnet</code>) The elasticnet mixing parameter
between 0 and 1. alpha equal to 1 is the lasso penalty, and alpha equal
to 0 the ridge penalty.</p>
</td></tr>
<tr><td><code id="q_model_+3A_s">s</code></td>
<td>
<p>(Only used by <code>q_glmnet</code>) Value(s) of the penalty parameter
lambda at which predictions are required, see <code><a href="glmnet.html#topic+predict.glmnet">glmnet::predict.glmnet()</a></code>.</p>
</td></tr>
<tr><td><code id="q_model_+3A_num.trees">num.trees</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Number of trees.</p>
</td></tr>
<tr><td><code id="q_model_+3A_mtry">mtry</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Number of variables to possibly split
at in each node.</p>
</td></tr>
<tr><td><code id="q_model_+3A_cv_args">cv_args</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Cross-validation parameters.
Only used if multiple hyper-parameters are given. <code>K</code> is the number
of folds and
<code>rep</code> is the number of replications.</p>
</td></tr>
<tr><td><code id="q_model_+3A_sl.library">SL.library</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Either a character vector of
prediction algorithms or a list containing character vectors,
see <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a>.</p>
</td></tr>
<tr><td><code id="q_model_+3A_env">env</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Environment containing the learner
functions. Defaults to the calling environment.</p>
</td></tr>
<tr><td><code id="q_model_+3A_onlysl">onlySL</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Logical. If TRUE, only saves and computes predictions
for algorithms with non-zero coefficients in the super learner object.</p>
</td></tr>
<tr><td><code id="q_model_+3A_discretesl">discreteSL</code></td>
<td>
<p>(Only used by <code>q_sl</code>) If TRUE, select the model with
the lowest cross-validated risk.</p>
</td></tr>
<tr><td><code id="q_model_+3A_objective">objective</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) specify the learning
task and the corresponding learning objective, see <a href="xgboost.html#topic+xgb.train">xgboost::xgboost</a>.</p>
</td></tr>
<tr><td><code id="q_model_+3A_params">params</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) list of parameters.</p>
</td></tr>
<tr><td><code id="q_model_+3A_nrounds">nrounds</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="q_model_+3A_max_depth">max_depth</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) maximum depth of a tree.</p>
</td></tr>
<tr><td><code id="q_model_+3A_eta">eta</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) learning rate.</p>
</td></tr>
<tr><td><code id="q_model_+3A_nthread">nthread</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) number of threads.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>q_glm()</code> is a wrapper of <code><a href="stats.html#topic+glm">glm()</a></code> (generalized linear model).<br />
<code>q_glmnet()</code> is a wrapper of <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> (generalized linear model via
penalized maximum likelihood).<br />
<code>q_rf()</code> is a wrapper of <code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code> (random forest).
When multiple hyper-parameters are given, the
model with the lowest cross-validation error is selected.<br />
<code>q_sl()</code> is a wrapper of <a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner</a> (ensemble model).
<code>q_xgboost()</code> is a wrapper of <a href="xgboost.html#topic+xgb.train">xgboost::xgboost</a>.
</p>


<h3>Value</h3>

<p>q_model object: function with arguments 'AH'
(combined action and history matrix) and 'V_res' (residual value/expected
utility).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_history_names">get_history_names()</a></code>, <code><a href="#topic+get_q_functions">get_q_functions()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage case
d1 &lt;- sim_single_stage(5e2, seed=1)
pd1 &lt;- policy_data(d1,
                   action="A",
                   covariates=list("Z", "B", "L"),
                   utility="U")
pd1

# available history variable names for the outcome regression:
get_history_names(pd1)

# evaluating the static policy a=1 using inverse
# propensity weighting based on the given Q-model:
pe1 &lt;- policy_eval(type = "or",
                   policy_data = pd1,
                   policy = policy_def(1, name = "A=1"),
                   q_model = q_glm(formula = ~A*.))
pe1

# getting the fitted Q-function values
head(predict(get_q_functions(pe1), pd1))

### Two stages:
d2 &lt;- sim_two_stage(5e2, seed=1)
pd2 &lt;- policy_data(d2,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2

# available full history variable names at each stage:
get_history_names(pd2, stage = 1)
get_history_names(pd2, stage = 2)

# evaluating the static policy a=1 using outcome
# regression based on a glm model for each stage:
pe2 &lt;- policy_eval(type = "or",
            policy_data = pd2,
            policy = policy_def(1, reuse = TRUE, name = "A=1"),
            q_model = list(q_glm(~ A * L_1),
                           q_glm(~ A * (L_1 + L_2))),
            q_full_history = TRUE)
pe2

# getting the fitted Q-function values
head(predict(get_q_functions(pe2), pd2))
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+IC'></span><span id='topic+estimate'></span><span id='topic+All'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>lava</dt><dd><p><code><a href="lava.html#topic+estimate.default">estimate</a></code>, <code><a href="lava.html#topic+IC">IC</a></code></p>
</dd>
<dt>SuperLearner</dt><dd><p><code><a href="SuperLearner.html#topic+write.screen.template">All</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sim_multi_stage'>Simulate Multi-Stage Data</h2><span id='topic+sim_multi_stage'></span>

<h3>Description</h3>

<p>Simulate Multi-Stage Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_multi_stage(
  n,
  par = list(tau = 10, gamma = c(0, -0.2, 0.3), alpha = c(0, 0.5, 0.2, -0.5, 0.4), beta =
    c(3, -0.5, -0.5), psi = 1, xi = 0.3),
  a = function(t, x, beta, ...) {
     prob &lt;- lava::expit(beta[1] + (beta[2] * t^2) +
    (beta[3] * x))
     stats::rbinom(n = 1, size = 1, prob = prob)
 },
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_multi_stage_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="sim_multi_stage_+3A_par">par</code></td>
<td>
<p>Named list with distributional parameters.
</p>

<ul>
<li> <p><code>tau</code>: <code class="reqn">\tau</code>
</p>
</li>
<li> <p><code>gamma</code>: <code class="reqn">\gamma</code>
</p>
</li>
<li> <p><code>alpha</code>: <code class="reqn">\alpha</code>
</p>
</li>
<li> <p><code>beta</code>: <code class="reqn">\beta</code>
</p>
</li>
<li> <p><code>psi</code>: <code class="reqn">\psi</code>
</p>
</li>
<li> <p><code>xi</code>: <code class="reqn">\xi</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="sim_multi_stage_+3A_a">a</code></td>
<td>
<p>Function used to specify the action/treatment at every stage.</p>
</td></tr>
<tr><td><code id="sim_multi_stage_+3A_seed">seed</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sim_multi_stage</code> samples <code>n</code> iid observation
<code class="reqn">O</code> with the following distribution:
</p>
<p style="text-align: center;"><code class="reqn">
W \sim \mathcal{N}(0, 1)\\
B \sim Ber(\xi)
</code>
</p>

<p>For <code class="reqn">k\geq 1</code> let
</p>
<p style="text-align: center;"><code class="reqn">
(T_k - T_{k-1})| X_{k-1}, A_{k-1}, W \sim
\begin{cases}
Exp\Big\{\exp\left(\gamma^T [1, X_{k-1}, W] \right) \Big\} + \psi \quad A_{k-1} = 1\\
\infty \quad A_{k-1} = 0
\end{cases}\\
X_{k}\mid T_k, X_{k-1}, B \sim
\begin{cases}
\mathcal{N}\left\{ \alpha^T [1, T_k, T^2_k, X_{k-1}, B], 1\right\} \quad T_k &lt; \infty \\
0 \quad T_k = \infty
\end{cases}\\
A_k \mid X_k, T_k \sim
\begin{cases}
Ber\left\{ expit\left(\beta^T[1, T_{k}^2, X_k] \right)\right\} \quad T_k &lt; \infty\\
0 \quad T_k = \infty,
\end{cases}
</code>
</p>

<p>Note that <code class="reqn">\psi</code> is the minimum increment.
</p>


<h3>Value</h3>

<p>list with elements <code>stage_data</code> (<a href="data.table.html#topic+data.table">data.table</a>) and
<code>baseline_data</code> (<a href="data.table.html#topic+data.table">data.table</a>).
</p>

<hr>
<h2 id='sim_single_stage'>Simulate Single-Stage Data</h2><span id='topic+sim_single_stage'></span>

<h3>Description</h3>

<p>Simulate Single-Stage Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_single_stage(
  n = 10000,
  par = c(k = 0.1, d = 0.5, a = 1, b = -2.5, c = 3, p = 0.3),
  action_model = function(Z, L, B, k, d) {
     k * (Z + L - 1) * Z^(-2) + d * (B == 1)

    },
  utility_model = function(Z, L, A, a, b, c) {
     Z + L + A * (c * Z + a * L + b)
 },
  seed = NULL,
  return_model = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_single_stage_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_par">par</code></td>
<td>
<p>Named vector with distributional parameters.
</p>

<ul>
<li> <p><code>k</code>: <code class="reqn">\kappa</code>
</p>
</li>
<li> <p><code>d</code>: <code class="reqn">\delta</code>
</p>
</li>
<li> <p><code>a</code>: <code class="reqn">\alpha</code>
</p>
</li>
<li> <p><code>b</code>: <code class="reqn">\beta</code>
</p>
</li>
<li> <p><code>c</code>: <code class="reqn">\gamma</code>
</p>
</li>
<li> <p><code>p</code>: <code class="reqn">\pi</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_action_model">action_model</code></td>
<td>
<p>Function used to specify the action/treatment probability (logit link).</p>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_utility_model">utility_model</code></td>
<td>
<p>Function used to specify the conditional mean utility.</p>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_seed">seed</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_return_model">return_model</code></td>
<td>
<p>If TRUE, the <a href="lava.html#topic+lvm">lava::lvm</a> model is returned.</p>
</td></tr>
<tr><td><code id="sim_single_stage_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="lava.html#topic+lvm">lava::lvm()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sim_single_stage</code> samples <code>n</code> iid observation
<code class="reqn">O = (B, Z, L, A, U)</code> with the following distribution:
</p>
<p style="text-align: center;"><code class="reqn">
B \sim Bernoulli(\pi)\\
Z, L \sim Uniform([0,1])\\
A\mid Z,L,B \sim Bernoulli(expit\{\kappa Z^{-2}(Z+L-1) + \delta B)\\
U\mid Z,L,A \sim \mathcal{N}(Z+L+A\cdot\{\gamma Z + \alpha L + \beta\}, 1)
</code>
</p>



<h3>Value</h3>

<p>data.frame with n rows and columns Z, L, B, A, and U.
</p>

<hr>
<h2 id='sim_single_stage_multi_actions'>Simulate Single-Stage Multi-Action Data</h2><span id='topic+sim_single_stage_multi_actions'></span>

<h3>Description</h3>

<p>Simulate Single-Stage Multi-Action Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_single_stage_multi_actions(n = 1000, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_single_stage_multi_actions_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="sim_single_stage_multi_actions_+3A_seed">seed</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sim_single_stage_multi_actions</code> samples <code>n</code> iid observation
<code class="reqn">O = (z, x, a, u)</code> with the following distribution:
</p>
<p style="text-align: center;"><code class="reqn">
z, x \sim Uniform([0,1])\\
\tilde a \sim \mathcal{N}(0,1)\\
a \mid \tilde a \sim
\begin{cases}
0 \quad if \quad \tilde a &lt; -1\\
1 \quad if \quad \tilde a -1 \leq a &lt; 0.5\\
2 \quad otherwise
\end{cases}\\
u \mid z, x \sim \mathcal{N}(x + z + I\{a=2\}(x-0.5) + I\{a=1\}(x^2 + z -0.5), 1)
</code>
</p>



<h3>Value</h3>

<p>data.frame with n rows and columns z, x, a, and u.
</p>

<hr>
<h2 id='sim_two_stage'>Simulate Two-Stage Data</h2><span id='topic+sim_two_stage'></span>

<h3>Description</h3>

<p>Simulate Two-Stage Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_two_stage(
  n = 10000,
  par = c(gamma = 0.5, beta = 1),
  seed = NULL,
  action_model_1 = function(C_1, beta, ...) stats::rbinom(n = NROW(C_1), size = 1, prob =
    lava::expit(beta * C_1)),
  action_model_2 = function(C_2, beta, ...) stats::rbinom(n = NROW(C_1), size = 1, prob =
    lava::expit(beta * C_2)),
  deterministic_rewards = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_two_stage_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="sim_two_stage_+3A_par">par</code></td>
<td>
<p>Named vector with distributional parameters.
</p>

<ul>
<li> <p><code>gamma</code>: <code class="reqn">\gamma</code>
</p>
</li>
<li> <p><code>beta</code>: <code class="reqn">\beta</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="sim_two_stage_+3A_seed">seed</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="sim_two_stage_+3A_action_model_1">action_model_1</code></td>
<td>
<p>Function used to specify the action/treatment at stage 1.</p>
</td></tr>
<tr><td><code id="sim_two_stage_+3A_action_model_2">action_model_2</code></td>
<td>
<p>Function used to specify the action/treatment at stage 2.</p>
</td></tr>
<tr><td><code id="sim_two_stage_+3A_deterministic_rewards">deterministic_rewards</code></td>
<td>
<p>Logical. If TRUE, the deterministic reward
contributions are returned as well (columns U_1_A0, U_1_A1, U_2_A0, U_2_A1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sim_two_stage</code> samples <code>n</code> iid observation
<code class="reqn">O</code> with the following distribution:
<code class="reqn">BB</code> is a random categorical variable with levels <code>group1</code>,
<code>group2</code>, and <code>group3</code>. Furthermore,
</p>
<p style="text-align: center;"><code class="reqn">
B \sim \mathcal{N}(0,1)\\
L_{1} \sim \mathcal{N}(0, 1)\\
C_{1} \mid L_{1} \sim \mathcal{N}(L_1, 1)\\
A_1 \mid C_1 \sim Bernoulli(expit(\beta C_1))\\
L_{2} \sim \mathcal{N} (0, 1)\\
C_{2} \mid A_1, L_1 \sim \mathcal{N}(\gamma L_1 + A_1, 1)\\
A_2 \mid C_2 \sim Bernoulli(expit(\beta C_2))\\
L_{3} \sim \mathcal{N} (0, 1)
</code>
</p>

<p>The rewards are calculated as
</p>
<p style="text-align: center;"><code class="reqn">
U_1 = L_1\\
U_2 = A_1\cdot C_1 + L_2 \\
U_3 = A_2\cdot C_2 + L_3.
</code>
</p>



<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with n rows and columns B, BB, L_1, C_1, A_1, L_2, C_2,
A_2, L_3, U_1, U_2, U_3 (,U_1_A0, U_1_A1, U_2_A0, U_2_A1).
</p>

<hr>
<h2 id='sim_two_stage_multi_actions'>Simulate Two-Stage Multi-Action Data</h2><span id='topic+sim_two_stage_multi_actions'></span>

<h3>Description</h3>

<p>Simulate Two-Stage Multi-Action Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_two_stage_multi_actions(
  n = 1000,
  par = list(gamma = 0.5, beta = 1, prob = c(0.2, 0.4, 0.4)),
  seed = NULL,
  action_model_1 = function(C_1, beta, ...) stats::rbinom(n = NROW(C_1), size = 1, prob =
    lava::expit(beta * C_1))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_two_stage_multi_actions_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="sim_two_stage_multi_actions_+3A_par">par</code></td>
<td>
<p>Named vector with distributional parameters.
</p>

<ul>
<li> <p><code>gamma</code>: <code class="reqn">\gamma</code>
</p>
</li>
<li> <p><code>beta</code>: <code class="reqn">\beta</code>
</p>
</li>
<li> <p><code>prob</code>: <code class="reqn">p</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="sim_two_stage_multi_actions_+3A_seed">seed</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="sim_two_stage_multi_actions_+3A_action_model_1">action_model_1</code></td>
<td>
<p>Function used to specify the dichotomous
action/treatment at stage 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sim_two_stage_multi_actions</code> samples <code>n</code> iid observation
<code class="reqn">O</code> with the following distribution:
<code class="reqn">BB</code> is a random categorical variable with levels <code>group1</code>,
<code>group2</code>, and <code>group3</code>. Furthermore,
</p>
<p style="text-align: center;"><code class="reqn">
B \sim \mathcal{N}(0,1)\\
L_{1} \sim \mathcal{N}(0, 1)\\
C_{1} \mid L_{1} \sim \mathcal{N}(L_1, 1)\\
P(A_1='yes'\mid C_1) =  expit(\beta C_1)\\
P(A_1='no'\mid C_1) = 1 - P(A_1='yes' \mid C_1)\\
L_{2} \sim \mathcal{N} (0, 1)\\
C_{2} \mid A_1, L_1 \sim \mathcal{N}(\gamma L_1 + A_1, 1)\\
P(A_2='yes') = p_1\\
P(A_2='no') = p_2\\
P(A_2='default') = p_3\\
L_{3} \sim \mathcal{N} (0, 1)
</code>
</p>

<p>The rewards are calculated as
</p>
<p style="text-align: center;"><code class="reqn">
U_1 = L_1\\
U_2 = A_1\cdot C_1 + L_2 \\
U_3 = A_2\cdot C_2 + L_3.
</code>
</p>



<h3>Value</h3>

<p><a href="data.table.html#topic+data.table">data.table</a> with n rows and columns B, BB, L_1, C_1, A_1, L_2, C_2,
A_2, L_3, U_1, U_2, U_3.
</p>

<hr>
<h2 id='subset_id'>Subset Policy Data on ID</h2><span id='topic+subset_id'></span>

<h3>Description</h3>

<p><code>subset_id</code> returns a policy data object containing the given IDs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subset_id(object, id, preserve_action_set = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset_id_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+policy_data">policy_data</a>.</p>
</td></tr>
<tr><td><code id="subset_id_+3A_id">id</code></td>
<td>
<p>character vectors of IDs.</p>
</td></tr>
<tr><td><code id="subset_id_+3A_preserve_action_set">preserve_action_set</code></td>
<td>
<p>If TRUE, the action sets must be preserved.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="#topic+policy_data">policy_data</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("polle")
### Single stage:
d &lt;- sim_single_stage(5e2, seed=1)
# constructing policy_data object:
pd &lt;- policy_data(d, action="A", covariates=list("Z", "B", "L"), utility="U")
pd

# getting the observation IDs:
get_id(pd)[1:10]

# subsetting on IDs:
pdsub &lt;- subset_id(pd, id = 250:500)
pdsub
get_id(pdsub)[1:10]
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
