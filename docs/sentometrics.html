<!DOCTYPE html><html><head><title>Help for package sentometrics</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sentometrics}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_features'><p>Add feature columns to a (sento_)corpus object</p></a></li>
<li><a href='#aggregate.sentiment'><p>Aggregate textual sentiment across sentences, documents and time</p></a></li>
<li><a href='#aggregate.sento_measures'><p>Aggregate sentiment measures</p></a></li>
<li><a href='#as.data.table.sento_measures'><p>Get the sentiment measures</p></a></li>
<li><a href='#as.sentiment'><p>Convert a sentiment table to a sentiment object</p></a></li>
<li><a href='#as.sento_corpus'><p>Convert a quanteda or tm corpus object into a sento_corpus object</p></a></li>
<li><a href='#attributions'><p>Retrieve top-down model sentiment attributions</p></a></li>
<li><a href='#compute_sentiment'><p>Compute textual sentiment across features and lexicons</p></a></li>
<li><a href='#corpus_summarize'><p>Summarize the sento_corpus object</p></a></li>
<li><a href='#ctr_agg'><p>Set up control for aggregation into sentiment measures</p></a></li>
<li><a href='#ctr_model'><p>Set up control for sentiment-based sparse regression modeling</p></a></li>
<li><a href='#data-defunct'><p>Datasets with defunct names</p></a></li>
<li><a href='#diff.sento_measures'><p>Differencing of sentiment measures</p></a></li>
<li><a href='#epu'><p>Monthly U.S. Economic Policy Uncertainty index</p></a></li>
<li><a href='#get_dates'><p>Get the dates of the sentiment measures/time series</p></a></li>
<li><a href='#get_dimensions'><p>Get the dimensions of the sentiment measures</p></a></li>
<li><a href='#get_hows'><p>Options supported to perform aggregation into sentiment measures</p></a></li>
<li><a href='#get_loss_data'><p>Retrieve loss data from a selection of models</p></a></li>
<li><a href='#list_lexicons'><p>Built-in lexicons</p></a></li>
<li><a href='#list_valence_shifters'><p>Built-in valence word lists</p></a></li>
<li><a href='#measures_fill'><p>Add and fill missing dates to sentiment measures</p></a></li>
<li><a href='#measures_update'><p>Update sentiment measures</p></a></li>
<li><a href='#merge.sentiment'><p>Merge sentiment objects horizontally and/or vertically</p></a></li>
<li><a href='#nmeasures'><p>Get number of sentiment measures</p></a></li>
<li><a href='#nobs.sento_measures'><p>Get number of observations in the sentiment measures</p></a></li>
<li><a href='#peakdates'><p>Extract dates related to sentiment time series peaks</p></a></li>
<li><a href='#peakdocs'><p>Extract documents related to sentiment peaks</p></a></li>
<li><a href='#plot.attributions'><p>Plot prediction attributions at specified level</p></a></li>
<li><a href='#plot.sento_measures'><p>Plot sentiment measures</p></a></li>
<li><a href='#plot.sento_modelIter'><p>Plot iterative predictions versus realized values</p></a></li>
<li><a href='#predict.sento_model'><p>Make predictions from a sento_model object</p></a></li>
<li><a href='#scale.sento_measures'><p>Scaling and centering of sentiment measures</p></a></li>
<li><a href='#sento_corpus'><p>Create a sento_corpus object</p></a></li>
<li><a href='#sento_lexicons'><p>Set up lexicons (and valence word list) for use in sentiment analysis</p></a></li>
<li><a href='#sento_measures'><p>One-way road towards a sento_measures object</p></a></li>
<li><a href='#sento_model'><p>Optimized and automated sentiment-based sparse regression</p></a></li>
<li><a href='#sentometrics-defunct'><p>Defunct functions</p></a></li>
<li><a href='#sentometrics-deprecated'><p>Deprecated functions</p></a></li>
<li><a href='#sentometrics-package'><p>sentometrics: An Integrated Framework for Textual Sentiment Time Series Aggregation and Prediction</p></a></li>
<li><a href='#subset.sento_measures'><p>Subset sentiment measures</p></a></li>
<li><a href='#usnews'><p>Texts (not) relevant to the U.S. economy</p></a></li>
<li><a href='#weights_almon'><p>Compute Almon polynomials</p></a></li>
<li><a href='#weights_beta'><p>Compute Beta weighting curves</p></a></li>
<li><a href='#weights_exponential'><p>Compute exponential weighting curves</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>An Integrated Framework for Textual Sentiment Time Series
Aggregation and Prediction</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Samuel Borms &lt;borms_sam@hotmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Optimized prediction based on textual sentiment, accounting for the intrinsic challenge that sentiment can be computed and pooled across texts and time in various ways. See Ardia et al. (2021) &lt;<a href="https://doi.org/10.18637%2Fjss.v099.i02">doi:10.18637/jss.v099.i02</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/SentometricsResearch/sentometrics/issues">https://github.com/SentometricsResearch/sentometrics/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://sentometrics-research.com/sentometrics/">https://sentometrics-research.com/sentometrics/</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, doParallel, e1071, lexicon, MCS, NLP, parallel,
randomForest, stopwords, testthat, tm</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, compiler, data.table, foreach, ggplot2, glmnet,
ISOweek, quanteda, Rcpp (&ge; 0.12.13), RcppRoll, RcppParallel,
stats, stringi, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppParallel</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-17 16:45:09 UTC; saborms</td>
</tr>
<tr>
<td>Author:</td>
<td>Samuel Borms <a href="https://orcid.org/0000-0001-9533-1870"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  David Ardia <a href="https://orcid.org/0000-0003-2823-782X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Keven Bluteau <a href="https://orcid.org/0000-0003-2990-4807"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Kris Boudt <a href="https://orcid.org/0000-0002-1000-5142"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jeroen Van Pelt [ctb],
  Andres Algaba [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-18 07:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_features'>Add feature columns to a (sento_)corpus object</h2><span id='topic+add_features'></span>

<h3>Description</h3>

<p>Adds new feature columns, either user-supplied or based on keyword(s)/regex pattern search, to
a provided <code>sento_corpus</code> or a <span class="pkg">quanteda</span> <code><a href="quanteda.html#topic+corpus">corpus</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_features(
  corpus,
  featuresdf = NULL,
  keywords = NULL,
  do.binary = TRUE,
  do.regex = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_features_+3A_corpus">corpus</code></td>
<td>
<p>a <code>sento_corpus</code> object created with <code><a href="#topic+sento_corpus">sento_corpus</a></code>, or a <span class="pkg">quanteda</span>
<code><a href="quanteda.html#topic+corpus">corpus</a></code> object.</p>
</td></tr>
<tr><td><code id="add_features_+3A_featuresdf">featuresdf</code></td>
<td>
<p>a named <code>data.frame</code> of type <code>numeric</code> where each columns is a new feature to be added to the
inputted <code>corpus</code> object. If the number of rows in <code>featuresdf</code> is not equal to the number of documents
in <code>corpus</code>, recycling will occur. The numeric values should be between 0 and 1 (included).</p>
</td></tr>
<tr><td><code id="add_features_+3A_keywords">keywords</code></td>
<td>
<p>a named <code>list</code>. For every element, a new feature column is added with a value of 1 for the texts
in which (at least one of) the keyword(s) appear(s), and 0 if not (for <code>do.binary = TRUE</code>), or with as value the
normalized number of times the keyword(s) occur(s) in the text (for <code>do.binary = FALSE</code>). If no texts match a
keyword, no column is added. The <code>list</code> names are used as the names of the new features. For more complex searching,
instead of just keywords, one can also directly use a single regex expression to define a new feature (see the details section).</p>
</td></tr>
<tr><td><code id="add_features_+3A_do.binary">do.binary</code></td>
<td>
<p>a <code>logical</code>, if <code>do.binary = FALSE</code>, the number of occurrences are normalized
between 0 and 1 (see argument <code>keywords</code>).</p>
</td></tr>
<tr><td><code id="add_features_+3A_do.regex">do.regex</code></td>
<td>
<p>a <code>logical</code> vector equal in length to the number of elements in the <code>keywords</code> argument
<code>list</code>, or a single value if it applies to all. It should be set to <code>TRUE</code> at those positions where a single
regex expression is used to identify the particular feature.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a provided feature name is already part of the corpus, it will be replaced. The <code>featuresdf</code> and
<code>keywords</code> arguments can be provided at the same time, or only one of them, leaving the other at <code>NULL</code>. We use
the <span class="pkg">stringi</span> package for searching the keywords. The <code>do.regex</code> argument points to the corresponding elements
in <code>keywords</code>. For <code>FALSE</code>, we transform the keywords into a simple regex expression, involving <code>"\b"</code> for
exact word boundary matching and (if multiple keywords) <code>|</code> as OR operator. The elements associated to <code>TRUE</code> do
not undergo this transformation, and are evaluated as given, if the corresponding keywords vector consists of only one
expression. For a large corpus and/or complex regex patterns, this function may require some patience. Scaling between 0
and 1 is performed via min-max normalization, per column.
</p>


<h3>Value</h3>

<p>An updated <code>corpus</code> object.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

# construct a corpus and add (a) feature(s) to it
corpus &lt;- quanteda::corpus_sample(
  sento_corpus(corpusdf = sentometrics::usnews), 500
)
corpus1 &lt;- add_features(corpus,
                        featuresdf = data.frame(random = runif(quanteda::ndoc(corpus))))
corpus2 &lt;- add_features(corpus,
                        keywords = list(pres = "president", war = "war"),
                        do.binary = FALSE)
corpus3 &lt;- add_features(corpus,
                        keywords = list(pres = c("Obama", "US president")))
corpus4 &lt;- add_features(corpus,
                        featuresdf = data.frame(all = 1),
                        keywords = list(pres1 = "Obama|US [p|P]resident",
                                        pres2 = "\\bObama\\b|\\bUS president\\b",
                                        war = "war"),
                        do.regex = c(TRUE, TRUE, FALSE))

sum(quanteda::docvars(corpus3, "pres")) ==
  sum(quanteda::docvars(corpus4, "pres2")) # TRUE

# adding a complementary feature
nonpres &lt;- data.frame(nonpres = as.numeric(!quanteda::docvars(corpus3, "pres")))
corpus3 &lt;- add_features(corpus3, featuresdf = nonpres)

</code></pre>

<hr>
<h2 id='aggregate.sentiment'>Aggregate textual sentiment across sentences, documents and time</h2><span id='topic+aggregate.sentiment'></span>

<h3>Description</h3>

<p>Aggregates textual sentiment scores at sentence- or document-level into a panel of textual
sentiment measures. Can also be used to aggregate sentence-level sentiment scores into
document-level sentiment scores. This function is called within the <code><a href="#topic+sento_measures">sento_measures</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sentiment'
aggregate(x, ctr, do.full = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate.sentiment_+3A_x">x</code></td>
<td>
<p>a <code>sentiment</code> object created using <code><a href="#topic+compute_sentiment">compute_sentiment</a></code> (from a <code>sento_corpus</code>
object) or using <code><a href="#topic+as.sentiment">as.sentiment</a></code>.</p>
</td></tr>
<tr><td><code id="aggregate.sentiment_+3A_ctr">ctr</code></td>
<td>
<p>output from a <code><a href="#topic+ctr_agg">ctr_agg</a></code> call. The <code>howWithin</code> and <code>nCore</code> elements are ignored.</p>
</td></tr>
<tr><td><code id="aggregate.sentiment_+3A_do.full">do.full</code></td>
<td>
<p>if <code>do.full = TRUE</code> (by default), does entire aggregation up to a <code>sento_measures</code>
object, else only goes from sentence-level to document-level. Ignored if no <code>"sentence_id"</code> column in
<code>sentiment</code> input object.</p>
</td></tr>
<tr><td><code id="aggregate.sentiment_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A document-level <code>sentiment</code> object or a fully aggregated <code>sento_measures</code> object.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute_sentiment">compute_sentiment</a></code>, <code><a href="#topic+ctr_agg">ctr_agg</a></code>, <code><a href="#topic+sento_measures">sento_measures</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# computation of sentiment
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l1 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                     list_valence_shifters[["en"]])
l2 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                     list_valence_shifters[["en"]][, c("x", "t")])
sent1 &lt;- compute_sentiment(corpusSample, l1, how = "counts")
sent2 &lt;- compute_sentiment(corpusSample, l2, do.sentence = TRUE)
sent3 &lt;- compute_sentiment(as.character(corpusSample), l2,
                           do.sentence = TRUE)
ctr &lt;- ctr_agg(howTime = c("linear"), by = "year", lag = 3)

# aggregate into sentiment measures
sm1 &lt;- aggregate(sent1, ctr)
sm2 &lt;- aggregate(sent2, ctr)

# two-step aggregation (first into document-level sentiment)
sd2 &lt;- aggregate(sent2, ctr, do.full = FALSE)
sm3 &lt;- aggregate(sd2, ctr)

# aggregation of a sentiment data.table
cols &lt;- c("word_count", names(l2)[-length(l2)])
sd3 &lt;- sent3[, lapply(.SD, sum), by = "id", .SDcols = cols]

</code></pre>

<hr>
<h2 id='aggregate.sento_measures'>Aggregate sentiment measures</h2><span id='topic+aggregate.sento_measures'></span>

<h3>Description</h3>

<p>Aggregates sentiment measures by combining across provided lexicons, features, and time weighting
schemes dimensions. For <code>do.global = FALSE</code>, the combination occurs by taking the mean of the relevant
measures. For <code>do.global = TRUE</code>, this function aggregates all sentiment measures into a weighted global textual
sentiment measure for each of the dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
aggregate(
  x,
  features = NULL,
  lexicons = NULL,
  time = NULL,
  do.global = FALSE,
  do.keep = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_features">features</code></td>
<td>
<p>a <code>list</code> with unique features to aggregate at given name, e.g., <br />
<code>list(feat12 = c("feat1", "feat2"))</code>. See <code>x$features</code> for the exact names to use. Use <code>NULL</code>
(default) to apply no merging across this dimension. If <code>do.global = TRUE</code>, should be a <code>numeric</code> vector of
weights, of size <code>length(x$features)</code>, in the same order. A value of <code>NULL</code> means equally weighted.</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_lexicons">lexicons</code></td>
<td>
<p>a <code>list</code> with unique lexicons to aggregate at given name, e.g., <br />
<code>list(lex12 = c("lex1", "lex2"))</code>. See <code>x$lexicons</code> for the exact names to use. Use <code>NULL</code>
(default) to apply no merging across this dimension. If <code>do.global = TRUE</code>, should be a <code>numeric</code> vector of
weights, of size <code>length(x$lexicons)</code>, in the same order. A value of <code>NULL</code> means equally weighted.</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_time">time</code></td>
<td>
<p>a <code>list</code> with unique time weighting schemes to aggregate at given name, e.g., <br />
<code>list(tw12 = c("tw1", "tw2"))</code>. See <code>x$time</code> for the exact names to use. Use <code>NULL</code> (default)
to apply no merging across this dimension. If <code>do.global = TRUE</code>, should be a <code>numeric</code> vector of
weights, of size <code>length(x$time)</code>, in the same order. A value of <code>NULL</code> means equally weighted.</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_do.global">do.global</code></td>
<td>
<p>a <code>logical</code> indicating if the sentiment measures should be aggregated into weighted
global sentiment indices.</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_do.keep">do.keep</code></td>
<td>
<p>a <code>logical</code> indicating if the original sentiment measures should be kept (i.e., the aggregated
sentiment measures will be added to the current sentiment measures as additional indices if <code>do.keep = TRUE</code>).</p>
</td></tr>
<tr><td><code id="aggregate.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>do.global = TRUE</code>, the measures are constructed from weights that indicate the importance (and sign)
along each component from the <code>lexicons</code>, <code>features</code>, and <code>time</code> dimensions. There is no restriction in
terms of allowed weights. For example, the global index based on the supplied lexicon weights (<code>"globLex"</code>) is obtained
first by multiplying every sentiment measure with its corresponding weight (meaning, the weight given to the lexicon the
sentiment is computed with), then by taking the average per date.
</p>


<h3>Value</h3>

<p>If <code>do.global = FALSE</code>, a modified <code>sento_measures</code> object, with the aggregated sentiment
measures, including updated information and statistics, but the original sentiment scores <code>data.table</code>
untouched.
</p>
<p>If <code>do.global = TRUE</code>, a <code>data.table</code> with the different types of weighted global sentiment measures,
named <code>"globLex"</code>, <code>"globFeat"</code>, <code>"globTime"</code> and <code>"global"</code>, with <code>"date"</code> as the first
column. The last measure is an average of the the three other measures.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                    list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"),
               by = "year", lag = 3)
sento_measures &lt;- sento_measures(corpusSample, l, ctr)

# aggregation across specified components
smAgg &lt;- aggregate(sento_measures,
                   time = list(W = c("equal_weight", "linear")),
                   features = list(journals = c("wsj", "wapo")),
                   do.keep = TRUE)

# aggregation in full
dims &lt;- get_dimensions(sento_measures)
smFull &lt;- aggregate(sento_measures,
                    lexicons = list(L = dims[["lexicons"]]),
                    time = list(T = dims[["time"]]),
                    features = list(F = dims[["features"]]))

# "global" aggregation
smGlobal &lt;- aggregate(sento_measures, do.global = TRUE,
                      lexicons = c(0.3, 0.1),
                      features = c(1, -0.5, 0.3, 1.2),
                      time = NULL)

## Not run: 
# aggregation won't work, but produces informative error message
aggregate(sento_measures,
          time = list(W = c("equal_weight", "almon1")),
          lexicons = list(LEX = c("LM_en")),
          features = list(journals = c("notInHere", "wapo")))
## End(Not run)

</code></pre>

<hr>
<h2 id='as.data.table.sento_measures'>Get the sentiment measures</h2><span id='topic+as.data.table.sento_measures'></span>

<h3>Description</h3>

<p>Extracts the sentiment measures <code>data.table</code> in either wide (by default)
or long format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
as.data.table(x, keep.rownames = FALSE, format = "wide", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.data.table.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="as.data.table.sento_measures_+3A_keep.rownames">keep.rownames</code></td>
<td>
<p>see <code><a href="data.table.html#topic+as.data.table">as.data.table</a></code>.</p>
</td></tr>
<tr><td><code id="as.data.table.sento_measures_+3A_format">format</code></td>
<td>
<p>a single <code>character</code> vector, one of <code>c("wide", "long")</code>.</p>
</td></tr>
<tr><td><code id="as.data.table.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The panel of sentiment measures under <code>sento_measures[["measures"]]</code>,
in wide or long format.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

sm &lt;- sento_measures(sento_corpus(corpusdf = usnews[1:200, ]),
                     sento_lexicons(list_lexicons["LM_en"]),
                     ctr_agg(lag = 3))

data.table::as.data.table(sm)
data.table::as.data.table(sm, format = "long")

</code></pre>

<hr>
<h2 id='as.sentiment'>Convert a sentiment table to a sentiment object</h2><span id='topic+as.sentiment'></span>

<h3>Description</h3>

<p>Converts a properly structured sentiment table into a <code>sentiment</code> object, that can be used
for further aggregation with the <code><a href="#topic+aggregate.sentiment">aggregate.sentiment</a></code> function. This allows to start from
sentiment scores not necessarily computed with <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.sentiment(s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.sentiment_+3A_s">s</code></td>
<td>
<p>a <code>data.table</code> or <code>data.frame</code> that can be converted into a <code>sentiment</code> object. It
should have at least an <code>"id"</code>, a <code>"date"</code>, a <code>"word_count"</code> and one sentiment scores column.
If other column names are provided with a separating <code>"--"</code>, the first part is considered the lexicon
(or more generally, the sentiment computation method), and the second part the feature. For sentiment column
names without any <code>"--"</code>, a <code>"dummyFeature"</code> component is added.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>sentiment</code> object.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")

ids &lt;- paste0("id", 1:200)
dates &lt;- sample(seq(as.Date("2015-01-01"), as.Date("2018-01-01"), by = "day"), 200, TRUE)
word_count &lt;- sample(150:850, 200, replace = TRUE)
sent &lt;- matrix(rnorm(200 * 8), nrow =  200)
s1 &lt;- s2 &lt;- data.table::data.table(id = ids, date = dates, word_count = word_count, sent)
s3 &lt;- data.frame(id = ids, date = dates, word_count = word_count, sent,
                 stringsAsFactors = FALSE)
s4 &lt;- compute_sentiment(usnews$texts[201:400],
                        sento_lexicons(list_lexicons["GI_en"]),
                        "counts", do.sentence = TRUE)
m &lt;- "method"

colnames(s1)[-c(1:3)] &lt;- paste0(m, 1:8)
sent1 &lt;- as.sentiment(s1)

colnames(s2)[-c(1:3)] &lt;- c(paste0(m, 1:4, "--", "feat1"), paste0(m, 1:4, "--", "feat2"))
sent2 &lt;- as.sentiment(s2)

colnames(s3)[-c(1:3)] &lt;- c(paste0(m, 1:3, "--", "feat1"), paste0(m, 1:3, "--", "feat2"),
                           paste0(m, 4:5))
sent3 &lt;- as.sentiment(s3)

s4[, "date" := rep(dates, s4[, max(sentence_id), by = id][[2]])]
sent4 &lt;- as.sentiment(s4)

# further aggregation from then on is easy...
sentMeas1 &lt;- aggregate(sent1, ctr_agg(lag = 10))
sent5 &lt;- aggregate(sent4, ctr_agg(howDocs = "proportional"), do.full = FALSE)

</code></pre>

<hr>
<h2 id='as.sento_corpus'>Convert a quanteda or tm corpus object into a sento_corpus object</h2><span id='topic+as.sento_corpus'></span>

<h3>Description</h3>

<p>Converts most common <span class="pkg">quanteda</span> and <span class="pkg">tm</span> corpus objects into a
<code>sento_corpus</code> object. Appropriate available metadata is integrated as features;
for a <span class="pkg">quanteda</span> corpus, this can come from <code>docvars(x)</code>, for a <span class="pkg">tm</span> corpus,
only <code>meta(x, type = "indexed")</code> metadata is considered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.sento_corpus(x, dates = NULL, do.clean = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.sento_corpus_+3A_x">x</code></td>
<td>
<p>a <span class="pkg">quanteda</span> <code><a href="quanteda.html#topic+corpus">corpus</a></code> object, a <span class="pkg">tm</span>
<code><a href="tm.html#topic+SimpleCorpus">SimpleCorpus</a></code> or a <span class="pkg">tm</span> <code><a href="tm.html#topic+VCorpus">VCorpus</a></code> object. For <span class="pkg">tm</span>
corpora, every corpus element should consist of a single <code>"content"</code> <code>character</code> vector
as the document unit.</p>
</td></tr>
<tr><td><code id="as.sento_corpus_+3A_dates">dates</code></td>
<td>
<p>an optional sequence of dates as <code>"yyyy-mm-dd"</code>, of the same length as the number
of documents in the input corpus, to define the <code>"date"</code> column. If <code>dates = NULL</code>, the
<code>"date"</code> metadata element in the input corpus, if available, will be used but should be in the
same <code>"yyyy-mm-dd"</code> format.</p>
</td></tr>
<tr><td><code id="as.sento_corpus_+3A_do.clean">do.clean</code></td>
<td>
<p>see <code><a href="#topic+sento_corpus">sento_corpus</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>sento_corpus</code> object, as returned by the <code><a href="#topic+sento_corpus">sento_corpus</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>See Also</h3>

<p><code><a href="quanteda.html#topic+corpus">corpus</a></code>, <code><a href="tm.html#topic+SimpleCorpus">SimpleCorpus</a></code>, <code><a href="tm.html#topic+VCorpus">VCorpus</a></code>,
<code><a href="#topic+sento_corpus">sento_corpus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
txt &lt;- system.file("texts", "txt", package = "tm")
reuters &lt;- system.file("texts", "crude", package = "tm")

# reshuffle usnews data.frame for use in quanteda and tm
dates &lt;- usnews$date
usnews$wrong &lt;- "notNumeric"
colnames(usnews)[c(1, 3)] &lt;- c("doc_id", "text")

# conversion from a quanteda corpus
qcorp &lt;- quanteda::corpus(usnews,
                          text_field = "text", docid_field = "doc_id")
corp1 &lt;- as.sento_corpus(qcorp)
corp2 &lt;- as.sento_corpus(qcorp, sample(dates)) # overwrites "date" column

# conversion from a tm SimpleCorpus corpus (DataframeSource)
tmSCdf &lt;- tm::SimpleCorpus(tm::DataframeSource(usnews))
corp3 &lt;- as.sento_corpus(tmSCdf)

# conversion from a tm SimpleCorpus corpus (DirSource)
tmSCdir &lt;- tm::SimpleCorpus(tm::DirSource(txt))
corp4 &lt;- as.sento_corpus(tmSCdir, dates[1:length(tmSCdir)])

# conversion from a tm VCorpus corpus (DataframeSource)
tmVCdf &lt;- tm::VCorpus(tm::DataframeSource(usnews))
corp5 &lt;- as.sento_corpus(tmVCdf)

# conversion from a tm VCorpus corpus (DirSource)
tmVCdir &lt;- tm::VCorpus(tm::DirSource(reuters),
                       list(reader = tm::readReut21578XMLasPlain))
corp6 &lt;- as.sento_corpus(tmVCdir, dates[1:length(tmVCdir)])

</code></pre>

<hr>
<h2 id='attributions'>Retrieve top-down model sentiment attributions</h2><span id='topic+attributions'></span>

<h3>Description</h3>

<p>Computes the attributions to predictions for a (given) number of dates at all possible sentiment dimensions,
based on the coefficients associated to each sentiment measure, as estimated in the provided model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>attributions(
  model,
  sento_measures,
  do.lags = TRUE,
  do.normalize = FALSE,
  refDates = NULL,
  factor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="attributions_+3A_model">model</code></td>
<td>
<p>a <code>sento_model</code> or a <code>sento_modelIter</code> object created with <code><a href="#topic+sento_model">sento_model</a></code>.</p>
</td></tr>
<tr><td><code id="attributions_+3A_sento_measures">sento_measures</code></td>
<td>
<p>the <code>sento_measures</code> object, as created with <code><a href="#topic+sento_measures">sento_measures</a></code>, used to estimate
the model from the first argument (make sure this is the case!).</p>
</td></tr>
<tr><td><code id="attributions_+3A_do.lags">do.lags</code></td>
<td>
<p>a <code>logical</code>, <code>TRUE</code> also computes the attribution to each time lag. For large time lags,
this is time-consuming.</p>
</td></tr>
<tr><td><code id="attributions_+3A_do.normalize">do.normalize</code></td>
<td>
<p>a <code>logical</code>, <code>TRUE</code> divides each element of every attribution vector at a given date by its
L2-norm at that date, normalizing the values between -1 and 1. The document attributions are not normalized.</p>
</td></tr>
<tr><td><code id="attributions_+3A_refdates">refDates</code></td>
<td>
<p>the dates (as <code>"yyyy-mm-dd"</code>) at which attribution is to be performed. These should be between the latest
date available in the input <code>sento_measures</code> object and the first estimation sample date (that is, <code>model$dates[1]</code>
if <code>model</code> is a <code>sento_model</code> object). All dates should also be in <code>get_dates(sento_measures)</code>. If
<code>NULL</code> (default), attribution is calculated for all in-sample dates. Ignored if <code>model</code> is a <code>sento_modelIter</code>
object, for which attribution is calculated for all out-of-sample prediction dates.</p>
</td></tr>
<tr><td><code id="attributions_+3A_factor">factor</code></td>
<td>
<p>the factor level as a single <code>character</code> vector to calculate attribution
for in case of (a) multinomial model(s). Ignored for linear and binomial models.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+sento_model">sento_model</a></code> for an elaborate modeling example including the calculation and plotting of
attributions. The attribution for logistic models is represented in terms of log odds. For binomial models, it is
calculated with respect to the last factor level or factor column. A <code>NULL</code> value for document-level attribution
on a given date means no documents are directly implicated in the associated prediction.
</p>


<h3>Value</h3>

<p>A <code>list</code> of class <code>attributions</code>, with <code>"documents"</code>, <code>"lags"</code>, <code>"lexicons"</code>,
<code>"features"</code> and <code>"time"</code> as attribution dimensions. The last four dimensions are
<code>data.table</code>s having a <code>"date"</code> column and the other columns the different components of the dimension, with
the attributions as values. Document-level attribution is further decomposed into a <code>data.table</code> per date, with
<code>"id"</code>, <code>"date"</code> and <code>"attrib"</code> columns. If <code>do.lags = FALSE</code>, the <code>"lags"</code> element is set
to <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sento_model">sento_model</a></code>
</p>

<hr>
<h2 id='compute_sentiment'>Compute textual sentiment across features and lexicons</h2><span id='topic+compute_sentiment'></span>

<h3>Description</h3>

<p>Given a corpus of texts, computes sentiment per document or sentence using the valence shifting
augmented bag-of-words approach, based on the lexicons provided and a choice of aggregation across words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_sentiment(
  x,
  lexicons,
  how = "proportional",
  tokens = NULL,
  do.sentence = FALSE,
  nCore = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_sentiment_+3A_x">x</code></td>
<td>
<p>either a <code>sento_corpus</code> object created with <code><a href="#topic+sento_corpus">sento_corpus</a></code>, a <span class="pkg">quanteda</span>
<code><a href="quanteda.html#topic+corpus">corpus</a></code> object, a <span class="pkg">tm</span> <code><a href="tm.html#topic+SimpleCorpus">SimpleCorpus</a></code> object, a <span class="pkg">tm</span>
<code><a href="tm.html#topic+VCorpus">VCorpus</a></code> object, or a <code>character</code> vector. Only a <code>sento_corpus</code> object incorporates
a date dimension. In case of a <code><a href="quanteda.html#topic+corpus">corpus</a></code> object, the <code>numeric</code> columns from the
<code><a href="quanteda.html#topic+docvars">docvars</a></code> are considered as features over which sentiment will be computed. In
case of a <code>character</code> vector, sentiment is only computed across lexicons.</p>
</td></tr>
<tr><td><code id="compute_sentiment_+3A_lexicons">lexicons</code></td>
<td>
<p>a <code>sento_lexicons</code> object created using <code><a href="#topic+sento_lexicons">sento_lexicons</a></code>.</p>
</td></tr>
<tr><td><code id="compute_sentiment_+3A_how">how</code></td>
<td>
<p>a single <code>character</code> vector defining how to perform aggregation within
documents or sentences. For available options, see <code><a href="#topic+get_hows">get_hows</a>()$words</code>.</p>
</td></tr>
<tr><td><code id="compute_sentiment_+3A_tokens">tokens</code></td>
<td>
<p>a <code>list</code> of tokenized documents, or if <code>do.sentence = TRUE</code> a <code>list</code> of
<code>list</code>s of tokenized sentences. This allows to specify your own tokenization scheme. Can indirectly result from
the <span class="pkg">quanteda</span>'s <code><a href="quanteda.html#topic+tokens">tokens</a></code> function, the <span class="pkg">tokenizers</span> package, or other (see examples).
Make sure the tokens are constructed from (the texts from) the <code>x</code> argument, are unigrams, and preferably
set to lowercase, otherwise, results may be spurious and errors could occur. By default set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="compute_sentiment_+3A_do.sentence">do.sentence</code></td>
<td>
<p>a <code>logical</code> to indicate whether the sentiment computation should be done on
sentence-level rather than document-level. By default <code>do.sentence = FALSE</code>.</p>
</td></tr>
<tr><td><code id="compute_sentiment_+3A_ncore">nCore</code></td>
<td>
<p>a positive <code>numeric</code> that will be passed on to the <code>numThreads</code> argument of the
<code><a href="RcppParallel.html#topic+setThreadOptions">setThreadOptions</a></code> function, to parallelize the sentiment computation across texts. A
value of 1 (default) implies no parallelization. Parallelization will improve speed of the sentiment
computation only for a sufficiently large corpus.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a separate calculation of positive (resp. negative) sentiment, provide distinct positive (resp.
negative) lexicons (see the <code>do.split</code> option in the <code><a href="#topic+sento_lexicons">sento_lexicons</a></code> function). All <code>NA</code>s
are converted to 0, under the assumption that this is equivalent to no sentiment. Per default <code>tokens = NULL</code>,
meaning the corpus is internally tokenized as unigrams, with punctuation and numbers but not stopwords removed.
All tokens are converted to lowercase, in line with what the <code><a href="#topic+sento_lexicons">sento_lexicons</a></code> function does for the
lexicons and valence shifters. Word counts are based on that same tokenization.
</p>


<h3>Value</h3>

<p>If <code>x</code> is a <code>sento_corpus</code> object: a <code>sentiment</code> object, i.e., a <code>data.table</code> containing
the sentiment scores <code>data.table</code> with an <code>"id"</code>, a <code>"date"</code> and a <code>"word_count"</code> column,
and all lexicon-feature sentiment scores columns. The tokenized sentences are not provided but can be
obtained as <code>stringi::stri_split_boundaries(texts, type = "sentence")</code>. A <code>sentiment</code> object can
be aggregated (into time series) with the <code><a href="#topic+aggregate.sentiment">aggregate.sentiment</a></code> function.
</p>
<p>If <code>x</code> is a <span class="pkg">quanteda</span> <code><a href="quanteda.html#topic+corpus">corpus</a></code> object: a sentiment scores
<code>data.table</code> with an <code>"id"</code> and a <code>"word_count"</code> column, and all lexicon-feature
sentiment scores columns.
</p>
<p>If <code>x</code> is a <span class="pkg">tm</span> <code>SimpleCorpus</code> object, a <span class="pkg">tm</span> <code>VCorpus</code> object, or a <code>character</code>
vector: a sentiment scores <code>data.table</code> with an auto-created <code>"id"</code> column, a <code>"word_count"</code>
column, and all lexicon sentiment scores columns.
</p>
<p>When <code>do.sentence = TRUE</code>, an additional <code>"sentence_id"</code> column along the
<code>"id"</code> column is added.
</p>


<h3>Calculation</h3>

<p>If the <code>lexicons</code> argument has no <code>"valence"</code> element, the sentiment computed corresponds to simple unigram
matching with the lexicons [<em>unigrams</em> approach]. If valence shifters are included in <code>lexicons</code> with a
corresponding <code>"y"</code> column, the polarity of a word detected from a lexicon gets multiplied with the associated
value of a valence shifter if it appears right before the detected word (examples: not good or can't defend) [<em>bigrams</em>
approach]. If the valence table contains a <code>"t"</code> column, valence shifters are searched for in a cluster centered around
a detected polarity word [<em>clusters</em> approach]. The latter approach is a simplified version of the one utilized by the
<span class="pkg">sentimentr</span> package. A cluster amounts to four words before and two words after a polarity word. A cluster never overlaps
with a preceding one. Roughly speaking, the polarity of a cluster is calculated as <code class="reqn">n(1 + 0.80d)S + \sum s</code>. The polarity
score of the detected word is <code class="reqn">S</code>, <code class="reqn">s</code> represents polarities of eventual other sentiment words, and <code class="reqn">d</code> is
the difference between the number of amplifiers (<code>t = 2</code>) and the number of deamplifiers (<code>t = 3</code>). If there
is an odd number of negators (<code>t = 1</code>), <code class="reqn">n = -1</code> and amplifiers are counted as deamplifiers, else <code class="reqn">n = 1</code>.
</p>
<p>The sentence-level sentiment calculation approaches each sentence as if it is a document. Depending on the input either
the unigrams, bigrams or clusters approach is used. We enhanced latter approach following more closely the default
<span class="pkg">sentimentr</span> settings. They use a cluster of five words before and two words after a polarized word. The cluster
is limited to the words after a previous comma and before a next comma. Adversative conjunctions (<code>t = 4</code>) are
accounted for here. The cluster is reweighted based on the value <code class="reqn">1 + 0.25adv</code>, where <code class="reqn">adv</code> is the difference
between the number of adversative conjunctions found before and after the polarized word.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Jeroen Van Pelt, Andres Algaba
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
txt &lt;- system.file("texts", "txt", package = "tm")
reuters &lt;- system.file("texts", "crude", package = "tm")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l1 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
l2 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                     list_valence_shifters[["en"]])
l3 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                     list_valence_shifters[["en"]][, c("x", "t")])

# from a sento_corpus object - unigrams approach
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 200)
sent1 &lt;- compute_sentiment(corpusSample, l1, how = "proportionalPol")

# from a character vector - bigrams approach
sent2 &lt;- compute_sentiment(usnews[["texts"]][1:200], l2, how = "counts")

# from a corpus object - clusters approach
corpusQ &lt;- quanteda::corpus(usnews, text_field = "texts")
corpusQSample &lt;- quanteda::corpus_sample(corpusQ, size = 200)
sent3 &lt;- compute_sentiment(corpusQSample, l3, how = "counts")

# from an already tokenized corpus - using the 'tokens' argument
toks &lt;- as.list(quanteda::tokens(corpusQSample, what = "fastestword"))
sent4 &lt;- compute_sentiment(corpusQSample, l1[1], how = "counts", tokens = toks)

# from a SimpleCorpus object - unigrams approach
scorp &lt;- tm::SimpleCorpus(tm::DirSource(txt))
sent5 &lt;- compute_sentiment(scorp, l1, how = "proportional")

# from a VCorpus object - unigrams approach
## in contrast to what as.sento_corpus(vcorp) would do, the
## sentiment calculator handles multiple character vectors within
## a single corpus element as separate documents
vcorp &lt;- tm::VCorpus(tm::DirSource(reuters))
sent6 &lt;- compute_sentiment(vcorp, l1)

# from a sento_corpus object - unigrams approach with tf-idf weighting
sent7 &lt;- compute_sentiment(corpusSample, l1, how = "TFIDF")

# sentence-by-sentence computation
sent8 &lt;- compute_sentiment(corpusSample, l1, how = "proportionalSquareRoot",
                           do.sentence = TRUE)

# from a (fake) multilingual corpus
usnews[["language"]] &lt;- "en" # add language column
usnews$language[1:100] &lt;- "fr"
lEn &lt;- sento_lexicons(list("FEEL_en" = list_lexicons$FEEL_en_tr,
                           "HENRY" = list_lexicons$HENRY_en),
                      list_valence_shifters$en)
lFr &lt;- sento_lexicons(list("FEEL_fr" = list_lexicons$FEEL_fr),
                      list_valence_shifters$fr)
lexicons &lt;- list(en = lEn, fr = lFr)
corpusLang &lt;- sento_corpus(corpusdf = usnews[1:250, ])
sent9 &lt;- compute_sentiment(corpusLang, lexicons, how = "proportional")

</code></pre>

<hr>
<h2 id='corpus_summarize'>Summarize the sento_corpus object</h2><span id='topic+corpus_summarize'></span>

<h3>Description</h3>

<p>Summarizes the <code>sento_corpus</code> object and returns insights about the evolution of
documents, features and tokens over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_summarize(x, by = "day", features = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_summarize_+3A_x">x</code></td>
<td>
<p>is a <code>sento_corpus</code> object created with <code><a href="#topic+sento_corpus">sento_corpus</a></code></p>
</td></tr>
<tr><td><code id="corpus_summarize_+3A_by">by</code></td>
<td>
<p>a single <code>character</code> vector to specify the frequency time interval over which the statistics
need to be calculated.</p>
</td></tr>
<tr><td><code id="corpus_summarize_+3A_features">features</code></td>
<td>
<p>a <code>character</code> vector that can be used to select a subset of the features to analyse.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarizes the <code>sento_corpus</code> object by generating statistics about
documents, features and tokens over time. The insights can be narrowed down to a chosen set of metadata
features. The same tokenization as in the sentiment calculation in <code><a href="#topic+compute_sentiment">compute_sentiment</a></code> is used.
</p>


<h3>Value</h3>

<p>returns a <code>list</code> containing:
</p>
<table>
<tr><td><code>stats</code></td>
<td>
<p>a <code>data.table</code> with statistics about the number of documents, total, average, minimum and maximum
number of tokens and the number of texts per features for each date.</p>
</td></tr>
<tr><td><code>plots</code></td>
<td>
<p>a <code>list</code> with three plots representing the above statistics.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeroen Van Pelt, Samuel Borms, Andres Algaba
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")

corpus &lt;- sento_corpus(usnews)

# summary of corpus by day
summary1 &lt;- corpus_summarize(corpus)

# summary of corpus by month for both journals
summary2 &lt;- corpus_summarize(corpus, by = "month",
                             features = c("wsj", "wapo"))

</code></pre>

<hr>
<h2 id='ctr_agg'>Set up control for aggregation into sentiment measures</h2><span id='topic+ctr_agg'></span>

<h3>Description</h3>

<p>Sets up control object for (computation of textual sentiment and) aggregation into textual
sentiment measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctr_agg(
  howWithin = "proportional",
  howDocs = "equal_weight",
  howTime = "equal_weight",
  do.sentence = FALSE,
  do.ignoreZeros = TRUE,
  by = "day",
  lag = 1,
  fill = "zero",
  alphaExpDocs = 0.1,
  alphasExp = seq(0.1, 0.5, by = 0.1),
  do.inverseExp = FALSE,
  ordersAlm = 1:3,
  do.inverseAlm = TRUE,
  aBeta = 1:4,
  bBeta = 1:4,
  weights = NULL,
  tokens = NULL,
  nCore = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctr_agg_+3A_howwithin">howWithin</code></td>
<td>
<p>a single <code>character</code> vector defining how to perform aggregation within
documents or sentences. Coincides with the <code>how</code> argument in the <code><a href="#topic+compute_sentiment">compute_sentiment</a></code> function. Should
<code>length(howWithin) &gt; 1</code>, the first element is used. For available options see <code><a href="#topic+get_hows">get_hows</a>()$words</code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_howdocs">howDocs</code></td>
<td>
<p>a single <code>character</code> vector defining how aggregation across documents (and/or sentences) per date will
be performed. Should <code>length(howDocs) &gt; 1</code>, the first element is used. For available options
see <code><a href="#topic+get_hows">get_hows</a>()$docs</code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_howtime">howTime</code></td>
<td>
<p>a <code>character</code> vector defining how aggregation across dates will be performed. More than one choice
is possible. For available options see <code><a href="#topic+get_hows">get_hows</a>()$time</code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_do.sentence">do.sentence</code></td>
<td>
<p>see <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_do.ignorezeros">do.ignoreZeros</code></td>
<td>
<p>a <code>logical</code> indicating whether zero sentiment values have to be ignored in the determination of
the document (and/or sentence) weights while aggregating across documents (and/or sentences). By default
<code>do.ignoreZeros = TRUE</code>, such that documents (and/or sentences) with a raw sentiment score of zero or for which
a given feature indicator is equal to zero are considered irrelevant.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_by">by</code></td>
<td>
<p>a single <code>character</code> vector, either <code>"day", "week", "month"</code> or <code>"year"</code>, to indicate at what
level the dates should be aggregated. Dates are displayed as the first day of the period, if applicable (e.g.,
<code>"2017-03-01"</code> for March 2017).</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_lag">lag</code></td>
<td>
<p>a single <code>integer</code> vector, being the time lag to be specified for aggregation across time. By default
equal to <code>1</code>, meaning no aggregation across time; a time weighting scheme named <code>"dummyTime"</code> is used in
this case.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_fill">fill</code></td>
<td>
<p>a single <code>character</code> vector, one of <code>c("zero", "latest", "none")</code>, to control how missing
sentiment values across the continuum of dates considered are added. This impacts the aggregation across time,
applying the <code><a href="#topic+measures_fill">measures_fill</a></code> function before aggregating, except if <code>fill = "none"</code>. By default equal to
<code>"zero"</code>, which sets the scores (and thus also the weights) of the added dates to zero in the time aggregation.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_alphaexpdocs">alphaExpDocs</code></td>
<td>
<p>a single <code>integer</code> vector. A weighting smoothing factor, used if <br />
<code>"exponential" %in% howDocs</code> or <code>"inverseExponential" %in% howDocs</code>. Value should be between 0 and 1
(both excluded); see <code><a href="#topic+weights_exponential">weights_exponential</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_alphasexp">alphasExp</code></td>
<td>
<p>a <code>numeric</code> vector of all exponential weighting smoothing factors, used if <br />
<code>"exponential" %in% howTime</code>. Values should be between 0 and 1 (both excluded); see
<code><a href="#topic+weights_exponential">weights_exponential</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_do.inverseexp">do.inverseExp</code></td>
<td>
<p>a <code>logical</code> indicating if for every exponential curve its inverse has to be added,
used if <code>"exponential" %in% howTime</code>; see <code><a href="#topic+weights_exponential">weights_exponential</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_ordersalm">ordersAlm</code></td>
<td>
<p>a <code>numeric</code> vector of all Almon polynomial orders (positive) to calculate weights for, used if
<code>"almon" %in% howTime</code>; see <code><a href="#topic+weights_almon">weights_almon</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_do.inversealm">do.inverseAlm</code></td>
<td>
<p>a <code>logical</code> indicating if for every Almon polynomial its inverse has to be added, used
if <code>"almon" %in% howTime</code>; see <code><a href="#topic+weights_almon">weights_almon</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_abeta">aBeta</code></td>
<td>
<p>a <code>numeric</code> vector of positive values as first Beta weighting decay parameter; see
<code><a href="#topic+weights_beta">weights_beta</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_bbeta">bBeta</code></td>
<td>
<p>a <code>numeric</code> vector of positive values as second Beta weighting decay parameter; see
<code><a href="#topic+weights_beta">weights_beta</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_weights">weights</code></td>
<td>
<p>optional own weighting scheme(s), used if provided as a <code>data.frame</code> with the number of rows
equal to the desired <code>lag</code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_tokens">tokens</code></td>
<td>
<p>see <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>.</p>
</td></tr>
<tr><td><code id="ctr_agg_+3A_ncore">nCore</code></td>
<td>
<p>see <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For available options on how aggregation can occur (via the <code>howWithin</code>,
<code>howDocs</code> and <code>howTime</code> arguments), inspect <code><a href="#topic+get_hows">get_hows</a></code>. The control parameters
associated to <code>howDocs</code> are used both for aggregation across documents and across sentences.
</p>


<h3>Value</h3>

<p>A <code>list</code> encapsulating the control parameters.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>See Also</h3>

<p><code><a href="#topic+measures_fill">measures_fill</a></code>, <code><a href="#topic+almons">almons</a></code>, <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

# simple control function
ctr1 &lt;- ctr_agg(howTime = "linear", by = "year", lag = 3)

# more elaborate control function (particular attention to time weighting schemes)
ctr2 &lt;- ctr_agg(howWithin = "proportionalPol",
                howDocs = "exponential",
                howTime = c("equal_weight", "linear", "almon", "beta", "exponential", "own"),
                do.ignoreZeros = TRUE,
                by = "day",
                lag = 20,
                ordersAlm = 1:3,
                do.inverseAlm = TRUE,
                alphasExp = c(0.20, 0.50, 0.70, 0.95),
                aBeta = c(1, 3),
                bBeta = c(1, 3, 4, 7),
                weights = data.frame(myWeights = runif(20)),
                alphaExp = 0.3)

# set up control function with one linear and two chosen Almon weighting schemes
a &lt;- weights_almon(n = 70, orders = 1:3, do.inverse = TRUE, do.normalize = TRUE)
ctr3 &lt;- ctr_agg(howTime = c("linear", "own"), by = "year", lag = 70,
                weights = data.frame(a1 = a[, 1], a2 = a[, 3]),
                do.sentence = TRUE)

</code></pre>

<hr>
<h2 id='ctr_model'>Set up control for sentiment-based sparse regression modeling</h2><span id='topic+ctr_model'></span>

<h3>Description</h3>

<p>Sets up control object for linear or nonlinear modeling of a response variable onto a large panel of
textual sentiment measures (and potentially other variables). See <code><a href="#topic+sento_model">sento_model</a></code> for details on the
estimation and calibration procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctr_model(
  model = c("gaussian", "binomial", "multinomial"),
  type = c("BIC", "AIC", "Cp", "cv"),
  do.intercept = TRUE,
  do.iter = FALSE,
  h = 0,
  oos = 0,
  do.difference = FALSE,
  alphas = seq(0, 1, by = 0.2),
  lambdas = NULL,
  nSample = NULL,
  trainWindow = NULL,
  testWindow = NULL,
  start = 1,
  do.shrinkage.x = FALSE,
  do.progress = TRUE,
  nCore = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctr_model_+3A_model">model</code></td>
<td>
<p>a <code>character</code> vector with one of the following: <code>"gaussian"</code> (linear regression), <code>"binomial"</code>
(binomial logistic regression), or <code>"multinomial"</code> (multinomial logistic regression).</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_type">type</code></td>
<td>
<p>a <code>character</code> vector indicating which model calibration approach to use. Supports &quot;<code>BIC</code>&quot;,
&quot;<code>AIC</code>&quot; and &quot;<code>Cp</code>&quot; (Mallows's Cp) as sparse regression adapted information criteria (Tibshirani and Taylor,
2012; Zou, Hastie and Tibshirani, 2007), and &quot;<code>cv</code>&quot; (cross-validation based on the <code><a href="caret.html#topic+train">train</a></code>
function from the <span class="pkg">caret</span> package). The adapted information criteria are only available for a linear regression.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_do.intercept">do.intercept</code></td>
<td>
<p>a <code>logical</code>, <code>TRUE</code> by default fits an intercept.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_do.iter">do.iter</code></td>
<td>
<p>a <code>logical</code>, <code>TRUE</code> induces an iterative estimation of models at the given <code>nSample</code> size and
performs the associated out-of-sample prediction exercise through time.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_h">h</code></td>
<td>
<p>an <code>integer</code> value that shifts the time series to have the desired prediction setup; <code>h = 0</code> means
no change to the input data (nowcasting assuming data is aligned properly), <code>h &gt; 0</code> shifts the dependent variable by
<code>h</code> periods (i.e., rows) further in time (forecasting), <code>h &lt; 0</code> shifts the independent variables by <code>h</code>
periods.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_oos">oos</code></td>
<td>
<p>a non-negative <code>integer</code> to indicate the number of periods to skip from the end of the training sample
up to the out-of-sample prediction(s). This is either used in the cross-validation based calibration approach
(if <code>type = </code> &quot;<code>cv</code>&quot;), or for the iterative out-of-sample prediction analysis (if <code>do.iter = TRUE</code>). For
instance, given <code class="reqn">t</code>, the (first) out-of-sample prediction is computed at <code class="reqn">t +</code> <code>oos</code> <code class="reqn">+ 1</code>.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_do.difference">do.difference</code></td>
<td>
<p>a <code>logical</code>, <code>TRUE</code> will difference the target variable <code>y</code> supplied in the
<code><a href="#topic+sento_model">sento_model</a></code> function with as lag the absolute value of the <code>h</code> argument, but
<code>abs(h) &gt; 0</code> is required. For example, if <code>h = 2</code>, and assuming the <code>y</code> variable is properly aligned
date-wise with the explanatory variables denoted by <code class="reqn">X</code> (the sentiment measures and other in <code>x</code>), the regression
will be of <code class="reqn">y_{t + 2} - y_t</code> on <code class="reqn">X_t</code>. If <code>h = -2</code>, the regression fitted is <code class="reqn">y_{t + 2} - y_t</code> on
<code class="reqn">X_{t+2}</code>. The argument is always kept at <code>FALSE</code> if the <code>model</code> argument is one of
<code>c("binomial", "multinomial")</code>.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_alphas">alphas</code></td>
<td>
<p>a <code>numeric</code> vector of the alphas to test for during calibration, between 0 and 1. A value of
0 pertains to Ridge regression, a value of 1 to LASSO regression; values in between are pure elastic net.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_lambdas">lambdas</code></td>
<td>
<p>a <code>numeric</code> vector of the lambdas to test for during calibration, <code class="reqn">&gt;= 0</code>.
A value of zero means no regularization, thus requires care when the data is fat. By default set to
<code>NULL</code>, such that the lambdas sequence is generated by the <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> function
or set to <code>10^seq(2, -2, length.out = 100)</code> in case of cross-validation.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_nsample">nSample</code></td>
<td>
<p>a positive <code>integer</code> as the size of the sample for model estimation at every iteration (ignored if
<code>do.iter = FALSE</code>).</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_trainwindow">trainWindow</code></td>
<td>
<p>a positive <code>integer</code> as the size of the training sample for cross-validation (ignored if
<code>type != </code> &quot;<code>cv</code>&quot;).</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_testwindow">testWindow</code></td>
<td>
<p>a positive <code>integer</code> as the size of the test sample for cross-validation (ignored if <code>type != </code>
&quot;<code>cv</code>&quot;).</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_start">start</code></td>
<td>
<p>a positive <code>integer</code> to indicate at which point the iteration has to start (ignored if
<code>do.iter = FALSE</code>). For example, given 100 possible iterations, <code>start = 70</code> leads to model estimations
only for the last 31 samples.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_do.shrinkage.x">do.shrinkage.x</code></td>
<td>
<p>a <code>logical</code> vector to indicate which of the other regressors provided through the <code>x</code>
argument of the <code><a href="#topic+sento_model">sento_model</a></code> function should be subject to shrinkage (<code>TRUE</code>). If argument is of
length one, it applies to all external regressors.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_do.progress">do.progress</code></td>
<td>
<p>a <code>logical</code>, if <code>TRUE</code> progress statements are displayed during model calibration.</p>
</td></tr>
<tr><td><code id="ctr_model_+3A_ncore">nCore</code></td>
<td>
<p>a positive <code>integer</code> to indicate the number of cores to use for a parallel iterative model
estimation (<code>do.iter = TRUE</code>). We use the <code>%dopar%</code> construct from the <span class="pkg">foreach</span> package. By default,
<code>nCore = 1</code>, which implies no parallelization. No progress statements are displayed whatsoever when <code>nCore &gt; 1</code>.
For cross-validation models, parallelization can also be carried out for a single-shot model (<code>do.iter = FALSE</code>),
whenever a parallel backend is set up. See the examples in <code><a href="#topic+sento_model">sento_model</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> encapsulating the control parameters.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>References</h3>

<p>Tibshirani and Taylor (2012). <strong>Degrees of freedom in LASSO problems</strong>.
<em>The Annals of Statistics 40, 1198-1232</em>, doi: <a href="https://doi.org/10.1214/12-AOS1003">10.1214/12-AOS1003</a>.
</p>
<p>Zou, Hastie and Tibshirani (2007). <strong>On the degrees of freedom of the LASSO</strong>.
<em>The Annals of Statistics 35, 2173-2192</em>, doi: <a href="https://doi.org/10.1214/009053607000000127">10.1214/009053607000000127</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sento_model">sento_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># information criterion based model control functions
ctrIC1 &lt;- ctr_model(model = "gaussian", type = "BIC", do.iter = FALSE, h = 0,
                    alphas = seq(0, 1, by = 0.10))
ctrIC2 &lt;- ctr_model(model = "gaussian", type = "AIC", do.iter = TRUE, h = 4, nSample = 100,
                    do.difference = TRUE, oos = 3)

# cross-validation based model control functions
ctrCV1 &lt;- ctr_model(model = "gaussian", type = "cv", do.iter = FALSE, h = 0,
                    trainWindow = 250, testWindow = 4, oos = 0, do.progress = TRUE)
ctrCV2 &lt;- ctr_model(model = "binomial", type = "cv", h = 0, trainWindow = 250,
                    testWindow = 4, oos = 0, do.progress = TRUE)
ctrCV3 &lt;- ctr_model(model = "multinomial", type = "cv", h = 2, trainWindow = 250,
                    testWindow = 4, oos = 2, do.progress = TRUE)
ctrCV4 &lt;- ctr_model(model = "gaussian", type = "cv", do.iter = TRUE, h = 0, trainWindow = 45,
                    testWindow = 4, oos = 0, nSample = 70, do.progress = TRUE)

</code></pre>

<hr>
<h2 id='data-defunct'>Datasets with defunct names</h2><span id='topic+data-defunct'></span><span id='topic+lexicons'></span><span id='topic+valence'></span>

<h3>Description</h3>

<p>These are datasets that have been renamed and removed.
</p>


<h3>Details</h3>

<p>The dataset <code>lexicons</code> is defunct, use <code>list_lexicons</code> instead.
</p>
<p>The dataset <code>valence</code> is defunct, use <code>list_valence_shifters</code> instead.
</p>

<hr>
<h2 id='diff.sento_measures'>Differencing of sentiment measures</h2><span id='topic+diff.sento_measures'></span>

<h3>Description</h3>

<p>Differences the sentiment measures from a <code>sento_measures</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
diff(x, lag = 1, differences = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diff.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="diff.sento_measures_+3A_lag">lag</code></td>
<td>
<p>a <code>numeric</code>, see documentation for the generic <code><a href="base.html#topic+diff">diff</a></code>.</p>
</td></tr>
<tr><td><code id="diff.sento_measures_+3A_differences">differences</code></td>
<td>
<p>a <code>numeric</code>, see documentation for the generic <code><a href="base.html#topic+diff">diff</a></code>.</p>
</td></tr>
<tr><td><code id="diff.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified <code>sento_measures</code> object, with the measures replaced by the differenced measures as well as updated
statistics.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "year", lag = 3)
sento_measures &lt;- sento_measures(corpusSample, l, ctr)

# first-order difference sentiment measures with a lag of two
diffed &lt;- diff(sento_measures, lag = 2, differences = 1)

</code></pre>

<hr>
<h2 id='epu'>Monthly U.S. Economic Policy Uncertainty index</h2><span id='topic+epu'></span>

<h3>Description</h3>

<p>Monthly news-based U.S. Economic Policy Uncertainty (EPU) index (Baker, Bloom and Davis, 2016). Goes from January 1985
to July 2018, and includes a binomial and a multinomial example series. Following columns are present:
</p>

<ul>
<li><p> date. Date as <code>"yyyy-mm-01"</code>.
</p>
</li>
<li><p> index. A <code>numeric</code> monthly index value.
</p>
</li>
<li><p> above. A <code>factor</code> with value <code>"above"</code> if the index is greater than the mean of the entire series, else
<code>"below"</code>.
</p>
</li>
<li><p> aboveMulti. A <code>factor</code> with values <code>"above+"</code>, <code>"above"</code>, <code>"below"</code> and <code>"below-"</code> if the
index is greater than the 75% quantile and the 50% quantile, or smaller than the 50% quantile and the 25% quantile,
respectively and in a mutually exclusive sense.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data("epu")
</code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code> with 403 rows and 4 columns.
</p>


<h3>Source</h3>

<p><a href="http://www.policyuncertainty.com/us_monthly.html">Measuring Economic Policy Uncertainty</a>. Retrieved
August 24, 2018.
</p>


<h3>References</h3>

<p>Baker, Bloom and Davis (2016). <strong>Measuring Economic Policy Uncertainty</strong>.
<em>The Quarterly Journal of Economics 131, 1593-1636</em>, doi: <a href="https://doi.org/10.1093/qje/qjw024">10.1093/qje/qjw024</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("epu", package = "sentometrics")
head(epu)

</code></pre>

<hr>
<h2 id='get_dates'>Get the dates of the sentiment measures/time series</h2><span id='topic+get_dates'></span>

<h3>Description</h3>

<p>Returns the dates of the sentiment time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dates(sento_measures)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dates_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>"date"</code> column in <code>sento_measures[["measures"]]</code> as a <code>character</code> vector.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>

<hr>
<h2 id='get_dimensions'>Get the dimensions of the sentiment measures</h2><span id='topic+get_dimensions'></span>

<h3>Description</h3>

<p>Returns the components across all three dimensions of the sentiment measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dimensions(sento_measures)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dimensions_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>"features"</code>, <code>"lexicons"</code> and <code>"time"</code> elements in <code>sento_measures</code>.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>

<hr>
<h2 id='get_hows'>Options supported to perform aggregation into sentiment measures</h2><span id='topic+get_hows'></span>

<h3>Description</h3>

<p>Outputs the supported aggregation arguments. Call for information purposes only. Used within
<code><a href="#topic+ctr_agg">ctr_agg</a></code> to check if supplied aggregation hows are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_hows()
</code></pre>


<h3>Details</h3>

<p>See the package's <a href="https://www.ssrn.com/abstract=3067734">vignette</a> for a detailed explanation of all
aggregation options.
</p>


<h3>Value</h3>

<p>A list with the supported aggregation hows for arguments <code>howWithin</code> (<code>"words"</code>), <code>howDows</code>
(<code>"docs"</code>) and <code>howTime</code> (<code>"time"</code>), to be supplied to <code><a href="#topic+ctr_agg">ctr_agg</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctr_agg">ctr_agg</a></code>
</p>

<hr>
<h2 id='get_loss_data'>Retrieve loss data from a selection of models</h2><span id='topic+get_loss_data'></span>

<h3>Description</h3>

<p>Structures specific performance data for a set of different <code>sento_modelIter</code> objects as loss data.
Can then be used, for instance, as an input to create a model confidence set (Hansen, Lunde and Nason, 2011) with
the <span class="pkg">MCS</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_loss_data(models, loss = c("DA", "error", "errorSq", "AD", "accuracy"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_loss_data_+3A_models">models</code></td>
<td>
<p>a named <code>list</code> of <code>sento_modelIter</code> objects. All models should be of the same family, being
either <code>"gaussian"</code>, <code>"binomial"</code> or <code>"multinomial"</code>, and have performance data of the same dimensions.</p>
</td></tr>
<tr><td><code id="get_loss_data_+3A_loss">loss</code></td>
<td>
<p>a single <code>character</code> vector, either <code>"DA"</code> (directional <em>in</em>accuracy), <code>"error"</code>
(predicted minus realized response variable), <code>"errorSq"</code> (squared errors), <code>"AD"</code> (absolute errors) or
<code>"accuracy"</code> (<em>in</em>accurate class predictions). This argument defines on what basis the model confidence set
is calculated. The first four options are available for <code>"gaussian"</code> models, the last option applies only to
<code>"binomial"</code> and <code>"multinomial"</code> models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>matrix</code> of loss data.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>References</h3>

<p>Hansen, Lunde and Nason (2011). <strong>The model confidence set</strong>. <em>Econometrica 79, 453-497</em>,
doi: <a href="https://doi.org/10.3982/ECTA5771">10.3982/ECTA5771</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sento_model">sento_model</a></code>, <code><a href="MCS.html#topic+MCSprocedure">MCSprocedure</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")
data("epu", package = "sentometrics")

set.seed(505)

# construct two sento_measures objects
corpusAll &lt;- sento_corpus(corpusdf = usnews)
corpus &lt;- quanteda::corpus_subset(corpusAll, date &gt;= "1997-01-01" &amp; date &lt; "2014-10-01")
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])

ctrA &lt;- ctr_agg(howWithin = "proportionalPol", howDocs = "proportional",
                howTime = c("equal_weight", "linear"), by = "month", lag = 3)
sentMeas &lt;- sento_measures(corpus, l, ctrA)

# prepare y and other x variables
y &lt;- epu[epu$date %in% get_dates(sentMeas), "index"]
length(y) == nobs(sentMeas) # TRUE
x &lt;- data.frame(runif(length(y)), rnorm(length(y))) # two other (random) x variables
colnames(x) &lt;- c("x1", "x2")

# estimate different type of regressions
ctrM &lt;- ctr_model(model = "gaussian", type = "AIC", do.iter = TRUE,
                 h = 0, nSample = 120, start = 50)
out1 &lt;- sento_model(sentMeas, y, x = x, ctr = ctrM)
out2 &lt;- sento_model(sentMeas, y, x = NULL, ctr = ctrM)
out3 &lt;- sento_model(subset(sentMeas, select = "linear"), y, x = x, ctr = ctrM)
out4 &lt;- sento_model(subset(sentMeas, select = "linear"), y, x = NULL, ctr = ctrM)

lossData &lt;- get_loss_data(models = list(m1 = out1, m2 = out2, m3 = out3, m4 = out4),
                          loss = "errorSq")

mcs &lt;- MCS::MCSprocedure(lossData)
## End(Not run)

</code></pre>

<hr>
<h2 id='list_lexicons'>Built-in lexicons</h2><span id='topic+list_lexicons'></span>

<h3>Description</h3>

<p>A <code>list</code> containing all built-in lexicons as a <code>data.table</code> with two columns: a <code>x</code> column with the words,
and a <code>y</code> column with the polarities. The <code>list</code> element names incorporate consecutively the name and language
(based on the two-letter ISO code convention as in <code><a href="stopwords.html#topic+stopwords">stopwords</a></code>), and <code>"_tr"</code> as
suffix if the lexicon is translated. The translation was done via Microsoft Translator through Microsoft
Word. Only the entries that conform to the original language entry after retranslation, and those that have actually been
translated, are kept. The last condition is assumed to be fulfilled when the translation differs from the original entry.
All words are unigrams and in lowercase. The built-in lexicons are the following:
</p>

<ul>
<li><p> FEEL_en_tr
</p>
</li>
<li><p> FEEL_fr (Abdaoui, Azé, Bringay and Poncelet, 2017)
</p>
</li>
<li><p> FEEL_nl_tr
</p>
</li>
<li><p> GI_en (General Inquirer, i.e. Harvard IV-4 combined with Laswell)
</p>
</li>
<li><p> GI_fr_tr
</p>
</li>
<li><p> GI_nl_tr
</p>
</li>
<li><p> HENRY_en (Henry, 2008)
</p>
</li>
<li><p> HENRY_fr_tr
</p>
</li>
<li><p> HENRY_nl_tr
</p>
</li>
<li><p> LM_en (Loughran and McDonald, 2011)
</p>
</li>
<li><p> LM_fr_tr
</p>
</li>
<li><p> LM_nl_tr
</p>
</li></ul>

<p>Other useful lexicons can be found in the <span class="pkg">lexicon</span> package, more specifically the datasets preceded by
<code>hash_sentiment_</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("list_lexicons")
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> with all built-in lexicons, appropriately named as <code>"NAME_language(_tr)"</code> .
</p>


<h3>Source</h3>

<p><a href="https://link.springer.com/article/10.1007%2Fs10579-016-9364-5">FEEL lexicon</a>. Retrieved November 1, 2017.
</p>
<p><a href="http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm">GI lexicon</a>. Retrieved November 1, 2017.
</p>
<p><a href="https://journals.sagepub.com/doi/abs/10.1177/0021943608319388">HENRY lexicon</a>. Retrieved
November 1, 2017.
</p>
<p><a href="https://sraf.nd.edu/textual-analysis/resources/">LM lexicon</a>. Retrieved
November 1, 2017.
</p>


<h3>References</h3>

<p>Abdaoui, Azé, Bringay and Poncelet (2017). <strong>FEEL: French Expanded Emotion Lexicon</strong>.
<em>Language Resources &amp; Evaluation 51, 833-855</em>, doi: <a href="https://doi.org/10.1007/s10579-016-9364-5">10.1007/s10579-016-9364-5</a>.
</p>
<p>Henry (2008). <strong>Are investors influenced by how earnings press releases are written?</strong>.
<em>Journal of Business Communication 45, 363-407</em>, doi: <a href="https://doi.org/10.1177/0021943608319388">10.1177/0021943608319388</a>.
</p>
<p>Loughran and McDonald (2011). <strong>When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks</strong>.
<em>Journal of Finance 66, 35-65</em>, doi: <a href="https://doi.org/10.1111/j.1540-6261.2010.01625.x">10.1111/j.1540-6261.2010.01625.x</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("list_lexicons", package = "sentometrics")
list_lexicons[c("FEEL_en_tr", "LM_en")]

</code></pre>

<hr>
<h2 id='list_valence_shifters'>Built-in valence word lists</h2><span id='topic+list_valence_shifters'></span>

<h3>Description</h3>

<p>A <code>list</code> containing all built-in valence word lists, as <code>data.table</code>s with three columns: a <code>x</code> column with
the words, a <code>y</code> column with the values associated to each word, and a <code>t</code> column with the type of valence
shifter (<code>1</code> = negators, <code>2</code> = amplifiers, <code>3</code> = deamplifiers,
<code>4</code> = adversative conjunctions). The <code>list</code> element names indicate the language
(based on the two-letter ISO code convention as in <code><a href="stopwords.html#topic+stopwords">stopwords</a></code>) of the valence word list.
All non-English word lists are translated via Microsoft Translator through Microsoft Word. Only the entries whose
translation differs from the original entry are kept. All words are unigrams and in lowercase. The built-in valence word
lists are available in following languages:
</p>

<ul>
<li><p> English (<code>"en"</code>)
</p>
</li>
<li><p> French (<code>"fr"</code>)
</p>
</li>
<li><p> Dutch (<code>"nl"</code>)
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data("list_valence_shifters")
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> with all built-in valence word lists, appropriately named.
</p>


<h3>Source</h3>

<p><code><a href="lexicon.html#topic+hash_valence_shifters">hash_valence_shifters</a></code> (English valence shifters). Retrieved August 24, 2018.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("list_valence_shifters", package = "sentometrics")
list_valence_shifters["en"]

</code></pre>

<hr>
<h2 id='measures_fill'>Add and fill missing dates to sentiment measures</h2><span id='topic+measures_fill'></span>

<h3>Description</h3>

<p>Adds missing dates between earliest and latest date of a <code>sento_measures</code> object or two more extreme
boundary dates, such that the time series are continuous date-wise. Fills in any missing date with either 0 or the
most recent non-missing value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measures_fill(
  sento_measures,
  fill = "zero",
  dateBefore = NULL,
  dateAfter = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measures_fill_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="measures_fill_+3A_fill">fill</code></td>
<td>
<p>an element of <code>c("zero", "latest")</code>; the first assumes missing dates represent zero sentiment,
the second assumes missing dates represent constant sentiment.</p>
</td></tr>
<tr><td><code id="measures_fill_+3A_datebefore">dateBefore</code></td>
<td>
<p>a date as <code>"yyyy-mm-dd"</code>, to stretch the sentiment time series from up to the first date. Should
be earlier than <code>get_dates(sento_measures)[1]</code> to take effect. The values for these dates are set to those at
<code>get_dates(sento_measures)[1]</code>. If <code>NULL</code>, then ignored.</p>
</td></tr>
<tr><td><code id="measures_fill_+3A_dateafter">dateAfter</code></td>
<td>
<p>a date as <code>"yyyy-mm-dd"</code>, to stretch the sentiment time series up to this date. Should be
later than <code>tail(get_dates(sento_measures), 1)</code> to take effect. If <code>NULL</code>, then ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>dateBefore</code> and <code>dateAfter</code> dates are converted according to the <code>sento_measures[["by"]]</code>
frequency.
</p>


<h3>Value</h3>

<p>A modified <code>sento_measures</code> object.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'># construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = sentometrics::usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(sentometrics::list_lexicons[c("LM_en", "HENRY_en")],
                    sentometrics::list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "day", lag = 7, fill = "none")
sento_measures &lt;- sento_measures(corpusSample, l, ctr)

# fill measures
f1 &lt;- measures_fill(sento_measures)
f2 &lt;- measures_fill(sento_measures, fill = "latest")
f3 &lt;- measures_fill(sento_measures, fill = "zero",
                    dateBefore = get_dates(sento_measures)[1] - 10,
                    dateAfter = tail(get_dates(sento_measures), 1) + 15)

</code></pre>

<hr>
<h2 id='measures_update'>Update sentiment measures</h2><span id='topic+measures_update'></span>

<h3>Description</h3>

<p>Updates a <code>sento_measures</code> object based on a new <code>sento_corpus</code> provided.
Sentiment for the unseen corpus texts calculated and aggregated applying the control variables
from the input <code>sento_measures</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measures_update(sento_measures, sento_corpus, lexicons)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measures_update_+3A_sento_measures">sento_measures</code></td>
<td>
<p><code>sento_measures</code> object created with <code><a href="#topic+sento_measures">sento_measures</a></code></p>
</td></tr>
<tr><td><code id="measures_update_+3A_sento_corpus">sento_corpus</code></td>
<td>
<p>a <code>sento_corpus</code> object created with <code><a href="#topic+sento_corpus">sento_corpus</a></code>.</p>
</td></tr>
<tr><td><code id="measures_update_+3A_lexicons">lexicons</code></td>
<td>
<p>a <code>sento_lexicons</code> object created with <code><a href="#topic+sento_lexicons">sento_lexicons</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated <code>sento_measures</code> object.
</p>


<h3>Author(s)</h3>

<p>Jeroen Van Pelt, Samuel Borms, Andres Algaba
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sento_measures">sento_measures</a></code>, <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")

corpus1 &lt;- sento_corpus(usnews[1:500, ])
corpus2 &lt;- sento_corpus(usnews[400:2000, ])

ctr &lt;- ctr_agg(howTime = "linear", by = "year", lag = 3)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                    list_valence_shifters[["en"]])
sento_measures &lt;- sento_measures(corpus1, l, ctr)
sento_measuresNew &lt;- measures_update(sento_measures, corpus2, l)

</code></pre>

<hr>
<h2 id='merge.sentiment'>Merge sentiment objects horizontally and/or vertically</h2><span id='topic+merge.sentiment'></span>

<h3>Description</h3>

<p>Combines multiple <code>sentiment</code> objects with possibly different column names
into a new <code>sentiment</code> object. Here, too, any resulting <code>NA</code> values are converted to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sentiment'
merge(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge.sentiment_+3A_...">...</code></td>
<td>
<p><code>sentiment</code> objects to merge.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The new, combined, <code>sentiment</code> object, ordered by <code>"date"</code> and <code>"id"</code>.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l1 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
l2 &lt;- sento_lexicons(list_lexicons[c("FEEL_en_tr")])
l3 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en", "FEEL_en_tr")])

corp1 &lt;- sento_corpus(corpusdf = usnews[1:200, ])
corp2 &lt;- sento_corpus(corpusdf = usnews[201:450, ])
corp3 &lt;- sento_corpus(corpusdf = usnews[401:700, ])

s1 &lt;- compute_sentiment(corp1, l1, "proportionalPol")
s2 &lt;- compute_sentiment(corp2, l1, "counts")
s3 &lt;- compute_sentiment(corp3, l1, "counts")
s4 &lt;- compute_sentiment(corp2, l1, "counts", do.sentence = TRUE)
s5 &lt;- compute_sentiment(corp3, l2, "proportional", do.sentence = TRUE)
s6 &lt;- compute_sentiment(corp3, l1, "counts", do.sentence = TRUE)
s7 &lt;- compute_sentiment(corp3, l3, "UShaped", do.sentence = TRUE)

# straightforward row-wise merge
m1 &lt;- merge(s1, s2, s3)
nrow(m1) == 700 # TRUE

# another straightforward row-wise merge
m2 &lt;- merge(s4, s6)

# merge of sentence and non-sentence calculations
m3 &lt;- merge(s3, s6)

# different methods adds columns
m4 &lt;- merge(s4, s5)
nrow(m4) == nrow(m2) # TRUE

# different methods and weighting adds rows and columns
## rows are added only when the different weighting
## approach for a specific method gives other sentiment values
m5 &lt;- merge(s4, s7)
nrow(m5) &gt; nrow(m4) # TRUE

</code></pre>

<hr>
<h2 id='nmeasures'>Get number of sentiment measures</h2><span id='topic+nmeasures'></span>

<h3>Description</h3>

<p>Returns the number of sentiment measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmeasures(sento_measures)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nmeasures_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of sentiment measures in the input <code>sento_measures</code> object.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>

<hr>
<h2 id='nobs.sento_measures'>Get number of observations in the sentiment measures</h2><span id='topic+nobs.sento_measures'></span>

<h3>Description</h3>

<p>Returns the number of data points available in the sentiment measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
nobs(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nobs.sento_measures_+3A_object">object</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="nobs.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of rows (observations/data points) in <code>object[["measures"]]</code>.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>

<hr>
<h2 id='peakdates'>Extract dates related to sentiment time series peaks</h2><span id='topic+peakdates'></span>

<h3>Description</h3>

<p>This function extracts the dates for which aggregated time series sentiment is most
extreme (lowest, highest or both in absolute terms). The extracted dates are unique, even when,
for example, all most extreme sentiment values (for different sentiment measures) occur on only
one date.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peakdates(sento_measures, n = 10, type = "both", do.average = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peakdates_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="peakdates_+3A_n">n</code></td>
<td>
<p>a positive <code>numeric</code> value to indicate the number of dates associated to sentiment peaks to extract.
If <code>n &lt; 1</code>, it is interpreted as a quantile (for example, 0.07 would mean the 7% most extreme dates).</p>
</td></tr>
<tr><td><code id="peakdates_+3A_type">type</code></td>
<td>
<p>a <code>character</code> value, either <code>"pos"</code>, <code>"neg"</code> or <code>"both"</code>, respectively to look
for the <code>n</code> dates related to the most positive, most negative or most extreme (in absolute terms) sentiment
occurrences.</p>
</td></tr>
<tr><td><code id="peakdates_+3A_do.average">do.average</code></td>
<td>
<p>a <code>logical</code> to indicate whether peaks should be selected based on the average sentiment
value per date.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of type <code>"Date"</code> corresponding to the <code>n</code> extracted sentiment peak dates.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "month", lag = 3)
sento_measures &lt;- sento_measures(corpusSample, l, ctr)

# extract the peaks
peaksAbs &lt;- peakdates(sento_measures, n = 5)
peaksAbsQuantile &lt;- peakdates(sento_measures, n = 0.50)
peaksPos &lt;- peakdates(sento_measures, n = 5, type = "pos")
peaksNeg &lt;- peakdates(sento_measures, n = 5, type = "neg")

</code></pre>

<hr>
<h2 id='peakdocs'>Extract documents related to sentiment peaks</h2><span id='topic+peakdocs'></span>

<h3>Description</h3>

<p>This function extracts the documents with most extreme sentiment (lowest, highest or both
in absolute terms). The extracted documents are unique, even when, for example, all most extreme
sentiment values (across sentiment calculation methods) occur only for one document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peakdocs(sentiment, n = 10, type = "both", do.average = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peakdocs_+3A_sentiment">sentiment</code></td>
<td>
<p>a <code>sentiment</code> object created using <code><a href="#topic+compute_sentiment">compute_sentiment</a></code> or
<code><a href="#topic+as.sentiment">as.sentiment</a></code>.</p>
</td></tr>
<tr><td><code id="peakdocs_+3A_n">n</code></td>
<td>
<p>a positive <code>numeric</code> value to indicate the number of documents associated to sentiment
peaks to extract. If <code>n &lt; 1</code>, it is interpreted as a quantile (for example, 0.07 would mean the
7% most extreme documents).</p>
</td></tr>
<tr><td><code id="peakdocs_+3A_type">type</code></td>
<td>
<p>a <code>character</code> value, either <code>"pos"</code>, <code>"neg"</code> or <code>"both"</code>, respectively to look
for the <code>n</code> documents related to the most positive, most negative or most extreme (in absolute terms) sentiment
occurrences.</p>
</td></tr>
<tr><td><code id="peakdocs_+3A_do.average">do.average</code></td>
<td>
<p>a <code>logical</code> to indicate whether peaks should be selected based on the average sentiment
value per document.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of type <code>"character"</code> corresponding to the <code>n</code> extracted document identifiers.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(505)

data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])

corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 200)
sent &lt;- compute_sentiment(corpusSample, l, how = "proportionalPol")

# extract the peaks
peaksAbs &lt;- peakdocs(sent, n = 5)
peaksAbsQuantile &lt;- peakdocs(sent, n = 0.50)
peaksPos &lt;- peakdocs(sent, n = 5, type = "pos")
peaksNeg &lt;- peakdocs(sent, n = 5, type = "neg")

</code></pre>

<hr>
<h2 id='plot.attributions'>Plot prediction attributions at specified level</h2><span id='topic+plot.attributions'></span>

<h3>Description</h3>

<p>Shows a plot of the attributions along the dimension provided, stacked per date.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'attributions'
plot(x, group = "features", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.attributions_+3A_x">x</code></td>
<td>
<p>an <code>attributions</code> object created with <code><a href="#topic+attributions">attributions</a></code>.</p>
</td></tr>
<tr><td><code id="plot.attributions_+3A_group">group</code></td>
<td>
<p>a value from <code>c("lags", "lexicons", "features", "time")</code>.</p>
</td></tr>
<tr><td><code id="plot.attributions_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+sento_model">sento_model</a></code> for an elaborate modeling example including the calculation and plotting of
attributions. This function does not handle the plotting of the attribution of individual documents, since there are
often a lot of documents involved and they appear only once at one date (even though a document may contribute to
predictions at several dates, depending on the number of lags in the time aggregation).
</p>


<h3>Value</h3>

<p>Returns a simple <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object, which can be added onto (or to alter its default elements) by using
the <code>+</code> operator. By default, a legend is positioned at the top if the number of components of the
dimension is at maximum twelve.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>

<hr>
<h2 id='plot.sento_measures'>Plot sentiment measures</h2><span id='topic+plot.sento_measures'></span>

<h3>Description</h3>

<p>Plotting method that shows all sentiment measures from the provided <code>sento_measures</code>
object in one plot, or the average along one of the lexicons, features and time weighting dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
plot(x, group = "all", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="plot.sento_measures_+3A_group">group</code></td>
<td>
<p>a value from <code>c("lexicons", "features", "time", "all")</code>. The first three choices display the average of
all measures from the same group, in a different color. The choice <code>"all"</code> displays every single sentiment measure
in a separate color, but this may look visually overwhelming very fast, and can be quite slow.</p>
</td></tr>
<tr><td><code id="plot.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a simple <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object, which can be added onto (or to alter its default elements) by using
the <code>+</code> operator (see example). By default, a legend is positioned at the top if there are at maximum twelve line
graphs plotted and <code>group</code> is different from <code>"all"</code>.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'># construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = sentometrics::usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(sentometrics::list_lexicons[c("LM_en")],
                    sentometrics::list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "month", lag = 3)
sm &lt;- sento_measures(corpusSample, l, ctr)

# plot sentiment measures
plot(sm, "features")

## Not run: 
# adjust appearance of plot
library("ggplot2")
p &lt;- plot(sm)
p &lt;- p +
  scale_x_date(name = "year", date_labels = "%Y") +
  scale_y_continuous(name = "newName")
p
## End(Not run)

</code></pre>

<hr>
<h2 id='plot.sento_modelIter'>Plot iterative predictions versus realized values</h2><span id='topic+plot.sento_modelIter'></span>

<h3>Description</h3>

<p>Displays a plot of all predictions made through the iterative model computation as incorporated in the
input <code>sento_modelIter</code> object, as well as the corresponding true values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_modelIter'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sento_modelIter_+3A_x">x</code></td>
<td>
<p>a <code>sento_modelIter</code> object created using <code><a href="#topic+sento_model">sento_model</a></code>.</p>
</td></tr>
<tr><td><code id="plot.sento_modelIter_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+sento_model">sento_model</a></code> for an elaborate modeling example including the plotting of out-of-sample
performance.
</p>


<h3>Value</h3>

<p>Returns a simple <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object, which can be added onto (or to alter its default elements) by using
the <code>+</code> operator.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>

<hr>
<h2 id='predict.sento_model'>Make predictions from a sento_model object</h2><span id='topic+predict.sento_model'></span>

<h3>Description</h3>

<p>Prediction method for <code>sento_model</code> class, with usage along the lines of
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>, but simplified in terms of parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_model'
predict(object, newx, type = "response", offset = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sento_model_+3A_object">object</code></td>
<td>
<p>a <code>sento_model</code> object created with <code><a href="#topic+sento_model">sento_model</a></code>.</p>
</td></tr>
<tr><td><code id="predict.sento_model_+3A_newx">newx</code></td>
<td>
<p>a data <code>matrix</code> used for the prediction(s), row-by-row; see
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>. The number of columns should be equal to <code>sum(sento_model$nVar)</code>, being the
number of original sentiment measures and other variables. The variables discarded in the regression process are
dealt with within this function, based on <code>sento_model$discarded</code>.</p>
</td></tr>
<tr><td><code id="predict.sento_model_+3A_type">type</code></td>
<td>
<p>type of prediction required, a value from <code>c("link", "response", "class")</code>, see documentation for
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="predict.sento_model_+3A_offset">offset</code></td>
<td>
<p>not used.</p>
</td></tr>
<tr><td><code id="predict.sento_model_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A prediction output depending on the <code>type</code> argument.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>See Also</h3>

<p><code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>, <code><a href="#topic+sento_model">sento_model</a></code>
</p>

<hr>
<h2 id='scale.sento_measures'>Scaling and centering of sentiment measures</h2><span id='topic+scale.sento_measures'></span>

<h3>Description</h3>

<p>Scales and centers the sentiment measures from a <code>sento_measures</code> object, column-per-column. By default,
the measures are normalized. <code>NA</code>s are removed first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
scale(x, center = TRUE, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scale.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="scale.sento_measures_+3A_center">center</code></td>
<td>
<p>a <code>logical</code> or a <code>numeric</code> vector, see documentation for the generic <code><a href="base.html#topic+scale">scale</a></code>.
Alternatively, one can provide a <code>matrix</code> of dimensions <code>nobs(sento_measures)</code> times <code>1</code> or
<code>nmeasures(sento_measures)</code> with values to subtract from each individual observation.</p>
</td></tr>
<tr><td><code id="scale.sento_measures_+3A_scale">scale</code></td>
<td>
<p>a <code>logical</code> or a <code>numeric</code> vector, see documentation for the generic <code><a href="base.html#topic+scale">scale</a></code>.
Alternatively, one can provide a <code>matrix</code> of dimensions <code>nobs(sento_measures)</code> times <code>1</code> or
<code>nmeasures(sento_measures)</code> with values to divide each individual observation by.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If one of the arguments <code>center</code> or <code>scale</code> is a <code>matrix</code>, this operation will be applied first,
and eventual other centering or scaling is computed on that data.
</p>


<h3>Value</h3>

<p>A modified <code>sento_measures</code> object, with the measures replaced by the scaled measures as well as updated
statistics.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

set.seed(505)

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "year", lag = 3)
sento_measures &lt;- sento_measures(corpusSample, l, ctr)

# scale sentiment measures to zero mean and unit standard deviation
sc1 &lt;- scale(sento_measures)

n &lt;- nobs(sento_measures)
m &lt;- nmeasures(sento_measures)

# subtract a matrix
sc2 &lt;- scale(sento_measures, center = matrix(runif(n * m), n, m), scale = FALSE)

# divide every row observation based on a one-column matrix, then center
sc3 &lt;- scale(sento_measures, center = TRUE, scale = matrix(runif(n)))

</code></pre>

<hr>
<h2 id='sento_corpus'>Create a sento_corpus object</h2><span id='topic+sento_corpus'></span>

<h3>Description</h3>

<p>Formalizes a collection of texts into a <code>sento_corpus</code> object derived from the <span class="pkg">quanteda</span>
<code><a href="quanteda.html#topic+corpus">corpus</a></code> object. The <span class="pkg">quanteda</span> package provides a robust text mining infrastructure
(see their <a href="http://quanteda.io/index.html">website</a>), including a handy corpus manipulation toolset. This function
performs a set of checks on the input data and prepares the corpus for further analysis by structurally
integrating a date dimension and numeric metadata features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sento_corpus(corpusdf, do.clean = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sento_corpus_+3A_corpusdf">corpusdf</code></td>
<td>
<p>a <code>data.frame</code> (or a <code>data.table</code>, or a <code>tbl</code>) with as named columns: a document <code>"id"</code>
column (coercible to <code>character</code> mode), a <code>"date"</code> column (as <code>"yyyy-mm-dd"</code>), a <code>"texts"</code> column
(in <code>character</code> mode), an optional <code>"language"</code> column (in <code>character</code> mode), and a series of
feature columns of type <code>numeric</code>, with values between 0 and 1 to specify the degree of connectedness of
a feature to a document. Features could be for instance topics (e.g., legal or economic) or article sources (e.g., online or
print). When no feature column is provided, a feature named <code>"dummyFeature"</code>
is added. All spaces in the names of the features are replaced by <code>'_'</code>. Feature columns with values not
between 0 and 1 are rescaled column-wise.</p>
</td></tr>
<tr><td><code id="sento_corpus_+3A_do.clean">do.clean</code></td>
<td>
<p>a <code>logical</code>, if <code>TRUE</code> all texts undergo a cleaning routine to eliminate common textual garbage.
This includes a brute force replacement of HTML tags and non-alphanumeric characters by an empty string. To use with care
if the text is meant to have non-alphanumeric characters! Preferably, cleaning is done outside of this function call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>sento_corpus</code> object is a specialized instance of a <span class="pkg">quanteda</span> <code><a href="quanteda.html#topic+corpus">corpus</a></code>. Any
<span class="pkg">quanteda</span> function applicable to its <code><a href="quanteda.html#topic+corpus">corpus</a></code> object can also be applied to a <code>sento_corpus</code>
object. However, changing a given <code>sento_corpus</code> object too drastically using some of <span class="pkg">quanteda</span>'s functions might
alter the very structure the corpus is meant to have (as defined in the <code>corpusdf</code> argument) to be able to be used as
an input in other functions of the <span class="pkg">sentometrics</span> package. There are functions, including
<code><a href="quanteda.html#topic+corpus_sample">corpus_sample</a></code> or <code><a href="quanteda.html#topic+corpus_subset">corpus_subset</a></code>, that do not change the actual corpus
structure and may come in handy.
</p>
<p>To add additional features, use <code><a href="#topic+add_features">add_features</a></code>. Binary features are useful as
a mechanism to select the texts which have to be integrated in the respective feature-based sentiment measure(s), but
applies only when <code>do.ignoreZeros = TRUE</code>. Because of this (implicit) selection that can be performed, having
complementary features (e.g., <code>"economy"</code> and <code>"noneconomy"</code>) makes sense.
</p>
<p>It is also possible to add one non-numerical feature, that is, <code>"language"</code>, to designate the language
of the corpus texts. When this feature is provided, a <code>list</code> of lexicons for different
languages is expected in the <code>compute_sentiment</code> function.
</p>


<h3>Value</h3>

<p>A <code>sento_corpus</code> object, derived from a <span class="pkg">quanteda</span> <code><a href="quanteda.html#topic+corpus">corpus</a></code>
object. The corpus is ordered by date.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>See Also</h3>

<p><code><a href="quanteda.html#topic+corpus">corpus</a></code>, <code><a href="#topic+add_features">add_features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")

# corpus construction
corp &lt;- sento_corpus(corpusdf = usnews)

# take a random subset making use of quanteda
corpusSmall &lt;- quanteda::corpus_sample(corp, size = 500)

# deleting a feature
quanteda::docvars(corp, field = "wapo") &lt;- NULL

# deleting all features results in the addition of a dummy feature
quanteda::docvars(corp, field = c("economy", "noneconomy", "wsj")) &lt;- NULL

## Not run: 
# to add or replace features, use the add_features() function...
quanteda::docvars(corp, field = c("wsj", "new")) &lt;- 1
## End(Not run)

# corpus creation when no features are present
corpusDummy &lt;- sento_corpus(corpusdf = usnews[, 1:3])

# corpus creation with a qualitative language feature
usnews[["language"]] &lt;- "en"
usnews[["language"]][c(200:400)] &lt;- "nl"
corpusLang &lt;- sento_corpus(corpusdf = usnews)

</code></pre>

<hr>
<h2 id='sento_lexicons'>Set up lexicons (and valence word list) for use in sentiment analysis</h2><span id='topic+sento_lexicons'></span>

<h3>Description</h3>

<p>Structures provided lexicon(s) and optionally valence words. One can for example combine (part of) the
built-in lexicons from <code>data("list_lexicons")</code> with other lexicons, and add one of the built-in valence word lists
from <code>data("list_valence_shifters")</code>. This function makes the output coherent, by converting all words to
lowercase and checking for duplicates. All entries consisting of more than one word are discarded, as required for
bag-of-words sentiment analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sento_lexicons(lexiconsIn, valenceIn = NULL, do.split = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sento_lexicons_+3A_lexiconsin">lexiconsIn</code></td>
<td>
<p>a named <code>list</code> of (raw) lexicons, each element as a <code>data.table</code> or a <code>data.frame</code> with
respectively a <code>character</code> column (the words) and a <code>numeric</code> column (the polarity scores). This argument can be
one of the built-in lexicons accessible via <code>sentometrics::list_lexicons</code>.</p>
</td></tr>
<tr><td><code id="sento_lexicons_+3A_valencein">valenceIn</code></td>
<td>
<p>a single valence word list as a <code>data.table</code> or a <code>data.frame</code> with respectively a <code>"x"</code>
and a <code>"y"</code> or <code>"t"</code> column. The first column has the words, <code>"y"</code> has the values for bigram
shifting, and <code>"t"</code> has the types of the valence shifter for a clustered approach to sentiment calculation
(supported types: <code>1</code> = negators, <code>2</code> = amplifiers, <code>3</code> = deamplifiers, <code>4</code> = adversative conjunctions).
Type <code>4</code> is only used in a clusters-based sentence-level sentiment calculation.
If three columns are provided, only the first two will be considered. This argument can be one of the
built-in valence word lists accessible via <code>sentometrics::list_valence_shifters</code>. A word that appears in both a
lexicon and the valence word list is prioritized as a lexical entry during sentiment calculation. If
<code>NULL</code>, valence shifting is not applied in the sentiment analysis.</p>
</td></tr>
<tr><td><code id="sento_lexicons_+3A_do.split">do.split</code></td>
<td>
<p>a <code>logical</code> that if <code>TRUE</code> splits every lexicon into a separate positive polarity and negative
polarity lexicon.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of class <code>sento_lexicons</code> with each lexicon as a separate element according to its name, as a
<code>data.table</code>, and optionally an element named <code>valence</code> that comprises the valence words. Every <code>"x"</code> column
contains the words, every <code>"y"</code> column contains the scores. The <code>"t"</code> column for valence shifters
contains the different types.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# lexicons straight from built-in word lists
l1 &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])

# including a self-made lexicon, with and without valence shifters
lexIn &lt;- c(list(myLexicon = data.table::data.table(w = c("nice", "boring"), s = c(2, -1))),
           list_lexicons[c("GI_en")])
valIn &lt;- list_valence_shifters[["en"]]
l2 &lt;- sento_lexicons(lexIn)
l3 &lt;- sento_lexicons(lexIn, valIn)
l4 &lt;- sento_lexicons(lexIn, valIn[, c("x", "y")], do.split = TRUE)
l5 &lt;- sento_lexicons(lexIn, valIn[, c("x", "t")], do.split = TRUE)
l6 &lt;- l5[c("GI_en_POS", "valence")] # preserves sento_lexicons class

## Not run: 
# include lexicons from lexicon package
lexIn2 &lt;- list(hul = lexicon::hash_sentiment_huliu, joc = lexicon::hash_sentiment_jockers)
l7 &lt;- sento_lexicons(c(lexIn, lexIn2), valIn)
## End(Not run)

## Not run: 
# faulty extraction, no replacement allowed
l5["valence"]
l2[0]
l3[22]
l4[1] &lt;- l2[1]
l4[[1]] &lt;- l2[[1]]
l4$GI_en_NEG &lt;- l2$myLexicon
## End(Not run)

</code></pre>

<hr>
<h2 id='sento_measures'>One-way road towards a sento_measures object</h2><span id='topic+sento_measures'></span>

<h3>Description</h3>

<p>Wrapper function which assembles calls to <code><a href="#topic+compute_sentiment">compute_sentiment</a></code> and <code><a href="stats.html#topic+aggregate">aggregate</a></code>.
Serves as the most direct way towards a panel of textual sentiment measures as a <code>sento_measures</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sento_measures(sento_corpus, lexicons, ctr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sento_measures_+3A_sento_corpus">sento_corpus</code></td>
<td>
<p>a <code>sento_corpus</code> object created with <code><a href="#topic+sento_corpus">sento_corpus</a></code>.</p>
</td></tr>
<tr><td><code id="sento_measures_+3A_lexicons">lexicons</code></td>
<td>
<p>a <code>sentolexicons</code> object created with <code><a href="#topic+sento_lexicons">sento_lexicons</a></code>.</p>
</td></tr>
<tr><td><code id="sento_measures_+3A_ctr">ctr</code></td>
<td>
<p>output from a <code><a href="#topic+ctr_agg">ctr_agg</a></code> call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As a general rule, neither the names of the features, lexicons or time weighting schemes may contain
any &lsquo;-&rsquo; symbol.
</p>


<h3>Value</h3>

<p>A <code>sento_measures</code> object, which is a <code>list</code> containing:
</p>
<table>
<tr><td><code>measures</code></td>
<td>
<p>a <code>data.table</code> with a <code>"date"</code> column and all textual sentiment measures as remaining columns.</p>
</td></tr>
<tr><td><code>features</code></td>
<td>
<p>a <code>character</code> vector of the different features.</p>
</td></tr>
<tr><td><code>lexicons</code></td>
<td>
<p>a <code>character</code> vector of the different lexicons used.</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>a <code>character</code> vector of the different time weighting schemes used.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>a <code>data.frame</code> with some elementary statistics (mean, standard deviation, maximum, minimum, and
average correlation with the other measures) for each individual sentiment measure. In all computations, NAs are
removed first.</p>
</td></tr>
<tr><td><code>sentiment</code></td>
<td>
<p>the document-level sentiment scores <code>data.table</code> with <code>"date"</code>,
<code>"word_count"</code> and lexicon-feature sentiment scores columns. The <code>"date"</code> column has the
dates converted at the frequency for across-document aggregation. All zeros are replaced by <code>NA</code>
if <code>ctr$docs$weightingParam$do.ignoreZeros = TRUE</code>.</p>
</td></tr>
<tr><td><code>attribWeights</code></td>
<td>
<p>a <code>list</code> of document and time weights used in the <code><a href="#topic+attributions">attributions</a></code> function.
Serves further no direct purpose.</p>
</td></tr>
<tr><td><code>ctr</code></td>
<td>
<p>a <code>list</code> encapsulating the control parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compute_sentiment">compute_sentiment</a></code>, <code><a href="stats.html#topic+aggregate">aggregate</a></code>, <code><a href="#topic+measures_update">measures_update</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])
ctr &lt;- ctr_agg(howWithin = "counts",
               howDocs = "proportional",
               howTime = c("equal_weight", "linear", "almon"),
               by = "month",
               lag = 3,
               ordersAlm = 1:3,
               do.inverseAlm = TRUE)
sento_measures &lt;- sento_measures(corpusSample, l, ctr)
summary(sento_measures)

</code></pre>

<hr>
<h2 id='sento_model'>Optimized and automated sentiment-based sparse regression</h2><span id='topic+sento_model'></span>

<h3>Description</h3>

<p>Linear or nonlinear penalized regression of any dependent variable on the wide number of sentiment measures and
potentially other explanatory variables. Either performs a regression given the provided variables at once, or computes
regressions sequentially for a given sample size over a longer time horizon, with associated prediction performance metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sento_model(sento_measures, y, x = NULL, ctr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sento_model_+3A_sento_measures">sento_measures</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="sento_model_+3A_y">y</code></td>
<td>
<p>a one-column <code>data.frame</code> or a <code>numeric</code> vector capturing the dependent (response) variable. In case of
a logistic regression, the response variable is either a <code>factor</code> or a <code>matrix</code> with the factors represented by
the columns as binary indicators, with the second factor level or column as the reference class in case of a binomial
regression. No <code>NA</code> values are allowed.</p>
</td></tr>
<tr><td><code id="sento_model_+3A_x">x</code></td>
<td>
<p>a named <code>data.table</code>, <code>data.frame</code> or <code>matrix</code> with other explanatory variables as <code>numeric</code>, by
default set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sento_model_+3A_ctr">ctr</code></td>
<td>
<p>output from a <code><a href="#topic+ctr_model">ctr_model</a></code> call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Models are computed using the elastic net regularization as implemented in the <span class="pkg">glmnet</span> package, to account for
the multidimensionality of the sentiment measures. Independent variables are normalized in the regression process, but
coefficients are returned in their original space. For a helpful introduction to <span class="pkg">glmnet</span>, we refer to their
<a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin">vignette</a>. The optimal elastic net parameters
<code>lambda</code> and <code>alpha</code> are calibrated either through a to specify information criterion or through
cross-validation (based on the &quot;rolling forecasting origin&quot; principle, using the <code><a href="caret.html#topic+train">train</a></code> function).
In the latter case, the training metric is automatically set to <code>"RMSE"</code> for a linear model and to <code>"Accuracy"</code>
for a logistic model. We suppress many of the details that can be supplied to the <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> and
<code><a href="caret.html#topic+train">train</a></code> functions we rely on, for the sake of user-friendliness.
</p>


<h3>Value</h3>

<p>If <code>ctr$do.iter = FALSE</code>, a <code>sento_model</code> object which is a <code>list</code> containing:
</p>
<table>
<tr><td><code>reg</code></td>
<td>
<p>optimized regression, i.e., a model-specific <span class="pkg">glmnet</span> object, including for example the estimated
coefficients.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the input argument <code>ctr$model</code>, to indicate the type of model estimated.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>calibrated alpha.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>calibrated lambda.</p>
</td></tr>
<tr><td><code>trained</code></td>
<td>
<p>output from <code><a href="caret.html#topic+train">train</a></code> call (if <code>ctr$type =</code> &quot;<code>cv</code>&quot;). There is no such
output if the control parameters <code>alphas</code> and <code>lambdas</code> both specify one value.</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>a <code>list</code> composed of two elements: under <code>"criterion"</code>, the type of information criterion used in the
calibration, and under <code>"matrix"</code>, a <code>matrix</code> of all information criterion values for <code>alphas</code> as rows
and the respective lambda values as columns (if <code>ctr$type != </code> &quot;<code>cv</code>&quot;). Any <code>NA</code> value in the latter
element means the specific information criterion could not be computed.</p>
</td></tr>
<tr><td><code>dates</code></td>
<td>
<p>sample reference dates as a two-element <code>character</code> vector, being the earliest and most recent date from
the <code>sento_measures</code> object accounted for in the estimation window.</p>
</td></tr>
<tr><td><code>nVar</code></td>
<td>
<p>a vector of size two, with respectively the number of sentiment measures, and the number of other explanatory
variables inputted.</p>
</td></tr>
<tr><td><code>discarded</code></td>
<td>
<p>a named <code>logical</code> vector of length equal to the number of sentiment measures, in which <code>TRUE</code>
indicates that the particular sentiment measure has not been considered in the regression process. A sentiment measure is
not considered when it is a duplicate of another, or when at least 50% of the observations are equal to zero.</p>
</td></tr>
</table>
<p>If <code>ctr$do.iter = TRUE</code>, a <code>sento_modelIter</code> object which is a <code>list</code> containing:
</p>
<table>
<tr><td><code>models</code></td>
<td>
<p>all sparse regressions, i.e., separate <code>sento_model</code> objects as above, as a <code>list</code> with as names the
dates from the perspective of the sentiment measures at which the out-of-sample predictions are carried out.</p>
</td></tr>
<tr><td><code>alphas</code></td>
<td>
<p>calibrated alphas.</p>
</td></tr>
<tr><td><code>lambdas</code></td>
<td>
<p>calibrated lambdas.</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>
<p>a <code>data.frame</code> with performance-related measures, being &quot;<code>RMSFE</code>&quot; (root mean squared
forecasting error), &quot;<code>MAD</code>&quot; (mean absolute deviation), &quot;<code>MDA</code>&quot; (mean directional accuracy, in which's calculation
zero is considered as a positive; in p.p.), &quot;<code>accuracy</code>&quot; (proportion of correctly predicted classes in case
of a logistic regression; in p.p.), and each's respective individual values in the sample. Directional accuracy
is measured by comparing the change in the realized response with the change in the prediction between two consecutive time
points (omitting the very first prediction as <code>NA</code>). Only the relevant performance statistics are given
depending on the type of regression. Dates are as in the <code>"models"</code> output element, i.e., from the perspective of the
sentiment measures.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Samuel Borms, Keven Bluteau
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctr_model">ctr_model</a></code>, <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>, <code><a href="caret.html#topic+train">train</a></code>, <code><a href="#topic+attributions">attributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")
data("epu", package = "sentometrics")

set.seed(505)

# construct a sento_measures object to start with
corpusAll &lt;- sento_corpus(corpusdf = usnews)
corpus &lt;- quanteda::corpus_subset(corpusAll, date &gt;= "2004-01-01")
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
ctr &lt;- ctr_agg(howWithin = "counts", howDocs = "proportional",
               howTime = c("equal_weight", "linear"),
               by = "month", lag = 3)
sento_measures &lt;- sento_measures(corpus, l, ctr)

# prepare y and other x variables
y &lt;- epu[epu$date %in% get_dates(sento_measures), "index"]
length(y) == nobs(sento_measures) # TRUE
x &lt;- data.frame(runif(length(y)), rnorm(length(y))) # two other (random) x variables
colnames(x) &lt;- c("x1", "x2")

# a linear model based on the Akaike information criterion
ctrIC &lt;- ctr_model(model = "gaussian", type = "AIC", do.iter = FALSE, h = 4,
                   do.difference = TRUE)
out1 &lt;- sento_model(sento_measures, y, x = x, ctr = ctrIC)

# attribution and prediction as post-analysis
attributions1 &lt;- attributions(out1, sento_measures,
                              refDates = get_dates(sento_measures)[20:25])
plot(attributions1, "features")

nx &lt;- nmeasures(sento_measures) + ncol(x)
newx &lt;- runif(nx) * cbind(data.table::as.data.table(sento_measures)[, -1], x)[30:40, ]
preds &lt;- predict(out1, newx = as.matrix(newx), type = "link")

# an iterative out-of-sample analysis, parallelized
ctrIter &lt;- ctr_model(model = "gaussian", type = "BIC", do.iter = TRUE, h = 3,
                     oos = 2, alphas = c(0.25, 0.75), nSample = 75, nCore = 2)
out2 &lt;- sento_model(sento_measures, y, x = x, ctr = ctrIter)
summary(out2)

# plot predicted vs. realized values
p &lt;- plot(out2)
p

# a cross-validation based model, parallelized
cl &lt;- parallel::makeCluster(2)
doParallel::registerDoParallel(cl)
ctrCV &lt;- ctr_model(model = "gaussian", type = "cv", do.iter = FALSE,
                   h = 0, alphas = c(0.10, 0.50, 0.90), trainWindow = 70,
                   testWindow = 10, oos = 0, do.progress = TRUE)
out3 &lt;- sento_model(sento_measures, y, x = x, ctr = ctrCV)
parallel::stopCluster(cl)
foreach::registerDoSEQ()
summary(out3)

# a cross-validation based model for a binomial target
yb &lt;- epu[epu$date %in% get_dates(sento_measures), "above"]
ctrCVb &lt;- ctr_model(model = "binomial", type = "cv", do.iter = FALSE,
                    h = 0, alphas = c(0.10, 0.50, 0.90), trainWindow = 70,
                    testWindow = 10, oos = 0, do.progress = TRUE)
out4 &lt;- sento_model(sento_measures, yb, x = x, ctr = ctrCVb)
summary(out4)
## End(Not run)

</code></pre>

<hr>
<h2 id='sentometrics-defunct'>Defunct functions</h2><span id='topic+sentometrics-defunct'></span><span id='topic+ctr_merge'></span><span id='topic+perform_MCS'></span><span id='topic+fill_measures'></span><span id='topic+merge_measures'></span><span id='topic+to_global'></span><span id='topic+subset_measures'></span><span id='topic+select_measures'></span><span id='topic+setup_lexicons'></span><span id='topic+retrieve_attributions'></span><span id='topic+perform_agg'></span><span id='topic+plot_attributions'></span><span id='topic+almons'></span><span id='topic+exponentials'></span><span id='topic+to_sentocorpus'></span><span id='topic+to_sentiment'></span><span id='topic+get_measures'></span><span id='topic+measures_subset'></span><span id='topic+measures_select'></span><span id='topic+measures_delete'></span><span id='topic+sentiment_bind'></span><span id='topic+measures_merge'></span><span id='topic+measures_global'></span><span id='topic+sento_app'></span>

<h3>Description</h3>

<p>Functions defunct due to changed naming or because functionality is discarded. See the NEWS file for more information
about since when or why functions have been defunct.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctr_merge(...)

perform_MCS(...)

fill_measures(...)

merge_measures(...)

to_global(...)

subset_measures(...)

select_measures(...)

setup_lexicons(...)

retrieve_attributions(...)

perform_agg(...)

plot_attributions(...)

almons(...)

exponentials(...)

to_sentocorpus(...)

to_sentiment(...)

get_measures(...)

measures_subset(...)

measures_select(...)

measures_delete(...)

sentiment_bind(...)

measures_merge(...)

measures_global(...)

sento_app(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sentometrics-defunct_+3A_...">...</code></td>
<td>
<p>allowed input arguments.</p>
</td></tr>
</table>

<hr>
<h2 id='sentometrics-deprecated'>Deprecated functions</h2><span id='topic+sentometrics-deprecated'></span>

<h3>Description</h3>

<p>Functions deprecated due to changed naming or because functionality is discarded. The general (but not
blindly followed) rule is that deprecated functions are made defunct every 1 major or every 2 minor
package updates. See the NEWS file for more information about since when or why functions have been
deprecated.
</p>

<hr>
<h2 id='sentometrics-package'>sentometrics: An Integrated Framework for Textual Sentiment Time Series Aggregation and Prediction</h2><span id='topic+sentometrics'></span><span id='topic+sentometrics-package'></span>

<h3>Description</h3>

<p>The <span class="pkg">sentometrics</span> package is an integrated framework for textual sentiment time series
aggregation and prediction. It accounts for the intrinsic challenge that, for a given text, sentiment can
be computed in many different ways, as well as the large number of possibilities to pool sentiment across
texts and time. This additional layer of manipulation does not exist in standard text mining and time series
analysis packages. The package therefore integrates the fast <em>quantification</em> of sentiment from texts,
the <em>aggregation</em> into different sentiment time series and the optimized <em>prediction</em> based on
these measures.
</p>


<h3>Main functions</h3>


<ul>
<li><p> Corpus (features) generation: <code><a href="#topic+sento_corpus">sento_corpus</a></code>, <code><a href="#topic+add_features">add_features</a></code>,
<code><a href="#topic+as.sento_corpus">as.sento_corpus</a></code>
</p>
</li>
<li><p> Sentiment computation and aggregation into sentiment measures: <code><a href="#topic+ctr_agg">ctr_agg</a></code>,
<code><a href="#topic+sento_lexicons">sento_lexicons</a></code>, <code><a href="#topic+compute_sentiment">compute_sentiment</a></code>, <code><a href="#topic+aggregate.sentiment">aggregate.sentiment</a></code>,
<code><a href="#topic+as.sentiment">as.sentiment</a></code>, <code><a href="#topic+sento_measures">sento_measures</a></code>, <code><a href="#topic+peakdocs">peakdocs</a></code>,
<code><a href="#topic+peakdates">peakdates</a></code>, <code><a href="#topic+aggregate.sento_measures">aggregate.sento_measures</a></code>
</p>
</li>
<li><p> Sparse modeling: <code><a href="#topic+ctr_model">ctr_model</a></code>, <code><a href="#topic+sento_model">sento_model</a></code>
</p>
</li>
<li><p> Prediction and post-modeling analysis: <code><a href="#topic+predict.sento_model">predict.sento_model</a></code>,
<code><a href="#topic+attributions">attributions</a></code>
</p>
</li></ul>



<h3>Note</h3>

<p>Please cite the package in publications. Use <code>citation("sentometrics")</code>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Samuel Borms <a href="mailto:borms_sam@hotmail.com">borms_sam@hotmail.com</a> (<a href="https://orcid.org/0000-0001-9533-1870">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> David Ardia <a href="mailto:david.ardia@hec.ca">david.ardia@hec.ca</a> (<a href="https://orcid.org/0000-0003-2823-782X">ORCID</a>)
</p>
</li>
<li><p> Keven Bluteau <a href="mailto:keven.bluteau@unine.ch">keven.bluteau@unine.ch</a> (<a href="https://orcid.org/0000-0003-2990-4807">ORCID</a>)
</p>
</li>
<li><p> Kris Boudt <a href="mailto:kris.boudt@vub.be">kris.boudt@vub.be</a> (<a href="https://orcid.org/0000-0002-1000-5142">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Jeroen Van Pelt <a href="mailto:jeroenvanpelt@hotmail.com">jeroenvanpelt@hotmail.com</a> [contributor]
</p>
</li>
<li><p> Andres Algaba <a href="mailto:andres.algaba@vub.be">andres.algaba@vub.be</a> [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Ardia, Bluteau, Borms and Boudt (2021). <strong>The R Package sentometrics to Compute, Aggregate, and
Predict with Textual Sentiment</strong>. <em>Journal of Statistical Software 99(2), 1-40</em>,
doi: <a href="https://doi.org/10.18637/jss.v099.i02">10.18637/jss.v099.i02</a>.
</p>
<p>Ardia, Bluteau and Boudt (2019). <strong>Questioning the news about economic growth: Sparse forecasting using
thousands of news-based sentiment values</strong>. <em>International Journal of Forecasting 35, 1370-1386</em>,
doi: <a href="https://doi.org/10.1016/j.ijforecast.2018.10.010">10.1016/j.ijforecast.2018.10.010</a>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://sentometrics-research.com/sentometrics/">https://sentometrics-research.com/sentometrics/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/SentometricsResearch/sentometrics/issues">https://github.com/SentometricsResearch/sentometrics/issues</a>
</p>
</li></ul>


<hr>
<h2 id='subset.sento_measures'>Subset sentiment measures</h2><span id='topic+subset.sento_measures'></span>

<h3>Description</h3>

<p>Subsets rows of the sentiment measures based on its columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sento_measures'
subset(x, subset = NULL, select = NULL, delete = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset.sento_measures_+3A_x">x</code></td>
<td>
<p>a <code>sento_measures</code> object created using <code><a href="#topic+sento_measures">sento_measures</a></code>.</p>
</td></tr>
<tr><td><code id="subset.sento_measures_+3A_subset">subset</code></td>
<td>
<p>a logical (non-<code>character</code>) expression indicating the rows to keep. If a
<code>numeric</code> input is given, it is used for row index subsetting.</p>
</td></tr>
<tr><td><code id="subset.sento_measures_+3A_select">select</code></td>
<td>
<p>a <code>character</code> vector of the lexicon, feature and time weighting scheme names, to indicate which
measures need to be selected, or as a <code>list</code> of <code>character</code> vectors, possibly with separately specified
combinations (consisting of one unique lexicon, one unique feature, and one unique time weighting scheme at maximum).</p>
</td></tr>
<tr><td><code id="subset.sento_measures_+3A_delete">delete</code></td>
<td>
<p>see the <code>select</code> argument, but to delete measures.</p>
</td></tr>
<tr><td><code id="subset.sento_measures_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified <code>sento_measures</code> object, with only the remaining rows and sentiment measures,
including updated information and statistics, but the original sentiment scores <code>data.table</code> untouched.
</p>


<h3>Author(s)</h3>

<p>Samuel Borms
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# construct a sento_measures object to start with
corpus &lt;- sento_corpus(corpusdf = usnews)
corpusSample &lt;- quanteda::corpus_sample(corpus, size = 500)
l &lt;- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
ctr &lt;- ctr_agg(howTime = c("equal_weight", "linear"), by = "year", lag = 3)
sm &lt;- sento_measures(corpusSample, l, ctr)

# three specified indices in required list format
three &lt;- as.list(
  stringi::stri_split(c("LM_en--economy--linear",
                        "HENRY_en--wsj--equal_weight",
                        "HENRY_en--wapo--equal_weight"),
                      regex = "--")
)

# different subsets
sub1 &lt;- subset(sm, HENRY_en--economy--equal_weight &gt;= 0.01)
sub2 &lt;- subset(sm, date %in% get_dates(sm)[3:12])
sub3 &lt;- subset(sm, 3:12)
sub4 &lt;- subset(sm, 1:100) # warning

# different selections
sel1 &lt;- subset(sm, select = "equal_weight")
sel2 &lt;- subset(sm, select = c("equal_weight", "linear"))
sel3 &lt;- subset(sm, select = c("linear", "LM_en"))
sel4 &lt;- subset(sm, select = list(c("linear", "wsj"), c("linear", "economy")))
sel5 &lt;- subset(sm, select = three)

# different deletions
del1 &lt;- subset(sm, delete = "equal_weight")
del2 &lt;- subset(sm, delete = c("linear", "LM_en"))
del3 &lt;- subset(sm, delete = list(c("linear", "wsj"), c("linear", "economy")))
del4 &lt;- subset(sm, delete = c("equal_weight", "linear")) # warning
del5 &lt;- subset(sm, delete = three)

</code></pre>

<hr>
<h2 id='usnews'>Texts (not) relevant to the U.S. economy</h2><span id='topic+usnews'></span>

<h3>Description</h3>

<p>A collection of texts annotated by humans in terms of relevance to the U.S. economy or not. The texts come from two major
journals in the U.S. (The Wall Street Journal and The Washington Post) and cover 4145 documents between 1995 and 2014. It
contains following information:
</p>

<ul>
<li><p> id. A <code>character</code> ID identifier.
</p>
</li>
<li><p> date. Date as <code>"yyyy-mm-dd"</code>.
</p>
</li>
<li><p> texts. Texts in <code>character</code> format.
</p>
</li>
<li><p> wsj. Equals 1 if the article comes from The Wall Street Journal.
</p>
</li>
<li><p> wapo. Equals 1 if the article comes from The Washington Post (complementary to &lsquo;wsj&rsquo;).
</p>
</li>
<li><p> economy. Equals 1 if the article is relevant to the U.S. economy.
</p>
</li>
<li><p> noneconomy. Equals 1 if the article is not relevant to the U.S. economy (complementary to &lsquo;economy&rsquo;).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data("usnews")
</code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code>, formatted as required to be an input for <code><a href="#topic+sento_corpus">sento_corpus</a></code>.
</p>


<h3>Source</h3>

<p><a href="https://appen.com/open-source-datasets/">Economic News Article Tone and Relevance</a>. Retrieved
November 1, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("usnews", package = "sentometrics")
usnews[3192, "texts"]
usnews[1:5, c("id", "date", "texts")]

</code></pre>

<hr>
<h2 id='weights_almon'>Compute Almon polynomials</h2><span id='topic+weights_almon'></span>

<h3>Description</h3>

<p>Computes Almon polynomial weighting curves. Handy to self-select specific time aggregation weighting schemes
for input in <code><a href="#topic+ctr_agg">ctr_agg</a></code> using the <code>weights</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights_almon(n, orders = 1:3, do.inverse = TRUE, do.normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights_almon_+3A_n">n</code></td>
<td>
<p>a single <code>numeric</code> to indicate the lag length (cf., <em>n</em>).</p>
</td></tr>
<tr><td><code id="weights_almon_+3A_orders">orders</code></td>
<td>
<p>a <code>numeric</code> vector as the sequence of the Almon orders (cf., <em>r</em>). The maximum value
corresponds to <em>R</em>.</p>
</td></tr>
<tr><td><code id="weights_almon_+3A_do.inverse">do.inverse</code></td>
<td>
<p><code>TRUE</code> if the inverse Almon polynomials should be calculated as well.</p>
</td></tr>
<tr><td><code id="weights_almon_+3A_do.normalize">do.normalize</code></td>
<td>
<p>a <code>logical</code>, if <code>TRUE</code> weights are normalized to unity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Almon polynomial formula implemented is:
<code class="reqn">(1 - (1 - i/n)^{r})(1 - i/n)^{R - r}</code>, where <code class="reqn">i</code> is the lag index ordered from
1 to <code class="reqn">n</code>. The inverse is computed by changing <code class="reqn">i/n</code> to <code class="reqn">1 - i/n</code>.
</p>


<h3>Value</h3>

<p>A <code>data.frame</code> of all Almon polynomial weighting curves, of size <code>length(orders)</code> (times two if
<code>do.inverse = TRUE</code>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctr_agg">ctr_agg</a></code>
</p>

<hr>
<h2 id='weights_beta'>Compute Beta weighting curves</h2><span id='topic+weights_beta'></span>

<h3>Description</h3>

<p>Computes Beta weighting curves as in Ghysels, Sinko and Valkanov (2007). Handy to self-select specific
time aggregation weighting schemes for input in <code><a href="#topic+ctr_agg">ctr_agg</a></code> using the <code>weights</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights_beta(n, a = 1:4, b = 1:4, do.normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights_beta_+3A_n">n</code></td>
<td>
<p>a single <code>numeric</code> to indicate the lag length (cf., <em>n</em>).</p>
</td></tr>
<tr><td><code id="weights_beta_+3A_a">a</code></td>
<td>
<p>a <code>numeric</code> as the first parameter (cf., <em>a</em>).</p>
</td></tr>
<tr><td><code id="weights_beta_+3A_b">b</code></td>
<td>
<p>a <code>numeric</code> as the second parameter (cf., <em>b</em>).</p>
</td></tr>
<tr><td><code id="weights_beta_+3A_do.normalize">do.normalize</code></td>
<td>
<p>a <code>logical</code>, if <code>TRUE</code> weights are normalized to unity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Beta weighting abides by following formula:
<code class="reqn">f(i/n; a, b) / \sum_{i}(i/n; a, b)</code>, where <code class="reqn">i</code> is the lag index ordered
from 1 to <code class="reqn">n</code>, <code class="reqn">a</code> and <code class="reqn">b</code> are two decay parameters, and
<code class="reqn">f(x; a, b) = (x^{a - 1}(1 - x)^{b - 1}\Gamma(a + b)) / (\Gamma(a)\Gamma(b))</code>, where <code class="reqn">\Gamma(.)</code> is
the <code><a href="base.html#topic+gamma">gamma</a></code> function.
</p>


<h3>Value</h3>

<p>A <code>data.frame</code> of beta weighting curves per combination of <code>a</code> and <code>b</code>. If <code>n = 1</code>,
all weights are set to 1.
</p>


<h3>References</h3>

<p>Ghysels, Sinko and Valkanov (2007). <strong>MIDAS regressions: Further results and new directions</strong>.
<em>Econometric Reviews 26, 53-90</em>, doi: <a href="https://doi.org/10.1080/07474930600972467">10.1080/07474930600972467</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctr_agg">ctr_agg</a></code>
</p>

<hr>
<h2 id='weights_exponential'>Compute exponential weighting curves</h2><span id='topic+weights_exponential'></span>

<h3>Description</h3>

<p>Computes exponential weighting curves. Handy to self-select specific time aggregation weighting schemes
for input in <code><a href="#topic+ctr_agg">ctr_agg</a></code> using the <code>weights</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights_exponential(
  n,
  alphas = seq(0.1, 0.5, by = 0.1),
  do.inverse = FALSE,
  do.normalize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights_exponential_+3A_n">n</code></td>
<td>
<p>a single <code>numeric</code> to indicate the lag length.</p>
</td></tr>
<tr><td><code id="weights_exponential_+3A_alphas">alphas</code></td>
<td>
<p>a <code>numeric</code> vector of decay factors, between 0 and 1, but multiplied by 10 in
the implementation.</p>
</td></tr>
<tr><td><code id="weights_exponential_+3A_do.inverse">do.inverse</code></td>
<td>
<p><code>TRUE</code> if the inverse exponential curves should be calculated as well.</p>
</td></tr>
<tr><td><code id="weights_exponential_+3A_do.normalize">do.normalize</code></td>
<td>
<p>a <code>logical</code>, if <code>TRUE</code> weights are normalized to unity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.frame</code> of exponential weighting curves per value of <code>alphas</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctr_agg">ctr_agg</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
