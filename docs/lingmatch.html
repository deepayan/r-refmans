<!DOCTYPE html><html lang="en"><head><title>Help for package lingmatch</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lingmatch}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dictionary_meta'><p>Assess Dictionary Categories Within a Latent Semantic Space</p></a></li>
<li><a href='#download.dict'><p>Download Dictionaries</p></a></li>
<li><a href='#download.lspace'><p>Download Latent Semantic Spaces</p></a></li>
<li><a href='#lingmatch'><p>Linguistic Matching and Accommodation</p></a></li>
<li><a href='#lma_dict'><p>English Function Word Category and Special Character Lists</p></a></li>
<li><a href='#lma_dtm'><p>Document-Term Matrix Creation</p></a></li>
<li><a href='#lma_initdirs'><p>Initialize Directories for Dictionaries and Latent Semantic Spaces</p></a></li>
<li><a href='#lma_lspace'><p>Latent Semantic Space (Embeddings) Operations</p></a></li>
<li><a href='#lma_meta'><p>Calculate Text-Based Metastatistics</p></a></li>
<li><a href='#lma_patcat'><p>Categorize Texts</p></a></li>
<li><a href='#lma_process'><p>Process Text</p></a></li>
<li><a href='#lma_simets'><p>Similarity Calculations</p></a></li>
<li><a href='#lma_termcat'><p>Document-Term Matrix Categorization</p></a></li>
<li><a href='#lma_weight'><p>Document-Term Matrix Weighting</p></a></li>
<li><a href='#read.dic'><p>Read/Write Dictionary Files</p></a></li>
<li><a href='#read.segments'><p>Read and Segment Multiple Texts</p></a></li>
<li><a href='#report_term_matches'><p>Generate a Report of Term Matches</p></a></li>
<li><a href='#select.dict'><p>Select Dictionaries</p></a></li>
<li><a href='#select.lspace'><p>Select Latent Semantic Spaces</p></a></li>
<li><a href='#standardize.lspace'><p>Standardize a Latent Semantic Space</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Linguistic Matching and Accommodation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.7</td>
</tr>
<tr>
<td>Author:</td>
<td>Micah Iserman</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Micah Iserman &lt;micah.iserman@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Measure similarity between texts. Offers a variety of processing
  tools and similarity metrics to facilitate flexible representation of texts and matching.
  Implements forms of Language Style Matching (Ireland &amp; Pennebaker, 2010) &lt;<a href="https://doi.org/10.1037%2Fa0020386">doi:10.1037/a0020386</a>&gt;
  and Latent Semantic Analysis (Landauer &amp; Dumais, 1997) &lt;<a href="https://doi.org/10.1037%2F0033-295X.104.2.211">doi:10.1037/0033-295X.104.2.211</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://miserman.github.io/lingmatch/">https://miserman.github.io/lingmatch/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/miserman/lingmatch/issues">https://github.com/miserman/lingmatch/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5), methods, Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, RcppParallel</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, splot, testthat (&ge; 2.1.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppParallel, BH</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-03 16:17:58 UTC; Admin</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-03 16:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='dictionary_meta'>Assess Dictionary Categories Within a Latent Semantic Space</h2><span id='topic+dictionary_meta'></span>

<h3>Description</h3>

<p>Assess Dictionary Categories Within a Latent Semantic Space
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictionary_meta(dict, space = "auto", n_spaces = 5, suggest = FALSE,
  suggestion_terms = 10, suggest_stopwords = FALSE,
  suggest_discriminate = TRUE, expand_cutoff_freq = 0.98,
  expand_cutoff_spaces = 10, dimension_prop = 1, pairwise = TRUE,
  glob = TRUE, space_dir = getOption("lingmatch.lspace.dir"),
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dictionary_meta_+3A_dict">dict</code></td>
<td>
<p>A vector of terms, list of such vectors, or a matrix-like object to be
categorized by <code><a href="#topic+read.dic">read.dic</a></code>.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_space">space</code></td>
<td>
<p>A vector space used to calculate similarities between terms.
Names of spaces (see <code><a href="#topic+select.lspace">select.lspace</a></code>), a matrix with terms as row names, or
<code>"auto"</code> to auto-select a space based on matched terms. This can also be <code>multi</code>
to use multiple spaces, which are combined after similarities are calculated.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_n_spaces">n_spaces</code></td>
<td>
<p>Number of spaces to draw from if <code>space</code> is <code>multi</code>.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_suggest">suggest</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will search for other terms for possible inclusion
in <code>space</code>.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_suggestion_terms">suggestion_terms</code></td>
<td>
<p>Number of terms to use when selecting suggested additions.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_suggest_stopwords">suggest_stopwords</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will suggest function words.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_suggest_discriminate">suggest_discriminate</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will adjust for similarity to other
categories when finding suggestions.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_expand_cutoff_freq">expand_cutoff_freq</code></td>
<td>
<p>Proportion of mapped terms to include when expanding dictionary terms.
Applies when <code>space</code> is a character (referring to a space to be loaded).</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_expand_cutoff_spaces">expand_cutoff_spaces</code></td>
<td>
<p>Number of spaces in which a term has to appear to be considered
for expansion. Applies when <code>space</code> is a character (referring to a space to be loaded).</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_dimension_prop">dimension_prop</code></td>
<td>
<p>Proportion of dimensions to use when searching for suggested additions,
where less than 1 will calculate similarities to the category core using fewer dimensions
of the space.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_pairwise">pairwise</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will compare candidate suggestion terms with a single,
averaged category vector rather than all category terms separately.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_glob">glob</code></td>
<td>
<p>Logical; if <code>TRUE</code>, converts globs (asterisk wildcards) to regular expressions.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_space_dir">space_dir</code></td>
<td>
<p>Directory from which <code>space</code> should be loaded.</p>
</td></tr>
<tr><td><code id="dictionary_meta_+3A_verbose">verbose</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will not show status messages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list:
</p>

<ul>
<li> <p><strong><code>expanded</code></strong>: A version of <code>dict</code> with fuzzy terms expanded.
</p>
</li>
<li> <p><strong><code>summary</code></strong>: A summary of each dictionary category.
</p>
</li>
<li> <p><strong><code>terms</code></strong>: Match (expanded term) similarities within terms and categories.
</p>
</li>
<li> <p><strong><code>suggested</code></strong>: If <code>suggest</code> is <code>TRUE</code>, a list with suggested
additions for each dictionary category. Each entry is a named numeric vector with
similarities for each suggested term.
</p>
</li></ul>



<h3>See Also</h3>

<p>To just expand fuzzy terms, see <code><a href="#topic+report_term_matches">report_term_matches</a>()</code>.
</p>
<p>Similar information is provided in the <a href="https://miserman.github.io/dictionary_builder/">dictionary builder</a> web tool.
</p>
<p>Other Dictionary functions: 
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (dir.exists("~/Latent Semantic Spaces")) {
  dict &lt;- list(
    furniture = c("table", "chair", "desk*", "couch*", "sofa*"),
    well_adjusted = c("happy", "bright*", "friend*", "she", "he", "they")
  )
  dictionary_meta(dict, space_dir = "~/Latent Semantic Spaces")
}
</code></pre>

<hr>
<h2 id='download.dict'>Download Dictionaries</h2><span id='topic+download.dict'></span>

<h3>Description</h3>

<p>Downloads the specified dictionaries from <a href="https://osf.io/y6g5b">osf.io/y6g5b</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download.dict(dict = "lusi", check.md5 = TRUE, mode = "wb",
  dir = getOption("lingmatch.dict.dir"), overwrite = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download.dict_+3A_dict">dict</code></td>
<td>
<p>One or more names of dictionaries to download, or <code>'all'</code> for all available. See
<a href="https://osf.io/y6g5b/wiki/home">osf.io/y6g5b/wiki</a> for more information, and a list of available dictionaries.</p>
</td></tr>
<tr><td><code id="download.dict_+3A_check.md5">check.md5</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), retrieves the MD5 checksum from OSF,
and compares it with that calculated from the downloaded file to check its integrity.</p>
</td></tr>
<tr><td><code id="download.dict_+3A_mode">mode</code></td>
<td>
<p>A character specifying the file write mode; default is 'wb'. See
<code><a href="utils.html#topic+download.file">download.file</a></code>.</p>
</td></tr>
<tr><td><code id="download.dict_+3A_dir">dir</code></td>
<td>
<p>Directory in which to save the dictionary; <br /> default is <code>getOption('lingmatch.dict.dir')</code>. <br />
This must be specified, or the option must be set &ndash; use <code><a href="#topic+lma_initdirs">lma_initdirs</a></code> to initialize a directory.</p>
</td></tr>
<tr><td><code id="download.dict_+3A_overwrite">overwrite</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will replace existing files.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Path to the downloaded dictionary, or a list of such if multiple were downloaded.
</p>


<h3>See Also</h3>

<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

download.dict("lusi", dir = "~/Dictionaries")

## End(Not run)
</code></pre>

<hr>
<h2 id='download.lspace'>Download Latent Semantic Spaces</h2><span id='topic+download.lspace'></span>

<h3>Description</h3>

<p>Downloads the specified semantic space from <a href="https://osf.io/489he">osf.io/489he</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download.lspace(space = "100k_lsa", decompress = TRUE, check.md5 = TRUE,
  mode = "wb", dir = getOption("lingmatch.lspace.dir"),
  overwrite = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download.lspace_+3A_space">space</code></td>
<td>
<p>Name of one or more spaces you want to download, or <code>'all'</code> for all available.
<code>'100k_lsa'</code> is the default, and some other common options might be <code>'google'</code>, <code>'facebook'</code>,
or <code>'glove'</code>. See <a href="https://osf.io/489he/wiki/home">osf.io/489he/wiki</a> for more information,
and a full list of spaces.</p>
</td></tr>
<tr><td><code id="download.lspace_+3A_decompress">decompress</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), decompresses the downloaded file
with the <code>bunzip2</code> system command assuming it is available <br /> (as indicated by
<code>Sys.which('bunzip2')</code>).</p>
</td></tr>
<tr><td><code id="download.lspace_+3A_check.md5">check.md5</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), retrieves the MD5 checksum from OSF,
and compares it with that calculated from the downloaded file to check its integrity.</p>
</td></tr>
<tr><td><code id="download.lspace_+3A_mode">mode</code></td>
<td>
<p>A character specifying the file write mode; default is 'wb'. See
<code><a href="utils.html#topic+download.file">download.file</a></code>.</p>
</td></tr>
<tr><td><code id="download.lspace_+3A_dir">dir</code></td>
<td>
<p>Directory in which to save the space. Specify this here, or set the lspace directory option
(e.g., <code>options(lingmatch.lspace.dir = '~/Latent Semantic Spaces')</code>), or use
<code><a href="#topic+lma_initdirs">lma_initdirs</a></code> to initialize a directory.</p>
</td></tr>
<tr><td><code id="download.lspace_+3A_overwrite">overwrite</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will replace existing files.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with paths to the [1] data and [2] term files.
</p>


<h3>See Also</h3>

<p>Other Latent Semantic Space functions: 
<code><a href="#topic+lma_lspace">lma_lspace</a>()</code>,
<code><a href="#topic+select.lspace">select.lspace</a>()</code>,
<code><a href="#topic+standardize.lspace">standardize.lspace</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

download.lspace("glove_crawl", dir = "~/Latent Semantic Spaces")

## End(Not run)
</code></pre>

<hr>
<h2 id='lingmatch'>Linguistic Matching and Accommodation</h2><span id='topic+lingmatch'></span>

<h3>Description</h3>

<p>Offers a variety of methods to assess linguistic matching or accommodation, where <em>matching</em>
is general similarity (sometimes called <em>homophily</em>), and <em>accommodation</em> is some form
of conditional similarity (accounting for some base-rate or precedent; sometimes called
<em>alignment</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lingmatch(input = NULL, comp = mean, data = NULL, group = NULL, ...,
  comp.data = NULL, comp.group = NULL, order = NULL, drop = FALSE,
  all.levels = FALSE, type = "lsm")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lingmatch_+3A_input">input</code></td>
<td>
<p>Texts to be compared; a vector, document-term matrix (dtm; with terms as column names),
or path to a file (.txt or .csv, with texts separated by one or more lines/rows).</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_comp">comp</code></td>
<td>
<p>Defines the comparison to be made:
</p>

<ul>
<li><p> If a <strong>function</strong>, this will be applied to <code>input</code> within each group (overall if there is
no group; i.e., <code>apply(input, 2, comp)</code>; e.g., <code>comp = mean</code> would compare each text to
the mean profile of its group).
</p>
</li>
<li><p> If a <strong>character</strong> with a length of 1 and no spaces:
</p>

<ul>
<li><p> If it partially matches one of <code>lsm_profiles</code>'s rownames, that row will be used as the comparison.
</p>
</li>
<li><p> If it partially matches <code>'auto'</code>, the highest correlating <code>lsm_profiles</code> row will be used.
</p>
</li>
<li><p> If it partially matches <code>'pairwise'</code>, each text will be compared to one another.
</p>
</li>
<li><p> If it partially matches <code>'sequential'</code>, the last variable in <code>group</code> will be treated as
a speaker ID (see the Grouping and Comparisons section).
</p>
</li></ul>

</li>
<li><p> If a <strong>character vector</strong>, this will be processed in the same way as <code>input</code>.
</p>
</li>
<li><p> If a <strong>vector</strong>, either (a) logical or factor-like (having n levels &lt; length) and of the same length as
<code>nrow(input)</code>, or (b) numeric or logical of length less than <code>nrow(input)</code>, this will be used to
select a subset of <code>input</code> (e.g., <code>1:10</code> would treat the first 10 rows of <code>input</code> as the
comparison; <code>lingmatch(text, type == 'prompt', data)</code> would use the texts in the <code>text</code> column
identified by the <code>type</code> column as the comparison).
</p>
</li>
<li><p> If a <strong>matrix-like object</strong> (having multiple rows and columns), or a named vector, this will
be treated as a sort of dtm, assuming there are common (column) names between <code>input</code> and
<code>comp</code> (e.g., if you had prompt and response texts that were already processed separately).
</p>
</li></ul>
</td></tr>
<tr><td><code id="lingmatch_+3A_data">data</code></td>
<td>
<p>A matrix-like object as a reference for column names, if variables are referred to in
other arguments (e.g., <code>lingmatch(text, data = data)</code> would be the same as
<code>lingmatch(data$text)</code>.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_group">group</code></td>
<td>
<p>A logical or factor-like vector the same length as <code>NROW(input)</code>, used to defined
groups.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_...">...</code></td>
<td>
<p>Passes arguments to <code><a href="#topic+lma_dtm">lma_dtm</a></code>, <code><a href="#topic+lma_weight">lma_weight</a></code>,
<code><a href="#topic+lma_termcat">lma_termcat</a></code>, and/or <code><a href="#topic+lma_lspace">lma_lspace</a></code> (depending on <code>input</code> and <code>comp</code>),
and <code><a href="#topic+lma_simets">lma_simets</a></code>.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_comp.data">comp.data</code></td>
<td>
<p>A matrix-like object as a source for <code>comp</code> variables.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_comp.group">comp.group</code></td>
<td>
<p>The column name of the grouping variable(s) in <code>comp.data</code>; if
<code>group</code> contains references to column names, and <code>comp.group</code> is not specified,
<code>group</code> variables will be looked for in <code>comp.data</code>.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_order">order</code></td>
<td>
<p>A numeric vector the same length as <code>nrow(input)</code> indicating the order of the
texts and grouping variables when the type of comparison is sequential. Only necessary if the
texts are not already ordered as desired.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_drop">drop</code></td>
<td>
<p>logical; if <code>TRUE</code>, will drop columns with a sum of 0.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_all.levels">all.levels</code></td>
<td>
<p>logical; if <code>FALSE</code>, multiple groups are combined. See the Grouping and
Comparisons section.</p>
</td></tr>
<tr><td><code id="lingmatch_+3A_type">type</code></td>
<td>
<p>A character at least partially matching 'lsm' or 'lsa'; applies default settings
aligning with the standard calculations of each type:
</p>

<table>
<tr>
 <td style="text-align: left;">
  LSM </td><td style="text-align: left;"> <code>lingmatch(text, weight = 'freq', dict = lma_dict(1:9), metric = 'canberra')</code></td>
</tr>
<tr>
 <td style="text-align: left;">
  LSA </td><td style="text-align: left;"> <code>lingmatch(text, weight = 'tfidf', space = '100k_lsa', metric = 'cosine')</code></td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
</td></tr>
</table>


<h3>Details</h3>

<p>There are a great many points of decision in the assessment of linguistic similarity and/or
accommodation, partly inherited from the great many point of decision inherent in the numerical
representation of language. Two general types of matching are implemented here as sets of
defaults: Language/Linguistic Style Matching (LSM; Niederhoffer &amp; Pennebaker, 2002; Ireland &amp;
Pennebaker, 2010), and Latent Semantic Analysis/Similarity (LSA; Landauer &amp; Dumais, 1997;
Babcock, Ta, &amp; Ickes, 2014). See the <code>type</code> argument for specifics.
</p>


<h3>Value</h3>

<p>A list with processed components of the input, information about the comparison, and results of
the comparison:
</p>

<ul>
<li> <p><strong><code>dtm</code></strong>: A sparse matrix; the raw count-dtm, or a version of the original input
if it is more processed.
</p>
</li>
<li> <p><strong><code>processed</code></strong>: A matrix-like object; a processed version of the input
(e.g., weighted and categorized).
</p>
</li>
<li> <p><strong><code>comp.type</code></strong>: A string describing the comparison if applicable.
</p>
</li>
<li> <p><strong><code>comp</code></strong>: A vector or matrix-like object; the comparison data if applicable.
</p>
</li>
<li> <p><strong><code>group</code></strong>: A string describing the group if applicable.
</p>
</li>
<li> <p><strong><code>sim</code></strong>: Result of <code><a href="#topic+lma_simets">lma_simets</a></code>.
</p>
</li></ul>



<h3>Grouping and Comparisons</h3>

<p>Defining groups and comparisons can sometimes be a bit complicated, and requires dataset
specific knowledge, so it can't always (readily) be done automatically. Variables entered in the
<code>group</code> argument are treated differently depending on their position and other arguments:
</p>

<dl>
<dt>Splitting</dt><dd><p>By default, groups are treated as if they define separate chunks of data in
which comparisons should be calculated. Functions used to calculated comparisons, and
pairwise comparisons are performed separately in each of these groups. For example, if you
wanted to compare each text with the mean of all texts in its condition, a <code>group</code>
variable could identify and split by condition. Given multiple grouping variables,
calculations will either be done in each split (if <code>all.levels = TRUE</code>; applied in
sequence so that groups become smaller and smaller), or once after all splits are made (if
<code>all.levels = FALSE</code>). This makes for 'one to many' comparisons with either calculated
or preexisting standards (i.e., the profile of the current data, or a precalculated profile,
respectively).</p>
</dd>
<dt>Comparison ID</dt><dd><p>When comparison data is identified in <code>comp</code>, groups are assumed
to apply to both <code>input</code> and <code>comp</code> (either both in <code>data</code>, or separately
between <code>data</code> and <code>comp.data</code>, in which case <code>comp.group</code> may be needed if
the same grouping variable have different names between <code>data</code> and <code>comp.data</code>).
In this case, multiple grouping variables are combined into a single factor assumed to
uniquely identify a comparison. This makes for 'one to many' comparisons with specific texts
(as in the case of manipulated prompts or text-based conditions).</p>
</dd>
<dt>Speaker ID</dt><dd><p>If <code>comp</code> matches <code>'sequential'</code>, the last grouping variable
entered is assumed to identify something like speakers (i.e., a factor with two or more
levels and multiple observations per level). In this case, the data are assumed to be ordered
(or ordered once sorted by <code>order</code> if specified). Any additional grouping variables
before the last are treated as splitting groups. This can set up for probabilistic
accommodation metrics. At the moment, when sequential comparisons are made within groups,
similarity scores between speakers are averaged, resulting in mean matching between speakers
within the group.</p>
</dd>
</dl>



<h3>References</h3>

<p>Babcock, M. J., Ta, V. P., &amp; Ickes, W. (2014). Latent semantic similarity and language style
matching in initial dyadic interactions. <em>Journal of Language and Social Psychology, 33</em>,
78-88.
</p>
<p>Ireland, M. E., &amp; Pennebaker, J. W. (2010). Language style matching in writing: synchrony in
essays, correspondence, and poetry. <em>Journal of Personality and Social Psychology, 99</em>,
549.
</p>
<p>Landauer, T. K., &amp; Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic
analysis theory of acquisition, induction, and representation of knowledge.
<em>Psychological Review, 104</em>, 211.
</p>
<p>Niederhoffer, K. G., &amp; Pennebaker, J. W. (2002). Linguistic style matching in social interaction.
<em>Journal of Language and Social Psychology, 21</em>, 337-360.
</p>


<h3>See Also</h3>

<p>For a general text processing function, see <code><a href="#topic+lma_process">lma_process</a>()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># compare single strings
lingmatch("Compare this sentence.", "With this other sentence.")

# compare each entry in a character vector with...
texts &lt;- c(
  "One bit of text as an entry...",
  "Maybe multiple sentences in an entry. Maybe essays or posts or a book.",
  "Could be lines or a column from a read-in file..."
)

## one another
lingmatch(texts)

## the first
lingmatch(texts, 1)

## the next
lingmatch(texts, "seq")

## the set average
lingmatch(texts, mean)

## other entries in a group
lingmatch(texts, group = c("a", "a", "b"))

## one another, without stop words
lingmatch(texts, exclude = "function")

## a standard average (based on function words)
lingmatch(texts, "auto", dict = lma_dict(1:9))

</code></pre>

<hr>
<h2 id='lma_dict'>English Function Word Category and Special Character Lists</h2><span id='topic+lma_dict'></span>

<h3>Description</h3>

<p>Returns a list of function words based on the Linguistic Inquiry and Word Count 2015 dictionary
(in terms of category names &ndash; words were selected independently), or a list of special characters and patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_dict(..., as.regex = TRUE, as.function = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_dict_+3A_...">...</code></td>
<td>
<p>Numbers or letters corresponding to category names: ppron, ipron, article,
adverb, conj, prep, auxverb, negate, quant, interrog, number, interjection, or special.</p>
</td></tr>
<tr><td><code id="lma_dict_+3A_as.regex">as.regex</code></td>
<td>
<p>Logical: if <code>FALSE</code>, lists are returned without regular expression.</p>
</td></tr>
<tr><td><code id="lma_dict_+3A_as.function">as.function</code></td>
<td>
<p>Logical or a function: if specified and <code>as.regex</code> is <code>TRUE</code>, the selected dictionary
will be collapsed to a regex string (terms separated by <code>|</code>), and a function for matching characters to that
string will be returned. The regex string is passed to the matching function (<code><a href="base.html#topic+grepl">grepl</a></code> by default)
as a 'pattern' argument, with the first argument of the returned function being passed as an 'x' argument.
See examples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with a vector of terms for each category, or (when <code>as.function = TRUE</code>) a function which
accepts an initial &quot;terms&quot; argument (a character vector), and any additional arguments determined by function
entered as <code>as.function</code> (<code><a href="base.html#topic+grepl">grepl</a></code> by default).
</p>


<h3>Note</h3>

<p>The <code>special</code> category is not returned unless specifically requested. It is a list of regular expression
strings attempting to capture special things like ellipses and emojis, or sets of special characters (those outside
of the Basic Latin range; <code>[^\u0020-\u007F]</code>), which can be used for character conversions.
If <code>special</code> is part of the returned list, <code>as.regex</code> is set to <code>TRUE</code>.
</p>
<p>The <code>special</code> list is always used by both <code><a href="#topic+lma_dtm">lma_dtm</a></code> and <code><a href="#topic+lma_termcat">lma_termcat</a></code>. When creating a
dtm, <code>special</code> is used to clean the original input (so that, by default, the punctuation involved in ellipses
and emojis are treated as different &ndash; as ellipses and emojis rather than as periods and parens and colons and such).
When categorizing a dtm, the input dictionary is passed by the special lists to be sure the terms in the dtm match up
with the dictionary (so, for example, &quot;: (&quot; would be replaced with &quot;repfrown&quot; in both the text and dictionary).
</p>


<h3>See Also</h3>

<p>To score texts with these categories, use <code><a href="#topic+lma_termcat">lma_termcat</a>()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># return the full dictionary (excluding special)
lma_dict()

# return the standard 7 category lsm categories
lma_dict(1:7)

# return just a few categories without regular expression
lma_dict(neg, ppron, aux, as.regex = FALSE)

# return special specifically
lma_dict(special)

# returning a function
is.ppron &lt;- lma_dict(ppron, as.function = TRUE)
is.ppron(c("i", "am", "you", "were"))

in.lsmcat &lt;- lma_dict(1:7, as.function = TRUE)
in.lsmcat(c("a", "frog", "for", "me"))

## use as a stopword filter
is.stopword &lt;- lma_dict(as.function = TRUE)
dtm &lt;- lma_dtm("Most of these words might not be all that relevant.")
dtm[, !is.stopword(colnames(dtm))]

## use to replace special characters
clean &lt;- lma_dict(special, as.function = gsub)
clean(c(
  "\u201Ccurly quotes\u201D", "na\u00EFve", "typographer\u2019s apostrophe",
  "en\u2013dash", "em\u2014dash"
))
</code></pre>

<hr>
<h2 id='lma_dtm'>Document-Term Matrix Creation</h2><span id='topic+lma_dtm'></span>

<h3>Description</h3>

<p>Creates a document-term matrix (dtm) from a set of texts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_dtm(text, exclude = NULL, context = NULL, replace.special = FALSE,
  numbers = FALSE, punct = FALSE, urls = TRUE, emojis = FALSE,
  to.lower = TRUE, word.break = " +", dc.min = 0, dc.max = Inf,
  sparse = TRUE, tokens.only = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_dtm_+3A_text">text</code></td>
<td>
<p>Texts to be processed. This can be a vector (such as a column in a data frame)
or list. When a list, these can be in the form returned with <code>tokens.only = TRUE</code>,
or a list with named vectors, where names are tokens and values are frequencies or the like.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_exclude">exclude</code></td>
<td>
<p>A character vector of words to be excluded. If <code>exclude</code> is a single string
matching <code>'function'</code>, <code>lma_dict(1:9)</code> will be used.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_context">context</code></td>
<td>
<p>A character vector used to reformat text based on look- ahead/behind. For example,
you might attempt to disambiguate <em>like</em> by reformatting certain <em>like</em>s
(e.g., <code>context = c('(i) like*', '(you) like*', '(do) like')</code>, where words in parentheses are
the context for the target word, and asterisks denote partial matching). This would be converted
to regular expression (i.e., <code>'(? &lt;= i) like\\b'</code>) which, if matched, would be
replaced with a coded version of the word (e.g., <code>"Hey, i like that!"</code> would become
<code>"Hey, i i-like that!"</code>). This would probably only be useful for categorization, where a
dictionary would only include one or another version of a word (e.g., the LIWC 2015 dictionary
does something like this with <em>like</em>, and LIWC 2007 did something like this with
<em>kind (of)</em>, both to try and clean up the posemo category).</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_replace.special">replace.special</code></td>
<td>
<p>Logical: if <code>TRUE</code>, special characters are replaced with regular
equivalents using the <code><a href="#topic+lma_dict">lma_dict</a></code> special function.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_numbers">numbers</code></td>
<td>
<p>Logical: if <code>TRUE</code>, numbers are preserved.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_punct">punct</code></td>
<td>
<p>Logical: if <code>TRUE</code>, punctuation is preserved.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_urls">urls</code></td>
<td>
<p>Logical: if <code>FALSE</code>, attempts to replace all urls with &quot;repurl&quot;.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_emojis">emojis</code></td>
<td>
<p>Logical: if <code>TRUE</code>, attempts to replace emojis (e.g., &quot;:(&quot; would be replaced
with &quot;repfrown&quot;).</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_to.lower">to.lower</code></td>
<td>
<p>Logical: if <code>FALSE</code>, words with different capitalization are treated as
different terms.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_word.break">word.break</code></td>
<td>
<p>A regular expression string determining the way words are split. Default is
<code>' +'</code> which breaks words at one or more blank spaces. You may also like to break by
dashes or slashes (<code>'[ /-]+'</code>), depending on the text.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_dc.min">dc.min</code></td>
<td>
<p>Numeric: excludes terms appearing in the set number or fewer documents.
Default is 0 (no limit).</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_dc.max">dc.max</code></td>
<td>
<p>Numeric: excludes terms appearing in the set number or more. Default
is Inf (no limit).</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_sparse">sparse</code></td>
<td>
<p>Logical: if <code>FALSE</code>, a regular dense matrix is returned.</p>
</td></tr>
<tr><td><code id="lma_dtm_+3A_tokens.only">tokens.only</code></td>
<td>
<p>Logical: if <code>TRUE</code>, returns a list rather than a matrix, with these entries:
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>tokens</code> </td><td style="text-align: left;"> A vector of indices with terms as names. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>frequencies</code> </td><td style="text-align: left;"> A vector of counts with terms as names. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>WC</code> </td><td style="text-align: left;"> A vector of term counts for each document. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>indices</code> </td><td style="text-align: left;"> A list with a vector of token indices for each document. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparse matrix (or regular matrix if <code>sparse = FALSE</code>), with a row per <code>text</code>,
and column per term, or a list if <code>tokens.only = TRUE</code>. Includes an attribute with options (<code>opts</code>),
and attributes with word count (<code>WC</code>) and column sums (<code>colsums</code>) if <code>tokens.only = FALSE</code>.
</p>


<h3>Note</h3>

<p>This is a relatively simple way to make a dtm. To calculate the (more or less) standard forms of
LSM and LSS, a somewhat raw dtm should be fine, because both processes essentially use
dictionaries (obviating stemming) and weighting or categorization (largely obviating 'stop word'
removal). The exact effect of additional processing will depend on the dictionary/semantic space
and weighting scheme used (particularly for LSA). This function also does some processing which
may matter if you plan on categorizing with categories that have terms with look- ahead/behind assertions
(like LIWC dictionaries). Otherwise, other methods may be faster, more memory efficient, and/or more featureful.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- c(
  "Why, hello there! How are you this evening?",
  "I am well, thank you for your inquiry!",
  "You are a most good at social interactions person!",
  "Why, thank you! You're not all bad yourself!"
)

lma_dtm(text)

# return tokens only
(tokens &lt;- lma_dtm(text, tokens.only = TRUE))

## convert those to a regular DTM
lma_dtm(tokens)

# convert a list-representation to a sparse matrix
lma_dtm(list(
  doc1 = c(why = 1, hello = 1, there = 1),
  doc2 = c(i = 1, am = 1, well = 1)
))
</code></pre>

<hr>
<h2 id='lma_initdirs'>Initialize Directories for Dictionaries and Latent Semantic Spaces</h2><span id='topic+lma_initdirs'></span>

<h3>Description</h3>

<p>Creates directories for dictionaries and latent semantic spaces if needed, sets them as the
<br /> <code>lingmatch.dict.dir</code> and <code>lingmatch.lspace.dir</code> options if they are not already set,
and creates links to them in their expected locations (<code>'~/Dictionaries'</code> and
<code>'~/Latent Semantic Spaces'</code>) by default if applicable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_initdirs(base = "", dict = "Dictionaries",
  lspace = "Latent Semantic Spaces", link = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_initdirs_+3A_base">base</code></td>
<td>
<p>Path to a directory in which to create the <code>dict</code> and <code>lspace</code> subdirectories.</p>
</td></tr>
<tr><td><code id="lma_initdirs_+3A_dict">dict</code></td>
<td>
<p>Path to the dictionaries directory relative to <code>base</code>.</p>
</td></tr>
<tr><td><code id="lma_initdirs_+3A_lspace">lspace</code></td>
<td>
<p>Path to the latent semantic spaces directory relative to <code>base</code>.</p>
</td></tr>
<tr><td><code id="lma_initdirs_+3A_link">link</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), the full <code>dict</code> and/or <code>lspace</code> paths exist
(potentially after being created), and they are not <code>'~/Dictionaries'</code> or <code>'~/Latent Semantic Spaces'</code>
respectively, junctions (Windows) or symbolic links will be created: <code>~/Dictionaries</code> <code>&lt;&lt;===&gt;&gt;</code>
<code>dict</code> and <code>~/Latent Semantic Spaces</code> <code>&lt;&lt;===&gt;&gt;</code> <code>lspace</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Paths to the [1] dictionaries and [2] latent semantic space directories, or a single path if only
<code>dict</code> or <code>lspace</code> is specified.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# set up the expected dictionary and latent semantic space directories
lma_initdirs("~")

# set up directories elsewhere, and links to the expected locations
lma_initdirs("d:")

# point options and create links to preexisting directories
lma_initdirs("~/NLP_Resources", "Dicts", "Dicts/Embeddings")

# create just a dictionaries directory and set the
# lingmatch.dict.dir option without creating a link
lma_initdirs(dict = "z:/external_dictionaries", link = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lma_lspace'>Latent Semantic Space (Embeddings) Operations</h2><span id='topic+lma_lspace'></span>

<h3>Description</h3>

<p>Map a document-term matrix onto a latent semantic space, extract terms from a
latent semantic space (if <code>dtm</code> is a character vector, or <code>map.space =</code> <code>FALSE</code>),
or perform a singular value decomposition of a document-term matrix (if <code>dtm</code> is a matrix
and <code>space</code> is missing).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_lspace(dtm = "", space, map.space = TRUE, fill.missing = FALSE,
  term.map = NULL, dim.cutoff = 0.5, keep.dim = FALSE,
  use.scan = FALSE, dir = getOption("lingmatch.lspace.dir"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_lspace_+3A_dtm">dtm</code></td>
<td>
<p>A matrix with terms as column names, or a character vector of terms to be extracted
from a specified space. If this is of length 1 and <code>space</code> is missing, it will be treated
as <code>space</code>.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_space">space</code></td>
<td>
<p>A matrix with terms as rownames. If missing, this will be the right singular vectors
of a singular value decomposition of <code>dtm</code>. If a character, a file matching the character
will be searched for in <code>dir</code> (e.g., <code>space = 'google'</code>). If a file is not found and
the character matches one of the <a href="https://osf.io/489he/wiki/home">available spaces</a>, you
will be given the option to download it, as handled by <code><a href="#topic+download.lspace">download.lspace</a></code>.
If <code>dtm</code> is missing, the entire space will be loaded and returned.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_map.space">map.space</code></td>
<td>
<p>Logical: if <code>FALSE</code>, the original vectors of <code>space</code> for terms
found in <code>dtm</code> are returned. Otherwise <code>dtm</code> <code>%*%</code> <code>space</code> is returned,
excluding uncommon columns of <code>dtm</code> and rows of <code>space</code>.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_fill.missing">fill.missing</code></td>
<td>
<p>Logical: if <code>TRUE</code> and terms are being extracted from a space, includes
terms not found in the space as rows of 0s, such that the returned matrix will have a row
for every requested term.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_term.map">term.map</code></td>
<td>
<p>A matrix with <code>space</code> as a column name, terms as row names, and indices of
the terms in the given space as values, or a numeric vector of indices with terms as names, or
a character vector of terms corresponding to rows of the space. This is used instead of reading
in an &quot;_terms.txt&quot; file corresponding to a <code>space</code> entered as a character (the name of a
space file).</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_dim.cutoff">dim.cutoff</code></td>
<td>
<p>If a <code>space</code> is calculated, this will be used to decide on the number of
dimensions to be retained: <code>cumsum(d) / sum(d) &lt; dim.cutoff</code>, where <code>d</code> is a vector
of singular values of <code>dtm</code> (i.e., <code>svd(dtm)$d</code>). The default is <code>.5</code>; lower
cutoffs result in fewer dimensions.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_keep.dim">keep.dim</code></td>
<td>
<p>Logical: if <code>TRUE</code>, and a space is being calculated from the input, a matrix
in the same dimensions as <code>dtm</code> is returned. Otherwise, a matrix with terms as rows and
dimensions as columns is returned.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_use.scan">use.scan</code></td>
<td>
<p>Logical: if <code>TRUE</code>, reads in the rows of <code>space</code> with <code><a href="base.html#topic+scan">scan</a></code>.</p>
</td></tr>
<tr><td><code id="lma_lspace_+3A_dir">dir</code></td>
<td>
<p>Path to a folder containing spaces. <br />
Set a session default with <code>options(lingmatch.lspace.dir = 'desired/path')</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix or sparse matrix with either (a) a row per term and column per latent dimension (a latent
space, either calculated from the input, or retrieved when <code>map.space = FALSE</code>), (b) a row per document
and column per latent dimension (when a dtm is mapped to a space), or (c) a row per document and
column per term (when a space is calculated and <code>keep.dim = TRUE</code>).
</p>


<h3>Note</h3>

<p>A traditional latent semantic space is a selection of right singular vectors from the singular
value decomposition of a dtm (<code>svd(dtm)$v[, 1:k]</code>, where <code>k</code> is the selected number of
dimensions, decided here by <code>dim.cutoff</code>).
</p>
<p>Mapping a new dtm into a latent semantic space consists of multiplying common terms:
<code>dtm[, ct]</code> <code>%*% space[ct, ]</code>, where <code>ct</code> <code>=</code> <code>colnames(dtm)[colnames(dtm)</code>
<code>%in%</code> <code>rownames(space)]</code> &ndash; the terms common between the dtm and the space. This
results in a matrix with documents as rows, and dimensions as columns, replacing terms.
</p>


<h3>See Also</h3>

<p>Other Latent Semantic Space functions: 
<code><a href="#topic+download.lspace">download.lspace</a>()</code>,
<code><a href="#topic+select.lspace">select.lspace</a>()</code>,
<code><a href="#topic+standardize.lspace">standardize.lspace</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- c(
  paste(
    "Hey, I like kittens. I think all kinds of cats really are just the",
    "best pet ever."
  ),
  paste(
    "Oh year? Well I really like cars. All the wheels and the turbos...",
    "I think that's the best ever."
  ),
  paste(
    "You know what? Poo on you. Cats, dogs, rabbits -- you know, living",
    "creatures... to think you'd care about anything else!"
  ),
  paste(
    "You can stick to your opinion. You can be wrong if you want. You know",
    "what life's about? Supercharging, diesel guzzling, exhaust spewing,",
    "piston moving ignitions."
  )
)

dtm &lt;- lma_dtm(text)

# calculate a latent semantic space from the example text
lss &lt;- lma_lspace(dtm)

# show that document similarities between the truncated and full space are the same
spaces &lt;- list(
  full = lma_lspace(dtm, keep.dim = TRUE),
  truncated = lma_lspace(dtm, lss)
)
sapply(spaces, lma_simets, metric = "cosine")

## Not run: 

# specify a directory containing spaces,
# or where you would like to download spaces
space_dir &lt;- "~/Latent Semantic Spaces"

# map to a pretrained space
ddm &lt;- lma_lspace(dtm, "100k", dir = space_dir)

# load the matching subset of the space
# without mapping
lss_100k_part &lt;- lma_lspace(colnames(dtm), "100k", dir = space_dir)

## or
lss_100k_part &lt;- lma_lspace(dtm, "100k", map.space = FALSE, dir = space_dir)

# load the full space
lss_100k &lt;- lma_lspace("100k", dir = space_dir)

## or
lss_100k &lt;- lma_lspace(space = "100k", dir = space_dir)

## End(Not run)
</code></pre>

<hr>
<h2 id='lma_meta'>Calculate Text-Based Metastatistics</h2><span id='topic+lma_meta'></span>

<h3>Description</h3>

<p>Calculate simple descriptive statistics from text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_meta(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_meta_+3A_text">text</code></td>
<td>
<p>A character vector of texts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame:
</p>

<ul>
<li> <p><strong><code>characters</code></strong>: Total number of characters.
</p>
</li>
<li> <p><strong><code>syllables</code></strong>: Total number of syllables, as estimated by split length of <br />
<code>'a+[eu]*|e+a*|i+|o+[ui]*|u+|y+[aeiou]*'</code> - 1.
</p>
</li>
<li> <p><strong><code>words</code></strong>: Total number of words (raw word count).
</p>
</li>
<li> <p><strong><code>unique_words</code></strong>: Number of unique words (binary word count).
</p>
</li>
<li> <p><strong><code>clauses</code></strong>: Number of clauses, as marked by commas, colons, semicolons, dashes, or brackets
within sentences.
</p>
</li>
<li> <p><strong><code>sentences</code></strong>: Number of sentences, as marked by periods, question marks, exclamation points,
or new line characters.
</p>
</li>
<li> <p><strong><code>words_per_clause</code></strong>: Average number of words per clause.
</p>
</li>
<li> <p><strong><code>words_per_sentence</code></strong>: Average number of words per sentence.
</p>
</li>
<li> <p><strong><code>sixltr</code></strong>: Number of words 6 or more characters long.
</p>
</li>
<li> <p><strong><code>characters_per_word</code></strong>: Average number of characters per word
(<code>characters</code> / <code>words</code>).
</p>
</li>
<li> <p><strong><code>syllables_per_word</code></strong>: Average number of syllables per word
(<code>syllables</code> / <code>words</code>).
</p>
</li>
<li> <p><strong><code>type_token_ratio</code></strong>: Ratio of unique to total words: <code>unique_words</code> / <code>words</code>.
</p>
</li>
<li> <p><strong><code>reading_grade</code></strong>: Flesch-Kincaid grade level: .39 * <code>words</code> / <code>sentences</code> +
11.8 * <code>syllables</code> / <code>words</code> - 15.59.
</p>
</li>
<li> <p><strong><code>numbers</code></strong>: Number of terms starting with numbers.
</p>
</li>
<li> <p><strong><code>punct</code></strong>: Number of terms starting with non-alphanumeric characters.
</p>
</li>
<li> <p><strong><code>periods</code></strong>: Number of periods.
</p>
</li>
<li> <p><strong><code>commas</code></strong>: Number of commas.
</p>
</li>
<li> <p><strong><code>qmarks</code></strong>: Number of question marks.
</p>
</li>
<li> <p><strong><code>exclams</code></strong>: Number of exclamation points.
</p>
</li>
<li> <p><strong><code>quotes</code></strong>: Number of quotation marks (single and double).
</p>
</li>
<li> <p><strong><code>apostrophes</code></strong>: Number of apostrophes, defined as any modified letter apostrophe, or backtick
or single straight or curly quote surrounded by letters.
</p>
</li>
<li> <p><strong><code>brackets</code></strong>: Number of bracketing characters (including parentheses, and square,
curly, and angle brackets).
</p>
</li>
<li> <p><strong><code>orgmarks</code></strong>: Number of characters used for organization or structuring (including
dashes, foreword slashes, colons, and semicolons).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- c(
  succinct = "It is here.",
  verbose = "Hear me now. I shall tell you about it. It is here. Do you hear?",
  couched = "I might be wrong, but it seems to me that it might be here.",
  bigwords = "Object located thither.",
  excited = "It's there! It's there! It's there!",
  drippy = "It's 'there', right? Not 'here'? 'there'? Are you Sure?",
  struggly = "It's here -- in that place where it is. Like... the 1st place (here)."
)
lma_meta(text)
</code></pre>

<hr>
<h2 id='lma_patcat'>Categorize Texts</h2><span id='topic+lma_patcat'></span>

<h3>Description</h3>

<p>Categorize raw texts using a pattern-based dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_patcat(text, dict = NULL, pattern.weights = "weight",
  pattern.categories = "category", bias = NULL, to.lower = TRUE,
  return.dtm = FALSE, drop.zeros = FALSE, exclusive = TRUE,
  boundary = NULL, fixed = TRUE, globtoregex = FALSE,
  name.map = c(intname = "_intercept", term = "term"),
  dir = getOption("lingmatch.dict.dir"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_patcat_+3A_text">text</code></td>
<td>
<p>A vector of text to be categorized. Texts are padded by 2 spaces, and potentially lowercased.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_dict">dict</code></td>
<td>
<p>At least a vector of terms (patterns), usually a matrix-like object with columns for terms,
categories, and weights.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_pattern.weights">pattern.weights</code></td>
<td>
<p>A vector of weights corresponding to terms in <code>dict</code>, or the column name of
weights found in <code>dict</code>.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_pattern.categories">pattern.categories</code></td>
<td>
<p>A vector of category names corresponding to terms in <code>dict</code>, or the column name of
category names found in <code>dict</code>.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_bias">bias</code></td>
<td>
<p>A constant to add to each category after weighting and summing. Can be a vector with names
corresponding to the unique values in <code>dict[, category]</code>, but is usually extracted from dict based
on the intercept included in each category (defined by <code>name.map['intname']</code>).</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_to.lower">to.lower</code></td>
<td>
<p>Logical indicating whether <code>text</code> should be converted to lowercase before processing.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_return.dtm">return.dtm</code></td>
<td>
<p>Logical; if <code>TRUE</code>, only a document-term matrix will be returned, rather than the
weighted, summed, and biased category values.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_drop.zeros">drop.zeros</code></td>
<td>
<p>logical; if <code>TRUE</code>, categories or terms with no matches will be removed.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_exclusive">exclusive</code></td>
<td>
<p>Logical; if <code>FALSE</code>, each dictionary term is searched for in the original text.
Otherwise (by default), terms are sorted by length (with longer terms being searched for first), and
matches are removed from the text (avoiding subsequent matches to matched patterns).</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_boundary">boundary</code></td>
<td>
<p>A string to add to the beginning and end of each dictionary term. If <code>TRUE</code>,
<code>boundary</code> will be set to <code>' '</code>, avoiding pattern matches within words. By default, dictionary
terms are left as entered.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_fixed">fixed</code></td>
<td>
<p>Logical; if <code>FALSE</code>, patterns are treated as regular expressions.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_globtoregex">globtoregex</code></td>
<td>
<p>Logical; if <code>TRUE</code>, initial and terminal asterisks are replaced with <code>\\b\\w*</code>
and <code>\\w*\\b</code> respectively. This will also set <code>fixed</code> to <code>FALSE</code> unless fixed is specified.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_name.map">name.map</code></td>
<td>
<p>A named character vector:
</p>

<ul>
<li> <p><strong><code>intname</code></strong>: term identifying category biases within the term list;
defaults to <code>'_intercept'</code>
</p>
</li>
<li> <p><strong><code>term</code></strong>: name of the column containing terms in <code>dict</code>; defaults to <code>'term'</code>
</p>
</li></ul>

<p>Missing names are added, so names can be specified positional (e.g., <code>c('_int',</code> <code>'terms')</code>),
or only some can be specified by name (e.g., <code>c(term =</code> <code>'patterns')</code>), leaving the rest default.</p>
</td></tr>
<tr><td><code id="lma_patcat_+3A_dir">dir</code></td>
<td>
<p>Path to a folder in which to look for <code>dict</code> if it is the name of a file to be passed to
<code><a href="#topic+read.dic">read.dic</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with a row per <code>text</code> and columns per dictionary category, or (when <code>return.dtm = TRUE</code>)
a sparse matrix with a row per <code>text</code> and column per term. Includes a <code>WC</code> attribute with original
word counts, and a <code>categories</code> attribute with row indices associated with each category if
<code>return.dtm = TRUE</code>.
</p>


<h3>See Also</h3>

<p>For applying term-based dictionaries (to a document-term matrix) see <code><a href="#topic+lma_termcat">lma_termcat</a>()</code>.
</p>
<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example text
text &lt;- c(
  paste(
    "Oh, what youth was! What I had and gave away.",
    "What I took and spent and saw. What I lost. And now? Ruin."
  ),
  paste(
    "God, are you so bored?! You just want what's gone from us all?",
    "I miss the you that was too. I love that you."
  ),
  paste(
    "Tomorrow! Tomorrow--nay, even tonight--you wait, as I am about to change.",
    "Soon I will off to revert. Please wait."
  )
)

# make a document-term matrix with pre-specified terms only
lma_patcat(text, c("bored?!", "i lo", ". "), return.dtm = TRUE)

# get counts of sets of letter
lma_patcat(text, list(c("a", "b", "c"), c("d", "e", "f")))

# same thing with regular expressions
lma_patcat(text, list("[abc]", "[def]"), fixed = FALSE)

# match only words
lma_patcat(text, list("i"), boundary = TRUE)

# match only words, ignoring punctuation
lma_patcat(
  text, c("you", "tomorrow", "was"),
  fixed = FALSE,
  boundary = "\\b", return.dtm = TRUE
)

## Not run: 

# read in the temporal orientation lexicon from the World Well-Being Project
tempori &lt;- read.csv(paste0(
  "https://raw.githubusercontent.com/wwbp/lexica/master/",
  "temporal_orientation/temporal_orientation_lexicon.csv"
))

lma_patcat(text, tempori)

# or use the standardized version
tempori_std &lt;- read.dic("wwbp_prospection", dir = "~/Dictionaries")

lma_patcat(text, tempori_std)

## get scores on the same scale by adjusting the standardized values
tempori_std[, -1] &lt;- tempori_std[, -1] / 100 *
  select.dict("wwbp_prospection")$selected[, "original_max"]

lma_patcat(text, tempori_std)[, unique(tempori$category)]

## End(Not run)
</code></pre>

<hr>
<h2 id='lma_process'>Process Text</h2><span id='topic+lma_process'></span>

<h3>Description</h3>

<p>A wrapper to other pre-processing functions, potentially from <code><a href="#topic+read.segments">read.segments</a></code>, to <code><a href="#topic+lma_dtm">lma_dtm</a></code>
or <code><a href="#topic+lma_patcat">lma_patcat</a></code>, to <code><a href="#topic+lma_weight">lma_weight</a></code>, then <code><a href="#topic+lma_termcat">lma_termcat</a></code> or <code><a href="#topic+lma_lspace">lma_lspace</a></code>,
and optionally including <code><a href="#topic+lma_meta">lma_meta</a></code> output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_process(input = NULL, ..., meta = TRUE, coverage = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_process_+3A_input">input</code></td>
<td>
<p>A vector of text, or path to a text file or folder.</p>
</td></tr>
<tr><td><code id="lma_process_+3A_...">...</code></td>
<td>
<p>arguments to be passed to <code><a href="#topic+lma_dtm">lma_dtm</a></code>, <code><a href="#topic+lma_patcat">lma_patcat</a></code>, <code><a href="#topic+lma_weight">lma_weight</a></code>,
<code><a href="#topic+lma_termcat">lma_termcat</a></code>, and/or <code><a href="#topic+lma_lspace">lma_lspace</a></code>. All arguments must be named.</p>
</td></tr>
<tr><td><code id="lma_process_+3A_meta">meta</code></td>
<td>
<p>Logical; if <code>FALSE</code>, metastatistics are not included. Only applies when raw text is available.
If included, meta categories are added as the last columns, with names starting with &quot;meta_&quot;.</p>
</td></tr>
<tr><td><code id="lma_process_+3A_coverage">coverage</code></td>
<td>
<p>Logical; if <code>TRUE</code> and a dictionary is provided (<code>dict</code>),
will calculate the coverage (number of unique term matches) of each dictionary category.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with texts represented by rows, and features in columns, unless there are multiple rows per output
(e.g., when a latent semantic space is applied without terms being mapped) in which case only the special output
is returned (e.g., a matrix with terms as rows and latent dimensions in columns).
</p>


<h3>See Also</h3>

<p>If you just want to compare texts, see the <code><a href="#topic+lingmatch">lingmatch</a>()</code> function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># starting with some texts in a vector
texts &lt;- c(
  "Firstly, I would like to say, and with all due respect...",
  "Please, proceed. I hope you feel you can speak freely...",
  "Oh, of course, I just hope to be clear, and not cause offense...",
  "Oh, no, don't monitor yourself on my account..."
)

# by default, term counts and metastatistics are returned
lma_process(texts)

# add dictionary and percent arguments for standard dictionary-based results
lma_process(texts, dict = lma_dict(), percent = TRUE)

# add space and weight arguments for standard word-centroid vectors
lma_process(texts, space = lma_lspace(texts), weight = "tfidf")
</code></pre>

<hr>
<h2 id='lma_simets'>Similarity Calculations</h2><span id='topic+lma_simets'></span>

<h3>Description</h3>

<p>Enter a numerical matrix, set of vectors, or set of matrices to calculate similarity per vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_simets(a, b = NULL, metric = NULL, group = NULL, lag = 0,
  agg = TRUE, agg.mean = TRUE, pairwise = TRUE, symmetrical = FALSE,
  mean = FALSE, return.list = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_simets_+3A_a">a</code></td>
<td>
<p>A vector or matrix. If a vector, <code>b</code> must also be provided. If a matrix and <code>b</code>
is missing, each row will be compared. If a matrix and <code>b</code> is not missing, each row will
be compared with <code>b</code> or each row of <code>b</code>.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_b">b</code></td>
<td>
<p>A vector or matrix to be compared with <code>a</code> or rows of <code>a</code>.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_metric">metric</code></td>
<td>
<p>A character or vector of characters at least partially matching one of the
available metric names (or 'all' to explicitly include all metrics),
or a number or vector of numbers indicating the metric by index:
</p>

<ul>
<li> <p><strong><code>jaccard</code></strong>: <code>sum(a &amp; b) / sum(a | b)</code>
</p>
</li>
<li> <p><strong><code>euclidean</code></strong>: <code>1 / (1 + sqrt(sum((a - b) ^ 2)))</code>
</p>
</li>
<li> <p><strong><code>canberra</code></strong>: <code>mean(1 - abs(a - b) / (a + b))</code>
</p>
</li>
<li> <p><strong><code>cosine</code></strong>: <code>sum(a * b) / sqrt(sum(a ^ 2 * sum(b ^ 2)))</code>
</p>
</li>
<li> <p><strong><code>pearson</code></strong>: <code>(mean(a * b) - (mean(a) * mean(b))) /</code> <br />
<code>sqrt(mean(a ^ 2) - mean(a) ^ 2) / sqrt(mean(b ^ 2) - mean(b) ^ 2)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="lma_simets_+3A_group">group</code></td>
<td>
<p>If <code>b</code> is missing and <code>a</code> has multiple rows, this will be used to make
comparisons between rows of <code>a</code>, as modified by <code>agg</code> and <code>agg.mean</code>.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_lag">lag</code></td>
<td>
<p>Amount to adjust the <code>b</code> index; either rows if <code>b</code> has multiple rows (e.g.,
for <code>lag = 1</code>, <code>a[1, ]</code> is compared with <code>b[2, ]</code>), or values otherwise (e.g.,
for <code>lag = 1</code>, <code>a[1]</code> is compared with <code>b[2]</code>). If <code>b</code> is not supplied,
<code>b</code> is a copy of <code>a</code>, resulting in lagged self-comparisons or autocorrelations.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_agg">agg</code></td>
<td>
<p>Logical: if <code>FALSE</code>, only the boundary rows between groups will be compared, see
example.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_agg.mean">agg.mean</code></td>
<td>
<p>Logical: if <code>FALSE</code> aggregated rows are summed instead of averaged.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_pairwise">pairwise</code></td>
<td>
<p>Logical: if <code>FALSE</code> and <code>a</code> and <code>b</code> are matrices with the same number of
rows, only paired rows are compared. Otherwise (and if only <code>a</code> is supplied), all pairwise
comparisons are made.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_symmetrical">symmetrical</code></td>
<td>
<p>Logical: if <code>TRUE</code> and pairwise comparisons between <code>a</code> rows were made,
the results in the lower triangle are copied to the upper triangle.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_mean">mean</code></td>
<td>
<p>Logical: if <code>TRUE</code>, a single mean for each metric is returned per row of <code>a</code>.</p>
</td></tr>
<tr><td><code id="lma_simets_+3A_return.list">return.list</code></td>
<td>
<p>Logical: if <code>TRUE</code>, a list-like object will always be returned, with an entry
for each metric, even when only one metric is requested.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use <code><a href="RcppParallel.html#topic+setThreadOptions">setThreadOptions</a></code> to change parallelization options; e.g., run
RcppParallel::setThreadOptions(4) before a call to lma_simets to set the number of CPU
threads to 4.
</p>


<h3>Value</h3>

<p>Output varies based on the dimensions of <code>a</code> and <code>b</code>:
</p>

<ul>
<li> <p><strong>Out:</strong> A vector with a value per metric. <br />
<strong>In:</strong> Only when <code>a</code> and <code>b</code> are both vectors.
</p>
</li>
<li> <p><strong>Out:</strong> A vector with a value per row. <br />
<strong>In:</strong> Any time a single value is expected per row: <code>a</code> or <code>b</code> is a vector,
<code>a</code> and <code>b</code> are matrices with the same number of rows and <code>pairwise = FALSE</code>, a group is
specified, or <code>mean = TRUE</code>, and only one metric is requested.
</p>
</li>
<li> <p><strong>Out:</strong> A data.frame with a column per metric. <br />
<strong>In:</strong> When multiple metrics are requested in the previous case.
</p>
</li>
<li> <p><strong>Out:</strong> A sparse matrix with a <code>metric</code> attribute with the metric name. <br />
<strong>In:</strong> Pairwise comparisons within an <code>a</code> matrix or between
an <code>a</code> and <code>b</code> matrix, when only 1 metric is requested.
</p>
</li>
<li> <p><strong>Out:</strong> A list with a sparse matrix per metric. <br />
<strong>In:</strong> When multiple metrics are requested in the previous case.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- c(
  "words of speaker A", "more words from speaker A",
  "words from speaker B", "more words from speaker B"
)
(dtm &lt;- lma_dtm(text))

# compare each entry
lma_simets(dtm)

# compare each entry with the mean of all entries
lma_simets(dtm, colMeans(dtm))

# compare by group (corresponding to speakers and turns in this case)
speaker &lt;- c("A", "A", "B", "B")

## by default, consecutive rows from the same group are averaged:
lma_simets(dtm, group = speaker)

## with agg = FALSE, only the rows at the boundary between
## groups (rows 2 and 3 in this case) are used:
lma_simets(dtm, group = speaker, agg = FALSE)
</code></pre>

<hr>
<h2 id='lma_termcat'>Document-Term Matrix Categorization</h2><span id='topic+lma_termcat'></span>

<h3>Description</h3>

<p>Reduces the dimensions of a document-term matrix by dictionary-based categorization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_termcat(dtm, dict, term.weights = NULL, bias = NULL,
  bias.name = "_intercept", escape = TRUE, partial = FALSE,
  glob = TRUE, term.filter = NULL, term.break = 20000,
  to.lower = FALSE, dir = getOption("lingmatch.dict.dir"),
  coverage = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_termcat_+3A_dtm">dtm</code></td>
<td>
<p>A matrix with terms as column names.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_dict">dict</code></td>
<td>
<p>The name of a provided dictionary
(<a href="https://osf.io/y6g5b/wiki/home">osf.io/y6g5b/wiki</a>) or of a file found in
<code>dir</code>, or a <code>list</code> object with named character vectors as word lists,
or the path to a file to be read in by <code><a href="#topic+read.dic">read.dic</a></code>.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_term.weights">term.weights</code></td>
<td>
<p>A <code>list</code> object with named numeric vectors lining up with the character
vectors in <code>dict</code>, used to weight the terms in each <code>dict</code> vector. If a category in
<code>dict</code> is not specified in <code>term.weights</code>, or the <code>dict</code> and <code>term.weights</code>
vectors aren't the same length, the weights for that category will be 1.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_bias">bias</code></td>
<td>
<p>A list or named vector specifying a constant to add to the named category. If a term
matching <code>bias.name</code> is included in a category, it's associated <code>weight</code> will be used
as the <code>bias</code> for that category.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_bias.name">bias.name</code></td>
<td>
<p>A character specifying a term to be used as a category bias; default is
<code>'_intercept'</code>.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_escape">escape</code></td>
<td>
<p>Logical indicating whether the terms in <code>dict</code> should not be treated as plain
text (including asterisk wild cards). If <code>TRUE</code>, regular expression related characters are
escaped. Set to <code>TRUE</code> if you get PCRE compilation errors.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_partial">partial</code></td>
<td>
<p>Logical; if <code>TRUE</code> terms are partially matched (not padded by ^ and $).</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_glob">glob</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), will convert initial and terminal asterisks to
partial matches.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_term.filter">term.filter</code></td>
<td>
<p>A regular expression string used to format the text of each term (passed to
<code>gsub</code>). For example, if terms are part-of-speech tagged (e.g.,
<code>'a_DT'</code>), <code>'_.*'</code> would remove the tag.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_term.break">term.break</code></td>
<td>
<p>If a category has more than <code>term.break</code> characters, it will be processed
in chunks. Reduce from 20000 if you get a PCRE compilation error.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_to.lower">to.lower</code></td>
<td>
<p>Logical; if <code>TRUE</code> will lowercase dictionary terms. Otherwise, dictionary
terms will be converted to match the terms if they are single-cased. Set to <code>FALSE</code> to
always keep dictionary terms as entered.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_dir">dir</code></td>
<td>
<p>Path to a folder in which to look for <code>dict</code>; <br />
will look in <code>'~/Dictionaries'</code> by default. <br />
Set a session default with <code>options(lingmatch.dict.dir = 'desired/path')</code>.</p>
</td></tr>
<tr><td><code id="lma_termcat_+3A_coverage">coverage</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will calculate coverage
(number of unique term matches) for each category.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with a row per <code>dtm</code> row and columns per dictionary category
(with added <code>coverage_</code> versions if <code>coverage</code> is <code>TRUE</code>),
and a <code>WC</code> attribute with original word counts.
</p>


<h3>See Also</h3>

<p>For applying pattern-based dictionaries (to raw text) see <code><a href="#topic+lma_patcat">lma_patcat</a>()</code>.
</p>
<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- list(category = c("cat", "dog", "pet*"))
lma_termcat(c(
  "cat, cat, cat, cat, cat, cat, cat, cat",
  "a cat, dog, or anything petlike, really",
  "petite petrochemical petitioned petty peter for petrified petunia petals"
), dict, coverage = TRUE)

## Not run: 

# Score texts with the NRC Affect Intensity Lexicon

dict &lt;- readLines("https://saifmohammad.com/WebDocs/NRC-AffectIntensity-Lexicon.txt")
dict &lt;- read.table(
  text = dict[-seq_len(grep("term\tscore", dict, fixed = TRUE)[[1]])],
  col.names = c("term", "weight", "category")
)

text &lt;- c(
  angry = paste(
    "We are outraged by their hateful brutality,",
    "and by the way they terrorize us with their hatred."
  ),
  fearful = paste(
    "The horrific torture of that terrorist was tantamount",
    "to the terrorism of terrorists."
  ),
  joyous = "I am jubilant to be celebrating the bliss of this happiest happiness.",
  sad = paste(
    "They are nearly suicidal in their mourning after",
    "the tragic and heartbreaking holocaust."
  )
)

emotion_scores &lt;- lma_termcat(text, dict)
if (require("splot")) splot(emotion_scores ~ names(text), leg = "out")

## or use the standardized version (which includes more categories)

emotion_scores &lt;- lma_termcat(text, "nrc_eil", dir = "~/Dictionaries")
emotion_scores &lt;- emotion_scores[, c("anger", "fear", "joy", "sadness")]
if (require("splot")) splot(emotion_scores ~ names(text), leg = "out")

## End(Not run)
</code></pre>

<hr>
<h2 id='lma_weight'>Document-Term Matrix Weighting</h2><span id='topic+lma_weight'></span>

<h3>Description</h3>

<p>Weight a document-term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lma_weight(dtm, weight = "count", normalize = TRUE, wc.complete = TRUE,
  log.base = 10, alpha = 1, pois.x = 1L, doc.only = FALSE,
  percent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lma_weight_+3A_dtm">dtm</code></td>
<td>
<p>A matrix with words as column names.</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_weight">weight</code></td>
<td>
<p>A string referring at least partially to one (or a combination; see note) of the
available weighting methods:
</p>
<p><strong>Term weights</strong> (applied uniquely to each cell)
</p>

<ul>
<li> <p><strong><code>binary</code></strong> <br />
<code>(dtm &gt; 0) * 1</code> <br />
Convert frequencies to 1s and 0s; remove differences in frequencies.
</p>
</li>
<li> <p><strong><code>log</code></strong> <br />
<code>log(dtm + 1, log.base)</code> <br />
Log of frequencies.
</p>
</li>
<li> <p><strong><code>sqrt</code></strong> <br />
<code>sqrt(dtm)</code> <br />
Square root of frequencies.
</p>
</li>
<li> <p><strong><code>count</code></strong> <br />
<code>dtm</code> <br />
Unaltered; sometimes called term frequencies (tf).
</p>
</li>
<li> <p><strong><code>amplify</code></strong> <br />
<code>dtm ^ alpha</code> <br />
Amplify difference in frequencies.
</p>
</li></ul>

<p><strong>Document weights</strong> (applied by column)
</p>

<ul>
<li> <p><strong><code>dflog</code></strong> <br />
<code>log(colSums(dtm &gt; 0), log.base)</code> <br />
Log of binary term sum.
</p>
</li>
<li> <p><strong><code>entropy</code></strong> <br />
<code>1 - rowSums(x *</code> <code>log(x + 1, log.base) /</code> <code>log(ncol(x), log.base),</code> <code>na.rm = TRUE)</code> <br />
Where <code>x = t(dtm) / colSums(dtm &gt; 0)</code>; entropy of term-conditional term distribution.
</p>
</li>
<li> <p><strong><code>ppois</code></strong> <br />
<code>1 - ppois(pois.x,</code> <code>colSums(dtm) / nrow(dtm))</code> <br />
Poisson-predicted term distribution.
</p>
</li>
<li> <p><strong><code>dpois</code></strong> <br />
<code>1 - dpois(pois.x, colSums(dtm) / nrow(dtm))</code> <br />
Poisson-predicted term density.
</p>
</li>
<li> <p><strong><code>dfmlog</code></strong> <br />
<code>log(diag(dtm[max.col(t(dtm)), ]), log.base)</code> <br />
Log of maximum term frequency.
</p>
</li>
<li> <p><strong><code>dfmax</code></strong> <br />
<code>diag(dtm[max.col(t(dtm)), ])</code> <br />
Maximum term frequency.
</p>
</li>
<li> <p><strong><code>df</code></strong> <br />
<code>colSums(dtm &gt; 0)</code> <br />
Sum of binary term occurrence across documents.
</p>
</li>
<li> <p><strong><code>idf</code></strong> <br />
<code>log(nrow(dtm) / colSums(dtm &gt; 0), log.base)</code> <br />
Inverse document frequency.
</p>
</li>
<li> <p><strong><code>ridf</code></strong> <br />
<code>idf - log(dpois, log.base)</code> <br />
Residual inverse document frequency.
</p>
</li>
<li> <p><strong><code>normal</code></strong> <br />
<code>sqrt(1 / colSums(dtm ^ 2))</code> <br />
Normalized document frequency.
</p>
</li></ul>

<p>Alternatively, <code>'pmi'</code> or <code>'ppmi'</code> will apply a pointwise mutual information weighting
scheme (with <code>'ppmi'</code> setting negative values to 0).</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_normalize">normalize</code></td>
<td>
<p>Logical: if <code>FALSE</code>, the dtm is not divided by document word-count before
being weighted.</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_wc.complete">wc.complete</code></td>
<td>
<p>If the dtm was made with <code><a href="#topic+lma_dtm">lma_dtm</a></code> (has a <code>'WC'</code>
attribute), word counts for
frequencies can be based on the raw count (default; <code>wc.complete = TRUE</code>). If
<code>wc.complete = FALSE</code>, or the dtm does not have a <code>'WC'</code> attribute,
<code>rowSums(dtm)</code> is used as word count.</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_log.base">log.base</code></td>
<td>
<p>The base of logs, applied to any weight using <code><a href="base.html#topic+log">log</a></code>.
Default is 10.</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_alpha">alpha</code></td>
<td>
<p>A scaling factor applied to document frequency as part of pointwise mutual
information weighting, or amplify's power (<code>dtm ^ alpha</code>, which defaults to 1.1).</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_pois.x">pois.x</code></td>
<td>
<p>integer; quantile or probability of the poisson distribution (<code>dpois(pois.x,
colSums(x,</code> <code>na.rm = TRUE) / nrow(x))</code>).</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_doc.only">doc.only</code></td>
<td>
<p>Logical: if <code>TRUE</code>, only document weights are returned (a single value for
each term).</p>
</td></tr>
<tr><td><code id="lma_weight_+3A_percent">percent</code></td>
<td>
<p>Logical; if <code>TRUE</code>, frequencies are multiplied by 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A weighted version of <code>dtm</code>, with a <code>type</code> attribute added (<code>attr(dtm, 'type')</code>).
</p>


<h3>Note</h3>

<p>Term weights works to adjust differences in counts within documents, with differences meaning
increasingly more from <code>binary</code> to <code>log</code> to <code>sqrt</code> to <code>count</code> to <code>amplify</code>.
</p>
<p>Document weights work to treat words differently based on their between-document or overall frequency.
When term frequencies are constant, <code>dpois</code>, <code>idf</code>, <code>ridf</code>, and <code>normal</code> give
less common words increasingly more weight, and <code>dfmax</code>, <code>dfmlog</code>, <code>ppois</code>, <code>df</code>,
<code>dflog</code>, and <code>entropy</code> give less common words increasingly less weight.
</p>
<p><code>weight</code> can either be a vector with two characters, corresponding to term weight and
document weight (e.g., <code>c('count', 'idf')</code>), or it can be a string with term and
document weights separated by any of <code>:\*_/; ,-</code> (e.g., <code>'count-idf'</code>).
<code>'tf'</code> is also acceptable for <code>'count'</code>, and <code>'tfidf'</code> will be parsed as
<code>c('count', 'idf')</code>, though this is a special case.
</p>
<p>For <code>weight</code>, term or document weights can be entered individually; term weights alone will
not apply any document weight, and document weights alone will apply a <code>'count'</code> term weight
(unless <code>doc.only = TRUE</code>, in which case a term-named vector of document weights is returned
instead of a weighted dtm).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># visualize term and document weights

## term weights
term_weights &lt;- c("binary", "log", "sqrt", "count", "amplify")
Weighted &lt;- sapply(term_weights, function(w) lma_weight(1:20, w, FALSE))
if (require(splot)) splot(Weighted ~ 1:20, labx = "Raw Count", lines = "co")

## document weights
doc_weights &lt;- c(
  "df", "dflog", "dfmax", "dfmlog", "idf", "ridf",
  "normal", "dpois", "ppois", "entropy"
)
weight_range &lt;- function(w, value = 1) {
  m &lt;- diag(20)
  m[upper.tri(m, TRUE)] &lt;- if (is.numeric(value)) {
    value
  } else {
    unlist(lapply(
      1:20, function(v) rep(if (value == "inverted") 21 - v else v, v)
    ))
  }
  lma_weight(m, w, FALSE, doc.only = TRUE)
}

if (require(splot)) {
  category &lt;- rep(c("df", "idf", "normal", "poisson", "entropy"), c(4, 2, 1, 2, 1))
  op &lt;- list(
    laby = "Relative (Scaled) Weight", labx = "Document Frequency",
    leg = "outside", lines = "connected", mv.scale = TRUE, note = FALSE
  )
  splot(
    sapply(doc_weights, weight_range) ~ 1:20,
    options = op, title = "Same Term, Varying Document Frequencies",
    sud = "All term frequencies are 1.",
    colorby = list(category, grade = TRUE)
  )
  splot(
    sapply(doc_weights, weight_range, value = "sequence") ~ 1:20,
    options = op, title = "Term as Document Frequencies",
    sud = "Non-zero terms are the number of non-zero terms.",
    colorby = list(category, grade = TRUE)
  )
  splot(
    sapply(doc_weights, weight_range, value = "inverted") ~ 1:20,
    options = op, title = "Term Opposite of Document Frequencies",
    sud = "Non-zero terms are the number of zero terms + 1.",
    colorby = list(category, grade = TRUE)
  )
}

</code></pre>

<hr>
<h2 id='read.dic'>Read/Write Dictionary Files</h2><span id='topic+read.dic'></span><span id='topic+write.dic'></span>

<h3>Description</h3>

<p>Read in or write dictionary files in Comma-Separated Values (.csv; weighted) or
Linguistic Inquiry and Word Count (.dic; non-weighted) format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.dic(path, cats = NULL, type = "asis", as.weighted = FALSE,
  dir = getOption("lingmatch.dict.dir"), ..., term.name = "term",
  category.name = "category", raw = FALSE)

write.dic(dict, filename = NULL, type = "asis", as.weighted = FALSE,
  save = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read.dic_+3A_path">path</code></td>
<td>
<p>Path to a file, a name corresponding to a file in <code>getOption('lingmatch.dict.dir')</code>
(or <code>'~/Dictionaries'</code>) or one of the dictionaries available at <a href="https://osf.io/y6g5b">osf.io/y6g5b</a>,
a matrix-like object to be categorized, or a list to be formatted.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_cats">cats</code></td>
<td>
<p>A character vector of category names to be returned. All categories are returned by default.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_type">type</code></td>
<td>
<p>A character indicating whether and how terms should be altered. Unspecified or matching 'asis'
leaves terms as they are. Other options change wildcards to regular expressions:
<code>'pattern'</code> (<code>'^[poi]'</code>) replaces initial asterisks with <code>'\\b\\w*'</code>,
and terminal asterisks with <code>'\\w*\\b'</code>, to match terms within raw text;
for anything else, terms are padded with <code>^</code> and <code>$</code>, then those bounding marks are removed
when an asterisk is present, to match tokenized terms.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_as.weighted">as.weighted</code></td>
<td>
<p>Logical; if <code>TRUE</code>, prevents weighted dictionaries from being converted to
unweighted versions, or converts unweighted dictionaries to a binary weighted version
&ndash; a data.frame with a &quot;term&quot; column of unique terms, and a column for each category.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_dir">dir</code></td>
<td>
<p>Path to a folder containing dictionaries, or where you would like dictionaries to be downloaded;
passed to <code><a href="#topic+select.dict">select.dict</a></code> and/or <code><a href="#topic+download.dict">download.dict</a></code>.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_...">...</code></td>
<td>
<p>Passes arguments to <code><a href="base.html#topic+readLines">readLines</a></code>.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_term.name">term.name</code>, <code id="read.dic_+3A_category.name">category.name</code></td>
<td>
<p>Strings identifying column names in <code>path</code> containing terms and categories
respectively.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_raw">raw</code></td>
<td>
<p>Logical or a character. As logical, indicates if <code>path</code> should be treated
as a raw dictionary (as might be read in from a .dic file). As a character, replaces <code>path</code>
as if it were read in from a file.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_dict">dict</code></td>
<td>
<p>A <code>list</code> with a named entry of terms for each category, or a <code>data.frame</code>
with terms in one column, and categories or weights in the rest.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_filename">filename</code></td>
<td>
<p>The name of the file to be saved.</p>
</td></tr>
<tr><td><code id="read.dic_+3A_save">save</code></td>
<td>
<p>Logical: if <code>FALSE</code>, does not write a file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>read.dic</code>: A <code>list</code> (unweighted) with an entry for each category containing
character vectors of terms, or a <code>data.frame</code> (weighted) with columns for terms (first, &quot;term&quot;) and
weights (all subsequent, with category labels as names).
</p>
<p><code>write.dic</code>: A version of the written dictionary &ndash; a raw character vector for
unweighted dictionaries, or a <code>data.frame</code> for weighted dictionaries.
</p>


<h3>See Also</h3>

<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># make a small murder related dictionary
dict &lt;- list(
  kill = c("kill*", "murd*", "wound*", "die*"),
  death = c("death*", "dying", "die*", "kill*")
)

# convert it to a weighted format
(dict_weighted &lt;- read.dic(dict, as.weighted = TRUE))

# categorize it back
read.dic(dict_weighted)

# convert it to a string without writing to a file
cat(raw_dict &lt;- write.dic(dict, save = FALSE))

# parse it back in
read.dic(raw = raw_dict)

## Not run: 

# save it as a .dic file
write.dic(dict, "murder")

# read it back in as a list
read.dic("murder.dic")

# read in the Moral Foundations or LUSI dictionaries from urls
moral_dict &lt;- read.dic("https://osf.io/download/whjt2")
lusi_dict &lt;- read.dic("https://osf.io/download/29ayf")

# save and read in a version of the General Inquirer dictionary
inquirer &lt;- read.dic("inquirer", dir = "~/Dictionaries")

## End(Not run)
</code></pre>

<hr>
<h2 id='read.segments'>Read and Segment Multiple Texts</h2><span id='topic+read.segments'></span>

<h3>Description</h3>

<p>Split texts by word count or specific characters. Input texts directly, or read them in from files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.segments(path = ".", segment = NULL, ext = ".txt", subdir = FALSE,
  segment.size = -1, bysentence = FALSE, end_in_quotes = TRUE,
  preclean = FALSE, text = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read.segments_+3A_path">path</code></td>
<td>
<p>Path to a folder containing files, or a vector of paths to files. If no folders or files are
recognized in <code>path</code>, it is treated as <code>text</code>.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_segment">segment</code></td>
<td>
<p>Specifies how the text of each file should be segmented. If a character, split at that character;
'\n' by default. If a number, texts will be broken into that many segments, each with a roughly equal number of
words.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_ext">ext</code></td>
<td>
<p>The extension of the files you want to read in. '.txt' by default.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_subdir">subdir</code></td>
<td>
<p>Logical; if <code>TRUE</code>, files in folders in <code>path</code> will also be included.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_segment.size">segment.size</code></td>
<td>
<p>Logical; if specified, <code>segment</code> will be ignored, and texts will be broken into
segments containing roughly <code>segment.size</code> number of words.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_bysentence">bysentence</code></td>
<td>
<p>Logical; if <code>TRUE</code>, and <code>segment</code> is a number or <code>segment.size</code> is specified,
sentences will be kept together, rather than potentially being broken across segments.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_end_in_quotes">end_in_quotes</code></td>
<td>
<p>Logical; if <code>FALSE</code>, sentence-ending marks (<code>.?!</code>) will not be considered when
immediately followed by a quotation mark. For example, <code>'"Word." Word.'</code> would be considered one sentence.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_preclean">preclean</code></td>
<td>
<p>Logical; if <code>TRUE</code>, text will be cleaned with <code>lma_dict(special)</code> before
segmentation.</p>
</td></tr>
<tr><td><code id="read.segments_+3A_text">text</code></td>
<td>
<p>A character vector with text to be split, used in place of <code>path</code>. Each entry is treated as a file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.frame</code> with columns for file names (<code>input</code>),
segment number within file (<code>segment</code>), word count for each segment (<code>WC</code>), and the text of
each segment (<code>text</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># split preloaded text
read.segments("split this text into two segments", 2)

## Not run: 

# read in all files from the package directory
texts &lt;- read.segments(path.package("lingmatch"), ext = "")
texts[, -4]

# segment .txt files in dir in a few ways:
dir &lt;- "path/to/files"

## into 1 line segments
texts_lines &lt;- read.segments(dir)

## into 5 even segments each
texts_5segs &lt;- read.segments(dir, 5)

## into 50 word segments
texts_50words &lt;- read.segments(dir, segment.size = 50)

## into 1 sentence segments
texts_1sent &lt;- read.segments(dir, segment.size = 1, bysentence = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='report_term_matches'>Generate a Report of Term Matches</h2><span id='topic+report_term_matches'></span>

<h3>Description</h3>

<p>Extract matches to fuzzy terms (globs/wildcards or regular expressions) from provided text, in order
to assess their appropriateness for inclusion in a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>report_term_matches(dict, text = NULL, space = NULL, glob = TRUE,
  parse_phrases = TRUE, tolower = TRUE, punct = TRUE, special = TRUE,
  as_terms = FALSE, bysentence = FALSE, as_string = TRUE,
  term_map_freq = 1, term_map_spaces = 1, outFile = NULL,
  space_dir = getOption("lingmatch.lspace.dir"), verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="report_term_matches_+3A_dict">dict</code></td>
<td>
<p>A vector of terms, list of such vectors, or a matrix-like object to be
categorized by <code><a href="#topic+read.dic">read.dic</a></code>.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_text">text</code></td>
<td>
<p>A vector of text to extract matches from. If not specified, will use the terms
in the <code>term_map</code> retrieved from <code><a href="#topic+select.lspace">select.lspace</a></code>.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_space">space</code></td>
<td>
<p>A vector space used to calculate similarities between term matches.
Name of a the space (see <code><a href="#topic+select.lspace">select.lspace</a></code>), a matrix with terms as row names, or
<code>TRUE</code> to auto-select a space based on matched terms.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_glob">glob</code></td>
<td>
<p>Logical; if <code>TRUE</code>, converts globs (asterisk wildcards) to regular expressions.
If not specified, this will be set automatically.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_parse_phrases">parse_phrases</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default) and <code>space</code> is specified, will
break unmatched phrases into single terms, and average across and matched vectors.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_tolower">tolower</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will retain <code>text</code>'s case.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_punct">punct</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will remove punctuation markings in <code>text</code>.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_special">special</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will attempt to replace special characters in <code>text</code>.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_as_terms">as_terms</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will treat <code>text</code> as terms, meaning <code>dict</code>
terms will only count as matches when matching the complete text.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_bysentence">bysentence</code></td>
<td>
<p>Logical; if <code>TRUE</code>, will split <code>text</code> into sentences, and only
consider unique sentences.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_as_string">as_string</code></td>
<td>
<p>Logical; if <code>FALSE</code>, returns matches as tables rather than a string.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_term_map_freq">term_map_freq</code></td>
<td>
<p>Proportion of terms to include when using the term map as a source
of terms. Applies when <code>text</code> is not specified.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_term_map_spaces">term_map_spaces</code></td>
<td>
<p>Number of spaces in which a term has to appear to be included.
Applies when <code>text</code> is not specified.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_outfile">outFile</code></td>
<td>
<p>File path to write results to, always ending in <code>.csv</code>.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_space_dir">space_dir</code></td>
<td>
<p>Directory from which <code>space</code> should be loaded.</p>
</td></tr>
<tr><td><code id="report_term_matches_+3A_verbose">verbose</code></td>
<td>
<p>Logical; if <code>FALSE</code>, will not display status messages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.frame</code> of results, with a row for each unique term, and the following columns:
</p>

<ul>
<li> <p><strong><code>term</code></strong>: The originally entered term.
</p>
</li>
<li> <p><strong><code>regex</code></strong>: The converted and applied regular expression form of the term.
</p>
</li>
<li> <p><strong><code>categories</code></strong>: Comma-separated category names,
if <code>dict</code> is a list with named entries.
</p>
</li>
<li> <p><strong><code>count</code></strong>: Total number of matches to the term.
</p>
</li>
<li> <p><strong><code>max_count</code></strong>: Number of matches to the most representative
(that with the highest average similarity) variant of the term.
</p>
</li>
<li> <p><strong><code>variants</code></strong>: Number of variants of the term.
</p>
</li>
<li> <p><strong><code>space</code></strong>: Name of the latent semantic space, if one was used.
</p>
</li>
<li> <p><strong><code>mean_sim</code></strong>: Average similarity to the most representative variant among terms
found in the space, if one was used.
</p>
</li>
<li> <p><strong><code>min_sim</code></strong>: Minimal similarity to the most representative variant.
</p>
</li>
<li> <p><strong><code>matches</code></strong>: Variants, with counts and similarity (Pearson's r) to the
most representative term (if a space was specified). Either in the form of a comma-separated
string or a <code>data.frame</code> (if <code>as_string</code> is <code>FALSE</code>).
</p>
</li></ul>



<h3>Note</h3>

<p>Matches are extracted for each term independently, so they may not align with some implementations
of dictionaries. For instance, by default <code><a href="#topic+lma_patcat">lma_patcat</a></code> matches destructively, and sorts
terms by length such that shorter terms will not match the same text and longer terms that overlap.
Here, the match would show up for both terms.
</p>


<h3>See Also</h3>

<p>For a more complete assessment of dictionaries, see <code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>.
</p>
<p>Similar information is provided in the <a href="https://miserman.github.io/dictionary_builder/">dictionary builder</a> web tool.
</p>
<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+select.dict">select.dict</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- c(
  "I am sadly homeless, and suffering from depression :(",
  "This wholesome happiness brings joy to my heart! :D:D:D",
  "They are joyous in these fearsome happenings D:",
  "I feel weightless now that my sadness has been depressed! :()"
)
dict &lt;- list(
  sad = c("*less", "sad*", "depres*", ":("),
  happy = c("*some", "happ*", "joy*", "d:"),
  self = c("i *", "my *")
)

report_term_matches(dict, text)
</code></pre>

<hr>
<h2 id='select.dict'>Select Dictionaries</h2><span id='topic+select.dict'></span>

<h3>Description</h3>

<p>Retrieve information and links to dictionaries
(lexicons/word lists) available at <a href="https://osf.io/y6g5b">osf.io/y6g5b</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.dict(query = NULL, dir = getOption("lingmatch.dict.dir"),
  check.md5 = TRUE, mode = "wb")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="select.dict_+3A_query">query</code></td>
<td>
<p>A character matching a dictionary name, or a set of keywords to search for in
dictionary information.</p>
</td></tr>
<tr><td><code id="select.dict_+3A_dir">dir</code></td>
<td>
<p>Path to a folder containing dictionaries, or where you want them to be saved.
Will look in getOption('lingmatch.dict.dir') and '~/Dictionaries' by default.</p>
</td></tr>
<tr><td><code id="select.dict_+3A_check.md5">check.md5</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), retrieves the MD5 checksum from OSF,
and compares it with that calculated from the downloaded file to check its integrity.</p>
</td></tr>
<tr><td><code id="select.dict_+3A_mode">mode</code></td>
<td>
<p>Passed to <code><a href="utils.html#topic+download.file">download.file</a></code> when downloading files.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with varying entries:
</p>

<ul>
<li> <p><strong><code>info</code></strong>: The version of <a href="https://osf.io/kjqb8">osf.io/kjqb8</a> stored internally; a
<code>data.frame</code>  with dictionary names as row names, and information about each dictionary in columns. <br />
Also described at
<a href="https://osf.io/y6g5b/wiki/dict_variables">osf.io/y6g5b/wiki/dict_variables</a>,
here <code>short</code> (corresponding to the file name [<code>{short}.(csv|dic)</code>] and
wiki urls [<code>https://osf.io/y6g5b/wiki/{short}</code>]) is set as row names and removed:
</p>

<ul>
<li> <p><strong><code>name</code></strong>: Full name of the dictionary.
</p>
</li>
<li> <p><strong><code>description</code></strong>: Description of the dictionary, relating to its purpose and
development.
</p>
</li>
<li> <p><strong><code>note</code></strong>: Notes about processing decisions that additionally alter the original.
</p>
</li>
<li> <p><strong><code>constructor</code></strong>: How the dictionary was constructed:
</p>

<ul>
<li> <p><strong><code>algorithm</code></strong>: Terms were selected by some automated process, potentially
learned from data or other resources.
</p>
</li>
<li> <p><strong><code>crowd</code></strong>: Several individuals rated the terms, and in aggregate those ratings
translate to categories and weights.
</p>
</li>
<li> <p><strong><code>mixed</code></strong>: Some combination of the other methods, usually in some iterative
process.
</p>
</li>
<li> <p><strong><code>team</code></strong>: One of more individuals make decisions about term inclusions,
categories, and weights.
</p>
</li></ul>

</li>
<li> <p><strong><code>subject</code></strong>: Broad, rough subject or purpose of the dictionary:
</p>

<ul>
<li> <p><strong><code>emotion</code></strong>: Terms relate to emotions, potentially exemplifying or expressing
them.
</p>
</li>
<li> <p><strong><code>general</code></strong>: A large range of categories, aiming to capture the content of the
text.
</p>
</li>
<li> <p><strong><code>impression</code></strong>: Terms are categorized and weighted based on the impression they
might give.
</p>
</li>
<li> <p><strong><code>language</code></strong>: Terms are categorized or weighted based on their linguistic
features, such as part of speech, specificity, or area of use.
</p>
</li>
<li> <p><strong><code>social</code></strong>: Terms relate to social phenomena, such as characteristics or concerns
of social entities.
</p>
</li></ul>

</li>
<li> <p><strong><code>terms</code></strong>: Number of unique terms across categories.
</p>
</li>
<li> <p><strong><code>term_type</code></strong>: Format of the terms:
</p>

<ul>
<li> <p><strong><code>glob</code></strong>: Include asterisks which denote inclusion of any characters until a
word boundary.
</p>
</li>
<li> <p><strong><code>glob+</code></strong>: Glob-style asterisks with regular expressions within terms.
</p>
</li>
<li> <p><strong><code>ngram</code></strong>: Includes any number of words as a term, separated by spaces.
</p>
</li>
<li> <p><strong><code>pattern</code></strong>: A string of characters, potentially within or between words, or
spanning words.
</p>
</li>
<li> <p><strong><code>regex</code></strong>: Regular expressions.
</p>
</li>
<li> <p><strong><code>stem</code></strong>: Unigrams with common endings removed.
</p>
</li>
<li> <p><strong><code>unigram</code></strong>: Complete single words.
</p>
</li></ul>

</li>
<li> <p><strong><code>weighted</code></strong>: Indicates whether weights are associated with terms. This
determines the file type of the dictionary: dictionaries with weights are stored
as .csv, and those without are stored as .dic files.
</p>
</li>
<li> <p><strong><code>regex_characters</code></strong>: Logical indicating whether special regular expression
characters are present in any term, which might need to be escaped if the terms are used
in regular expressions. Glob-type terms allow complete parens (at least one open and one
closed, indicating preceding or following words), and initial and terminal asterisks. For
all other terms, <code>[](){}*.^$+?\|</code> are counted as regex characters. These could be
escaped in R with <code>gsub('([][)(}{*.^$+?\\|])', '\\\1', terms)</code> if <code>terms</code>
is a character vector, and in Python with (importing re)
<code>[re.sub(r'([][(){}*.^$+?\|])', r'\\1', term)</code> <code>for term in terms]</code> if <code>terms</code>
is a list.
</p>
</li>
<li> <p><strong><code>categories</code></strong>: Category names in the order in which they appear in the dictionary
file, separated by commas.
</p>
</li>
<li> <p><strong><code>ncategories</code></strong>: Number of categories.
</p>
</li>
<li> <p><strong><code>original_max</code></strong>: Maximum value of the original dictionary before standardization:
<code>original values / max(original values) * 100</code>. Dictionaries with no weights are
considered to have a max of <code>1</code>.
</p>
</li>
<li> <p><strong><code>osf</code></strong>: ID of the file on OSF, translating to the file's URL:
https://osf.io/<code>osf</code>.
</p>
</li>
<li> <p><strong><code>wiki</code></strong>: URL of the dictionary's wiki.
</p>
</li>
<li> <p><strong><code>downloaded</code></strong>: Path to the file if downloaded, and <code>''</code> otherwise.
</p>
</li></ul>

</li>
<li> <p><strong><code>selected</code></strong>: A subset of <code>info</code> selected by <code>query</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Dictionary functions: 
<code><a href="#topic+dictionary_meta">dictionary_meta</a>()</code>,
<code><a href="#topic+download.dict">download.dict</a>()</code>,
<code><a href="#topic+lma_patcat">lma_patcat</a>()</code>,
<code><a href="#topic+lma_termcat">lma_termcat</a>()</code>,
<code><a href="#topic+read.dic">read.dic</a>()</code>,
<code><a href="#topic+report_term_matches">report_term_matches</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># just retrieve information about available dictionaries
dicts &lt;- select.dict()$info
dicts[1:10, 4:9]

# select all dictionaries mentioning sentiment or emotion
sentiment_dicts &lt;- select.dict("sentiment emotion")$selected
sentiment_dicts[1:10, 4:9]
</code></pre>

<hr>
<h2 id='select.lspace'>Select Latent Semantic Spaces</h2><span id='topic+select.lspace'></span>

<h3>Description</h3>

<p>Retrieve information and links to latent semantic spaces
(sets of word vectors/embeddings) available at <a href="https://osf.io/489he">osf.io/489he</a>,
and optionally download their term mappings (<a href="https://osf.io/xr7jv">osf.io/xr7jv</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.lspace(query = NULL, dir = getOption("lingmatch.lspace.dir"),
  terms = NULL, get.map = FALSE, check.md5 = TRUE, mode = "wb")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="select.lspace_+3A_query">query</code></td>
<td>
<p>A character used to select spaces, based on names or other features.
If length is over 1, <code>get.map</code> is set to <code>TRUE</code>. Use <code>terms</code> alone to select
spaces based on term coverage.</p>
</td></tr>
<tr><td><code id="select.lspace_+3A_dir">dir</code></td>
<td>
<p>Path to a directory containing <code>lma_term_map.rda</code> and downloaded spaces; <br /> will look in
<code>getOption('lingmatch.lspace.dir')</code> and <code>'~/Latent Semantic Spaces'</code> by default.</p>
</td></tr>
<tr><td><code id="select.lspace_+3A_terms">terms</code></td>
<td>
<p>A character vector of terms to search for in the downloaded term map, to calculate
coverage of spaces, or select by coverage if <code>query</code> is not specified.</p>
</td></tr>
<tr><td><code id="select.lspace_+3A_get.map">get.map</code></td>
<td>
<p>Logical; if <code>TRUE</code> and <code>lma_term_map.rda</code> is not found in
<code>dir</code>, the term map (<a href="https://osf.io/xr7jv">lma_term_map.rda</a>) is
downloaded and decompressed.</p>
</td></tr>
<tr><td><code id="select.lspace_+3A_check.md5">check.md5</code></td>
<td>
<p>Logical; if <code>TRUE</code> (default), retrieves the MD5 checksum from OSF,
and compares it with that calculated from the downloaded file to check its integrity.</p>
</td></tr>
<tr><td><code id="select.lspace_+3A_mode">mode</code></td>
<td>
<p>Passed to <code><a href="utils.html#topic+download.file">download.file</a></code> when downloading the term map.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with varying entries:
</p>

<ul>
<li> <p><strong><code>info</code></strong>: The version of <a href="https://osf.io/9yzca">osf.io/9yzca</a> stored internally; a
<code>data.frame</code>  with spaces as row names, and information about each space in columns:
</p>

<ul>
<li> <p><strong><code>terms</code></strong>: number of terms in the space
</p>
</li>
<li> <p><strong><code>corpus</code></strong>: corpus(es) on which the space was trained
</p>
</li>
<li> <p><strong><code>model</code></strong>: model from which the space was trained
</p>
</li>
<li> <p><strong><code>dimensions</code></strong>: number of dimensions in the model (columns of the space)
</p>
</li>
<li> <p><strong><code>model_info</code></strong>: some parameter details about the model
</p>
</li>
<li> <p><strong><code>original_max</code></strong>: maximum value used to normalize the space; the original
space would be <code>(vectors *</code> <code>original_max) /</code> <code>100</code>
</p>
</li>
<li> <p><strong><code>osf_dat</code></strong>: OSF id for the <code>.dat</code> files; the URL would be
https://osf.io/<code>osf_dat</code>
</p>
</li>
<li> <p><strong><code>osf_terms</code></strong>: OSF id for the <code>_terms.txt</code> files; the URL would be
https://osf.io/<code>osf_terms</code>
</p>
</li>
<li> <p><strong><code>wiki</code></strong>: link to the wiki for the space
</p>
</li>
<li> <p><strong><code>downloaded</code></strong>: path to the <code>.dat</code> file if downloaded,
and <code>''</code> otherwise.
</p>
</li></ul>

</li>
<li> <p><strong><code>selected</code></strong>: A subset of <code>info</code> selected by <code>query</code>.
</p>
</li>
<li> <p><strong><code>term_map</code></strong>: If <code>get.map</code> is <code>TRUE</code> or <code>lma_term_map.rda</code> is found in
<code>dir</code>, a copy of <a href="https://osf.io/xr7jv">osf.io/xr7jv</a>, which has space names as
column names, terms as row names, and indices as values, with 0 indicating the term is not
present in the associated space.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Latent Semantic Space functions: 
<code><a href="#topic+download.lspace">download.lspace</a>()</code>,
<code><a href="#topic+lma_lspace">lma_lspace</a>()</code>,
<code><a href="#topic+standardize.lspace">standardize.lspace</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># just retrieve information about available spaces
spaces &lt;- select.lspace()
spaces$info[1:10, c("terms", "dimensions", "original_max")]

# retrieve all spaces that used word2vec
w2v_spaces &lt;- select.lspace("word2vec")$selected
w2v_spaces[, c("terms", "dimensions", "original_max")]

## Not run: 

# select spaces by terms
select.lspace(terms = c(
  "part-time", "i/o", "'cause", "brexit", "debuffs"
))$selected[, c("terms", "coverage")]

## End(Not run)
</code></pre>

<hr>
<h2 id='standardize.lspace'>Standardize a Latent Semantic Space</h2><span id='topic+standardize.lspace'></span>

<h3>Description</h3>

<p>Reformat a .rda file which has a matrix with terms as row names, or a plain-text embeddings file
which has a term at the start of each line, and consistent delimiting characters. Plain-text files
are processed line-by-line, so large spaces can be reformatted RAM-conservatively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardize.lspace(infile, name, sep = " ", digits = 9,
  dir = getOption("lingmatch.lspace.dir"), outdir = dir, remove = "",
  term_check = "^[a-zA-Z]+$|^['a-zA-Z][a-zA-Z.'\\/-]*[a-zA-Z.]$",
  verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="standardize.lspace_+3A_infile">infile</code></td>
<td>
<p>Name of the .rda or plain-text file relative to <code>dir</code>, <br />
e.g., &quot;default.rda&quot; or &quot;glove/glove.6B.300d.txt&quot;.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_name">name</code></td>
<td>
<p>Base name of the reformatted file and term file; e.g., &quot;glove&quot; would result in
<code>glove.dat</code> and <code>glove_terms.txt</code> in <code>outdir</code>.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_sep">sep</code></td>
<td>
<p>Delimiting character between values in each line, e.g., <code>" "</code> or <code>"\t"</code>.
Only applies to plain-text files.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_digits">digits</code></td>
<td>
<p>Number of digits to round values to; default is 9.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_dir">dir</code></td>
<td>
<p>Path to folder containing <code>infile</code>s. <br /> Default is <code>getOption('lingmatch.lspace.dir')</code>,
which must be set in the current session. If this is not specified and <code>infile</code> is a full path,
<code>dir</code> will be set to <code>infile</code>'s parent directory.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_outdir">outdir</code></td>
<td>
<p>Path to folder in which to save standardized files; default is <code>dir</code>.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_remove">remove</code></td>
<td>
<p>A string with a regex pattern to be removed from term names <br /> (i.e., <code>gsub(remove,</code>
<code>"", term)</code>); default is <code>""</code>, which is ignored.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_term_check">term_check</code></td>
<td>
<p>A string with a regex pattern by which to filter terms; i.e., only lines with fully
matched terms are written to the reformatted file. The default attempts to retain only regular words, including
those with dashes, foreword slashes, and periods. Set to an empty string (<code>""</code>) to write all lines
regardless of term.</p>
</td></tr>
<tr><td><code id="standardize.lspace_+3A_verbose">verbose</code></td>
<td>
<p>Logical: if <code>TRUE</code>, prints the current line number and its term to the console every 1,000 lines.
Only applies to plain-text files.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Path to the standardized [1] data file and [2] terms file if applicable.
</p>


<h3>See Also</h3>

<p>Other Latent Semantic Space functions: 
<code><a href="#topic+download.lspace">download.lspace</a>()</code>,
<code><a href="#topic+lma_lspace">lma_lspace</a>()</code>,
<code><a href="#topic+select.lspace">select.lspace</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# from https://sites.google.com/site/fritzgntr/software-resources/semantic_spaces
standardize.lspace("EN_100k_lsa.rda", "100k_lsa")

# from https://fasttext.cc/docs/en/english-vectors.html
standardize.lspace("crawl-300d-2M.vec", "facebook_crawl")

# Standardized versions of these spaces can also be downloaded with download.lspace.

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
