<!DOCTYPE html><html><head><title>Help for package MTE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MTE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#huber.lasso'><p>Huber-Lasso estimator</p></a></li>
<li><a href='#huber.reg'><p>Huber estimation for linear regression</p></a></li>
<li><a href='#huberloss'><p>Huber Loss</p></a></li>
<li><a href='#LAD'><p>Least Absolute Deviance Estimator for Linear Regression</p></a></li>
<li><a href='#LADlasso'><p>LAD-Lasso for Linear Regression</p></a></li>
<li><a href='#MTE'><p>Maximum Tangent-likelihood Estimation</p></a></li>
<li><a href='#MTElasso'><p>MTE-Lasso estimator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Maximum Tangent Likelihood Estimation for Robust Linear
Regression and Variable Selection</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-08</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shaobo Li &lt;shaobo.li@ku.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Several robust estimators for linear regression and variable selection are provided. 
              Included are Maximum tangent likelihood estimator by Qin, et al., (2017) &lt;<a href="https://arxiv.org/abs/1708.05439">arXiv:1708.05439</a>&gt;, 
              least absolute deviance estimator and Huber regression. The penalized version of each of these 
              estimator incorporates L1 penalty function, i.e., LASSO and Adaptive Lasso. They are able to 
              produce consistent estimates for both fixed and high-dimensional settings. </td>
</tr>
<tr>
<td>URL:</td>
<td>GitHub: <a href="https://github.com/shaobo-li/MTE">https://github.com/shaobo-li/MTE</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, quantreg, glmnet, rqPen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-10 02:07:20 UTC; lisha</td>
</tr>
<tr>
<td>Author:</td>
<td>Shaobo Li [aut, cre],
  Yichen Qin [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-11 09:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='huber.lasso'>Huber-Lasso estimator</h2><span id='topic+huber.lasso'></span>

<h3>Description</h3>

<p>This function is L1 penalized Huber estimator for linear regression under both fixed and high-dimensional settings.
Currently, the function does not support automatic selection of huber tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huber.lasso(
  X,
  y,
  beta.ini,
  lambda,
  alpha = 2,
  adaptive = TRUE,
  intercept = TRUE,
  penalty.factor = rep(1, ncol(X))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huber.lasso_+3A_x">X</code></td>
<td>
<p>design matrix, standardization is recommended.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_y">y</code></td>
<td>
<p>response vector.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_beta.ini">beta.ini</code></td>
<td>
<p>initial estimates of beta. If not specified, LADLasso estimates from <code>rq.lasso.fit()</code> in <code>rqPen</code>
is used. Otherwise, robust estimators are strongly recommended.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter of Lasso or adaptive Lasso (if adaptive=TRUE).</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_alpha">alpha</code></td>
<td>
<p>1/alpha is the huber tuning parameter. Larger alpha results in smaller portion of squared loss.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_adaptive">adaptive</code></td>
<td>
<p>logical input that indicates if adaptive Lasso is used. Default is TRUE.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
<tr><td><code id="huber.lasso_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>can be used to force nonzero coefficients. Default is rep(1, ncol(X)) as in glmnet.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>the regression coefficient estimates.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>predicted response.</p>
</td></tr>
<tr><td><code>iter.steps</code></td>
<td>
<p>iteration steps.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2017)
n=200; d=500
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(rep(2,6), rep(0, d-6))
y=X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
output.HuberLasso=huber.lasso(X,y)
beta.est=output.HuberLasso$beta

</code></pre>

<hr>
<h2 id='huber.reg'>Huber estimation for linear regression</h2><span id='topic+huber.reg'></span>

<h3>Description</h3>

<p>This function produces Huber estimates for linear regression. Initial estimates is required.
Currently, the function does not support automatic selection of huber tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huber.reg(y, X, beta.ini, alpha, intercept = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huber.reg_+3A_y">y</code></td>
<td>
<p>the response vector</p>
</td></tr>
<tr><td><code id="huber.reg_+3A_x">X</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="huber.reg_+3A_beta.ini">beta.ini</code></td>
<td>
<p>initial value of estimates, could be from OLS.</p>
</td></tr>
<tr><td><code id="huber.reg_+3A_alpha">alpha</code></td>
<td>
<p>1/alpha is the huber tuning parameter delta. Larger alpha results in smaller portion of squared loss.</p>
</td></tr>
<tr><td><code id="huber.reg_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>the regression coefficient estimates</p>
</td></tr>
<tr><td><code>fitted.value</code></td>
<td>
<p>predicted response</p>
</td></tr>
<tr><td><code>iter.steps</code></td>
<td>
<p>iteration steps.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2017)
n=200; d=4
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(1, -1, 2, -2)
y=-2+X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
beta0=beta.ls=lm(y~X)$coeff
beta.huber=huber.reg(y, X, beta0, 2, intercept=TRUE)$beta
cbind(c(-2,beta), beta.ls, beta.huber)
</code></pre>

<hr>
<h2 id='huberloss'>Huber Loss</h2><span id='topic+huberloss'></span>

<h3>Description</h3>

<p>Huber Loss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huberloss(r, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huberloss_+3A_r">r</code></td>
<td>
<p>residual, y-Xbeta</p>
</td></tr>
<tr><td><code id="huberloss_+3A_alpha">alpha</code></td>
<td>
<p>1/alpha is the huber tuning parameter delta. Larger alpha results in smaller portion of squared loss.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>it returns huber loss that will be called in Huber estimation.
</p>

<hr>
<h2 id='LAD'>Least Absolute Deviance Estimator for Linear Regression</h2><span id='topic+LAD'></span>

<h3>Description</h3>

<p>Least Absolute Deviance Estimator for Linear Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LAD(X, y, intercept = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LAD_+3A_x">X</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="LAD_+3A_y">y</code></td>
<td>
<p>reponse vector</p>
</td></tr>
<tr><td><code id="LAD_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>coefficient estimates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1989)
n=200; d=4
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(1, -1, 2, -2)
y=-2+X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
beta.ls=lm(y~X)$coeff
beta.LAD=LAD(X,y,intercept=TRUE)
cbind(c(-2,beta), beta.ls, beta.LAD)

</code></pre>

<hr>
<h2 id='LADlasso'>LAD-Lasso for Linear Regression</h2><span id='topic+LADlasso'></span>

<h3>Description</h3>

<p>LAD-Lasso for Linear Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LADlasso(
  X,
  y,
  beta.ini,
  lambda = NULL,
  adaptive = TRUE,
  intercept = FALSE,
  penalty.factor = rep(1, ncol(X))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LADlasso_+3A_x">X</code></td>
<td>
<p>design matrix, standardization is recommended.</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_y">y</code></td>
<td>
<p>reponse vector</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_beta.ini">beta.ini</code></td>
<td>
<p>initial estimates of beta. Using unpenalized LAD is recommended under high-dimensional setting.</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter of Lasso or adaptive Lasso (if adaptive=TRUE).</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_adaptive">adaptive</code></td>
<td>
<p>logical input that indicates if adaptive Lasso is used. Default is TRUE.</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
<tr><td><code id="LADlasso_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>can be used to force nonzero coefficients. Default is rep(1, ncol(X)) as in glmnet.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>the regression coefficient estimates.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>predicted response.</p>
</td></tr>
<tr><td><code>iter.steps</code></td>
<td>
<p>iteration steps.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2017)
n=200; d=50
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(rep(2,6), rep(0, 44))
y=X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
output.LADLasso=LADlasso(X, y, beta.ini=LAD(X, y))
beta.est=output.LADLasso$beta

</code></pre>

<hr>
<h2 id='MTE'>Maximum Tangent-likelihood Estimation</h2><span id='topic+MTE'></span>

<h3>Description</h3>

<p>It estimates linear regression coefficient using MTE.
The function produces robust estimates of linear regression. Outliers and contamination would be downweighted.
It is robust to Gaussian assumption of the error term. Initial estimates need to be provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MTE(y, X, beta.ini, t, p, intercept = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MTE_+3A_y">y</code></td>
<td>
<p>the response vector</p>
</td></tr>
<tr><td><code id="MTE_+3A_x">X</code></td>
<td>
<p>design matrix</p>
</td></tr>
<tr><td><code id="MTE_+3A_beta.ini">beta.ini</code></td>
<td>
<p>initial value of estimates, could be from OLS.</p>
</td></tr>
<tr><td><code id="MTE_+3A_t">t</code></td>
<td>
<p>the tangent point. You may specify a sequence of values, so that the function automatically select the optimal one.</p>
</td></tr>
<tr><td><code id="MTE_+3A_p">p</code></td>
<td>
<p>Taylor expansion order, up to 3.</p>
</td></tr>
<tr><td><code id="MTE_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns estimates from MTE method.
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the regression coefficient estimates</p>
</td></tr>
<tr><td><code>fitted.value</code></td>
<td>
<p>predicted response</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>the optimal tangent point through data-driven method</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2017)
n=200; d=4
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(1, -1, 2, -2)
y=-2+X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
beta0=beta.ls=lm(y~X)$coeff
beta.MTE=MTE(y,X,beta0,0.1,2, intercept=TRUE)$beta
cbind(c(-2,beta), beta.ls, beta.MTE)

</code></pre>

<hr>
<h2 id='MTElasso'>MTE-Lasso estimator</h2><span id='topic+MTElasso'></span>

<h3>Description</h3>

<p>MTELasso is the penalized MTE for robust estimation and variable selection for linear regression.
It can deal with both fixed and high-dimensional settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MTElasso(
  X,
  y,
  beta.ini,
  p = 2,
  lambda = NULL,
  adaptive = TRUE,
  t = 0.01,
  intercept = TRUE,
  penalty.factor = rep(1, ncol(X)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MTElasso_+3A_x">X</code></td>
<td>
<p>design matrix, standardization is recommended.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_y">y</code></td>
<td>
<p>response vector.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_beta.ini">beta.ini</code></td>
<td>
<p>initial estimates of beta. If not specified, LADLasso estimates from <code>rq.lasso.fit()</code> in <code>rqPen</code>
is used. Otherwise, robust estimators are strongly recommended.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_p">p</code></td>
<td>
<p>Taylor expansion order.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for LASSO, but not necessary if &quot;adaptive=TRUE&quot;.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_adaptive">adaptive</code></td>
<td>
<p>logic argument to indicate if Adaptive-Lasso is used. Default is TRUE.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_t">t</code></td>
<td>
<p>the tuning parameter that controls for the tradeoff between robustness and efficiency. Default is t=0.01.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_intercept">intercept</code></td>
<td>
<p>logical input that indicates if intercept needs to be estimated. Default is FALSE.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>can be used to force nonzero coefficients. Default is rep(1, ncol(X)) as in glmnet.</p>
</td></tr>
<tr><td><code id="MTElasso_+3A_...">...</code></td>
<td>
<p>other arguments that are used in <code>glmnet</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns a sparse vector of estimates of linear regression. It has two types of penalty, LASSO and AdaLasso.
Coordinate descent algorithm is used for iteratively updating coefficients.
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>sparse regression coefficient</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>predicted response</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2017)
n=200; d=500
X=matrix(rnorm(n*d), nrow=n, ncol=d)
beta=c(rep(2,6), rep(0, d-6))
y=X%*%beta+c(rnorm(150), rnorm(30,10,10), rnorm(20,0,100))
output.MTELasso=MTElasso(X, y, p=2, t=0.01)
beta.est=output.MTELasso$beta

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
