<!DOCTYPE html><html><head><title>Help for package SetTest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SetTest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pbj'><p>CDF of Berk-Jones statitic under the null hypothesis.</p></a></li>
<li><a href='#phc'><p>CDF of Higher Criticism statitic under the null hypothesis.</p></a></li>
<li><a href='#power.bj'><p>Statistical power of Berk and Jones test.</p></a></li>
<li><a href='#power.hc'><p>Statistical power of Higher Criticism test.</p></a></li>
<li><a href='#power.phi'><p>Statistical power of phi-divergence test.</p></a></li>
<li><a href='#pphi'><p>calculate the left-tail probability of phi-divergence under general correlation matrix.</p></a></li>
<li><a href='#pphi.omni'><p>calculate the left-tail probability of omnibus phi-divergence statistics under general correlation matrix.</p></a></li>
<li><a href='#qbj'><p>Quantile of Berk-Jones statitic under the null hypothesis.</p></a></li>
<li><a href='#qhc'><p>Quantile of Higher Criticism statitics under the null hypothesis.</p></a></li>
<li><a href='#qphi'><p>Quantile of phi-divergence statitic under the null hypothesis.</p></a></li>
<li><a href='#stat.bj'><p>Construct Berk and Jones (BJ) statitics.</p></a></li>
<li><a href='#stat.hc'><p>Construct Higher Criticism (HC) statitics.</p></a></li>
<li><a href='#stat.phi'><p>Construct phi-divergence statitics.</p></a></li>
<li><a href='#stat.phi.omni'><p>calculate the omnibus phi-divergence statistics under general correlation matrix.</p></a></li>
<li><a href='#test.bj'><p>Multiple comparison test using Berk and Jones (BJ) statitics.</p></a></li>
<li><a href='#test.hc'><p>Multiple comparison test using Higher Criticism (HC) statitics.</p></a></li>
<li><a href='#test.phi'><p>Multiple comparison test using phi-divergence statitics.</p></a></li>
<li><a href='#test.phi.omni'><p>calculate the right-tail probability of omnibus phi-divergence statistics under general correlation matrix.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Group Testing Procedures for Signal Detection and
Goodness-of-Fit</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Hong Zhang and Zheyang Wu</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hong Zhang &lt;hzhang@wpi.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>It provides cumulative distribution function (CDF),
    quantile, p-value, statistical power calculator and random number generator
    for a collection of group-testing procedures, including the Higher Criticism
    tests, the one-sided Kolmogorov-Smirnov tests, the one-sided Berk-Jones tests,
    the one-sided phi-divergence tests, etc. The input are a group of p-values.
    The null hypothesis is that they are i.i.d. Uniform(0,1). In the context of
    signal detection, the null hypothesis means no signals. In the context of the
    goodness-of-fit testing, which contrasts a group of i.i.d. random variables to
    a given continuous distribution, the input p-values can be obtained by the CDF
    transformation. The null hypothesis means that these random variables follow the
    given distribution. For reference, see Hong Zhang, Jiashun Jin and Zheyang Wu. 
    "Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases",
    submitted.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-03-22 04:28:36 UTC; Hong</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-03-22 05:24:28 UTC</td>
</tr>
</table>
<hr>
<h2 id='pbj'>CDF of Berk-Jones statitic under the null hypothesis.</h2><span id='topic+pbj'></span>

<h3>Description</h3>

<p>CDF of Berk-Jones statitic under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbj(q, M, k0, k1, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pbj_+3A_q">q</code></td>
<td>
<p>- quantile, must be a scalar.</p>
</td></tr>
<tr><td><code id="pbj_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="pbj_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="pbj_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="pbj_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The left-tail probability of the null distribution of B-J statistic at the given quantile.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Berk, R.H. &amp; Jones, D.H. Z. &quot;Goodness-of-fit test statistics that dominate the Kolmogorov statistics&quot;. Wahrscheinlichkeitstheorie verw Gebiete (1979) 47: 47.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.bj">stat.bj</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pval &lt;- runif(10)
bjstat &lt;- stat.phi(pval, s=1, k0=1, k1=10)$value
pbj(q=bjstat, M=diag(10), k0=1, k1=10)
</code></pre>

<hr>
<h2 id='phc'>CDF of Higher Criticism statitic under the null hypothesis.</h2><span id='topic+phc'></span>

<h3>Description</h3>

<p>CDF of Higher Criticism statitic under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phc(q, M, k0, k1, LS = F, ZW = F, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phc_+3A_q">q</code></td>
<td>
<p>- quantile, must be a scalar.</p>
</td></tr>
<tr><td><code id="phc_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="phc_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="phc_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="phc_+3A_ls">LS</code></td>
<td>
<p>- if LS = T, then method of Li and Siegmund (2015) will be implemented (for independence case only).</p>
</td></tr>
<tr><td><code id="phc_+3A_zw">ZW</code></td>
<td>
<p>- if ZW = T, then approximation method of Zhang and Wu will be implemented.</p>
</td></tr>
<tr><td><code id="phc_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The left-tail probability of the null distribution of HC statistic at the given quantile.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Li, Jian; Siegmund, David. &quot;Higher criticism: p-values and criticism&quot;. Annals of Statistics 43 (2015).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.hc">stat.hc</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pval &lt;- runif(10)
hcstat &lt;- stat.phi(pval, s=2, k0=1, k1=5)$value
phc(q=hcstat, M=diag(10), k0=1, k1=10)
</code></pre>

<hr>
<h2 id='power.bj'>Statistical power of Berk and Jones test.</h2><span id='topic+power.bj'></span>

<h3>Description</h3>

<p>Statistical power of Berk and Jones test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.bj(alpha, n, beta, method = "gaussian-gaussian", eps = 0, mu = 0,
  df = 1, delta = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.bj_+3A_alpha">alpha</code></td>
<td>
<p>- type-I error rate.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_n">n</code></td>
<td>
<p>- dimension parameter, i.e. the number of input statitics to construct B-J statistic.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_beta">beta</code></td>
<td>
<p>- search range parameter. Search range = (1, beta*n). Beta must be between 1/n and 1.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_method">method</code></td>
<td>
<p>- different alternative hypothesis, including mixtures such as, &quot;gaussian-gaussian&quot;, &quot;gaussian-t&quot;, &quot;t-t&quot;, &quot;chisq-chisq&quot;, and &quot;exp-chisq&quot;. By default, we use Gaussian mixture.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_eps">eps</code></td>
<td>
<p>- mixing parameter of the mixture.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_mu">mu</code></td>
<td>
<p>- mean of non standard Gaussian model.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_df">df</code></td>
<td>
<p>- degree of freedom of t/Chisq distribution and exp distribution.</p>
</td></tr>
<tr><td><code id="power.bj_+3A_delta">delta</code></td>
<td>
<p>- non-cenrality of t/Chisq distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We consider the following hypothesis test,
</p>
<p style="text-align: center;"><code class="reqn">H_0: X_i\sim F, H_a: X_i\sim G</code>
</p>

<p>Specifically, <code class="reqn">F = F_0</code> and <code class="reqn">G = (1-\epsilon)F_0+\epsilon F_1</code>, where <code class="reqn">\epsilon</code> is the mixing parameter, <code class="reqn">F_0</code> and <code class="reqn">F_1</code> is
speified by the &quot;method&quot; argument:
</p>
<p>&quot;gaussian-gaussian&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of normal distribution with <code class="reqn">\mu</code> defined by mu and <code class="reqn">\sigma = 1</code>.
</p>
<p>&quot;gaussian-t&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of t distribution with degree of freedom defined by df.
</p>
<p>&quot;t-t&quot;: <code class="reqn">F_0</code> is the CDF of t distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central t distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>
<p>&quot;chisq-chisq&quot;: <code class="reqn">F_0</code> is the CDF of Chisquare distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisquare distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>
<p>&quot;exp-chisq&quot;: <code class="reqn">F_0</code> is the CDF of exponential distribution with parameter defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisqaure distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>


<h3>Value</h3>

<p>Power of BJ test.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Jager, Leah; Wellner, Jon A. &quot;Goodness-of-fit tests via phi-divergences&quot;. Annals of Statistics 35 (2007).
</p>
<p>4. Berk, R.H. &amp; Jones, D.H. Z. &quot;Goodness-of-fit test statistics that dominate the Kolmogorov statistics&quot;. Wahrscheinlichkeitstheorie verw Gebiete (1979) 47: 47.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.bj">stat.bj</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>power.bj(0.05, n=10, beta=0.5, eps = 0.1, mu = 1.2)
</code></pre>

<hr>
<h2 id='power.hc'>Statistical power of Higher Criticism test.</h2><span id='topic+power.hc'></span>

<h3>Description</h3>

<p>Statistical power of Higher Criticism test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.hc(alpha, n, beta, method = "gaussian-gaussian", eps = 0, mu = 0,
  df = 1, delta = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.hc_+3A_alpha">alpha</code></td>
<td>
<p>- type-I error rate.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_n">n</code></td>
<td>
<p>- dimension parameter, i.e. the number of input statitics to construct Higher Criticism statistic.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_beta">beta</code></td>
<td>
<p>- search range parameter. Search range = (1, beta*n). Beta must be between 1/n and 1.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_method">method</code></td>
<td>
<p>- different alternative hypothesis, including mixtures such as, &quot;gaussian-gaussian&quot;, &quot;gaussian-t&quot;, &quot;t-t&quot;, &quot;chisq-chisq&quot;, and &quot;exp-chisq&quot;. By default, we use Gaussian mixture.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_eps">eps</code></td>
<td>
<p>- mixing parameter of the mixture.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_mu">mu</code></td>
<td>
<p>- mean of non standard Gaussian model.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_df">df</code></td>
<td>
<p>- degree of freedom of t/Chisq distribution and exp distribution.</p>
</td></tr>
<tr><td><code id="power.hc_+3A_delta">delta</code></td>
<td>
<p>- non-cenrality of t/Chisq distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We consider the following hypothesis test,
</p>
<p style="text-align: center;"><code class="reqn">H_0: X_i\sim F, H_a: X_i\sim G</code>
</p>

<p>Specifically, <code class="reqn">F = F_0</code> and <code class="reqn">G = (1-\epsilon)F_0+\epsilon F_1</code>, where <code class="reqn">\epsilon</code> is the mixing parameter, <code class="reqn">F_0</code> and <code class="reqn">F_1</code> is
speified by the &quot;method&quot; argument:
</p>
<p>&quot;gaussian-gaussian&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of normal distribution with <code class="reqn">\mu</code> defined by mu and <code class="reqn">\sigma = 1</code>.
</p>
<p>&quot;gaussian-t&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of t distribution with degree of freedom defined by df.
</p>
<p>&quot;t-t&quot;: <code class="reqn">F_0</code> is the CDF of t distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central t distribution with degree of freedom defined by df and non-centrality defined by delta.
&quot;chisq-chisq&quot;: <code class="reqn">F_0</code> is the CDF of Chisquare distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisquare distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>
<p>&quot;exp-chisq&quot;: <code class="reqn">F_0</code> is the CDF of exponential distribution with parameter defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisqaure distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>


<h3>Value</h3>

<p>Power of HC test.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.hc">stat.hc</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>power.hc(0.05, n=10, beta=0.5, eps = 0.1, mu = 1.2)
</code></pre>

<hr>
<h2 id='power.phi'>Statistical power of phi-divergence test.</h2><span id='topic+power.phi'></span>

<h3>Description</h3>

<p>Statistical power of phi-divergence test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.phi(alpha, n, s, beta, method = "gaussian-gaussian", eps = 0,
  mu = 0, df = 1, delta = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.phi_+3A_alpha">alpha</code></td>
<td>
<p>- type-I error rate.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_n">n</code></td>
<td>
<p>- dimension parameter, i.e. the number of input statitics to construct phi-divergence statistic.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_s">s</code></td>
<td>
<p>- phi-divergence parameter. s = 2 is the higher criticism statitic.s = 1 is the Berk and Jones statistic.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_beta">beta</code></td>
<td>
<p>- search range parameter. Search range = (1, beta*n). Beta must be between 1/n and 1.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_method">method</code></td>
<td>
<p>- different alternative hypothesis, including mixtures such as, &quot;gaussian-gaussian&quot;, &quot;gaussian-t&quot;, &quot;t-t&quot;, &quot;chisq-chisq&quot;, and &quot;exp-chisq&quot;. By default, we use Gaussian mixture.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_eps">eps</code></td>
<td>
<p>- mixing parameter of the mixture.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_mu">mu</code></td>
<td>
<p>- mean of non standard Gaussian model.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_df">df</code></td>
<td>
<p>- degree of freedom of t/Chisq distribution and exp distribution.</p>
</td></tr>
<tr><td><code id="power.phi_+3A_delta">delta</code></td>
<td>
<p>- non-cenrality of t/Chisq distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We consider the following hypothesis test,
</p>
<p style="text-align: center;"><code class="reqn">H_0: X_i\sim F, H_a: X_i\sim G</code>
</p>

<p>Specifically, <code class="reqn">F = F_0</code> and <code class="reqn">G = (1-\epsilon)F_0+\epsilon F_1</code>, where <code class="reqn">\epsilon</code> is the mixing parameter, <code class="reqn">F_0</code> and <code class="reqn">F_1</code> is
speified by the &quot;method&quot; argument:
</p>
<p>&quot;gaussian-gaussian&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of normal distribution with <code class="reqn">\mu</code> defined by mu and <code class="reqn">\sigma = 1</code>.
</p>
<p>&quot;gaussian-t&quot;: <code class="reqn">F_0</code> is the standard normal CDF and <code class="reqn">F = F_1</code> is the CDF of t distribution with degree of freedom defined by df.
</p>
<p>&quot;t-t&quot;: <code class="reqn">F_0</code> is the CDF of t distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central t distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>
<p>&quot;chisq-chisq&quot;: <code class="reqn">F_0</code> is the CDF of Chisquare distribution with degree of freedom defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisquare distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>
<p>&quot;exp-chisq&quot;: <code class="reqn">F_0</code> is the CDF of exponential distribution with parameter defined by df and <code class="reqn">F = F_1</code> is the CDF of non-central Chisqaure distribution with degree of freedom defined by df and non-centrality defined by delta.
</p>


<h3>Value</h3>

<p>Power of phi-divergence test.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.phi">stat.phi</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#If the alternative hypothesis Gaussian mixture with eps = 0.1 and mu = 1.2:#
power.phi(0.05, n=10, s=2, beta=0.5, eps = 0.1, mu = 1.2)
</code></pre>

<hr>
<h2 id='pphi'>calculate the left-tail probability of phi-divergence under general correlation matrix.</h2><span id='topic+pphi'></span>

<h3>Description</h3>

<p>calculate the left-tail probability of phi-divergence under general correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pphi(q, M, k0, k1, s = 2, t = 30, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pphi_+3A_q">q</code></td>
<td>
<p>- quantile, must be a scalar.</p>
</td></tr>
<tr><td><code id="pphi_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="pphi_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="pphi_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="pphi_+3A_s">s</code></td>
<td>
<p>- the phi-divergence test parameter.</p>
</td></tr>
<tr><td><code id="pphi_+3A_t">t</code></td>
<td>
<p>- numerical truncation parameter.</p>
</td></tr>
<tr><td><code id="pphi_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M = toeplitz(1/(1:10)*(-1)^(0:9)) #alternating polynomial decaying correlation matrix
pphi(q=2, M=M, k0=1, k1=5, s=2)
pphi(q=2, M=diag(10), k0=1, k1=5, s=2)
</code></pre>

<hr>
<h2 id='pphi.omni'>calculate the left-tail probability of omnibus phi-divergence statistics under general correlation matrix.</h2><span id='topic+pphi.omni'></span>

<h3>Description</h3>

<p>calculate the left-tail probability of omnibus phi-divergence statistics under general correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pphi.omni(q, M, K0, K1, S, t = 30, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pphi.omni_+3A_q">q</code></td>
<td>
<p>- quantile, must be a scalar.</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_k0">K0</code></td>
<td>
<p>- vector of search range starts (from the k0th smallest p-value).</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_k1">K1</code></td>
<td>
<p>- vector of search range ends (at the k1th smallest p-value).</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_s">S</code></td>
<td>
<p>- vector of the phi-divergence test parameters.</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_t">t</code></td>
<td>
<p>- numerical truncation parameter.</p>
</td></tr>
<tr><td><code id="pphi.omni_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M = matrix(0.3,10,10) + diag(1-0.3, 10)
pphi.omni(0.05, M=M, K0=rep(1,4), K1=rep(5,4), S=c(-1,0,1,2))
</code></pre>

<hr>
<h2 id='qbj'>Quantile of Berk-Jones statitic under the null hypothesis.</h2><span id='topic+qbj'></span>

<h3>Description</h3>

<p>Quantile of Berk-Jones statitic under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qbj(p, M, k0, k1, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qbj_+3A_p">p</code></td>
<td>
<p>- a scalar left probability that defines the quantile.</p>
</td></tr>
<tr><td><code id="qbj_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="qbj_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="qbj_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="qbj_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Quantile of BJ statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Berk, R.H. &amp; Jones, D.H. Z. &quot;Goodness-of-fit test statistics that dominate the Kolmogorov statistics&quot;. Wahrscheinlichkeitstheorie verw Gebiete (1979) 47: 47.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.bj">stat.bj</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The 0.05 critical value of BJ statistic when n = 10:
qbj(p=.95, M=diag(10), k0=1, k1=5, onesided=FALSE)
</code></pre>

<hr>
<h2 id='qhc'>Quantile of Higher Criticism statitics under the null hypothesis.</h2><span id='topic+qhc'></span>

<h3>Description</h3>

<p>Quantile of Higher Criticism statitics under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qhc(p, M, k0, k1, onesided = FALSE, LS = F, ZW = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qhc_+3A_p">p</code></td>
<td>
<p>-  a scalar left probability that defines the quantile.</p>
</td></tr>
<tr><td><code id="qhc_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="qhc_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="qhc_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="qhc_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
<tr><td><code id="qhc_+3A_ls">LS</code></td>
<td>
<p>- if LS = T, then method of Li and Siegmund (2015) will be implemented.When n and q is very large, approximation method is prefered.</p>
</td></tr>
<tr><td><code id="qhc_+3A_zw">ZW</code></td>
<td>
<p>- if ZW = T, then approximation method of Zhang and Wu will be implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Quantile of HC statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Li, Jian; Siegmund, David. &quot;Higher criticism: p-values and criticism&quot;. Annals of Statistics 43 (2015).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.hc">stat.hc</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The 0.05 critical value of HC statistic when n = 10:
qhc(p=.95, M=diag(10), k0=1, k1=5, onesided=FALSE)
</code></pre>

<hr>
<h2 id='qphi'>Quantile of phi-divergence statitic under the null hypothesis.</h2><span id='topic+qphi'></span>

<h3>Description</h3>

<p>Quantile of phi-divergence statitic under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qphi(p, M, k0, k1, s = 2, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qphi_+3A_p">p</code></td>
<td>
<p>-  a scalar left probability that defines the quantile.</p>
</td></tr>
<tr><td><code id="qphi_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="qphi_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="qphi_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="qphi_+3A_s">s</code></td>
<td>
<p>- the phi-divergence test parameter.</p>
</td></tr>
<tr><td><code id="qphi_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Quantile of phi-divergence statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.phi">stat.phi</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The 0.05 critical value of HC statistic when n = 10:
qphi(p=.95, M=diag(10), k0=1, k1=5, s=2, onesided=FALSE)
</code></pre>

<hr>
<h2 id='stat.bj'>Construct Berk and Jones (BJ) statitics.</h2><span id='topic+stat.bj'></span>

<h3>Description</h3>

<p>Construct Berk and Jones (BJ) statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.bj(p, k0 = 1, k1 = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.bj_+3A_p">p</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="stat.bj_+3A_k0">k0</code></td>
<td>
<p>- search range left end parameter. Default k0 = 1.</p>
</td></tr>
<tr><td><code id="stat.bj_+3A_k1">k1</code></td>
<td>
<p>- search range right end parameter. Default k1 = 0.5*number of input p-values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">p_{(i)}</code>, <code class="reqn">i = 1,...,n</code> be a sequence of ordered p-values, the Berk and Jones statistic
</p>
<p style="text-align: center;"><code class="reqn">BJ = \sqrt{2n} \max_{1 \leq i\leq \lfloor \beta n \rfloor} (-1)^j \sqrt{i/n * \log(i/n/p_{(i)}) + (1-i/n) * \log((1-i/n)/(1-p_{(i)}))}</code>
</p>

<p>and when <code class="reqn">p_{(i)} &gt; i/n</code>, <code class="reqn">j = 1</code>, otherwise <code class="reqn">j = 0</code>.
</p>


<h3>Value</h3>

<p>value - BJ statistic constructed from a vector of p-values.
</p>
<p>location - the order of the p-values to obtain BJ statistic.
</p>
<p>stat - vector of marginal BJ statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Jager, Leah; Wellner, Jon A. &quot;Goodness-of-fit tests via phi-divergences&quot;. Annals of Statistics 35 (2007).
</p>
<p>3. Berk, R.H. &amp; Jones, D.H. Z. &quot;Goodness-of-fit test statistics that dominate the Kolmogorov statistics&quot;. Wahrscheinlichkeitstheorie verw Gebiete (1979) 47: 47.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stat.bj(runif(10))
#When the input are statistics#
stat.test = rnorm(20)
p.test = 1 - pnorm(stat.test)
stat.bj(p.test, k0 = 2, k1 = 20)
</code></pre>

<hr>
<h2 id='stat.hc'>Construct Higher Criticism (HC) statitics.</h2><span id='topic+stat.hc'></span>

<h3>Description</h3>

<p>Construct Higher Criticism (HC) statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.hc(p, k0 = 1, k1 = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.hc_+3A_p">p</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="stat.hc_+3A_k0">k0</code></td>
<td>
<p>- search range left end parameter. Default k0 = 1.</p>
</td></tr>
<tr><td><code id="stat.hc_+3A_k1">k1</code></td>
<td>
<p>- search range right end parameter. Default k1 = 0.5*number of input p-values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">p_{(i)}</code>, <code class="reqn">i = 1,...,n</code> be a sequence of ordered p-values, the higher criticism statistic
</p>
<p style="text-align: center;"><code class="reqn">HC = \sqrt{n} \max_{1 \leq i\leq \lfloor \beta n \rfloor} [i/n - p_{(i)}] /\sqrt{p_{(i)}(1 - p_{(i)})}</code>
</p>



<h3>Value</h3>

<p>value - HC statistic constructed from a vector of p-values.
</p>
<p>location - the order of the p-values to obtain HC statistic.
</p>
<p>stat - vector of marginal HC statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stat.hc(runif(10))
#When the input are statistics#
stat.test = rnorm(20)
p.test = 1 - pnorm(stat.test)
stat.hc(p.test, k0 = 1, k1 = 10)
</code></pre>

<hr>
<h2 id='stat.phi'>Construct phi-divergence statitics.</h2><span id='topic+stat.phi'></span>

<h3>Description</h3>

<p>Construct phi-divergence statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.phi(p, s, k0 = 1, k1 = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.phi_+3A_p">p</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="stat.phi_+3A_s">s</code></td>
<td>
<p>- phi-divergence parameter. s = 2 is the higher criticism statitic.s = 1 is the Berk and Jones statistic.</p>
</td></tr>
<tr><td><code id="stat.phi_+3A_k0">k0</code></td>
<td>
<p>- search range left end parameter. Default k0 = 1.</p>
</td></tr>
<tr><td><code id="stat.phi_+3A_k1">k1</code></td>
<td>
<p>- search range right end parameter. Default k1 = 0.5*number of input p-values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">p_{(i)}</code>, <code class="reqn">i = 1,...,n</code> be a sequence of ordered p-values, the phi-divergence statistic
</p>
<p style="text-align: center;"><code class="reqn">PHI = \sqrt{2n}/(s - s^2) \max_{1 \leq i\leq \lfloor \beta n \rfloor} (-1)^j \sqrt{1 - (i/n)^s  (p_{(i)})^s - (1-i/n)^{(1-s)} * (1-p_{(i)})^{(1-s)}}</code>
</p>

<p>and when <code class="reqn">p_{(i)} &gt; i/n</code>, <code class="reqn">j = 1</code>, otherwise <code class="reqn">j = 0</code>.
</p>


<h3>Value</h3>

<p>value - phi-divergence statistic constructed from a vector of p-values.
</p>
<p>location - the order of the p-values to obtain phi-divergence statistic.
</p>
<p>stat - vector of marginal phi-divergence statistics.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Jager, Leah; Wellner, Jon A. &quot;Goodness-of-fit tests via phi-divergences&quot;. Annals of Statistics 35 (2007).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>stat.phi(runif(10), s = 2)
#When the input are statistics#
stat.test = rnorm(20)
p.test = 1 - pnorm(stat.test)
stat.phi(p.test, s = 0.5, k0 = 2, k1 = 5)
</code></pre>

<hr>
<h2 id='stat.phi.omni'>calculate the omnibus phi-divergence statistics under general correlation matrix.</h2><span id='topic+stat.phi.omni'></span>

<h3>Description</h3>

<p>calculate the omnibus phi-divergence statistics under general correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.phi.omni(p, M, K0 = rep(1, 4), K1 = rep(length(M[1, ]), 4), S = c(-1,
  0, 1, 2), t = 30, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.phi.omni_+3A_p">p</code></td>
<td>
<p>- input pvalues.</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_k0">K0</code></td>
<td>
<p>- vector of search range starts (from the k0th smallest p-value).</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_k1">K1</code></td>
<td>
<p>- vector of search range ends (at the k1th smallest p-value).</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_s">S</code></td>
<td>
<p>- vector of the phi-divergence test parameters.</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_t">t</code></td>
<td>
<p>- numerical truncation parameter.</p>
</td></tr>
<tr><td><code id="stat.phi.omni_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M = toeplitz(1/(1:10)*(-1)^(0:9)) #alternating polynomial decaying correlation matrix
stat.phi.omni(runif(10), M=M, K0=rep(1,4), K1=rep(5,4), S=c(-1,0,1,2))
</code></pre>

<hr>
<h2 id='test.bj'>Multiple comparison test using Berk and Jones (BJ) statitics.</h2><span id='topic+test.bj'></span>

<h3>Description</h3>

<p>Multiple comparison test using Berk and Jones (BJ) statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.bj(prob, M, k0, k1, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.bj_+3A_prob">prob</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="test.bj_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="test.bj_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.bj_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.bj_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>pvalue - the p-value of the Berk-Jones test.
</p>
<p>bjstat - the Berk-Jones statistic.
</p>
<p>location - the order of the input p-values to obtain BJ statistic.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Jager, Leah; Wellner, Jon A. &quot;Goodness-of-fit tests via phi-divergences&quot;. Annals of Statistics 35 (2007).
</p>
<p>3. Berk, R.H. &amp; Jones, D.H. Z. &quot;Goodness-of-fit test statistics that dominate the Kolmogorov statistics&quot;. Wahrscheinlichkeitstheorie verw Gebiete (1979) 47: 47.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.bj">stat.bj</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test.bj(runif(10), M=diag(10), k0=1, k1=10)
#When the input are statistics#
stat.test = rnorm(20)
p.test = 2*(1 - pnorm(abs(stat.test)))
test.bj(p.test, M=diag(20), k0=1, k1=10)
</code></pre>

<hr>
<h2 id='test.hc'>Multiple comparison test using Higher Criticism (HC) statitics.</h2><span id='topic+test.hc'></span>

<h3>Description</h3>

<p>Multiple comparison test using Higher Criticism (HC) statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.hc(prob, M, k0, k1, LS = F, ZW = F, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.hc_+3A_prob">prob</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="test.hc_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="test.hc_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.hc_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.hc_+3A_ls">LS</code></td>
<td>
<p>- if LS = T, then method of Li and Siegmund (2015) will be implemented.When n and q is very large, approximation method is prefered.</p>
</td></tr>
<tr><td><code id="test.hc_+3A_zw">ZW</code></td>
<td>
<p>- if ZW = T, then approximation method of Zhang and Wu will be implemented.</p>
</td></tr>
<tr><td><code id="test.hc_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>pvalue - The p-value of the HC test.
</p>
<p>hcstat - HC statistic.
</p>
<p>location - the order of the input p-values to obtain HC statistic.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Donoho, David; Jin, Jiashun. &quot;Higher criticism for detecting sparse heterogeneous mixtures&quot;. Annals of Statistics 32 (2004).
</p>
<p>3. Li, Jian; Siegmund, David. &quot;Higher criticism: p-values and criticism&quot;. Annals of Statistics 43 (2015).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.hc">stat.hc</a></code> for the definition of the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pval.test = runif(10)
test.hc(pval.test, M=diag(10), k0=1, k1=10)
test.hc(pval.test, M=diag(10), k0=1, k1=10, LS = TRUE)
test.hc(pval.test, M=diag(10), k0=1, k1=10, ZW = TRUE)
#When the input are statistics#
stat.test = rnorm(20)
p.test = 2*(1 - pnorm(abs(stat.test)))
test.hc(p.test, M=diag(20), k0=1, k1=10)
</code></pre>

<hr>
<h2 id='test.phi'>Multiple comparison test using phi-divergence statitics.</h2><span id='topic+test.phi'></span>

<h3>Description</h3>

<p>Multiple comparison test using phi-divergence statitics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.phi(prob, M, k0, k1, s = 2, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.phi_+3A_prob">prob</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="test.phi_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="test.phi_+3A_k0">k0</code></td>
<td>
<p>- search range starts from the k0th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.phi_+3A_k1">k1</code></td>
<td>
<p>- search range ends at the k1th smallest p-value.</p>
</td></tr>
<tr><td><code id="test.phi_+3A_s">s</code></td>
<td>
<p>- phi-divergence parameter. s = 2 is the higher criticism statitic.s = 1 is the Berk and Jones statistic.</p>
</td></tr>
<tr><td><code id="test.phi_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>pvalue - The p-value of the phi-divergence test.
</p>
<p>phistat - phi-diergence statistic.
</p>
<p>location - the order of the input p-values to obtain phi-divergence statistic.
</p>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>
<p>2. Jager, Leah; Wellner, Jon A. &quot;Goodness-of-fit tests via phi-divergences&quot;. Annals of Statistics 35 (2007).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stat.phi">stat.phi</a></code> for the definition of the statistic.v
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test.phi(runif(10), M=diag(10), s = 0.5, k0=1, k1=10)
#When the input are statistics#
stat.test = rnorm(20)
p.test = 2*(1 - pnorm(abs(stat.test)))
test.phi(p.test, M=diag(20), s = 0.5, k0=1, k1=10)
</code></pre>

<hr>
<h2 id='test.phi.omni'>calculate the right-tail probability of omnibus phi-divergence statistics under general correlation matrix.</h2><span id='topic+test.phi.omni'></span>

<h3>Description</h3>

<p>calculate the right-tail probability of omnibus phi-divergence statistics under general correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.phi.omni(prob, M, K0, K1, S, onesided = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test.phi.omni_+3A_prob">prob</code></td>
<td>
<p>- vector of input p-values.</p>
</td></tr>
<tr><td><code id="test.phi.omni_+3A_m">M</code></td>
<td>
<p>- correlation matrix of input statistics (of the input p-values).</p>
</td></tr>
<tr><td><code id="test.phi.omni_+3A_k0">K0</code></td>
<td>
<p>- vector of search range starts (from the k0th smallest p-value).</p>
</td></tr>
<tr><td><code id="test.phi.omni_+3A_k1">K1</code></td>
<td>
<p>- vector of search range ends (at the k1th smallest p-value).</p>
</td></tr>
<tr><td><code id="test.phi.omni_+3A_s">S</code></td>
<td>
<p>- vector of the phi-divergence test parameters.</p>
</td></tr>
<tr><td><code id="test.phi.omni_+3A_onesided">onesided</code></td>
<td>
<p>- TRUE if the input p-values are one-sided.</p>
</td></tr>
</table>


<h3>References</h3>

<p>1. Hong Zhang, Jiashun Jin and Zheyang Wu. &quot;Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases&quot;, submitted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M = matrix(0.3,10,10) + diag(1-0.3, 10)
test.phi.omni(runif(10), M=M, K0=rep(1,4), K1=rep(5,4), S=c(-1,0,1,2))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
