<!DOCTYPE html><html lang="en-US"><head><title>Help for package kerntools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kerntools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kerntools-package'><p>kerntools: Kernel Functions and Tools for Machine Learning Applications</p></a></li>
<li><a href='#Acc'><p>Accuracy</p></a></li>
<li><a href='#Acc_rnd'><p>Accuracy of a random model</p></a></li>
<li><a href='#aggregate_imp'><p>Aggregate importances</p></a></li>
<li><a href='#Boots_CI'><p>Confidence Interval using Bootstrap</p></a></li>
<li><a href='#BrayCurtis'><p>Kernels for count data</p></a></li>
<li><a href='#centerK'><p>Centering a kernel matrix</p></a></li>
<li><a href='#centerX'><p>Centering a squared matrix by row or column</p></a></li>
<li><a href='#Chi2'><p>Chi-squared kernel</p></a></li>
<li><a href='#cLinear'><p>Compositional kernels</p></a></li>
<li><a href='#cosNorm'><p>Cosine normalization of a kernel matrix</p></a></li>
<li><a href='#cosnormX'><p>Cosine normalization of a matrix</p></a></li>
<li><a href='#desparsify'><p>This function deletes those columns and/or rows in a matrix/data.frame that</p>
only contain 0s.</a></li>
<li><a href='#Dirac'><p>Kernels for categorical variables</p></a></li>
<li><a href='#dummy_data'><p>Convert categorical data to dummies.</p></a></li>
<li><a href='#dummy_var'><p>Levels per factor variable</p></a></li>
<li><a href='#estimate_gamma'><p>Gamma hyperparameter estimation (RBF kernel)</p></a></li>
<li><a href='#F1'><p>F1 score</p></a></li>
<li><a href='#Frobenius'><p>Frobenius kernel</p></a></li>
<li><a href='#frobNorm'><p>Frobenius normalization</p></a></li>
<li><a href='#heatK'><p>Kernel matrix heatmap</p></a></li>
<li><a href='#histK'><p>Kernel matrix histogram</p></a></li>
<li><a href='#Jaccard'><p>Kernels for sets</p></a></li>
<li><a href='#Kendall'><p>Kendall's tau kernel</p></a></li>
<li><a href='#kPCA'><p>Kernel PCA</p></a></li>
<li><a href='#kPCA_arrows'><p>Plot the original variables' contribution to a PCA plot</p></a></li>
<li><a href='#kPCA_imp'><p>Contributions of the variables to the Principal Components (&quot;loadings&quot;)</p></a></li>
<li><a href='#KTA'><p>Kernel-target alignment</p></a></li>
<li><a href='#Laplace'><p>Laplacian kernel</p></a></li>
<li><a href='#Linear'><p>Linear kernel</p></a></li>
<li><a href='#minmax'><p>Minmax normalization</p></a></li>
<li><a href='#MKC'><p>Multiple Kernel (Matrices) Combination</p></a></li>
<li><a href='#nmse'><p>NMSE (Normalized Mean Squared Error)</p></a></li>
<li><a href='#Normal_CI'><p>Confidence Interval using Normal Approximation</p></a></li>
<li><a href='#plotImp'><p>Importance barplot</p></a></li>
<li><a href='#Prec'><p>Precision or PPV</p></a></li>
<li><a href='#Procrustes'><p>Procrustes Analysis</p></a></li>
<li><a href='#RBF'><p>Gaussian RBF (Radial Basis Function) kernel</p></a></li>
<li><a href='#Rec'><p>Recall or Sensitivity or TPR</p></a></li>
<li><a href='#showdata'><p>Showdata</p></a></li>
<li><a href='#simK'><p>Kernel matrix similarity</p></a></li>
<li><a href='#soil'><p>Soil microbiota (raw counts)</p></a></li>
<li><a href='#Spe'><p>Specificity or TNR</p></a></li>
<li><a href='#Spectrum'><p>Spectrum kernel</p></a></li>
<li><a href='#svm_imp'><p>SVM feature importance</p></a></li>
<li><a href='#TSS'><p>Total Sum Scaling</p></a></li>
<li><a href='#vonNeumann'><p>Von Neumann entropy</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel Functions and Tools for Machine Learning Applications</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Kernel functions for diverse types of data (including, but not
    restricted to: nonnegative and real vectors, real matrices, categorical
    and ordinal variables, sets, strings), plus other utilities like kernel
    similarity, kernel Principal Components Analysis (PCA) and features'
    importance for Support Vector Machines (SVMs), which expand other 'R'
    packages like 'kernlab'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, spelling, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, ggplot2, kernlab, methods, reshape2, stats, stringi</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/elies-ramon/kerntools">https://github.com/elies-ramon/kerntools</a>,
<a href="https://elies-ramon.github.io/kerntools/">https://elies-ramon.github.io/kerntools/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/elies-ramon/kerntools/issues">https://github.com/elies-ramon/kerntools/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-19 14:52:02 UTC; syko</td>
</tr>
<tr>
<td>Author:</td>
<td>Elies Ramon <a href="https://orcid.org/0000-0002-7953-8115"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Elies Ramon &lt;eramon@everlyrusher.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-19 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='kerntools-package'>kerntools: Kernel Functions and Tools for Machine Learning Applications</h2><span id='topic+kerntools'></span><span id='topic+kerntools-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Kernel functions for diverse types of data (including, but not restricted to: nonnegative and real vectors, real matrices, categorical and ordinal variables, sets, strings), plus other utilities like kernel similarity, kernel Principal Components Analysis (PCA) and features' importance for Support Vector Machines (SVMs), which expand other 'R' packages like 'kernlab'.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Elies Ramon <a href="mailto:eramon@everlyrusher.com">eramon@everlyrusher.com</a> (<a href="https://orcid.org/0000-0002-7953-8115">ORCID</a>) [copyright holder]
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/elies-ramon/kerntools">https://github.com/elies-ramon/kerntools</a>
</p>
</li>
<li> <p><a href="https://elies-ramon.github.io/kerntools/">https://elies-ramon.github.io/kerntools/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/elies-ramon/kerntools/issues">https://github.com/elies-ramon/kerntools/issues</a>
</p>
</li></ul>


<hr>
<h2 id='Acc'>Accuracy</h2><span id='topic+Acc'></span>

<h3>Description</h3>

<p>'Acc()' computes the accuracy between the output
of a classification model and the actual values of the target.
It can also compute the weighted accuracy, which is useful in
imbalanced classification problems. The weighting is applied according
to the class frequencies in the target. In balanced problems, weighted Acc = Acc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Acc(ct, weighted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Acc_+3A_ct">ct</code></td>
<td>
<p>Confusion Matrix.</p>
</td></tr>
<tr><td><code id="Acc_+3A_weighted">weighted</code></td>
<td>
<p>If TRUE, the weighted accuracy is returned. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Accuracy of the model (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",3),rep("b",2))
y_pred &lt;- c(rep("a",2),rep("b",3))
ct &lt;- table(y,y_pred)
Acc(ct)
Acc(ct,weighted=TRUE)
</code></pre>

<hr>
<h2 id='Acc_rnd'>Accuracy of a random model</h2><span id='topic+Acc_rnd'></span>

<h3>Description</h3>

<p>'Acc_rnd()' computes the expected accuracy of a random classifier based on the
class frequencies of the target. This measure can be used as a benchmark when contrasted
to the accuracy (in test) of a given prediction model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Acc_rnd(target, freq = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Acc_rnd_+3A_target">target</code></td>
<td>
<p>A character vector or a factor. Alternatively, a numeric vector
(see below).</p>
</td></tr>
<tr><td><code id="Acc_rnd_+3A_freq">freq</code></td>
<td>
<p>TRUE if 'target' contains the frequencies of the classes (in this
case, 'target' should be numeric), FALSE otherwise. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Expected accuracy of a random classification model (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Expected accuracy of a random model:
target &lt;- c(rep("a",5),rep("b",2))
Acc_rnd(target)
# This is the same than:
freqs &lt;- c(5/7,2/7)
Acc_rnd(freqs,freq=TRUE)
</code></pre>

<hr>
<h2 id='aggregate_imp'>Aggregate importances</h2><span id='topic+aggregate_imp'></span>

<h3>Description</h3>

<p>'aggregate_imp()' sums the importances present in a matrix or data.frame according
to some user-specified grouping criterion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_imp(X, lev = NULL, samples = "rows")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aggregate_imp_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame containing the importances (in rows or in columns).</p>
</td></tr>
<tr><td><code id="aggregate_imp_+3A_lev">lev</code></td>
<td>
<p>(optional) The grouping elements. 'lev' should be as long as the dimension (cols or rows)
that one wants to aggregate. If this parameter is absent, the colnames (if samples=&quot;rows&quot;) or rownames will
be used to that effect. In that case, it is expected that the col/rownames follow
this pattern: &quot;V_Y&quot;, and the variables with the same &quot;V&quot; will be summed. (Check the colnames of
a typical output of 'dummy_data()' for more info).</p>
</td></tr>
<tr><td><code id="aggregate_imp_+3A_samples">samples</code></td>
<td>
<p>Samples are in rows or in columns? (Defaults: &quot;rows&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>X, a matrix or data.frame containing the aggregated importances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>importances &lt;- matrix(rnorm(90),nrow=3,ncol=30)
rownames(importances) &lt;- c("sample1","sample2","sample3")
colnames(importances) &lt;- paste0("Feat",
rep(1:5,times=2*(1:5)), "_", unlist(lapply(2*(1:5),function(x)LETTERS[1:x])))

## The grouping criterion is:
groups &lt;- paste0("Feat",1:5)
aggregate_imp(X=importances,samples="rows",lev=groups)
## We can also use the colnames:
colnames(importances)
aggregate_imp(X=importances,samples="rows")
</code></pre>

<hr>
<h2 id='Boots_CI'>Confidence Interval using Bootstrap</h2><span id='topic+Boots_CI'></span>

<h3>Description</h3>

<p>'Boots_CI()' computes the Confidence Interval (CI) of a performance measure
(for instance, accuracy) via bootstrapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Boots_CI(target, pred, index = "acc", nboots, confidence = 95, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Boots_CI_+3A_target">target</code></td>
<td>
<p>Numeric vector containing the actual values.</p>
</td></tr>
<tr><td><code id="Boots_CI_+3A_pred">pred</code></td>
<td>
<p>Numeric vector containing the predicted values.
(The order should be the same than the target's).</p>
</td></tr>
<tr><td><code id="Boots_CI_+3A_index">index</code></td>
<td>
<p>Performance measure name, in lowercase. (Defaults: &quot;acc&quot;).</p>
</td></tr>
<tr><td><code id="Boots_CI_+3A_nboots">nboots</code></td>
<td>
<p>Number of bootstrapping replicas.</p>
</td></tr>
<tr><td><code id="Boots_CI_+3A_confidence">confidence</code></td>
<td>
<p>Confidence level; for instance, 95% or 99%. (Defaults: 95).</p>
</td></tr>
<tr><td><code id="Boots_CI_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the performance measures functions;
notably, multi.class=&quot;macro&quot; or multi.class=&quot;micro&quot; for the macro or micro
performance measures. (Defaults: &quot;macro&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the bootstrap estimate of the performance and its CI.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",30),rep("b",20))
y_pred &lt;- c(rep("a",20),rep("b",30))
# Computing Accuracy with their 95%CI
Boots_CI(target=y, pred=y_pred, index="acc", nboots=1000, confidence=95)
</code></pre>

<hr>
<h2 id='BrayCurtis'>Kernels for count data</h2><span id='topic+BrayCurtis'></span><span id='topic+Ruzicka'></span>

<h3>Description</h3>

<p>Ruzicka and Bray-Curtis are kernel functions for absolute or relative
frequencies and count data. Both kernels have as input a matrix or data.frame
with dimension <em>NxD</em> and <em>N</em>&gt;1, <em>D</em>&gt;1, containing strictly non-negative real numbers.
Samples should be in the rows. Thus, when working with relative frequencies,
'rowSums(X)' should be 1 (or 100, or another arbitrary number) for <em>all</em> rows
(samples) of the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BrayCurtis(X)

Ruzicka(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BrayCurtis_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains absolute or relative frequencies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more info about these measures, please check Details in
?vegan::vegdist(). Note that, in the vegan help page, &quot;Ruzicka&quot; corresponds to
&quot;quantitative Jaccard&quot;. 'BrayCurtis(X)' gives the same result than
'1-vegan::vegdist(X,method=&quot;bray&quot;)', and the same with 'Ruzicka(data)' and
'1-vegan::vegdist(data,method=&quot;jaccard&quot;)'.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- soil$abund
Kruz &lt;- Ruzicka(data)
Kbray &lt;- BrayCurtis(data)
Kruz[1:5,1:5]
Kbray[1:5,1:5]
</code></pre>

<hr>
<h2 id='centerK'>Centering a kernel matrix</h2><span id='topic+centerK'></span>

<h3>Description</h3>

<p>It is equivalent to compute 'K' over centered data (i.e. the mean of each
column is subtracted) in Feature Space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centerK(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="centerK_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Centered 'K' (class &quot;matrix&quot;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(250),ncol=50,nrow=5)
K &lt;- Linear(dat)
centerK(K)
</code></pre>

<hr>
<h2 id='centerX'>Centering a squared matrix by row or column</h2><span id='topic+centerX'></span>

<h3>Description</h3>

<p>It centers a numeric matrix with dimension <em>N x N</em> by row (rows=TRUE) or column
(rows=FALSE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centerX(X, rows = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="centerX_+3A_x">X</code></td>
<td>
<p>Numeric matrix or data.frame of any size.</p>
</td></tr>
<tr><td><code id="centerX_+3A_rows">rows</code></td>
<td>
<p>If TRUE, the operation is done by row; otherwise, it is done by
column. (Defaults: TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Centered X (class &quot;matrix&quot;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(25),ncol=5,nrow=5)
centerX(dat)
</code></pre>

<hr>
<h2 id='Chi2'>Chi-squared kernel</h2><span id='topic+Chi2'></span>

<h3>Description</h3>

<p>'Chi2()' computes the basic <code class="reqn">\chi^2</code> kernel for bag-of-words (BoW) or bag-of-visual-words
data. This kernel computes the similarity between two nonnegative vectors that represent
the occurrence counts of words in two different documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Chi2(X, g = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Chi2_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame (dimension <em>NxD</em>) that contains nonnegative numbers. Each row represents
the counts of words of <em>N</em> documents, while each column is a word.</p>
</td></tr>
<tr><td><code id="Chi2_+3A_g">g</code></td>
<td>
<p>Gamma hyperparameter. If g=0 or NULL, 'Chi2()' returns the LeCam
distances between the documents instead of the <code class="reqn">\chi^2</code> kernel matrix.
(Defaults=NULL).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>References</h3>

<p>Zhang, Jianguo, et al. Local features and kernels for classification
of texture and object categories: A comprehensive study. International journal of computer
vision 73 (2007): 213-238. <a href="https://inria.hal.science/inria-00548574/document">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example dataset: word counts in 4 documents
documents &lt;- matrix( c(0, 1, 3, 2, 1, 0,  1, 1, 6,4,3,1,3,5,6,2), nrow=4,byrow=TRUE)
rownames(documents) &lt;- paste0("doc",1:4)
colnames(documents) &lt;- c("animal","life","tree","ecosystem")
documents
Chi2(documents,g=NULL)
</code></pre>

<hr>
<h2 id='cLinear'>Compositional kernels</h2><span id='topic+cLinear'></span><span id='topic+Aitchison'></span>

<h3>Description</h3>

<p>'cLinear()' is the compositional-linear kernel, which is useful for compositional
data (relative frequencies or proportions). 'Aitchison()' is akin to the RBF kernel for this
type of data. Thus, the expected input for both kernels is a matrix or data.frame containing
strictly non-negative or (even better) positive numbers. This input has dimension <em>NxD</em>, with <em>N</em>&gt;1
samples and <em>D</em>&gt;1 compositional features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cLinear(X, cos.norm = FALSE, feat_space = FALSE, zeros = "none")

Aitchison(X, g = NULL, zeros = "none")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cLinear_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains the compositional data.</p>
</td></tr>
<tr><td><code id="cLinear_+3A_cos.norm">cos.norm</code></td>
<td>
<p>Should the resulting kernel matrix be cosine normalized? (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="cLinear_+3A_feat_space">feat_space</code></td>
<td>
<p>If FALSE, only the kernel matrix is returned. Otherwise,
the feature space is also returned. (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="cLinear_+3A_zeros">zeros</code></td>
<td>
<p>&quot;none&quot; to warrant that there are no zeroes in X, &quot;pseudo&quot; to replace
zeroes by a pseudocount. (Defaults=&quot;none&quot;).</p>
</td></tr>
<tr><td><code id="cLinear_+3A_g">g</code></td>
<td>
<p>Gamma hyperparameter. If g=0 or NULL, the matrix of squared Aitchison
distances is returned instead of the Aitchison kernel matrix. (Defaults=NULL).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In compositional data, samples (rows) sum to an arbitrary or irrelevant
number. This is most clear when working with relative frequencies, as all samples
add to 1 (or 100, or other uninformative value). Zeroes are a typical challenge
when using compositional approaches. They introduce ambiguity because they can
have multiple causes; a zero may signal a true absence, or a value so small that
it is below the detection threshold of an instrument. A simple approach to deal
with zeroes is replacing them by a pseudocount. More sophisticated approaches are
reviewed elsewhere; see for instance the R package 'zCompositions'.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>References</h3>

<p>Ramon, E., Belanche-Muñoz, L. et al (2021). kernInt: A kernel framework for
integrating supervised and unsupervised analyses in spatio-temporal metagenomic
datasets. Frontiers in microbiology 12 (2021): 609048.
doi: 10.3389/fmicb.2021.609048
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- soil$abund

## This data is sparse and contains a lot of zeroes. We can replace them by pseudocounts:
Kclin &lt;- cLinear(data,zeros="pseudo")
Kclin[1:5,1:5]

## With the feature space:
Kclin &lt;- cLinear(data,zeros="pseudo",feat_space=TRUE)

## With cosine normalization:
Kcos &lt;- cLinear(data,zeros="pseudo",cos.norm=TRUE)
Kcos[1:5,1:5]

## Aitchison kernel:
Kait &lt;- Aitchison(data,g=0.0001,zeros="pseudo")
Kait[1:5,1:5]
</code></pre>

<hr>
<h2 id='cosNorm'>Cosine normalization of a kernel matrix</h2><span id='topic+cosNorm'></span>

<h3>Description</h3>

<p>It is equivalent to compute K using the normalization 'X/sqrt(sum(X^2))' in Feature Space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosNorm(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cosNorm_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cosine-normalized K (class &quot;matrix&quot;).
</p>


<h3>References</h3>

<p>Ah-Pine, J. (2010). Normalized kernels as similarity indices.
In Advances in Knowledge Discovery and Data Mining: 14th Pacific-Asia Conference,
PAKDD 2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part II 14 (pp. 362-373).
Springer Berlin Heidelberg. <a href="https://hal.science/hal-01504523/document">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(250),ncol=50,nrow=5)
K &lt;- Linear(dat)
cosNorm(K)
</code></pre>

<hr>
<h2 id='cosnormX'>Cosine normalization of a matrix</h2><span id='topic+cosnormX'></span>

<h3>Description</h3>

<p>Normalizes a numeric matrix dividing each row (if rows=TRUE) or column (if rows=FALSE)
by their L2 norm. Thus, each row (or column) has unit norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosnormX(X, rows = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cosnormX_+3A_x">X</code></td>
<td>
<p>Numeric matrix or data.frame of any size.</p>
</td></tr>
<tr><td><code id="cosnormX_+3A_rows">rows</code></td>
<td>
<p>If TRUE, the operation is done by row; otherwise, it is done by
column. (Defaults: TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cosine-normalized X.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(50),ncol=5,nrow=10)
cosnormX(dat)
</code></pre>

<hr>
<h2 id='desparsify'>This function deletes those columns and/or rows in a matrix/data.frame that
only contain 0s.</h2><span id='topic+desparsify'></span>

<h3>Description</h3>

<p>This function deletes those columns and/or rows in a matrix/data.frame that
only contain 0s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>desparsify(X, dim = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="desparsify_+3A_x">X</code></td>
<td>
<p>Numeric matrix or data.frame of any size.</p>
</td></tr>
<tr><td><code id="desparsify_+3A_dim">dim</code></td>
<td>
<p>A numeric vector. 1 indicates that the function should be applied
to rows, 2 to columns, c(1, 2) indicates rows and columns. (Defaults: 2).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>X with less rows or columns. (Class: the same than X).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(150),ncol=50,nrow=30)
dat[c(2,6,12),] &lt;- 0
dat[,c(30,40,50)] &lt;- 0
dim(desparsify(dat))
dim(desparsify(dat,dim=c(1,2)))
</code></pre>

<hr>
<h2 id='Dirac'>Kernels for categorical variables</h2><span id='topic+Dirac'></span>

<h3>Description</h3>

<p>From a matrix or data.frame with dimension <em>NxD</em>, where <em>N</em>&gt;1, <em>D</em>&gt;0,
'Dirac()' computes the simplest kernel for categorical data. Samples
should be in the rows and features in the columns. When there is a single feature,
'Dirac()' returns 1 if the category (or class, or level) is the same in
two given samples, and 0 otherwise. Instead, when <em>D</em>&gt;1, the results for the
<em>D</em> features are combined doing a sum, a mean, or a weighted mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dirac(X, comp = "mean", coeff = NULL, feat_space = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Dirac_+3A_x">X</code></td>
<td>
<p>Matrix (class &quot;character&quot;) or data.frame (class &quot;character&quot;, or columns = &quot;factor&quot;).
The elements in X are assumed to be categorical in nature.</p>
</td></tr>
<tr><td><code id="Dirac_+3A_comp">comp</code></td>
<td>
<p>When <em>D</em>&gt;1, this argument indicates how the variables
of the dataset are combined. Options are: &quot;mean&quot;, &quot;sum&quot; and &quot;weighted&quot;. (Defaults: &quot;mean&quot;)
</p>

<ul>
<li><p> &quot;sum&quot; gives the same importance to all variables, and returns an
unnormalized kernel matrix.
</p>
</li>
<li><p> &quot;mean&quot; gives the same importance to all variables, and returns a
normalized kernel matrix (all its elements range between 0 and 1).
</p>
</li>
<li><p> &quot;weighted&quot; weights each variable according to the 'coeff' parameter, and returns a
normalized kernel matrix.
</p>
</li></ul>
</td></tr>
<tr><td><code id="Dirac_+3A_coeff">coeff</code></td>
<td>
<p>(optional) A vector of weights with length <em>D</em>.</p>
</td></tr>
<tr><td><code id="Dirac_+3A_feat_space">feat_space</code></td>
<td>
<p>If FALSE, only the kernel matrix is returned. Otherwise,
the feature space is also returned. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>), or a list with the kernel matrix and the
feature space.
</p>


<h3>References</h3>

<p>Belanche, L. A., and Villegas, M. A. (2013).
Kernel functions for categorical variables with application to problems in the life sciences.
Artificial Intelligence Research and Development (pp. 171-180). IOS Press.
<a href="https://upcommons.upc.edu/bitstream/handle/2117/23347/KernelCATEG_CCIA2013.pdf">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Categorical data
summary(CO2)
Kdirac &lt;- Dirac(CO2[,1:3])
## Display a subset of the kernel matrix:
Kdirac[c(1,15,50,65),c(1,15,50,65)]
</code></pre>

<hr>
<h2 id='dummy_data'>Convert categorical data to dummies.</h2><span id='topic+dummy_data'></span>

<h3>Description</h3>

<p>Given a matrix or data.frame containing character/factors, this function
performs one-hot-encoding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy_data(X, lev = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dummy_data_+3A_x">X</code></td>
<td>
<p>A matrix, or a data.frame containing factors. (If the columns are of
any other class, they will be coerced into factors anyway).</p>
</td></tr>
<tr><td><code id="dummy_data_+3A_lev">lev</code></td>
<td>
<p>(optional) A vector with the categories (&quot;levels&quot;) of each factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>X (class: &quot;matrix&quot;) after performing one-hot-encoding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(CO2)
CO2_dummy &lt;- dummy_data(CO2[,1:3],lev=dummy_var(CO2[,1:3]))
CO2_dummy[1:10,1:5]
</code></pre>

<hr>
<h2 id='dummy_var'>Levels per factor variable</h2><span id='topic+dummy_var'></span>

<h3>Description</h3>

<p>This function gives the categories (&quot;levels&quot;) per categorical variable (&quot;factor&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy_var(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dummy_var_+3A_x">X</code></td>
<td>
<p>A matrix, or a data.frame containing factors. (If the columns are of
any other class, they will be coerced into factors anyway).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the levels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(showdata)
dummy_var(showdata)
</code></pre>

<hr>
<h2 id='estimate_gamma'>Gamma hyperparameter estimation (RBF kernel)</h2><span id='topic+estimate_gamma'></span>

<h3>Description</h3>

<p>This function returns an estimation of the optimum value for the gamma hyperparameter
(required by the RBF kernel function) using different heuristics:
</p>

<dl>
<dt><em>D</em> criterion</dt><dd><p>It returns the inverse of the number of features in X.</p>
</dd>
<dt>Scale criterion</dt><dd><p>It returns the inverse of the number of features,
normalized by the total variance of X.</p>
</dd>
<dt>Quantiles criterion</dt><dd><p>A range of values, computed with the function
'kernlab::sigest()'.</p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>estimate_gamma(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimate_gamma_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains real numbers (&quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the gamma value estimation according to different criteria.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(rnorm(150),ncol=50,nrow=30)
gamma &lt;- estimate_gamma(data)
gamma
K &lt;- RBF(data, g = gamma$scale_criterion)
K[1:5,1:5]
</code></pre>

<hr>
<h2 id='F1'>F1 score</h2><span id='topic+F1'></span>

<h3>Description</h3>

<p>'F1()' computes the F1 score between the output of a classification prediction model
and the actual values of the target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>F1(ct, multi.class = "macro")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="F1_+3A_ct">ct</code></td>
<td>
<p>Confusion Matrix.</p>
</td></tr>
<tr><td><code id="F1_+3A_multi.class">multi.class</code></td>
<td>
<p>Should the results of each class be aggregated, and how?
Options: &quot;none&quot;, &quot;macro&quot;, &quot;micro&quot;. (Defaults: &quot;macro&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>F1 corresponds to the harmonic mean of Precision and Recall.
</p>


<h3>Value</h3>

<p>F1 (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",3),rep("b",2))
y_pred &lt;- c(rep("a",2),rep("b",3))
ct &lt;- table(y,y_pred)
F1(ct)
</code></pre>

<hr>
<h2 id='Frobenius'>Frobenius kernel</h2><span id='topic+Frobenius'></span>

<h3>Description</h3>

<p>'Frobenius()' computes the Frobenius kernel between numeric matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Frobenius(DATA, cos.norm = FALSE, feat_space = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Frobenius_+3A_data">DATA</code></td>
<td>
<p>A list of <em>M</em> matrices or data.frames containing only real
numbers (class &quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).
All matrices or data.frames should have the same number of rows and columns.</p>
</td></tr>
<tr><td><code id="Frobenius_+3A_cos.norm">cos.norm</code></td>
<td>
<p>Should the resulting kernel matrix be cosine normalized? (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="Frobenius_+3A_feat_space">feat_space</code></td>
<td>
<p>If FALSE, only the kernel matrix is returned. Otherwise,
the feature space is also returned. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Frobenius kernel is the same than the Frobenius inner product between
matrices.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension:<em>NxN</em>), or a list with the kernel matrix and the
feature space.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data1 &lt;- matrix(rnorm(250000),ncol=500,nrow=500)
data2 &lt;- matrix(rnorm(250000),ncol=500,nrow=500)
data3 &lt;- matrix(rnorm(250000),ncol=500,nrow=500)

Frobenius(list(data1,data2,data3))
</code></pre>

<hr>
<h2 id='frobNorm'>Frobenius normalization</h2><span id='topic+frobNorm'></span>

<h3>Description</h3>

<p>This function computes the Frobenius normalization of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frobNorm(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="frobNorm_+3A_x">X</code></td>
<td>
<p>Numeric matrix of any size. It may be a kernel matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Frobenius-normalized X (class: &quot;matrix&quot;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(50),ncol=5,nrow=10)
frobNorm(dat)
</code></pre>

<hr>
<h2 id='heatK'>Kernel matrix heatmap</h2><span id='topic+heatK'></span>

<h3>Description</h3>

<p>'heatK()' plots the heatmap of a kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatK(
  K,
  cos.norm = FALSE,
  title = NULL,
  color = c("red", "yellow"),
  name_leg = NULL,
  raster = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="heatK_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
<tr><td><code id="heatK_+3A_cos.norm">cos.norm</code></td>
<td>
<p>If TRUE, the cosine normalization is applied to the kernel matrix
so its elements have a maximum value of 1. (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="heatK_+3A_title">title</code></td>
<td>
<p>Heatmap title (optional).</p>
</td></tr>
<tr><td><code id="heatK_+3A_color">color</code></td>
<td>
<p>A vector of length 2 containing two colors. The first color will be
used to represent the minimum value and the second the maximum value of the kernel matrix.</p>
</td></tr>
<tr><td><code id="heatK_+3A_name_leg">name_leg</code></td>
<td>
<p>Title of the legend.</p>
</td></tr>
<tr><td><code id="heatK_+3A_raster">raster</code></td>
<td>
<p>In large kernel matrices, raster = TRUE will draw quicker and
better-looking heatmaps. (Defaults=FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'ggplot2' heatmap.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(rnorm(150),ncol=50,nrow=30)
K &lt;- Linear(data)
heatK(K)
</code></pre>

<hr>
<h2 id='histK'>Kernel matrix histogram</h2><span id='topic+histK'></span>

<h3>Description</h3>

<p>'histK()' plots the histogram of a kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>histK(K, main = "Histogram of K", vn = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="histK_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
<tr><td><code id="histK_+3A_main">main</code></td>
<td>
<p>Plot title.</p>
</td></tr>
<tr><td><code id="histK_+3A_vn">vn</code></td>
<td>
<p>If TRUE, the value of the von Neumann entropy is shown in the plot.
(Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="histK_+3A_...">...</code></td>
<td>
<p>further arguments and graphical parameters passed to 'plot.histogram'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Information about the von Neumann entropy can be found at '?vonNeumann()'.
</p>


<h3>Value</h3>

<p>An object of class &quot;histogram&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(rnorm(150),ncol=50,nrow=30)
K &lt;- RBF(data,g=0.01)
histK(K)
</code></pre>

<hr>
<h2 id='Jaccard'>Kernels for sets</h2><span id='topic+Jaccard'></span><span id='topic+Intersect'></span>

<h3>Description</h3>

<p>'Intersect()' or 'Jaccard()' compute the kernel functions of the same name,
which are useful for set data. Their input is a matrix or data.frame with
dimension <em>NxD</em>, where <em>N</em>&gt;1, <em>D</em>&gt;0. Samples should be in the
rows and features in the columns. When there is a single feature,
'Jaccard()' returns 1 if the elements of the set are exactly the same in
two given samples, and 0 if they are completely different (see Details). Instead,
in the multivariate case (<em>D</em>&gt;1), the results (for both 'Intersect()' and
'Jaccard()') of the <em>D</em> features are combined with a sum, a mean, or a
weighted mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Jaccard(X, elements = LETTERS, comp = "sum", coeff = NULL)

Intersect(
  X,
  elements = LETTERS,
  comp = "sum",
  coeff = NULL,
  feat_space = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Jaccard_+3A_x">X</code></td>
<td>
<p>Matrix (class &quot;character&quot;) or data.frame (class &quot;character&quot;, or columns = &quot;factor&quot;).
The elements in X are assumed to be categorical in nature.</p>
</td></tr>
<tr><td><code id="Jaccard_+3A_elements">elements</code></td>
<td>
<p>All potential elements (symbols) that can appear in the sets. If there are
some elements that are not of interest, they can be excluded so they are not
taken into account by these kernels. (Defaults: LETTERS).</p>
</td></tr>
<tr><td><code id="Jaccard_+3A_comp">comp</code></td>
<td>
<p>When <em>D</em>&gt;1, this argument indicates how the variables
of the dataset are combined. Options are: &quot;mean&quot;, &quot;sum&quot; and &quot;weighted&quot;. (Defaults: &quot;mean&quot;)
</p>

<ul>
<li><p> &quot;sum&quot; gives the same importance to all variables, and returns an
unnormalized kernel matrix.
</p>
</li>
<li><p> &quot;mean&quot; gives the same importance to all variables, and returns a
normalized kernel matrix (all its elements range between 0 and 1).
</p>
</li>
<li><p> &quot;weighted&quot; weights each variable according to the 'coeff' parameter, and returns a
normalized kernel matrix.
</p>
</li></ul>
</td></tr>
<tr><td><code id="Jaccard_+3A_coeff">coeff</code></td>
<td>
<p>(optional) A vector of weights with length <em>D</em>.</p>
</td></tr>
<tr><td><code id="Jaccard_+3A_feat_space">feat_space</code></td>
<td>
<p>(not available for the Jaccard kernel). If FALSE, only the
kernel matrix is returned. Otherwise, the feature space is returned too. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">A,B</code> be two sets. Then, the Intersect
kernel is defined as:
</p>
<p style="text-align: center;"><code class="reqn">K_{Intersect}(A,B)=|A \cap B| </code>
</p>

<p>And the Jaccard kernel is defined as:
</p>
<p style="text-align: center;"><code class="reqn">K_{Jaccard}(A,B)=|A \cap B| / |A \cup B|</code>
</p>

<p>This specific implementation of the Intersect and Jaccard kernels expects
that the set members (elements) are character symbols (length=1). In case the
set data is multivariate (<em>D</em>&gt;1 columns, and each one contains a set feature),
elements for the <em>D</em> sets should come from the same domain (universe).
For instance, a dataset with two variables, so the elements
in the first one are colors c(&quot;green&quot;,&quot;black&quot;,&quot;white&quot;,&quot;red&quot;) and the second are names
c(&quot;Anna&quot;,&quot;Elsa&quot;,&quot;Maria&quot;) is not allowed. In that case, set factors should be recoded
to colors c(&quot;g&quot;,&quot;b&quot;,&quot;w&quot;,&quot;r&quot;) and names c(&quot;A&quot;,&quot;E&quot;,&quot;M&quot;) and, if necessary, 'Intersect()'
(or 'Jaccard()') should be called twice.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>), or a list with the kernel matrix and the
feature space.
</p>


<h3>References</h3>

<p>Bouchard, M., Jousselme, A. L., and Doré, P. E. (2013).
A proof for the positive definiteness of the Jaccard index matrix.
International Journal of Approximate Reasoning, 54(5), 615-626.
</p>
<p>Ruiz, F., Angulo, C., and Agell, N. (2008).
Intersection and Signed-Intersection Kernels for Intervals.
Frontiers in Artificial Intelligence and Applications. 184. 262-270.
doi: 10.3233/978-1-58603-925-7-262.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sets data
## Generating a dataset with sets containing uppercase letters
random_set &lt;- function(x)paste(sort(sample(LETTERS,x,FALSE)),sep="",collapse = "")
max_setsize &lt;- 4
setsdata &lt;- matrix(replicate(20,random_set(sample(2:max_setsize,1))),nrow=4,ncol=5)

## Computing the Intersect kernel:
Intersect(setsdata,elements=LETTERS,comp="sum")

## Computing the Jaccard kernel weighting the variables:
coeffs &lt;- c(0.1,0.15,0.15,0.4,0.20)
Jaccard(setsdata,elements=LETTERS,comp="weighted",coeff=coeffs)
</code></pre>

<hr>
<h2 id='Kendall'>Kendall's tau kernel</h2><span id='topic+Kendall'></span>

<h3>Description</h3>

<p>&lsquo;Kendall()' computes the Kendall&rsquo;s tau, which happens to be a kernel function
for ordinal variables, ranks or permutations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Kendall(X, NA.as.0 = TRUE, samples.in.rows = FALSE, comp = "mean")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Kendall_+3A_x">X</code></td>
<td>
<p>When evaluating a single ordinal feature, X should be a numeric matrix
or data.frame. If data is multivariate, X should be a list, and each ordinal/ranking
feature should be placed in a different element of the list (see Examples).</p>
</td></tr>
<tr><td><code id="Kendall_+3A_na.as.0">NA.as.0</code></td>
<td>
<p>Should NAs be converted to 0s? (Defaults: TRUE).</p>
</td></tr>
<tr><td><code id="Kendall_+3A_samples.in.rows">samples.in.rows</code></td>
<td>
<p>If TRUE, the samples are considered to be in the rows.
Otherwise, it is assumed that they are in the columns. (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="Kendall_+3A_comp">comp</code></td>
<td>
<p>If X is a list, this argument indicates how the ordinal/ranking variables
are combined. Options are: &quot;mean&quot; and &quot;sum&quot;. (Defaults: &quot;mean&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>References</h3>

<p>Jiao, Y. and Vert, J.P.
The Kendall and Mallows kernels for permutations. International Conference on Machine Learning.
PMLR, 2015. <a href="https://proceedings.mlr.press/v37/jiao15.html">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 3 people are given a list of 10 colors. They rank them from most (1) to least
# (10) favorite
color_list &lt;-  c("black","blue","green","grey","lightblue","orange","purple",
"red","white","yellow")
survey1 &lt;- 1:10
survey2 &lt;- 10:1
survey3 &lt;- sample(10)
color &lt;- cbind(survey1,survey2,survey3) # Samples in columns
rownames(color) &lt;- color_list
Kendall(color)

# The same 3 people are asked the number of times they ate 5 different kinds of
# food during the last month:
food &lt;- matrix(c(10, 1,18, 25,30, 7, 5,20, 5, 12, 7,20, 20, 3,22),ncol=5,nrow=3)
rownames(food) &lt;- colnames(color)
colnames(food) &lt;- c("spinach", "chicken", "beef" , "salad","lentils")
# (we can observe that, for person 2, vegetables &lt;&lt; meat, while for person 3
# is the other way around)
Kendall(food,samples.in.rows=TRUE)

# We can combine this results:
dataset &lt;- list(color=color,food=t(food)) #All samples in columns
Kendall(dataset)
</code></pre>

<hr>
<h2 id='kPCA'>Kernel PCA</h2><span id='topic+kPCA'></span>

<h3>Description</h3>

<p>'kPCA()' computes the kernel PCA from a kernel matrix and, if desired, produces
a plot. The contribution of the original variables to the Principal Components (PCs),
sometimes referred as &quot;loadings&quot;, is NOT returned (to do so, go to 'kPCA_imp()').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kPCA(
  K,
  center = TRUE,
  Ktest = NULL,
  plot = NULL,
  y = NULL,
  colors = "black",
  na_col = "grey70",
  title = "Kernel PCA",
  pos_leg = "right",
  name_leg = "",
  labels = NULL,
  ellipse = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kPCA_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
<tr><td><code id="kPCA_+3A_center">center</code></td>
<td>
<p>A logical value. If TRUE, the variables are zero-centered before
the PCA. (Defaults: TRUE).</p>
</td></tr>
<tr><td><code id="kPCA_+3A_ktest">Ktest</code></td>
<td>
<p>(optional) An additional kernel matrix corresponding to test samples,
with dimension <em>Ntest x Ntraining</em>. These new samples are projected
(using the color defined by 'na_col') over the kernel PCA computed from K.
Remember than the data that generated 'Ktest' should be centered beforehand, using
the same values used for centering 'K'.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_plot">plot</code></td>
<td>
<p>(optional) A 'ggplot2' is displayed. The input should be a vector of
integers with length 2, corresponding to the two Principal Components to be displayed in the plot.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_y">y</code></td>
<td>
<p>(optional) A factor, or a numeric vector, with length equal to 'nrow(K)'
(number of samples). This parameter allows to paint the points with different colors.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_colors">colors</code></td>
<td>
<p>A single color, or a vector of colors. If 'y' is numeric, a gradient of colors
between the first and the second entry will be used to paint the points. (Defaults: &quot;black&quot;).</p>
</td></tr>
<tr><td><code id="kPCA_+3A_na_col">na_col</code></td>
<td>
<p>Color of the entries that have a NA in the parameter 'y', or the entries
corresponding to 'Ktest' (when 'Ktest' is not NULL). Otherwise, this parameter is ignored.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_title">title</code></td>
<td>
<p>Plot title.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_pos_leg">pos_leg</code></td>
<td>
<p>Position of the legend.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_name_leg">name_leg</code></td>
<td>
<p>Title of the legend. (Defaults: blank)</p>
</td></tr>
<tr><td><code id="kPCA_+3A_labels">labels</code></td>
<td>
<p>(optional) A vector of the same length than nrow(K). A name will be
displayed next to each point.</p>
</td></tr>
<tr><td><code id="kPCA_+3A_ellipse">ellipse</code></td>
<td>
<p>(optional) A float between 0 and 1. An ellipse will be drawn for
each group of points defined by 'y'. Here 'y' should be of class &quot;factor.&quot; This
parameter will indicate the spread of the ellipse.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As the ordinary PCA, kernel PCA can be used to summarize, visualize and/or
create new features of a dataset. Data can be projected in a linear or nonlinear
way, depending on the kernel used. When the kernel is 'Linear()', kernel PCA
is equivalent to ordinary PCA.
</p>


<h3>Value</h3>

<p>A list with two objects:
</p>
<p>* The PCA projection (class &quot;matrix&quot;). Please note that if K was computed from a <em>NxD</em>
table with <em>N &gt; D</em>, only the first <em>N-D</em> PCs may be useful.
</p>
<p>* (optional) A 'ggplot2' plot of the selected PCs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(150),ncol=50,nrow=30)
K &lt;- Linear(dat)

## Projection's coordinates only:
pca &lt;- kPCA(K)

## Coordinates + plot of the two first principal components (PC1 and PC2):
pca &lt;- kPCA(K,plot=1:2, colors = "coral2")
pca$plot
</code></pre>

<hr>
<h2 id='kPCA_arrows'>Plot the original variables' contribution to a PCA plot</h2><span id='topic+kPCA_arrows'></span>

<h3>Description</h3>

<p>'kPCA_arrows()' draws arrows on a (kernel) PCA plot to represent the contribution
of the original variables to the two displayed Principal Components (PCs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kPCA_arrows(plot, contributions, colour = "steelblue", size = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kPCA_arrows_+3A_plot">plot</code></td>
<td>
<p>A kernel PCA plot generated by 'kPCA()'.</p>
</td></tr>
<tr><td><code id="kPCA_arrows_+3A_contributions">contributions</code></td>
<td>
<p>The variables contributions, for instance obtained via 'kPCA_imp()'.
It is not mandatory to draw all the original variables; a subset of interest
can be passed on to this argument.</p>
</td></tr>
<tr><td><code id="kPCA_arrows_+3A_colour">colour</code></td>
<td>
<p>Color of arrows and labels. (Defaults: &quot;steelblue&quot;).</p>
</td></tr>
<tr><td><code id="kPCA_arrows_+3A_size">size</code></td>
<td>
<p>Size of the labels. (Defaults: 4).</p>
</td></tr>
<tr><td><code id="kPCA_arrows_+3A_...">...</code></td>
<td>
<p>Additional parameters passed on to geom_segments() and geom_text().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is important to note that the arrows are scaled to match the samples' projection
plot. Thus, arrows' directions are correct, but do not expect that their magnitudes
match the output of 'kPCA_imp()' or other functions('prcomp', 'princomp...').
(Nevertheless, they should at least be proportional to the real magnitudes.)
</p>


<h3>Value</h3>

<p>The PCA plot with the arrows ('ggplot2' object).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(500),ncol=10,nrow=50)
K &lt;- Linear(dat)

## Computing the kernel PCA. The plot represents PC1 and PC2:
kpca &lt;- kPCA(K,plot=1:2)

## Computing the contributions to all the PCS:
pcs &lt;- kPCA_imp(dat,secure=FALSE)

## We will draw the arrows for PC1 and PC2.
contributions &lt;- t(pcs$loadings[1:2,])
rownames(contributions) &lt;- 1:10
kPCA_arrows(plot=kpca$plot,contributions=contributions)
</code></pre>

<hr>
<h2 id='kPCA_imp'>Contributions of the variables to the Principal Components (&quot;loadings&quot;)</h2><span id='topic+kPCA_imp'></span>

<h3>Description</h3>

<p>'kPCA_imp()' performs a PCA and a kernel PCA simultaneously and returns
the contributions of the variables to the Principal Components (sometimes, these
contributions are called &quot;loadings&quot;) in Feature Space. Optionally, it can also
return the samples' projection (cropped to the relevant PCs) and the values used
to centering the variables in Feature Space.
It does not return any plot, nor it projects test data. To do so, please use 'kPCA()'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kPCA_imp(DATA, center = TRUE, projected = NULL, secure = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kPCA_imp_+3A_data">DATA</code></td>
<td>
<p>A matrix or data.frame (NOT a kernel matrix) containing the data in
feature space. Please note that nrow(DATA) should be higher than ncol(DATA).
If the Linear kernel is used, this feature space is simply the original space.</p>
</td></tr>
<tr><td><code id="kPCA_imp_+3A_center">center</code></td>
<td>
<p>A logical value. If TRUE, the variables are zero-centered. (Defaults: TRUE).</p>
</td></tr>
<tr><td><code id="kPCA_imp_+3A_projected">projected</code></td>
<td>
<p>(optional) If desired, the PCA projection (generated, for example, by 'kPCA()')
can be included. If DATA is big (especially in the number of rows) this may save
some computation time.</p>
</td></tr>
<tr><td><code id="kPCA_imp_+3A_secure">secure</code></td>
<td>
<p>(optional) If TRUE, it tests the quality of the loadings
This may be slow. (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function may be not valid for all kernels. Do not use it with
the RBF, Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels unless
you know exactly what you are doing.
</p>


<h3>Value</h3>

<p>A list with three objects:
</p>
<p>* The PCA projection (class &quot;matrix&quot;) using only the relevant Principal Components.
</p>
<p>* The loadings.
</p>
<p>* The values used to center each variable in Feature Space.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(150),ncol=30,nrow=50)
contributions &lt;- kPCA_imp(dat)
contributions$loadings[c("PC1","PC2"),1:5]
</code></pre>

<hr>
<h2 id='KTA'>Kernel-target alignment</h2><span id='topic+KTA'></span>

<h3>Description</h3>

<p>'KTA()' computes the alignment between a kernel matrix and a target variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KTA(K, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KTA_+3A_k">K</code></td>
<td>
<p>A kernel matrix (class: &quot;matrix&quot;).</p>
</td></tr>
<tr><td><code id="KTA_+3A_y">y</code></td>
<td>
<p>The target variable. A factor with two levels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Alignment value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>K1 &lt;- RBF(iris[1:100,1:4],g=0.1)
y &lt;- factor(iris[1:100,5])
KTA(K1,y)
</code></pre>

<hr>
<h2 id='Laplace'>Laplacian kernel</h2><span id='topic+Laplace'></span>

<h3>Description</h3>

<p>'Laplace()' computes the laplacian kernel between all possible pairs of rows of a
matrix or data.frame with dimension <em>NxD</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Laplace(X, g = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Laplace_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains real numbers (&quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).</p>
</td></tr>
<tr><td><code id="Laplace_+3A_g">g</code></td>
<td>
<p>Gamma hyperparameter. If g=0 or NULL, 'Laplace()' returns the Manhattan distance
(L1 norm between two vectors). (Defaults=NULL)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">x_i,x_j</code> be two real vectors. Then, the laplacian kernel is defined as:
</p>
<p style="text-align: center;"><code class="reqn">K_{Lapl}(x_i,x_j)=\exp(-\gamma \|x_i - x_j \|_1)</code>
</p>



<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(250),ncol=50,nrow=5)
Laplace(dat,g=0.1)
</code></pre>

<hr>
<h2 id='Linear'>Linear kernel</h2><span id='topic+Linear'></span>

<h3>Description</h3>

<p>'Linear()' computes the inner product between all possible pairs of rows of a
matrix or data.frame with dimension <em>NxD</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Linear(X, cos.norm = FALSE, coeff = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Linear_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains real numbers (&quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).</p>
</td></tr>
<tr><td><code id="Linear_+3A_cos.norm">cos.norm</code></td>
<td>
<p>Should the resulting kernel matrix be cosine normalized? (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="Linear_+3A_coeff">coeff</code></td>
<td>
<p>(optional) A vector of length <em>D</em> that weights each one of the
features (columns). When cos.norm=TRUE, 'Linear()' first does the weighting and
then the cosine-normalization.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(250),ncol=50,nrow=5)
Linear(dat)
</code></pre>

<hr>
<h2 id='minmax'>Minmax normalization</h2><span id='topic+minmax'></span>

<h3>Description</h3>

<p>Minmax normalization. Custom min/max values may be passed to the function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minmax(X, rows = FALSE, values = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minmax_+3A_x">X</code></td>
<td>
<p>Numeric matrix or data.frame of any size.</p>
</td></tr>
<tr><td><code id="minmax_+3A_rows">rows</code></td>
<td>
<p>If TRUE, the minmax normalization is done by row; otherwise, it
is done by column. (Defaults: FALSE)</p>
</td></tr>
<tr><td><code id="minmax_+3A_values">values</code></td>
<td>
<p>(optional) A list containing two elements, the &quot;max&quot;
values and the &quot;min&quot; values. If no value is passed, the typical minmax normalization
(which normalizes the dataset between 0 and 1) is computed with the observed
maximum and minimum value in each column (or row) of X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Minmax-normalized X.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(100),ncol=10,nrow=10)
dat_minmax &lt;- minmax(dat)
apply(dat_minmax,2,min) ## Min values = 0
apply(dat_minmax,2,max) ## Max values = 1
# We can also explicitly state the max and min values:
values &lt;- list(min=apply(dat,2,min),max=apply(dat,2,max))
dat_minmax &lt;- minmax(dat,values=values)
</code></pre>

<hr>
<h2 id='MKC'>Multiple Kernel (Matrices) Combination</h2><span id='topic+MKC'></span>

<h3>Description</h3>

<p>Combination of kernel matrices coming from different datasets / feature types
into a single kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MKC(K, coeff = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MKC_+3A_k">K</code></td>
<td>
<p>A three-dimensional <em>NxDxM</em> array containing <em>M</em> kernel matrices.</p>
</td></tr>
<tr><td><code id="MKC_+3A_coeff">coeff</code></td>
<td>
<p>A vector of length <em>M</em> with the weight of each kernel matrix.
If NULL, all kernel matrices have the same weight. (Defaults: NULL)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A kernel matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# For illustrating a possible use of this function, we work with a dataset
# that contains numeric and categorical features.

summary(mtcars)
cat_feat_idx &lt;- which(colnames(mtcars) %in% c("vs", "am"))

# vs and am are categorical variables. We make a list, with the numeric features
# in the first element and the categorical features in the second:
DATA &lt;- list(num=mtcars[,-cat_feat_idx], cat=mtcars[,cat_feat_idx])
# Our N, D and M dimensions are:
N &lt;- nrow(mtcars); D &lt;- ncol(mtcars); M &lt;- length(DATA)

# Now we prepare a kernel matrix:
K &lt;- array(dim=c(N,N,M))
K[,,1] &lt;- Linear(DATA[[1]],cos.norm = TRUE) ## Kernel for numeric data
K[,,2] &lt;- Dirac(DATA[[2]]) ## Kernel for categorical data

# Here, K1 has the same weight than K2 when computing the final kernel, although
# K1 has 9 variables and K2 has only 2.
Kconsensus &lt;- MKC(K)
Kconsensus[1:5,1:5]

# If we want to weight equally each one of the 11 variables in the final
# kernel, K1 will weight 9/11 and K2 2/11.
coeff &lt;- sapply(DATA,ncol)
coeff
Kweighted &lt;- MKC(K,coeff=coeff)
Kweighted[1:5,1:5]

</code></pre>

<hr>
<h2 id='nmse'>NMSE (Normalized Mean Squared Error)</h2><span id='topic+nmse'></span>

<h3>Description</h3>

<p>'nmse()' computes the Normalized Mean Squared Error between the output
of a regression model and the actual values of the target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmse(target, pred)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nmse_+3A_target">target</code></td>
<td>
<p>Numeric vector containing the actual values.</p>
</td></tr>
<tr><td><code id="nmse_+3A_pred">pred</code></td>
<td>
<p>Numeric vector containing the predicted values.
(The order should be the same than in the target)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Normalized Mean Squared error is defined as:
</p>
<p style="text-align: center;"><code class="reqn">NMSE=MSE/((N-1)*var(target))</code>
</p>

<p>where MSE is the Mean Squared Error.
</p>


<h3>Value</h3>

<p>The normalized mean squared error (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- 1:10
y_pred &lt;- y+rnorm(10)
nmse(y,y_pred)
</code></pre>

<hr>
<h2 id='Normal_CI'>Confidence Interval using Normal Approximation</h2><span id='topic+Normal_CI'></span>

<h3>Description</h3>

<p>'Normal_CI()' computes the Confidence Interval (CI) of a performance measure
(for instance, accuracy) using normal approximation. Thus, it is advisable
that the test has a size of, at least, 30 instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Normal_CI(value, ntest, confidence = 95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Normal_CI_+3A_value">value</code></td>
<td>
<p>Performance value (a single value).</p>
</td></tr>
<tr><td><code id="Normal_CI_+3A_ntest">ntest</code></td>
<td>
<p>Test set size (a single value).</p>
</td></tr>
<tr><td><code id="Normal_CI_+3A_confidence">confidence</code></td>
<td>
<p>Confidence level; for instance, 95% or 99%. (Defaults: 95).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the CI.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Computing accuracy
y &lt;- c(rep("a",30),rep("b",20))
y_pred &lt;- c(rep("a",20),rep("b",30))
ct &lt;- table(y,y_pred)
accuracy &lt;- Acc(ct)
# Computing 95%CI
Normal_CI(accuracy, ntest=length(y), confidence=95)
</code></pre>

<hr>
<h2 id='plotImp'>Importance barplot</h2><span id='topic+plotImp'></span>

<h3>Description</h3>

<p>'plotImp()' displays the barplot of a numeric vector, which is assumed to contain the
features importance (from a prediction model) or the contribution of each
original variable to a Principal Component (PCA). In the barplot, features/PCs
are sorted by decreasing importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotImp(
  x,
  y = NULL,
  relative = TRUE,
  absolute = TRUE,
  nfeat = NULL,
  names = NULL,
  main = NULL,
  xlim = NULL,
  color = "grey",
  leftmargin = NULL,
  ylegend = NULL,
  leg_pos = "right",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotImp_+3A_x">x</code></td>
<td>
<p>Numeric vector containing the importances.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_y">y</code></td>
<td>
<p>(optional) Numeric vector containing a different, independent variable to
be contrasted with the feature importances. Should have the same length and order
than 'x'.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_relative">relative</code></td>
<td>
<p>If TRUE, the barplot will display relative importances. (Defaults: TRUE).</p>
</td></tr>
<tr><td><code id="plotImp_+3A_absolute">absolute</code></td>
<td>
<p>If FALSE, the bars may be positive or negative, which will affect
the order of the features Otherwise, the absolute value of 'x' will be taken (Defaults: TRUE).</p>
</td></tr>
<tr><td><code id="plotImp_+3A_nfeat">nfeat</code></td>
<td>
<p>(optional) The number of top (most important) features displayed in the plot.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_names">names</code></td>
<td>
<p>(optional) The names of the features, in the same order than 'x'.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_main">main</code></td>
<td>
<p>(optional) Plot title.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_xlim">xlim</code></td>
<td>
<p>(optional) A numeric vector. If absent, the minimum and maximum
value of &lsquo;x' will be used to establish the axis&rsquo; range.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_color">color</code></td>
<td>
<p>Color(s) chosen for the bars. A single value or a vector. (Defaults: &quot;grey&quot;).</p>
</td></tr>
<tr><td><code id="plotImp_+3A_leftmargin">leftmargin</code></td>
<td>
<p>(optional) Left margin space for the plot.</p>
</td></tr>
<tr><td><code id="plotImp_+3A_ylegend">ylegend</code></td>
<td>
<p>(optional) It allows to add a text explaining what is 'y' (only
if 'y' is not NULL).</p>
</td></tr>
<tr><td><code id="plotImp_+3A_leg_pos">leg_pos</code></td>
<td>
<p>If 'ylegend' is TRUE, the position of the legend. (Defaults: &quot;right&quot;).</p>
</td></tr>
<tr><td><code id="plotImp_+3A_...">...</code></td>
<td>
<p>(optional) Additional arguments (such as 'axes', 'asp',...) and graphical
parameters (such as 'par'). See '?graphics::barplot()'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<p>* The vector of importances in decreasing order. When 'nfeat' is not NULL, only
the top 'nfeat' are returned.
</p>
<p>* The cumulative sum of (absolute) importances.
</p>
<p>* A numeric vector giving the coordinates of all the drawn bars' midpoints.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>importances &lt;- rnorm(30)
names_imp &lt;- paste0("Feat",1:length(importances))

plot1 &lt;- plotImp(x=importances,names=names_imp,main="Barplot")
plot2 &lt;- plotImp(x=importances,names=names_imp,relative=FALSE,
main="Barplot",nfeat=10)
plot3 &lt;- plotImp(x=importances,names=names_imp,absolute=FALSE,
main="Barplot",color="coral2")
</code></pre>

<hr>
<h2 id='Prec'>Precision or PPV</h2><span id='topic+Prec'></span>

<h3>Description</h3>

<p>'Prec()' computes the Precision of PPV (Positive Predictive Value) between the output
of a classification model and the actual values of the target.
The precision of each class can be aggregated. Macro-precision is the average of the
precision of each classes. Micro-precision is the weighted average.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Prec(ct, multi.class = "macro")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Prec_+3A_ct">ct</code></td>
<td>
<p>Confusion Matrix.</p>
</td></tr>
<tr><td><code id="Prec_+3A_multi.class">multi.class</code></td>
<td>
<p>Should the results of each class be aggregated, and how?
Options: &quot;none&quot;, &quot;macro&quot;, &quot;micro&quot;. (Defaults: &quot;macro&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>PPV (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",3),rep("b",2))
y_pred &lt;- c(rep("a",2),rep("b",3))
ct &lt;- table(y,y_pred)
Prec(ct)
</code></pre>

<hr>
<h2 id='Procrustes'>Procrustes Analysis</h2><span id='topic+Procrustes'></span>

<h3>Description</h3>

<p>Procrustes Analysis compares two PCA/PCoA/MDS/other ordination methods'
projections after &quot;removing&quot; the translation, scaling and rotation effects.
Thus, they are compared in their configuration of &quot;maximum similarity&quot;.
Samples in the two projections should be related. The similarity of the projections
X1 and X2 is quantified using a correlation-like statistic derived from the
symmetric Procrustes sum of squared differences between X1 and X2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Procrustes(X1, X2, plot = NULL, labels = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Procrustes_+3A_x1">X1</code></td>
<td>
<p>A matrix or data.frame containing a PCA/PCoA/MDS projection.</p>
</td></tr>
<tr><td><code id="Procrustes_+3A_x2">X2</code></td>
<td>
<p>A second matrix or data.frame containing a different PCA/PCoA/MDS projection,
with the same number of rows than X1.</p>
</td></tr>
<tr><td><code id="Procrustes_+3A_plot">plot</code></td>
<td>
<p>(optional) A 'ggplot2' is displayed. The input should be a vector of
integers with length 2, corresponding to the two Principal Components to be displayed in the plot.</p>
</td></tr>
<tr><td><code id="Procrustes_+3A_labels">labels</code></td>
<td>
<p>(optional) A vector of the same length than nrow(X1), or instead,
nrow(X1)+nrow(X2). A name will be displayed next to each point.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'Procrustes()' performs a Procrustes Analysis equivalent to
'vegan::procrustes(X,Y,scale=FALSE,symmetric=TRUE)'.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<p>* X1 (zero-centered and scaled).
</p>
<p>* X2 superimposed over X1 (after translating, scaling and rotating X2).
</p>
<p>* Procrustes correlation between X1 and X2.
</p>
<p>* (optional) A 'ggplot2' plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data1 &lt;- matrix(rnorm(900),ncol=30,nrow=30)
data2 &lt;- matrix(rnorm(900),ncol=30,nrow=30)
pca1 &lt;- kPCA(Linear(data1),center=TRUE)
pca2 &lt;- kPCA(Linear(data2),center=TRUE)
procr &lt;- Procrustes(pca1,pca2)
# Procrustean correlation between pca1 and pca2:
procr$pro.cor
# With plot (first two axes):
procr &lt;- Procrustes(pca1,pca2,plot=1:2,labels=1:30)
procr$plot
</code></pre>

<hr>
<h2 id='RBF'>Gaussian RBF (Radial Basis Function) kernel</h2><span id='topic+RBF'></span>

<h3>Description</h3>

<p>'RBF()' computes the RBF kernel between all possible pairs of rows of a
matrix or data.frame with dimension <em>NxD</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RBF(X, g = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RBF_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains real numbers (&quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).</p>
</td></tr>
<tr><td><code id="RBF_+3A_g">g</code></td>
<td>
<p>Gamma hyperparameter. If g=0 or NULL, 'RBF()' returns the matrix of squared Euclidean
distances instead of the RBF kernel matrix. (Defaults=NULL).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">x_i,x_j</code> be two real vectors. Then, the RBF kernel is defined as:
</p>
<p style="text-align: center;"><code class="reqn">K_{RBF}(x_i,x_j)=\exp(-\gamma \|x_i - x_j \|^2)</code>
</p>

<p>Sometimes the RBF kernel is given a hyperparameter called sigma. In that case:
<code class="reqn">\gamma = 1/\sigma^2</code>.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(250),ncol=50,nrow=5)
RBF(dat,g=0.1)
</code></pre>

<hr>
<h2 id='Rec'>Recall or Sensitivity or TPR</h2><span id='topic+Rec'></span>

<h3>Description</h3>

<p>'Rec()' computes the Recall, also known as Sensitivity or TPR (True Positive Rate),
between the output of a classification model and the actual values of the target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rec(ct, multi.class = "macro")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Rec_+3A_ct">ct</code></td>
<td>
<p>Confusion Matrix.</p>
</td></tr>
<tr><td><code id="Rec_+3A_multi.class">multi.class</code></td>
<td>
<p>Should the results of each class be aggregated, and how?
Options: &quot;none&quot;, &quot;macro&quot;, &quot;micro&quot;. (Defaults: &quot;macro&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TPR (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",3),rep("b",2))
y_pred &lt;- c(rep("a",2),rep("b",3))
ct &lt;- table(y,y_pred)
Rec(ct)
</code></pre>

<hr>
<h2 id='showdata'>Showdata</h2><span id='topic+showdata'></span>

<h3>Description</h3>

<p>A toy dataset that contains the results of a (fictional) survey
commissioned from a well-known streaming platform. The platform invited 100
people to watch footage of their new show before the premiere. After that,
the participants were asked to pick their favorite color, actress, actors
and shows from a list. Finally, they were asked to disclose if they liked the new show.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>showdata
</code></pre>


<h3>Format</h3>

<p>A data.frame with 100 rows and 5 factor variables:
</p>

<dl>
<dt>Favorite.color</dt><dd><p>Favorite color</p>
</dd>
<dt>Favorite.actress</dt><dd><p>Favorite actress</p>
</dd>
<dt>Favorite.actor</dt><dd><p>Favorite actor</p>
</dd>
<dt>Favorite.show</dt><dd><p>Favorite show</p>
</dd>
<dt>Liked.new.show</dt><dd><p>Do you like the new show?</p>
</dd>
</dl>



<h3>Source</h3>

<p>Own
</p>

<hr>
<h2 id='simK'>Kernel matrix similarity</h2><span id='topic+simK'></span>

<h3>Description</h3>

<p>'simK()' computes the similarity between kernel matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simK(Klist)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simK_+3A_klist">Klist</code></td>
<td>
<p>A list of <em>M</em> kernel matrices with identical <em>NxN</em> dimension.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is a wrapper of 'Frobenius()'.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>MxM</em>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>K1 &lt;- Linear(matrix(rnorm(7500),ncol=150,nrow=50))
K2 &lt;- Linear(matrix(rnorm(7500),ncol=150,nrow=50))
K3 &lt;- Linear(matrix(rnorm(7500),ncol=150,nrow=50))

simK(list(K1,K2,K3))
</code></pre>

<hr>
<h2 id='soil'>Soil microbiota (raw counts)</h2><span id='topic+soil'></span>

<h3>Description</h3>

<p>Bacterial abundances in 89 soils from across North and South America.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soil
</code></pre>


<h3>Format</h3>

<p>A list containing the following elements:
</p>

<dl>
<dt>abund</dt><dd><p>Bacterial abundances of 7396 taxa in 88 sites.</p>
</dd>
<dt>metadata</dt><dd><p>Samples' metadata</p>
</dd>
<dt>taxonomy</dt><dd><p>Taxonomic information</p>
</dd>
</dl>



<h3>References</h3>

<p>Lauber CL, Hamady M, Knight R, Fierer N. Pyrosequencing-based
assessment of soil pH as a predictor of soil bacterial community structure at
the continental scale. Appl Environ Microbiol. 2009 Aug;75(15):5111-20.
doi: 10.1128/AEM.00335-09.
</p>

<hr>
<h2 id='Spe'>Specificity or TNR</h2><span id='topic+Spe'></span>

<h3>Description</h3>

<p>'Spe()' computes the Specificity or TNR (True Negative Rate) between the output
of a classification prediction model and the actual values of the target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Spe(ct, multi.class = "macro")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Spe_+3A_ct">ct</code></td>
<td>
<p>Confusion Matrix.</p>
</td></tr>
<tr><td><code id="Spe_+3A_multi.class">multi.class</code></td>
<td>
<p>Should the results of each class be aggregated, and how?
Options: &quot;none&quot;, &quot;macro&quot;, &quot;micro&quot;. (Defaults: &quot;macro&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TNR (a single value).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(rep("a",3),rep("b",2))
y_pred &lt;- c(rep("a",2),rep("b",3))
ct &lt;- table(y,y_pred)
Spe(ct)
</code></pre>

<hr>
<h2 id='Spectrum'>Spectrum kernel</h2><span id='topic+Spectrum'></span>

<h3>Description</h3>

<p>'Spectrum()' computes the basic Spectrum kernel between strings. This kernel
computes the similarity of two strings by counting how many matching substrings
of length <em>l</em> are present in each one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Spectrum(
  x,
  alphabet,
  l = 1,
  group.ids = NULL,
  weights = NULL,
  feat_space = FALSE,
  cos.norm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Spectrum_+3A_x">x</code></td>
<td>
<p>Vector of strings (length <em>N</em>).</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_alphabet">alphabet</code></td>
<td>
<p>Alphabet of reference.</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_l">l</code></td>
<td>
<p>Length of the substrings.</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_group.ids">group.ids</code></td>
<td>
<p>(optional) A vector with ids. It allows to compute the kernel
over groups of strings within x, instead of the individual strings.</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_weights">weights</code></td>
<td>
<p>(optional) A numeric vector as long as x. It allows to weight differently
each one of the strings.</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_feat_space">feat_space</code></td>
<td>
<p>If FALSE, only the kernel matrix is returned. Otherwise,
the feature space (i.e. a table with the number of times that a substring of
length <em>l</em> appears in each string) is also returned (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_cos.norm">cos.norm</code></td>
<td>
<p>Should the resulting kernel matrix be cosine normalized? (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In large datasets this function may be slow. In that case, you may use the 'stringdot()'
function of the 'kernlab' package, or the 'spectrumKernel()' function of the 'kebabs' package.
</p>


<h3>Value</h3>

<p>Kernel matrix (dimension: <em>NxN</em>), or a list with the kernel matrix and the
feature space.
</p>


<h3>References</h3>

<p>Leslie, C., Eskin, E., and Noble, W.S.
The spectrum kernel: a string kernel for SVM protein classification.
Pac Symp Biocomput. 2002:564-75. PMID: 11928508.
<a href="http://psb.stanford.edu/psb-online/proceedings/psb02/abstracts/p564.html">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Examples of alphabets. _ stands for a blank space, a gap, or the
## start or the end of sequence)
NT &lt;- c("A","C","G","T","_") # DNA nucleotides
AA &lt;- c("A","C","D","E","F","G","H","I","K","L","M","N","P","Q","R","S","T",
"V","W","Y","_") ##canonical aminoacids
letters_ &lt;- c(letters,"_")
## Example of data
strings &lt;- c("hello_world","hello_word","hola_mon","kaixo_mundua",
"saluton_mondo","ola_mundo", "bonjour_le_monde")
names(strings) &lt;- c("english1","english_typo","catalan","basque",
"esperanto","galician","french")
## Computing the kernel:
Spectrum(strings,alphabet=letters_,l=2)
</code></pre>

<hr>
<h2 id='svm_imp'>SVM feature importance</h2><span id='topic+svm_imp'></span>

<h3>Description</h3>

<p>Recovering the features importances from a SVM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svm_imp(
  X,
  svindx,
  coeff,
  result = "absolute",
  cos.norm = FALSE,
  center = FALSE,
  scale = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svm_imp_+3A_x">X</code></td>
<td>
<p>Matrix or data.frame that contains real numbers (&quot;integer&quot;, &quot;float&quot; or &quot;double&quot;).
X is NOT the kernel matrix, but the original dataset used to compute the kernel matrix.</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_svindx">svindx</code></td>
<td>
<p>Indices of the support vectors.</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_coeff">coeff</code></td>
<td>
<p>target * alpha.</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_result">result</code></td>
<td>
<p>A string. If &quot;absolute&quot;, the absolute values of the importances
are returned. If &quot;squared&quot;, the squared values are returned. Any other input will
result in the original (positive and/or negative) importance values (see Details). (Defaults: &quot;absolute&quot;).</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_cos.norm">cos.norm</code></td>
<td>
<p>Boolean. Was the data cosine normalized prior to training the model? (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_center">center</code></td>
<td>
<p>Boolean. Was the data centered prior to training the model? (Defaults: FALSE).</p>
</td></tr>
<tr><td><code id="svm_imp_+3A_scale">scale</code></td>
<td>
<p>Boolean. Was the data scaled prior to training the model? (Defaults: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function may be not valid for all kernels. Do not use it with
the RBF, Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels unless
you know exactly what you are doing.
</p>
<p>Usually the sign of the importances is irrelevant, thus justifying working with the
absolute or squared values; see for instance Guyon et al. (2002). Some classification
tasks are an exception to this, when it can be demonstrated that the feature space
is strictly nonnegative. In that case, a positive importance implies that a feature
contributes to the &quot;positive&quot; class, and the same with a negative importance
and the &quot;negative&quot; class.
</p>


<h3>Value</h3>

<p>The importance of each feature (a vector).
</p>


<h3>References</h3>

<p>Guyon, I., Weston, J., Barnhill, S., and Vapnik, V. (2002) Gene selection
for cancer classification using support vector machines. Machine learning, 46, 389-422.
<a href="https://link.springer.com/content/pdf/10.1023/a:1012487302797.pdf">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data1 &lt;- iris[1:100,]
sv_index &lt;- c( 24, 42, 58, 99)
coefficients &lt;- c(-0.2670988, -0.3582848,  0.2129282,  0.4124554)
# This SV and coefficients were obtained from a model generated with kernlab:
# model &lt;- kernlab::ksvm(Species ~ .,data=data1, kernel="vanilladot",scaled = TRUE)
# sv_index &lt;- unlist(kernlab::alphaindex(model))
# coefficients &lt;- kernlab::unlist(coef(model))
# Now we compute the importances:
svm_imp(X=data1[,-5],svindx=sv_index,coeff=coefficients,center=TRUE,scale=TRUE)
</code></pre>

<hr>
<h2 id='TSS'>Total Sum Scaling</h2><span id='topic+TSS'></span>

<h3>Description</h3>

<p>This function transforms a dataset from absolute to relative frequencies
(by row or column).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TSS(X, rows = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TSS_+3A_x">X</code></td>
<td>
<p>Numeric matrix or data.frame of any size containing absolute frequencies.</p>
</td></tr>
<tr><td><code id="TSS_+3A_rows">rows</code></td>
<td>
<p>If TRUE, the operation is done by row; otherwise, it is done by
column. (Defaults: TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A relative frequency matrix or data.frame with the same dimension than X.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(50),ncol=5,nrow=10)
TSS(dat) #It can be checked that, after scaling, the sum of each row is equal to 1.
</code></pre>

<hr>
<h2 id='vonNeumann'>Von Neumann entropy</h2><span id='topic+vonNeumann'></span>

<h3>Description</h3>

<p>'vonNeumann()' computes the von Neumann entropy of a kernel matrix.
Entropy values close to 0 indicate that all its elements are very similar,
which may result in underfitting when training a prediction model. Instead,
values close to 1 indicate a high variability which may produce overfitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vonNeumann(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vonNeumann_+3A_k">K</code></td>
<td>
<p>Kernel matrix (class &quot;matrix&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Von Neumann entropy (a single value).
</p>


<h3>References</h3>

<p>Belanche-Muñoz, L.A. and Wiejacha, M. (2023)
Analysis of Kernel Matrices via the von Neumann Entropy and Its Relation to RVM Performances.
Entropy, 25, 154. doi:10.3390/e25010154. <a href="https://www.mdpi.com/1099-4300/25/1/154">Link</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(rnorm(150),ncol=50,nrow=30)
K &lt;- Linear(data)
vonNeumann(K)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
