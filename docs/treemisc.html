<!DOCTYPE html><html><head><title>Help for package treemisc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {treemisc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#banknote'><p>Swiss banknote data</p></a></li>
<li><a href='#banknote2'><p>Swiss banknote data (UCI version)</p></a></li>
<li><a href='#calibrate'><p>External probability calibration</p></a></li>
<li><a href='#cummean'><p>Cumulative means</p></a></li>
<li><a href='#decision_boundary'><p>Add decision boundary to a scatterplot.</p></a></li>
<li><a href='#gbm_2way'><p>Two-way interactions</p></a></li>
<li><a href='#gen_friedman1'><p>Friedman benchmark data</p></a></li>
<li><a href='#gen_mease'><p>Generate data from the Mease model</p></a></li>
<li><a href='#guide_setup'><p>Generate GUIDE input files</p></a></li>
<li><a href='#hitters'><p>Baseball data (corrected)</p></a></li>
<li><a href='#isle_post'><p>Importance sampled learning ensemble</p></a></li>
<li><a href='#ladboost'><p>Gradient tree boosting with least absolute deviation (LAD) loss</p></a></li>
<li><a href='#lift'><p>Gain and lift charts</p></a></li>
<li><a href='#load_eslmix'><p>Gaussian mixture data</p></a></li>
<li><a href='#lsboost'><p>Gradient tree boosting with least squares (LS) loss</p></a></li>
<li><a href='#mushroom'><p>Mushroom edibility</p></a></li>
<li><a href='#predict.rforest'><p>Random forest predictions</p></a></li>
<li><a href='#predict.rftree'><p>Model predictions</p></a></li>
<li><a href='#proximity'><p>Proximity matrix</p></a></li>
<li><a href='#prune_se'><p>Prune an <code>rpart</code> object</p></a></li>
<li><a href='#rforest'><p>Random forest</p></a></li>
<li><a href='#rftree'><p>Random forest tree</p></a></li>
<li><a href='#rrm'><p>Random rotation matrix</p></a></li>
<li><a href='#suppressRegressionWarning'><p>Suppress randomForest() warning message</p></a></li>
<li><a href='#tree_diagram'><p>Tree diagram</p></a></li>
<li><a href='#treemisc-package'><p>Data Sets and Functions to Accompany &quot;Tree-Based Methods for Statistical</p>
Learning in R&quot;</a></li>
<li><a href='#wilson_hilferty'><p>Modified Wilson-Hilferty approximation</p></a></li>
<li><a href='#wine'><p>Wine quality</p></a></li>
<li><a href='#xy_grid'><p>Create a Cartesian product from evenly spaced values of two variables</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Data Sets and Functions to Accompany "Tree-Based Methods for
Statistical Learning in R"</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Miscellaneous data sets and functions to accompany Greenwell (2022) 
    "Tree-Based Methods for Statistical Learning in R" &lt;<a href="https://doi.org/10.1201%2F9781003089032">doi:10.1201/9781003089032</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, Rcpp (&ge; 1.0.1), rpart, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>AmesHousing, gbm, glmnet (&ge; 3.0), graphics, mlbench, party,
partykit, randomForest, ranger, rpart.plot, tinytest</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-18 12:14:08 UTC; bgreenwell</td>
</tr>
<tr>
<td>Author:</td>
<td>Brandon M. Greenwell
    <a href="https://orcid.org/0000-0002-8120-0084"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brandon M. Greenwell &lt;greenwell.brandon@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-19 10:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='banknote'>Swiss banknote data</h2><span id='topic+banknote'></span>

<h3>Description</h3>

<p>Measurements from 200 Swiss 1000-franc banknotes: 100 genuine and 100 counterfeit.
</p>


<h3>Format</h3>

<p>A data frame with 200 rows and 7 columns.
</p>


<h3>Details</h3>


<dl>
<dt>length</dt><dd><p>The length of the bill in mm.</p>
</dd>
<dt>left</dt><dd><p>The length of the left edge in mm.</p>
</dd>
<dt>right</dt><dd><p>The length of the right edge in mm.</p>
</dd>
<dt>bottom</dt><dd><p>The length of the botttom edge in mm.</p>
</dd>
<dt>top</dt><dd><p>The length of the top edge in mm.</p>
</dd>
<dt>diagonal</dt><dd><p>The length of the diagonal in mm.</p>
</dd>
<dt>y</dt><dd><p>Integer specifying whether or not the bill was genuine
(<code>y = 0</code>) or counterfeit (<code>y = 1</code>).</p>
</dd>
</dl>



<h3>Source</h3>

<p>Flury, B. and Riedwyl, H. (1988). <em>Multivariate Statistics: A practical 
approach</em>. London: Chapman &amp; Hall.
</p>

<hr>
<h2 id='banknote2'>Swiss banknote data (UCI version)</h2><span id='topic+banknote2'></span>

<h3>Description</h3>

<p>Data were extracted from images that were taken from genuine <code>class = 1</code> 
and forged <code>class = 0</code> banknote-like specimens. For digitization, an 
industrial camera usually used for print inspection was used. The final 
images contained 400 x 400 pixels. Due to the object lens and distance to the 
investigated object, gray-scale pictures with a resolution of about 660 dpi 
were gained. Wavelet transformation tools were used to extract features from 
the images.
</p>


<h3>Format</h3>

<p>A data frame with 1372 rows and 5 variables.
</p>


<h3>Details</h3>


<dl>
<dt>vow</dt><dd><p>Variance of the wavelet transformed image (continuous)</p>
</dd>
<dt>sow</dt><dd><p>Skewness of the wavelet transformed image (continuous)</p>
</dd>
<dt>kow</dt><dd><p>Kurtosis of the wavelet transformed image (continuous)</p>
</dd>
<dt>eoi</dt><dd><p>Entropy of the image (continuous)</p>
</dd>
<dt>class</dt><dd><p>Integer specifying whether or not the specimen was genuine
(<code>class = 1</code>) or forged (<code>class = 0</code>).</p>
</dd>
</dl>



<h3>Source</h3>

<p>Dua, D. and Graff, C. (2019). <em>UCI Machine Learning Repository</em>
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, 
School of Information and Computer Science.
</p>

<hr>
<h2 id='calibrate'>External probability calibration</h2><span id='topic+calibrate'></span><span id='topic+print.calibrate'></span><span id='topic+plot.calibrate'></span>

<h3>Description</h3>

<p>Validates predicted probabilities against a set of observed (binary)
outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(
  prob,
  y,
  method = c("pratt", "iso", "ns", "bins"),
  pos.class = NULL,
  probs = c(0.05, 0.35, 0.65, 0.95),
  nbins = 10
)

## S3 method for class 'calibrate'
print(x, ...)

## S3 method for class 'calibrate'
plot(
  x,
  refline = TRUE,
  refline.col = 2,
  refline.lty = "dashed",
  refline.lwd = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_prob">prob</code></td>
<td>
<p>Vector of predicted probabilities.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_y">y</code></td>
<td>
<p>Vector of binary (i.e., 0/1) outcomes. If <code>y</code> is coded as
anything other than 0/1, then you must specify which of the two categories
represents the &quot;positive&quot; class (i.e., the class for which the probabilities
specified in <code>prob</code> correspond to) via the <code>pos.class</code> argument.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_method">method</code></td>
<td>
<p>Character string specifying which calibration method to use.
Current options include:
</p>

<dl>
<dt><code>"pratt"</code></dt><dd><p>Pratt scaling.</p>
</dd>
<dt><code>"iso"</code></dt><dd><p>Isotonic (i.e., monotonic) calibration.</p>
</dd>
<dt><code>"ns"</code></dt><dd><p>Natural (i.e., restricted) cubic splines; essentially,
a spline-based nonparametric version of Pratt scaling.</p>
</dd>
<dt><code>"binned"</code></dt><dd><p>TBD.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="calibrate_+3A_pos.class">pos.class</code></td>
<td>
<p>Numeric/character string specifying which values in <code>y</code>
correspond to the &quot;positive&quot; class. Default is <code>NULL</code>. (Must be
specified whenever <code>y</code> is not coded as 0/1., where 1 is assumed to
represent the &quot;positive&quot; class.)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_probs">probs</code></td>
<td>
<p>Numeric vector specifying the probabilities for generating the
quantiles of <code>prob</code> on the logit scale; these are used for the knot
locations defining the spline whenever <code>method = "ns"</code>. The default
corresponds to a good choice based on four knots; see
Harrel (2015, pp. 26-28) for details.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_nbins">nbins</code></td>
<td>
<p>Integer specifying the number of bins to use for grouping the
probabilities.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_x">x</code></td>
<td>
<p>An object of class <code>"calibrate"</code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>
<p>Additional optional argument to be passed on to other methods.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_refline">refline</code></td>
<td>
<p>Logical indicating whether or not to include a reference line.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_refline.col">refline.col</code></td>
<td>
<p>The color to use for the reference line. Default is
<code>"red"</code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_refline.lty">refline.lty</code></td>
<td>
<p>The type of line to use for the reference line. Default is
<code>"dashed"</code>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_refline.lwd">refline.lwd</code></td>
<td>
<p>The width of the reference line. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>"calibrate"</code> object, which is essentially a list with the
following components:
</p>

<dl>
<dt><code>"probs"</code></dt><dd><p>A data frame containing two columns: <code>original</code>
(the original probability estimates) and <code>calibrated</code> (the calibrated
probability estimates).</p>
</dd>
<dt><code>"calibrater"</code></dt><dd><p>The calibration function (essentially a fitted
model object) which can be used to calibrate new probabilities.</p>
</dd>
<dt><code>"bs"</code></dt><dd><p>The Brier score between <code>prob</code> and <code>y</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Harrell, Frank. (2015). Regression Modeling Strategies. Springer Series in
Statistics. Springer International Publishing.
</p>

<hr>
<h2 id='cummean'>Cumulative means</h2><span id='topic+cummean'></span>

<h3>Description</h3>

<p>Returns a vector whose elements are the cumulative means of the argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cummean(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cummean_+3A_x">x</code></td>
<td>
<p>A numeric object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the same length and type as <code>x</code> (after coercion). 
Names are preserved.
</p>
<p>An <code>NA</code> value in <code>x</code> causes the corresponding and following 
elements of the return value to be <code>NA</code>, as does integer overflow (with 
a warning).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:10
cummean(x)
cumsum(x) / seq_along(x)  # equivalent using cumulative sums
</code></pre>

<hr>
<h2 id='decision_boundary'>Add decision boundary to a scatterplot.</h2><span id='topic+decision_boundary'></span><span id='topic+decision_boundary.default'></span>

<h3>Description</h3>

<p>Adds the decision boundary from a classification model (binary or multiclass)
to an existing scatterplot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decision_boundary(model, train, y, x1, x2, pfun, grid.resolution = 100, ...)

## Default S3 method:
decision_boundary(
  model,
  train,
  y,
  x1,
  x2,
  pfun = NULL,
  grid.resolution = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decision_boundary_+3A_model">model</code></td>
<td>
<p>The associated model object.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_train">train</code></td>
<td>
<p>Data frame of training observations.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_y">y</code></td>
<td>
<p>Character string giving the name of the outcome variable in 
<code>train</code>.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_x1">x1</code></td>
<td>
<p>Character string giving the name of the predictor in <code>train</code> 
that corresponds to the x-axis.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_x2">x2</code></td>
<td>
<p>Character string giving the name of the predictor in <code>train</code> 
that corresponds to the y-axis.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_pfun">pfun</code></td>
<td>
<p>Optional prediction wrapper that returns a vector of predicted
class labels. It must have exactly two arguments: <code>object</code> and
<code>newdata</code>.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_grid.resolution">grid.resolution</code></td>
<td>
<p>Integer specifying the resolution of the contour plot.
Default is <code>100</code>.</p>
</td></tr>
<tr><td><code id="decision_boundary_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed on to 
<code><a href="graphics.html#topic+contour">contour</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, only called for side effects; in this case, a 
contour displaying the decision boundary of a classifier is added to an
existing scatterplot.
</p>


<h3>Note</h3>

<p>Based on a function written by Michael Hahsler; see
https://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(mlbench)
library(rpart)
library(treemisc)

# Generate training data from the twonorm benchmark problem
set.seed(1050)  # for reproducibility
trn &lt;- as.data.frame(mlbench.twonorm(500, d = 2))

# Fit a default classification tree
tree &lt;- rpart(classes ~ ., data = trn)

# Scatterplot of training data
palette("Okabe-Ito")
plot(x.2 ~ x.1, data = trn, col = as.integer(trn$classes) + 1,
     xlab = expression(x[1]), ylab = expression(x[2]))
palette("default")

# Add a decision boundary
decision_boundary(tree, train = trn, y = "y", x1 = "x.1", x2 = "x.2")

## End(Not run)
</code></pre>

<hr>
<h2 id='gbm_2way'>Two-way interactions</h2><span id='topic+gbm_2way'></span>

<h3>Description</h3>

<p>Computes Friedman's H-statistic (Friedman &amp; Popescu, 2008) for all pairwise 
interaction effects in a <code><a href="gbm.html#topic+gbm">gbm</a></code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm_2way(object, data, var.names = object$var.names, n.trees = object$n.trees)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm_2way_+3A_object">object</code></td>
<td>
<p>A <code><a href="gbm.html#topic+gbm">gbm</a></code> object.</p>
</td></tr>
<tr><td><code id="gbm_2way_+3A_data">data</code></td>
<td>
<p>Data frame containing the original training data (or 
representative sample thereof).</p>
</td></tr>
<tr><td><code id="gbm_2way_+3A_var.names">var.names</code></td>
<td>
<p>Character string specifying the predictor names to consider.</p>
</td></tr>
<tr><td><code id="gbm_2way_+3A_n.trees">n.trees</code></td>
<td>
<p>Integer specifying the number of trees to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the following three columns:
</p>
 
<dl>
<dt>var1</dt><dd><p>The name of the first feature.</p>
</dd> 
<dt>var2</dt><dd><p>The name of the second feature.</p>
</dd> 
<dt>h</dt><dd><p>The corresponding H-statistic.</p>
</dd> 
</dl>

<p>The resulting rows are sorted in descending order of <code>h</code>.
</p>


<h3>References</h3>

<p>Friedman, J. H., &amp; Popescu, B. E. (2008). Predictive Learning via Rule 
Ensembles. The Annals of Applied Statistics, 2(3), 916–954. 
http://www.jstor.org/stable/30245114
</p>

<hr>
<h2 id='gen_friedman1'>Friedman benchmark data</h2><span id='topic+gen_friedman1'></span>

<h3>Description</h3>

<p>Simulate data from the Friedman 1 benchmark problem. See
<code><a href="mlbench.html#topic+mlbench.friedman1">mlbench.friedman1</a></code> for details and references.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_friedman1(n = 100, nx = 10, sigma = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_friedman1_+3A_n">n</code></td>
<td>
<p>Integer specifying the number of samples (i.e., rows) to
generate. Default is 100.</p>
</td></tr>
<tr><td><code id="gen_friedman1_+3A_nx">nx</code></td>
<td>
<p>Integer specifying the number of predictor variables to generate.
Default is 10. Note that <code>nx &gt;= 5</code>.</p>
</td></tr>
<tr><td><code id="gen_friedman1_+3A_sigma">sigma</code></td>
<td>
<p>Numeric specifying the standard deviation of the standard 
Gaussian noise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with <code>n</code> rows and <code>nx</code> + 1 columns (for
<code>nx</code> features and the response).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2319)  # for reproducibility
friedman1 &lt;- gen_friedman1(nx = 5)
pairs(friedman1, col = "purple2")
</code></pre>

<hr>
<h2 id='gen_mease'>Generate data from the Mease model</h2><span id='topic+gen_mease'></span>

<h3>Description</h3>

<p>Generate binary classification data from the Mease model Mease et al. (2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_mease(n = 1000, nsim = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_mease_+3A_n">n</code></td>
<td>
<p>Integer specifying the number of observations. Default is 
<code>1000</code>.</p>
</td></tr>
<tr><td><code id="gen_mease_+3A_nsim">nsim</code></td>
<td>
<p>Integer specifying the number of binary repsonses to generate.
Default is <code>1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with <code>3 + nsim</code> columns. The first two columns 
give the values of the numeric features <code>x1</code> and <code>x2</code>. The third
column (<code>yprob</code>) gives the true probabilities (i.e., PrY = 1 | X = x). 
The remaining <code>nsim</code> columns (<code>yclass&lt;i&gt;</code>, 
<code>i = 1, 2, ..., nsim</code>) give the simulated binary outcomes corresponding 
to <code>yprob</code>.
</p>


<h3>References</h3>

<p>Mease D, Wyner AJ, Buja A. Boosted classification trees and class probability
quantile estimation. Journal of Machine Learning Research. 2007; 8:409–439.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate N = 1000 observations from the Mease model
set.seed(2254)  # for reproducibility 
mease &lt;- gen_mease(1000, nsim = 1)

# Plot predictor values colored by binary outcome
cols &lt;- palette.colors(2, palette = "Okabe-Ito", alpha = 0.3)
plot(x2 ~ x1, data = mease, col = cols[mease$yclass1 + 1], pch = 19)
</code></pre>

<hr>
<h2 id='guide_setup'>Generate GUIDE input files</h2><span id='topic+guide_setup'></span>

<h3>Description</h3>

<p>Just a simple helper function I found useful while using the GUIDE terminal 
application (http://pages.stat.wisc.edu/~loh/guide.html). It creates two 
input text files required by GUIDE: a data file and description file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guide_setup(
  data,
  path,
  dv = NULL,
  var.roles = NULL,
  na = "NA",
  file.name = NULL,
  data.loc = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guide_setup_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_path">path</code></td>
<td>
<p>Character string specifying the full path to where the GUIDE 
input files will be written to. If the given path does not exist, it will be
created automatically using <code>dir.create()</code>.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_dv">dv</code></td>
<td>
<p>Character string specifying which column represents the target/
dependent variable.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_var.roles">var.roles</code></td>
<td>
<p>A named character vector specifying the role of each column.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_na">na</code></td>
<td>
<p>Character string specifying the missing value indicator.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_file.name">file.name</code></td>
<td>
<p>Character string giving the file name (or prefix) to use for
the generated input files. If <code>NULL</code>, the default, it will be parsed 
from the <code>data</code> argument.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_data.loc">data.loc</code></td>
<td>
<p>Character string specifying the the full path to the data
input file, which is used for the first line of the generated description 
file. If <code>NULL</code>, the default, it will be determined automatically by 
<code>path</code> and <code>file.name</code>. This is useful if the data input file does 
not reside in the same directory as the GUIDE executable.</p>
</td></tr>
<tr><td><code id="guide_setup_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print progress 
information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, only called for side effects; in this case, two
text file are created for consumption by the GUIDE terminal application
</p>


<h3>Note</h3>

<p>This function assumes that the GUIDE executable is located in the same 
directory specified by the 'path' argument. For details, see the official 
software manual for GUIDE: 
<a href="http://pages.stat.wisc.edu/~loh/treeprogs/guide/guideman.pdf">http://pages.stat.wisc.edu/~loh/treeprogs/guide/guideman.pdf</a>. This 
function has only been tested on GUIDE v38.0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# New York air quality measurements
aq &lt;- airquality
aq &lt;- aq[!is.na(aq$Ozone), ]  # remove rows with missing response values

# Default variable roles
guide_setup(aq, path = "some/path/aq", dv = "Ozone")

# User specified variable roles
var.roles &lt;- c("Ozone" = "d", "Solar.R" = "n", "Wind" = "n", "Temp" = "c",
               "Month" = "p", "Day" = "p")
guide_setup(aq, path = "some/path/aq", var.roles = var.roles)

## End(Not run)
</code></pre>

<hr>
<h2 id='hitters'>Baseball data (corrected)</h2><span id='topic+hitters'></span>

<h3>Description</h3>

<p>Major League Baseball data from the 1986 and 1987 seasons.
</p>


<h3>Format</h3>

<p>A data frame with 322 observations of major league players on the
following 20 variables.  
</p>
 
<dl>
<dt>AtBat</dt><dd><p>Number of times at bat in 1986.</p>
</dd> 
<dt>Hits</dt><dd><p>Number of hits in 1986.</p>
</dd>
<dt>HmRun</dt><dd><p>Number of home runs in 1986.</p>
</dd> 
<dt>Runs</dt><dd><p>Number of runs in 1986.</p>
</dd> 
<dt>RBI</dt><dd><p>Number of runs batted in in 1986.</p>
</dd>
<dt>Walks</dt><dd><p>Number of walks in 1986.</p>
</dd> 
<dt>Years</dt><dd><p>Number of years in the major leagues.</p>
</dd> 
<dt>CAtBat</dt><dd><p>Number of times at bat during his career.</p>
</dd> 
<dt>CHits</dt><dd><p>Number of hits during his career.</p>
</dd>
<dt>CHmRun</dt><dd><p>Number of home runs during his career.</p>
</dd>
<dt>CRuns</dt><dd><p>Number of runs during his career.</p>
</dd>
<dt>CRBI</dt><dd><p>Number of runs batted in during his career.</p>
</dd>
<dt>CWalks</dt><dd><p>Number of walks during his career.</p>
</dd>
<dt>League</dt><dd><p>A factor with levels <code>A</code> and <code>N</code> indicating 
player's league at the end of 1986.</p>
</dd> 
<dt>Division</dt><dd><p>A factor with levels <code>E</code> and <code>W</code> 
indicating player's division at the end of 1986.</p>
</dd> 
<dt>PutOuts</dt><dd><p>Number of put outs in 1986.</p>
</dd>
<dt>Assists</dt><dd><p>Number of assists in 1986.</p>
</dd>
<dt>Errors</dt><dd><p>Number of errors in 1986.</p>
</dd> 
<dt>Salary</dt><dd><p>1987 annual salary on opening day in thousands of dollars.</p>
</dd>
<dt>NewLeague</dt><dd><p>A factor with levels <code>A</code> and <code>N</code> indicating 
player's league at the beginning of 1987.</p>
</dd> 
</dl>



<h3>Note</h3>

<p>This is a corrected version of the <code><a href="ISLR.html#topic+Hitters">Hitters</a></code> data set 
based on the corrections listed in Hoaglin and Velleman (1995).
</p>


<h3>References</h3>

<p>James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013)
<em>An Introduction to Statistical Learning with applications in R</em>,
<a href="https://www.statlearning.com">https://www.statlearning.com</a>, Springer-Verlag, New York.
</p>
<p>Hoaglin, D. C. and Velleman, P. F. (1995). 
<em>A critical look at some analyses ofmajor league baseball salaries</em>.
The American Statistician, 49(3):277-285.
</p>


<h3>See Also</h3>

<p><code><a href="ISLR.html#topic+Hitters">Hitters</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(hitters &lt;-  treemisc::hitters)
plot(log(Salary) ~ Years, data = hitters)
</code></pre>

<hr>
<h2 id='isle_post'>Importance sampled learning ensemble</h2><span id='topic+isle_post'></span>

<h3>Description</h3>

<p>Uses <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> or <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> to fit 
the entire LASSO path for post-processing the individual trees of a 
tree-based ensemble (e.g., a random forest).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isle_post(
  X,
  y,
  newX = NULL,
  newy = NULL,
  cv = FALSE,
  nfolds = 5,
  family = NULL,
  loss = "default",
  offset = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isle_post_+3A_x">X</code></td>
<td>
<p>A matrix of training predictions, one column for each tree in the
ensemble.</p>
</td></tr>
<tr><td><code id="isle_post_+3A_y">y</code></td>
<td>
<p>Vector of training response values. See <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> 
for acceptable values (e.g., numeric for <code>family = "gaussian"</code>).</p>
</td></tr>
<tr><td><code id="isle_post_+3A_newx">newX</code></td>
<td>
<p>Same as argument <code>X</code>, but should correspond to an 
independent test set. (Required whenever <code>cv = FALSE</code>.)</p>
</td></tr>
<tr><td><code id="isle_post_+3A_newy">newy</code></td>
<td>
<p>Same as argument <code>y</code>, but should correspond to an 
independent test set. (Required whenever <code>cv = FALSE</code>.)</p>
</td></tr>
<tr><td><code id="isle_post_+3A_cv">cv</code></td>
<td>
<p>Logical indicating whether or not to use n-fold cross-validation.
Default is <code>FALSE</code> (Must be <code>TRUE</code> whenever <code>newX = NULL</code> and 
<code>newy = NULL</code>.)</p>
</td></tr>
<tr><td><code id="isle_post_+3A_nfolds">nfolds</code></td>
<td>
<p>Integer specifying the number of folds to use for 
cross-validation (i.e., whenever <code>cv = TRUE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="isle_post_+3A_family">family</code></td>
<td>
<p>The model fitting family (e.g., <code>family = "binomial"</code> for
binary outcomes); see <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> for details on acceptable 
values.</p>
</td></tr>
<tr><td><code id="isle_post_+3A_loss">loss</code></td>
<td>
<p>Optional character string specifying the loss to use for 
n-fold cross-validation. Default is <code>"default"</code>; see
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> for details. (Only used when 
<code>cv = TRUE</code>.)</p>
</td></tr>
<tr><td><code id="isle_post_+3A_offset">offset</code></td>
<td>
<p>Optional value for the offset. Default is <code>NULL</code>, which
corresponds to no offset.</p>
</td></tr>
<tr><td><code id="isle_post_+3A_...">...</code></td>
<td>
<p>Additional (optional) arguments to be passed on to 
<code><a href="glmnet.html#topic+glmnet">glmnet</a></code> (e.g., <code>intercept = FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two components:
</p>

<dl>
<dt><code>results</code></dt><dd><p>A data frame with one row for each value of lambda in
the coefficient path and columns giving the corresponding number of 
trees/non-zero coefficients, error metric(s), and the corresponding value 
of lambda.</p>
</dd>
<dt>lasso.fit</dt><dd><p>The fitted <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> or 
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> object.</p>
</dd>
</dl>


<hr>
<h2 id='ladboost'>Gradient tree boosting with least absolute deviation (LAD) loss</h2><span id='topic+ladboost'></span><span id='topic+print.ladboost'></span><span id='topic+predict.ladboost'></span>

<h3>Description</h3>

<p>A poor-man's implementation of stochastic gradient tree boosting with LAD 
loss.
</p>
<p>Print basic information about a fitted <code>"ladboost"</code> object.
</p>
<p>Compute predictions from an <code>"ladboost"</code> object using new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ladboost(
  X,
  y,
  ntree = 100,
  shrinkage = 0.1,
  depth = 6,
  subsample = 0.5,
  init = median(y)
)

## S3 method for class 'ladboost'
print(x, ...)

## S3 method for class 'ladboost'
predict(object, newdata, ntree = NULL, individual = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ladboost_+3A_x">X</code></td>
<td>
<p>A data frame of only predictors.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_y">y</code></td>
<td>
<p>A vector of response values</p>
</td></tr>
<tr><td><code id="ladboost_+3A_ntree">ntree</code></td>
<td>
<p>Integer specifying the number of trees in the ensemble to use.
Defaults to using all the trees in the ensemble.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>Numeric specifying the shrinkage factor.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_depth">depth</code></td>
<td>
<p>Integer specifying the depth of each tree.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_subsample">subsample</code></td>
<td>
<p>Numeric specifying the proportion of the training data to
randomly sample before building each tree. Default is <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_init">init</code></td>
<td>
<p>Numeric specifying the initial value to boost from. Defaults to
the median response (i.e., <code>median(y)</code>).</p>
</td></tr>
<tr><td><code id="ladboost_+3A_x">x</code></td>
<td>
<p>An object of class <code>"ladboost"</code>.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_...">...</code></td>
<td>
<p>Additional optional arguments. (Currently ignored.)</p>
</td></tr>
<tr><td><code id="ladboost_+3A_object">object</code></td>
<td>
<p>An object of class <code>"ladboost"</code>.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_newdata">newdata</code></td>
<td>
<p>Data frame of new observations for making predictions.</p>
</td></tr>
<tr><td><code id="ladboost_+3A_individual">individual</code></td>
<td>
<p>Logical indicating whether or not to return the (shrunken)
predictions from each tree individually (<code>TRUE</code>) or the overall ensemble
prediction (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"ladboost"</code> which is just a list with the 
following components:
</p>

<ul>
<li> <p><code>trees</code> A list of length <code>ntree</code> containing the individual 
<code><a href="rpart.html#topic+rpart">rpart</a></code> tree fits.
</p>
</li>
<li> <p><code>shrinkage</code> The corresponding shrinkage parameter.
</p>
</li>
<li> <p><code>depth</code> The maximum depth of each tree.
</p>
</li>
<li> <p><code>subsample</code> The (row) subsampling rate.
</p>
</li>
<li> <p><code>init</code> The initial constant fit.
</p>
</li></ul>

<p>A vector (<code>individual = TRUE</code>) or matrix 
(<code>individual = FALSE</code>) of predictions.
</p>


<h3>Note</h3>

<p>By design, the final model does not include the predictions from the initial
(constant) fit. So the constant is stored in the <code>init</code> component of the
returned output to be used later by <code>predict.ladboost()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from the Friedman 1 benchmark problem
set.seed(1025)  # for reproducibility
trn &lt;- gen_friedman1(500)  # training data
tst &lt;- gen_friedman1(500)  # test data

# Gradient boosted decision trees
set.seed(1027)  # for reproducibility
bst &lt;- ladboost(subset(trn, select = -y), y = trn$y, depth = 2)
pred &lt;- predict(bst, newdata = tst)
mean((pred - tst$y) ^ 2)
</code></pre>

<hr>
<h2 id='lift'>Gain and lift charts</h2><span id='topic+lift'></span><span id='topic+plot.lift'></span>

<h3>Description</h3>

<p>Validates predicted probabilities against a set of observed (binary)
outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lift(prob, y, pos.class = NULL, cumulative = TRUE, nbins = 0)

## S3 method for class 'lift'
plot(
  x,
  refline = TRUE,
  refline.col = 2,
  refline.fill = 2,
  refline.lty = "dashed",
  refline.lwd = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lift_+3A_prob">prob</code></td>
<td>
<p>Vector of predicted probabilities.</p>
</td></tr>
<tr><td><code id="lift_+3A_y">y</code></td>
<td>
<p>Vector of binary (i.e., 0/1) outcomes. If <code>y</code> is coded as
anything other than 0/1, then you must specify which of the two categories
represents the &quot;positive&quot; class (i.e., the class for which the probabilities
specified in <code>prob</code> correspond to) via the <code>pos.class</code> argument.</p>
</td></tr>
<tr><td><code id="lift_+3A_pos.class">pos.class</code></td>
<td>
<p>Numeric/character string specifying which values in <code>y</code>
correspond to the &quot;positive&quot; class. Default is <code>NULL</code>. (Must be
specified whenever <code>y</code> is not coded as 0/1, where 1 is assumed to
represent the &quot;positive&quot; class.)</p>
</td></tr>
<tr><td><code id="lift_+3A_cumulative">cumulative</code></td>
<td>
<p>Logical indicating whether or not to compute cumulative
lift (i.e., gain). Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_nbins">nbins</code></td>
<td>
<p>Integer specifying the number of bins to use when computing
lift. Default is 0, which corresponds to no binning. For example, setting
<code>nbins = 10</code> will result in computing lift within each decile of the
sorted probabilities.</p>
</td></tr>
<tr><td><code id="lift_+3A_x">x</code></td>
<td>
<p>An object of class <code>"lift"</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_refline">refline</code></td>
<td>
<p>Logical indicating whether or not to include a reference line.</p>
</td></tr>
<tr><td><code id="lift_+3A_refline.col">refline.col</code></td>
<td>
<p>The color to use for the reference line. Default is
<code>2</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_refline.fill">refline.fill</code></td>
<td>
<p>The color to use for filling in the polygon-shaped 
reference line. Default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_refline.lty">refline.lty</code></td>
<td>
<p>The type of line to use for the reference line. Default is
<code>"dashed"</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_refline.lwd">refline.lwd</code></td>
<td>
<p>The width of the reference line. Default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="lift_+3A_...">...</code></td>
<td>
<p>Additional optional argument to be passed on to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>"lift"</code> object, which is essentially a list with the
following components:
</p>

<dl>
<dt><code>"lift"</code></dt><dd><p>A numeric vector containing the computed lift values.</p>
</dd>
<dt><code>"prop"</code></dt><dd><p>The corresponding proportion of cases associated with
each lift value.</p>
</dd>
<dt><code>"cumulative"</code></dt><dd><p>Same value as that supplied via the
<code>cumulative</code> argument. (Used by the <code>plot.lift()</code> method.)</p>
</dd>
</dl>


<hr>
<h2 id='load_eslmix'>Gaussian mixture data</h2><span id='topic+load_eslmix'></span>

<h3>Description</h3>

<p>Load the Gaussian mixture data from Hastie et al. (2009, Sec. 2.3.3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_eslmix()
</code></pre>


<h3>Value</h3>

<p>A list with the following components:
</p>
 
<dl>
<dt>x</dt><dd><p>200 x 2 matrix of training predictors.</p>
</dd>
<dt>y</dt><dd><p>class variable; logical vector of 0s and 1s - 100 of each.</p>
</dd>
<dt>xnew</dt><dd><p>matrix 6831 x 2 of lattice points in predictor space.</p>
</dd>
<dt>prob</dt><dd><p>vector of 6831 probabilities (of class TRUE) at each lattice point.</p>
</dd>
<dt>marginal</dt><dd><p>marginal probability at each lattice point.</p>
</dd>
<dt>px1</dt><dd><p>69 lattice coordinates for <code>x1</code>.</p>
</dd>
<dt>px2</dt><dd><p>99 lattice values for <code>x2</code>  (69*99=6831).</p>
</dd>
<dt>means</dt><dd><p>20 x 2 matrix of the mixture centers, first ten for one class, next ten for the other.</p>
</dd>
</dl>



<h3>Source</h3>

<p>https://web.stanford.edu/~hastie/ElemStatLearn/datasets/mixture.example.info.txt
</p>


<h3>References</h3>

<p>Trevor Hastie, Robert. Tibshirani, and Jerome Friedman. The Elements of 
Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. 
Springer Series in Statistics. Springer-Verlag, 2009.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>eslmix &lt;- load_eslmix()
names(eslmix)
</code></pre>

<hr>
<h2 id='lsboost'>Gradient tree boosting with least squares (LS) loss</h2><span id='topic+lsboost'></span><span id='topic+print.lsboost'></span><span id='topic+predict.lsboost'></span>

<h3>Description</h3>

<p>A poor-man's implementation of stochastic gradient tree boosting with LS 
loss.
</p>
<p>Print basic information about a fitted <code>"lsboost"</code> object.
</p>
<p>Compute predictions from an <code>"lsboost"</code> object using new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsboost(
  X,
  y,
  ntree = 100,
  shrinkage = 0.1,
  depth = 6,
  subsample = 0.5,
  init = mean(y)
)

## S3 method for class 'lsboost'
print(x, ...)

## S3 method for class 'lsboost'
predict(object, newdata, ntree = NULL, individual = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsboost_+3A_x">X</code></td>
<td>
<p>A data frame of only predictors.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_y">y</code></td>
<td>
<p>A vector of response values</p>
</td></tr>
<tr><td><code id="lsboost_+3A_ntree">ntree</code></td>
<td>
<p>Integer specifying the number of trees in the ensemble to use.
Defaults to using all the trees in the ensemble.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>Numeric specifying the shrinkage factor.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_depth">depth</code></td>
<td>
<p>Integer specifying the depth of each tree.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_subsample">subsample</code></td>
<td>
<p>Numeric specifying the proportion of the training data to
randomly sample before building each tree. Default is <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_init">init</code></td>
<td>
<p>Numeric specifying the initial value to boost from. Defaults to
the mean response (i.e., <code>mean(y)</code>).</p>
</td></tr>
<tr><td><code id="lsboost_+3A_x">x</code></td>
<td>
<p>An object of class <code>"lsboost"</code>.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_...">...</code></td>
<td>
<p>Additional optional arguments. (Currently ignored.)</p>
</td></tr>
<tr><td><code id="lsboost_+3A_object">object</code></td>
<td>
<p>An object of class <code>"lsboost"</code>.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_newdata">newdata</code></td>
<td>
<p>Data frame of new observations for making predictions.</p>
</td></tr>
<tr><td><code id="lsboost_+3A_individual">individual</code></td>
<td>
<p>Logical indicating whether or not to return the (shrunken)
predictions from each tree individually (<code>TRUE</code>) or the overall ensemble
prediction (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"lsboost"</code> which is just a list with the 
following components:
</p>

<ul>
<li> <p><code>trees</code> A list of length <code>ntree</code> containing the individual 
<code><a href="rpart.html#topic+rpart">rpart</a></code> tree fits.
</p>
</li>
<li> <p><code>shrinkage</code> The corresponding shrinkage parameter.
</p>
</li>
<li> <p><code>depth</code> The maximum depth of each tree.
</p>
</li>
<li> <p><code>subsample</code> The (row) subsampling rate.
</p>
</li>
<li> <p><code>init</code> The initial constant fit.
</p>
</li></ul>

<p>A vector (<code>individual = TRUE</code>) or matrix 
(<code>individual = FALSE</code>) of predictions.
</p>


<h3>Note</h3>

<p>By design, the final model does not include the predictions from the initial
(constant) fit. So the constant is stored in the <code>init</code> component of the
returned output to be used later by <code>predict.lsboost()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data from the Friedman 1 benchmark problem
set.seed(1025)  # for reproducibility
trn &lt;- gen_friedman1(500)  # training data
tst &lt;- gen_friedman1(500)  # test data

# Gradient boosted decision trees
set.seed(1027)  # for reproducibility
bst &lt;- lsboost(subset(trn, select = -y), y = trn$y, depth = 2)
pred &lt;- predict(bst, newdata = tst)
mean((pred - tst$y) ^ 2)
</code></pre>

<hr>
<h2 id='mushroom'>Mushroom edibility</h2><span id='topic+mushroom'></span>

<h3>Description</h3>

<p>Mushrooms described in terms of physical characteristics.
</p>


<h3>Format</h3>

<p>A data frame with 8124 rows and 23 variables.
</p>


<h3>Source</h3>

<p>Knopf, Alfred A. <em>The Audubon Society Field Guide to North American
Mushrooms</em>, G. H. Lincoff (Pres.), 1981.
</p>

<hr>
<h2 id='predict.rforest'>Random forest predictions</h2><span id='topic+predict.rforest'></span>

<h3>Description</h3>

<p>Compute predictions from an <code>"rftree"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rforest'
predict(object, newX, predict.all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rforest_+3A_object">object</code></td>
<td>
<p>An object of class <code>"rftree"</code>.</p>
</td></tr>
<tr><td><code id="predict.rforest_+3A_newx">newX</code></td>
<td>
<p>Data frame or matrix of new feature values.</p>
</td></tr>
<tr><td><code id="predict.rforest_+3A_predict.all">predict.all</code></td>
<td>
<p>Logical indicating whether or not to return predictions
for each individual tree. Default is <code>FALSE</code>, which only returns the 
aggregated predictions.</p>
</td></tr>
<tr><td><code id="predict.rforest_+3A_...">...</code></td>
<td>
<p>Additional optional arguments. (Currently ignored.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions. For binary outcomes coded as 0/1, the 
predictions represent Pr(Y = 1).
</p>

<hr>
<h2 id='predict.rftree'>Model predictions</h2><span id='topic+predict.rftree'></span>

<h3>Description</h3>

<p>Compute predictions from an <code>"rftree"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rftree'
predict(object, newX)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rftree_+3A_object">object</code></td>
<td>
<p>An object of class <code>"rftree"</code>.</p>
</td></tr>
<tr><td><code id="predict.rftree_+3A_newx">newX</code></td>
<td>
<p>Data frame or matrix of new feature values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Has the same return type as 
<code><a href="randomForest.html#topic+predict.randomForest">predict.randomForest</a></code>.
</p>

<hr>
<h2 id='proximity'>Proximity matrix</h2><span id='topic+proximity'></span><span id='topic+proximity.default'></span><span id='topic+proximity.matrix'></span><span id='topic+proximity.ranger'></span>

<h3>Description</h3>

<p>Compute proximity matrix from a random forest or matrix of terminal node
assignments (one row for each observation and one column for each tree in the 
forest).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proximity(x, ...)

## Default S3 method:
proximity(x, sparse = NULL, upper = TRUE, ...)

## S3 method for class 'matrix'
proximity(x, sparse = NULL, upper = TRUE, ...)

## S3 method for class 'ranger'
proximity(x, data = NULL, sparse = NULL, upper = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proximity_+3A_x">x</code></td>
<td>
<p>Either a <code><a href="ranger.html#topic+ranger">ranger</a></code> object or a matrix of terminal 
node assignments (one row for each observation and one column for each tree 
in the forest).</p>
</td></tr>
<tr><td><code id="proximity_+3A_...">...</code></td>
<td>
<p>Additional optional argument. (Currently ignored.)</p>
</td></tr>
<tr><td><code id="proximity_+3A_sparse">sparse</code></td>
<td>
<p>Logical or <code>NULL</code> indicating whether or not the resulting 
matrix should be sparse. If <code>NULL</code> (the default) it is made sparse when 
more than half the entries are 0.</p>
</td></tr>
<tr><td><code id="proximity_+3A_upper">upper</code></td>
<td>
<p>Logical indicating whether or not to return the proximities in
upper triangular form (<code>TRUE</code>) or as a symmetric matrix (<code>FALSE</code>).
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="proximity_+3A_data">data</code></td>
<td>
<p>Optional data frame passed on to 
<code><a href="ranger.html#topic+predict.ranger">predict.ranger</a></code>. It's a good idea to pass the data via 
this argument whenever <code>x</code> is a <code><a href="ranger.html#topic+ranger">ranger</a></code> object. If
<code>NULL</code> (the default) it will be looked for recursively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix or sparse Matrix (<code>sparse = TRUE</code>) of pairwise 
proximity (i.e., similarity) scores between training observations.
</p>

<hr>
<h2 id='prune_se'>Prune an <code>rpart</code> object</h2><span id='topic+prune_se'></span>

<h3>Description</h3>

<p>Prune an <code>rpart</code> object using the standard error (SE) of the 
cross-validation results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune_se(object, prune = TRUE, se = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune_se_+3A_object">object</code></td>
<td>
<p>An object that inherits from class <code>"rpart"</code>.</p>
</td></tr>
<tr><td><code id="prune_se_+3A_prune">prune</code></td>
<td>
<p>Logical indicating whether or not to return the pruned decision
tree. Default is <code>TRUE</code>. If <code>FALSE</code>, the optimal value of the 
cost-complexity parameter is returned instead.</p>
</td></tr>
<tr><td><code id="prune_se_+3A_se">se</code></td>
<td>
<p>Numeric specifying the number of standard errors to use when 
pruning the tree. Default is <code>1</code>, which corresponds to the 1-SE rule
described in Breiman et al. (1984).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either an object that inherits from class <code>"rpart"</code> (ideally, 
one that's been simplified using cost-complexity pruning with the 1-SE rule)
or a numeric value representing the cost-complexity parameter to use for 
pruning.
</p>


<h3>References</h3>

<p>Breiman, L., Friedman, J., and Charles J. Stone, R. A. O. (1984). 
Classification and Regression Trees. The Wadsworth and Brooks-Cole 
statistics-probability series. Taylor &amp; Francis.
</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+prune">prune</a></code>
</p>

<hr>
<h2 id='rforest'>Random forest</h2><span id='topic+rforest'></span>

<h3>Description</h3>

<p>A poor man's implementation of random forest (Breiman, 2001) with the option
to incorporate random rotations as described in Blaser and Fryzlewicz (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rforest(X, y, mtry = NULL, ntree = 500, rotate = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rforest_+3A_x">X</code></td>
<td>
<p>A data frame or a matrix of predictors.</p>
</td></tr>
<tr><td><code id="rforest_+3A_y">y</code></td>
<td>
<p>Numeric vector of response value. For binary outcomes, <code>y</code> 
should be mapped to {0, 1}. Note that multiclass outcomes are not supported.</p>
</td></tr>
<tr><td><code id="rforest_+3A_mtry">mtry</code></td>
<td>
<p>Integer specifying the number of variables randomly sampled as 
candidates splitters at each node in a tree. Note that the default values are 
different for classification (<code>floor(sqrt(p))</code> where <code>p</code> is number 
of columns of <code>X</code>) and regression <code>floor(p/3)</code>).</p>
</td></tr>
<tr><td><code id="rforest_+3A_ntree">ntree</code></td>
<td>
<p>Integer specifying the number of trees to grow. This should not 
be set to too small a number, to ensure that every input row gets predicted 
at least a few times. Default is <code>500</code>.</p>
</td></tr>
<tr><td><code id="rforest_+3A_rotate">rotate</code></td>
<td>
<p>Logical indicating whether or not to randomly rotate the 
feature values prior to fitting each tree. Default is <code>FALSE</code> which
results in a traditional random forest.</p>
</td></tr>
<tr><td><code id="rforest_+3A_...">...</code></td>
<td>
<p>Optional arguments to be passed on to 
<code><a href="randomForest.html#topic+randomForest">randomForest</a></code> (e.g., <code>nodesize = 10</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"rforest"</code>, which is essentially a list of
<a href="#topic+rftree">rftree</a> objects.
</p>


<h3>References</h3>

<p>Breiman, Leo. (2001), Random Forests, Machine Learning 45(1), 5-32.
</p>
<p>Rico Blaser and Piotr Fryzlewicz. Random rotation ensembles. Journal of 
Machine Learning Research, 17:1–26, 2016.
</p>

<hr>
<h2 id='rftree'>Random forest tree</h2><span id='topic+rftree'></span>

<h3>Description</h3>

<p>Fits a single <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rftree(X, y, rotate = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rftree_+3A_x">X</code></td>
<td>
<p>A data frame or a matrix of predictors.</p>
</td></tr>
<tr><td><code id="rftree_+3A_y">y</code></td>
<td>
<p>Numeric vector of response value. For binary outcomes, <code>y</code> 
should be mapped to {0, 1}. Note that multiclass outcomes are not supported.</p>
</td></tr>
<tr><td><code id="rftree_+3A_rotate">rotate</code></td>
<td>
<p>Logical indicating whether or not to randomly rotate the 
feature values prior to fitting the tree. Default is <code>FALSE</code> which
results in a traditional random forest tree.</p>
</td></tr>
<tr><td><code id="rftree_+3A_...">...</code></td>
<td>
<p>Optional arguments to be passed on to 
<code><a href="randomForest.html#topic+randomForest">randomForest</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='rrm'>Random rotation matrix</h2><span id='topic+rrm'></span>

<h3>Description</h3>

<p>Generates a random rotation matrix as described in Blaser and Fryzlewicz 
(2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rrm(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rrm_+3A_n">n</code></td>
<td>
<p>Integer specifying the dimension of the resulting (square) random 
rotation matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>n</code>-by-<code>n</code> matrix (i.e., an object of class 
<code>c("matrix" "array")</code>.
</p>


<h3>Source</h3>

<p>https://www.jmlr.org/papers/volume17/blaser16a/blaser16a.pdf.
</p>


<h3>References</h3>

<p>Rico Blaser and Piotr Fryzlewicz. Random rotation ensembles. Journal of 
Machine Learning Research, 17:1–26, 2016.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(R &lt;- rrm(3))
det(R)  # determinant should always be +1
solve(R)  # R^{-1} = R'
t(R)  # R^{-1} = R'
</code></pre>

<hr>
<h2 id='suppressRegressionWarning'>Suppress randomForest() warning message</h2><span id='topic+suppressRegressionWarning'></span>

<h3>Description</h3>

<p>Suppresses a specific <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> warning 
message regarding the response having &quot;...five or fewer unique values.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>suppressRegressionWarning(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="suppressRegressionWarning_+3A_expr">expr</code></td>
<td>
<p>Expression to evaluate.</p>
</td></tr>
</table>

<hr>
<h2 id='tree_diagram'>Tree diagram</h2><span id='topic+tree_diagram'></span>

<h3>Description</h3>

<p>Draw a decision tree diagram from a fitted <code><a href="rpart.html#topic+rpart">rpart</a></code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree_diagram(object, prob = TRUE, box.palette = "BuOr", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tree_diagram_+3A_object">object</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="tree_diagram_+3A_prob">prob</code></td>
<td>
<p>Logical indicating whether or not to display leaf node probability
estimates for classification trees; default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tree_diagram_+3A_box.palette">box.palette</code></td>
<td>
<p>Chjaracter string specifying the palette to use for 
coloring the nodes; see <code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code> for details.</p>
</td></tr>
<tr><td><code id="tree_diagram_+3A_...">...</code></td>
<td>
<p>Additional optional argumebts to be passed onto 
<code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, only called for side effects; in this case, a 
decision tree diagram constructed by a simple call to 
<code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code>.
</p>


<h3>Note</h3>

<p>This function is just a light wrapper around 
<code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code> and was used to produce several of the
tree diagrams in the accompanying book.
</p>

<hr>
<h2 id='treemisc-package'>Data Sets and Functions to Accompany &quot;Tree-Based Methods for Statistical
Learning in R&quot;</h2><span id='topic+treemisc-package'></span><span id='topic+treemisc'></span>

<h3>Description</h3>

<p>Miscellaneous data sets and functions to accompany Greenwell (2022) 
    &quot;Tree-Based Methods for Statistical Learning in R&quot; &lt;doi:10.1201/9781003089032&gt;.</p>


<h3>Details</h3>

<p>The DESCRIPTION file: </p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> treemisc</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Data Sets and Functions to Accompany "Tree-Based Methods for Statistical
Learning in R"</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> person(c("Brandon", "M."), family = "Greenwell", 
    email = "greenwell.brandon@gmail.com", 
    role = c("aut", "cre"), comment = c(ORCID = "0000-0002-8120-0084"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Miscellaneous data sets and functions to accompany Greenwell (2022) 
    "Tree-Based Methods for Statistical Learning in R" &lt;doi:10.1201/9781003089032&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 4.0.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Matrix,
Rcpp (&gt;= 1.0.1),
rpart,
stats,
utils</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> AmesHousing,
gbm,
glmnet (&gt;= 3.0),
graphics,
mlbench,
party,
partykit,
randomForest,
ranger,
rpart.plot,
tinytest</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 7.2.1</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyData: </td><td style="text-align: left;"> TRUE</td>
</tr>
<tr>
 <td style="text-align: left;">
Encoding: </td><td style="text-align: left;"> UTF-8</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Brandon M. Greenwell [aut, cre]
    (&lt;https://orcid.org/0000-0002-8120-0084&gt;)</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Brandon M. Greenwell &lt;greenwell.brandon@gmail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
banknote                Swiss banknote data
banknote2               Swiss banknote data (UCI version)
calibrate               External probability calibration
cummean                 Cumulative means
decision_boundary       Add decision boundary to a scatterplot.
gbm_2way                Two-way interactions
gen_friedman1           Friedman benchmark data
gen_mease               Generate data from the Mease model
guide_setup             Generate GUIDE input files
hitters                 Baseball data (corrected)
isle_post               Importance sampled learning ensemble
ladboost                Gradient tree boosting with least absolute
                        deviation (LAD) loss
lift                    Gain and lift charts
load_eslmix             Gaussian mixture data
lsboost                 Gradient tree boosting with least squares (LS)
                        loss
mushroom                Mushroom edibility
predict.rforest         Random forest predictions
proximity               Proximity matrix
prune_se                Prune an 'rpart' object
rforest                 Random forest
rrm                     Random rotation matrix
tree_diagram            Tree diagram
treemisc-package        Data Sets and Functions to Accompany
                        "Tree-Based Methods for Statistical Learning in
                        R"
wilson_hilferty         Modified Wilson-Hilferty approximation
wine                    Wine quality
xy_grid                 Create a Cartesian product from evenly spaced
                        values of two variables
</pre>
<p>This section should provide a more detailed overview of how to use the
package, including the most important functions.
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>


<h3>References</h3>

<p>This optional section can contain literature or other references for
background information.
</p>


<h3>See Also</h3>

<p>Optional links to other man pages
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Optional simple examples of the most important functions
  ## Use \dontrun{} around code to be shown but not executed
</code></pre>

<hr>
<h2 id='wilson_hilferty'>Modified Wilson-Hilferty approximation</h2><span id='topic+wilson_hilferty'></span>

<h3>Description</h3>

<p>Implements the modified Wilson-Hilferty (1931) approximation used by GUIDE to 
convert a chi-square random variable to an approximate chi-square random
variable with one degree of freedom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wilson_hilferty(x, df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wilson_hilferty_+3A_x">x</code></td>
<td>
<p>Numeric value of the observed chi-square statistic.</p>
</td></tr>
<tr><td><code id="wilson_hilferty_+3A_df">df</code></td>
<td>
<p>Integer specifying the degrees of freedom associated with <code>x</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximate chi-square statistic with one degree of freedom.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wilson_hilferty(2056, df = 4)
wilson_hilferty(1831, df = 20)

set.seed(1144)  # for reproducibility
x &lt;- rchisq(1000, df = 10)
w &lt;- sapply(x, FUN = function(x) wilson_hilferty(x, df = 10))
px &lt;- pchisq(x, df = 10)
pw &lt;- pchisq(round(w), df = 1)
plot(px, pw)
abline(0, 1, lty = 2, col = 2)
cor(px, pw)
</code></pre>

<hr>
<h2 id='wine'>Wine quality</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>These data are related to red and white variants of the Portuguese &quot;Vinho 
Verde&quot; wine. For details, see Cortez et al. (2009). Due to privacy and 
logistic issues, only physicochemical (inputs) and sensory (the output) 
variables are available (e.g., there is no data about grape types, wine 
brand, wine selling price, etc.). These data can be used for classification 
or regression tasks. The classes are ordered and not balanced (e.g., there 
are many more normal wines than excellent or poor ones). Outlier detection 
algorithms could be used to detect the few excellent or poor wines. Also, it 
is not known if all input variables are relevant. So it could be interesting 
to test feature selection methods.
</p>


<h3>Format</h3>

<p>A data frame with 6497 rows and 13 variables.
</p>


<h3>Details</h3>


<dl>
<dt>fixed.acidity</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>volatile.acidity</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>citric.acid</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>residual.sugar</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>chlorides</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>free.sulfur.dioxide</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>total.sulfur.dioxide</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>density</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>pH</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>sulphates</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>alcohol</dt><dd><p>Input feature (continuous).</p>
</dd>
<dt>quality</dt><dd><p>Target variable (continuous).</p>
</dd>
<dt>type</dt><dd><p>Input feature specifying whether it's a red or white wine 
(factor).</p>
</dd>
</dl>



<h3>Source</h3>

<p>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 
Modeling wine preferences by data mining from physicochemical properties. 
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.
</p>

<hr>
<h2 id='xy_grid'>Create a Cartesian product from evenly spaced values of two variables</h2><span id='topic+xy_grid'></span><span id='topic+xy_grid.default'></span><span id='topic+xy_grid.formula'></span><span id='topic+xy_grid.matrix'></span><span id='topic+xy_grid.data.frame'></span>

<h3>Description</h3>

<p>Create a Cartesian product from evenly spaced values of two variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xy_grid(x, ...)

## Default S3 method:
xy_grid(x, y, grid.resolution = 51, col.names = NULL, ...)

## S3 method for class 'formula'
xy_grid(x, data, grid.resolution = 51, ...)

## S3 method for class 'matrix'
xy_grid(x, grid.resolution = 51, ...)

## S3 method for class 'data.frame'
xy_grid(x, grid.resolution = 51, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xy_grid_+3A_x">x</code></td>
<td>
<p>Either a numeric vector (if argument <code>y</code> is also specified),
a matrix-like object (e.g., a data frame), or two-variable formula of the
form <code>y ~ x</code>.</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_...">...</code></td>
<td>
<p>Additional (optional) arguments. (Currently ignored.)</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_y">y</code></td>
<td>
<p>A numeric vector representing the second variable (only required if
<code>x</code> is a numeric vector).</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_grid.resolution">grid.resolution</code></td>
<td>
<p>Integer specifying the number of equally-spaced values
to use for each numeric variable. For example, if <code>grid.resolution = k</code>, 
then the final data frame will have <code>k^2</code> rows (formed by a Cartesian
product).</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_col.names">col.names</code></td>
<td>
<p>Optional vector of column names to use for the output
whenever both <code>x</code> and <code>y</code> are supplied.</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_data">data</code></td>
<td>
<p>A data frame containing the variables specified in
<code>x</code> if <code>x</code> is a formula.</p>
</td></tr>
<tr><td><code id="xy_grid_+3A_formula">formula</code></td>
<td>
<p>A two-variable formula of the form <code>y ~ x</code>. The response
(i.e., the variable on the left side of the formula) corresponds to the
second column of the output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame representing the Cartesian product between equally
spaced values from each variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x1 &lt;- 1:3
x2 &lt;- letters[1L:3L]
xy_grid(x1, x2, gr = 3, col.names = c("x1", "x2"))  # will have 3^2=9 rows
xy_grid(m &lt;- cbind(x1, x2), gr = 3)     # equivalent
xy_grid(d &lt;- as.data.frame(m), gr = 3)  # equivalent
xy_grid(x2 ~ x1, data = d, gr = 3)      # equivalent
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
