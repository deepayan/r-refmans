<!DOCTYPE html><html><head><title>Help for package highfrequency</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {highfrequency}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aggregatePrice'><p>Aggregate a time series but keep first and last observation</p></a></li>
<li><a href='#aggregateQuotes'><p>Aggregate a <code>data.table</code> or <code>xts</code> object containing quote data</p></a></li>
<li><a href='#aggregateTrades'><p>Aggregate a <code>data.table</code> or <code>xts</code> object containing trades dataÂ´</p></a></li>
<li><a href='#aggregateTS'><p>Aggregate a time series</p></a></li>
<li><a href='#AJjumpTest'><p>Ait-Sahalia and Jacod (2009) tests for the presence of jumps in the price series.</p></a></li>
<li><a href='#autoSelectExchangeQuotes'><p>Retain only data from the stock exchange with the highest volume</p></a></li>
<li><a href='#autoSelectExchangeTrades'><p>Retain only data from the stock exchange with the highest trading volume</p></a></li>
<li><a href='#Bj'><p>Internal HEAVY functions</p></a></li>
<li><a href='#BNSjumpTest'><p>Barndorff-Nielsen and Shephard (2006) tests for the presence of jumps in the price series.</p></a></li>
<li><a href='#businessTimeAggregation'><p>Business time aggregation</p></a></li>
<li><a href='#cholCovrMRCov'><p>#' @keywords internal</p>
zgamma &lt;- function (x, y, gamma_power)
if (x^2 &lt; y)
out &lt;- abs(x)^gamma_power
else
if (gamma_power == 1)
out &lt;- 1.094 * sqrt(y)

if (gamma_power == 2)
out &lt;- 1.207 * y

if (gamma_power == 4/3)
out &lt;- 1.129 * y^(2/3)


</p>
<p>return(out)</p></a></li>
<li><a href='#driftBursts'><p>Inference on drift burst hypothesis</p></a></li>
<li><a href='#exchangeHoursOnly'><p>Extract data from an <code>xts</code> object for the exchange hours only</p></a></li>
<li><a href='#gatherPrices'><p>Make TAQ format</p></a></li>
<li><a href='#getAlphaVantageData'><p>Get high frequency data from Alpha Vantage</p></a></li>
<li><a href='#getCriticalValues'><p>Get critical value for the drift burst hypothesis t-statistic</p></a></li>
<li><a href='#getLiquidityMeasures'><p>Compute Liquidity Measure</p></a></li>
<li><a href='#getTradeDirection'><p>Get trade direction</p></a></li>
<li><a href='#HARmodel'><p>Heterogeneous autoregressive (HAR) model for realized volatility model estimation</p></a></li>
<li><a href='#HEAVYmodel'><p>HEAVY model estimation</p></a></li>
<li><a href='#highfrequency-package'><p>highfrequency: Tools for Highfrequency Data Analysis</p></a></li>
<li><a href='#ICov'><p>Estimators of the integrated covariance</p></a></li>
<li><a href='#intradayJumpTest'><p>Intraday jump tests</p></a></li>
<li><a href='#IVar'><p>Estimators of the integrated variance</p></a></li>
<li><a href='#IVinference'><p>Function returns the value, the standard error and the confidence band of the integrated variance (IV) estimator.</p></a></li>
<li><a href='#JOjumpTest'><p>Jiang and Oomen (2008) tests for the presence of jumps in the price series.</p></a></li>
<li><a href='#knChooseReMeDI'><p>ReMeDI tuning parameter</p></a></li>
<li><a href='#leadLag'><p>Lead-Lag estimation</p></a></li>
<li><a href='#listAvailableKernels'><p>Available kernels</p></a></li>
<li><a href='#listCholCovEstimators'><p>Utility function listing the available estimators for the CholCov estimation</p></a></li>
<li><a href='#makeOHLCV'><p>Make Open-High-Low-Close-Volume bars</p></a></li>
<li><a href='#makePsd'><p>Returns the positive semidefinite projection of a symmetric matrix using the eigenvalue method</p></a></li>
<li><a href='#makeReturns'><p>Compute log returns</p></a></li>
<li><a href='#makeRMFormat'><p>DEPRECATED</p>
use <code>spreadPrices</code></a></li>
<li><a href='#matchTradesQuotes'><p>Match trade and quote data</p></a></li>
<li><a href='#MDtest'><p># Difference of medians test</p>
# See Fried (2012)
# Returns TRUE if H0 is rejected
# importFrom stats density
# keywords internal
DMtest &lt;- function(x, y, alpha = 0.005)
m &lt;- length(x)
n &lt;- length(y)
xmed &lt;- median(x)
ymed &lt;- median(y)
xcor &lt;- x - xmed
ycor &lt;- y - ymed
delta1 &lt;- ymed - xmed
out &lt;- density(c(xcor, ycor), kernel = &quot;epanechnikov&quot;)
fmed &lt;- as.numeric(BMS::quantile.density(out, probs = 0.5))
fmedvalue &lt;- (out$y[max(which(out$x &lt; fmed))] +
out$y[max(which(out$x &lt; fmed))+1])/2
test &lt;- sqrt((m*n)/(m + n))*2*fmedvalue*delta1
return(abs(test) &gt; qnorm(1-alpha/2))</a></li>
<li><a href='#mergeQuotesSameTimestamp'><p>Merge multiple quote entries with the same time stamp</p></a></li>
<li><a href='#mergeTradesSameTimestamp'><p>Merge multiple transactions with the same time stamp</p></a></li>
<li><a href='#mukp'><p>to use when p,k different from range [4,6]</p></a></li>
<li><a href='#noZeroPrices'><p>Delete the observations where the price is zero</p></a></li>
<li><a href='#noZeroQuotes'><p>Delete the observations where the bid or ask is zero</p></a></li>
<li><a href='#plot.DBH'><p>Plotting method for <code>DBH</code> objects</p></a></li>
<li><a href='#plot.HARmodel'><p>Plotting method for HARmodel objects</p></a></li>
<li><a href='#plot.HEAVYmodel'><p>Plotting method for HEAVYmodel objects</p></a></li>
<li><a href='#plotTQData'><p>Plot Trade and Quote data</p></a></li>
<li><a href='#predict.HARmodel'><p>Predict method for objects of type <code>HARmodel</code></p></a></li>
<li><a href='#predict.HEAVYmodel'><p>Iterative multi-step-ahead forecasting for HEAVY models</p></a></li>
<li><a href='#print.DBH'><p>Printing method for <code>DBH</code> objects</p></a></li>
<li><a href='#print.HARmodel'><p>Printing method for <code>HARmodel</code> objects</p></a></li>
<li><a href='#quotesCleanup'><p>Cleans quote data</p></a></li>
<li><a href='#rankJumpTest'><p>Rank jump test</p></a></li>
<li><a href='#rAVGCov'><p>Realized covariances via subsample averaging</p></a></li>
<li><a href='#rBACov'><p>rBACov</p></a></li>
<li><a href='#rBeta'><p>Realized beta</p></a></li>
<li><a href='#rBPCov'><p>Realized bipower covariance</p></a></li>
<li><a href='#RBPCov_bi'><p># Check data:</p>
#' @keywords internal
rdatacheck &lt;- function (rData, multi = FALSE)
if ((dim(rData)[2] &lt; 2) &amp; (multi))
stop(&quot;Your rData object should have at least 2 columns&quot;)
</a></li>
<li><a href='#rCholCov'><p>CholCov estimator</p></a></li>
<li><a href='#rCov'><p>Realized covariance</p></a></li>
<li><a href='#refreshTime'><p>Synchronize (multiple) irregular timeseries by refresh time</p></a></li>
<li><a href='#ReMeDI'><p>ReMeDI</p></a></li>
<li><a href='#ReMeDIAsymptoticVariance'><p>Asymptotic variance of ReMeDI estimator</p></a></li>
<li><a href='#rHYCov'><p>Hayashi-Yoshida covariance</p></a></li>
<li><a href='#rKernelCov'><p>Realized kernel estimator</p></a></li>
<li><a href='#rKurt'><p>Realized kurtosis of highfrequency return series.</p></a></li>
<li><a href='#rMedRQ'><p>DEPRECATED</p></a></li>
<li><a href='#rMedRQuar'><p>An estimator of integrated quarticity from applying the median operator on blocks of three returns</p></a></li>
<li><a href='#rMedRV'><p>DEPRECATED</p></a></li>
<li><a href='#rMedRVar'><p>rMedRVar</p></a></li>
<li><a href='#rMinRQ'><p>DEPRECATED</p></a></li>
<li><a href='#rMinRQuar'><p>An estimator of integrated quarticity from applying the minimum operator on blocks of two returns</p></a></li>
<li><a href='#rMinRV'><p>DEPRECATED</p></a></li>
<li><a href='#rMinRVar'><p>rMinRVar</p></a></li>
<li><a href='#rmLargeSpread'><p>Delete entries for which the spread is more than <code>maxi</code> times the median spread</p></a></li>
<li><a href='#rmNegativeSpread'><p>Delete entries for which the spread is negative</p></a></li>
<li><a href='#rmOutliersQuotes'><p>Remove outliers in quotes</p></a></li>
<li><a href='#rmOutliersTrades'><p>Remove outliers in trades without using quote data</p></a></li>
<li><a href='#rMPV'><p>DEPRECATED</p></a></li>
<li><a href='#rMPVar'><p>Realized multipower variation</p></a></li>
<li><a href='#rMRC'><p>DEPRECATED rMRC</p></a></li>
<li><a href='#rMRCov'><p>Modulated realized covariance</p></a></li>
<li><a href='#rmTradeOutliersUsingQuotes'><p>Delete transactions with unlikely transaction prices</p></a></li>
<li><a href='#rOWCov'><p>Realized outlyingness weighted covariance</p></a></li>
<li><a href='#rQPVar'><p>Realized quad-power variation of intraday returns</p></a></li>
<li><a href='#rQuar'><p>Realized quarticity</p></a></li>
<li><a href='#rRTSCov'><p>Robust two time scale covariance estimation</p></a></li>
<li><a href='#rRVar'><p>An estimator of realized variance.</p></a></li>
<li><a href='#rSemiCov'><p>Realized semicovariance</p></a></li>
<li><a href='#rSkew'><p>Realized skewness</p></a></li>
<li><a href='#rSV'><p>DEPRECATED</p></a></li>
<li><a href='#rSVar'><p>Realized semivariance of highfrequency return series</p></a></li>
<li><a href='#rThresholdCov'><p>Threshold Covariance</p></a></li>
<li><a href='#rTPQuar'><p>Realized tri-power quarticity</p></a></li>
<li><a href='#rTSCov'><p>Two time scale covariance estimation</p></a></li>
<li><a href='#RV'><p>DEPRECATED</p>
DEPRECATED USE <code>rRVar</code></a></li>
<li><a href='#salesCondition'><p>salesCondition is deprecated. Use tradesCondition instead.</p></a></li>
<li><a href='#sampleMultiTradeData'><p>Multivariate tick by tick data</p></a></li>
<li><a href='#sampleOneMinuteData'><p>One minute data</p></a></li>
<li><a href='#sampleQData'><p>Sample of cleaned quotes for stock XXX for 2 days measured in microseconds</p></a></li>
<li><a href='#sampleQDataRaw'><p>Sample of raw quotes for stock XXX for 2 days measured in microseconds</p></a></li>
<li><a href='#sampleTData'><p>Sample of cleaned trades for stock XXX for 2 days</p></a></li>
<li><a href='#sampleTDataEurope'><p>European data</p></a></li>
<li><a href='#sampleTDataRaw'><p>Sample of raw trades for stock XXX for 2 days</p></a></li>
<li><a href='#selectExchange'><p>Retain only data from a single stock exchange</p></a></li>
<li><a href='#spotDrift'><p>Spot Drift Estimation</p></a></li>
<li><a href='#spotVol'><p>Spot volatility estimation</p></a></li>
<li><a href='#spreadPrices'><p>Convert to format for realized measures</p></a></li>
<li><a href='#SPYRM'><p>SPY realized measures</p></a></li>
<li><a href='#summary.HARmodel'><p>Summary for <code>HARmodel</code> objects</p></a></li>
<li><a href='#tradesCleanup'><p>Cleans trade data</p></a></li>
<li><a href='#tradesCleanupUsingQuotes'><p>Perform a final cleaning procedure on trade data</p></a></li>
<li><a href='#tradesCondition'><p>Delete entries with abnormal trades condition.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-04</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Highfrequency Data Analysis</td>
</tr>
<tr>
<td>Description:</td>
<td>Provide functionality to manage, clean and match highfrequency
    trades and quotes data, calculate various liquidity measures, estimate and
    forecast volatility, detect price jumps and investigate microstructure noise and intraday
    periodicity. A detailed vignette can be found in the open-access paper 
    "Analyzing Intraday Financial Data in R: The highfrequency Package" 
    by Boudt, Kleen, and Sjoerup (2022, &lt;<a href="https://doi.org/10.18637%2Fjss.v104.i08">doi:10.18637/jss.v104.i08</a>&gt;). </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jonathancornelissen/highfrequency">https://github.com/jonathancornelissen/highfrequency</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jonathancornelissen/highfrequency/issues">https://github.com/jonathancornelissen/highfrequency/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>xts, zoo, Rcpp, graphics, methods, stats, utils, grDevices,
robustbase, data.table (&ge; 1.12.0), RcppRoll, quantmod,
sandwich, numDeriv, Rsolnp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mvtnorm, covr, FKF, rugarch, testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-04 12:07:14 UTC; onnokleen</td>
</tr>
<tr>
<td>Author:</td>
<td>Kris Boudt <a href="https://orcid.org/0000-0002-1000-5142"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Jonathan Cornelissen [aut],
  Scott Payseur [aut],
  Giang Nguyen [ctb],
  Onno Kleen <a href="https://orcid.org/0000-0003-4731-4640"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Emil Sjoerup [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kris Boudt &lt;kris.boudt@ugent.be&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-04 15:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aggregatePrice'>Aggregate a time series but keep first and last observation</h2><span id='topic+aggregatePrice'></span>

<h3>Description</h3>

<p>Function to aggregate high frequency data by last tick aggregation to an arbitrary periodicity based on wall clocks.
Alternatively the aggregation can be done by number of ticks. In case we DON'T do tick-based aggregation, 
this function accepts arbitrary number of symbols over a arbitrary number of days. Although the function has the word Price in the name,
the function is general and works on arbitrary time series, either <code>xts</code> or <code>data.table</code> objects the latter requires a <code>DT</code>
column containing POSIXct time stamps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregatePrice(
  pData,
  alignBy = "minutes",
  alignPeriod = 1,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  fill = FALSE,
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregatePrice_+3A_pdata">pData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object to be aggregated containing the intraday price series, possibly across multiple days.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. Possible values are: &quot;secs&quot;, &quot;seconds&quot;, &quot;mins&quot;, &quot;minutes&quot;,&quot;hours&quot;, and &quot;ticks&quot;.
To aggregate based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time, by default: <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time, by default: <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_fill">fill</code></td>
<td>
<p>indicates whether rows without trades should be added with the most recent value, FALSE by default.</p>
</td></tr>
<tr><td><code id="aggregatePrice_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The time stamps of the new time series are the closing times and/or days of the intervals. 
The element of the returned series with e.g. time stamp 09:35:00 contains
the last observation up to that point, including the value at 09:35:00 itself.
</p>
<p>In case <code>alignBy = "ticks"</code>, the sampling is done such the sampling starts on the first tick, and the last tick is always included.
For example, if 14 observations are made on one day, and these are 1, 2, 3, ... 14.
Then, with <code>alignBy = "ticks"</code> and <code>alignPeriod = 3</code>, the output will be 1, 4, 7, 10, 13, 14.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> or <code>xts</code> object containing the aggregated time series.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Aggregate price data to the 30-second frequency
aggregatePrice(sampleTData, alignBy = "secs", alignPeriod = 30)

# Aggregate price data to 30-minute frequency including zero return price changes
aggregatePrice(sampleTData, alignBy = "minutes", alignPeriod = 30, fill = TRUE)

</code></pre>

<hr>
<h2 id='aggregateQuotes'>Aggregate a <code>data.table</code> or <code>xts</code> object containing quote data</h2><span id='topic+aggregateQuotes'></span>

<h3>Description</h3>

<p>Aggregate tick-by-tick quote data and return a <code>data.table</code> or <code>xts</code> object containing the aggregated quote data.
See <code><a href="#topic+sampleQData">sampleQData</a></code> for an example of the argument qData. This function accepts arbitrary number of symbols over an arbitrary number of days.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregateQuotes(
  qData,
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregateQuotes_+3A_qdata">qData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object to be aggregated, containing the intraday quote data of a stock for one day.</p>
</td></tr>
<tr><td><code id="aggregateQuotes_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. Possible values are: &quot;secs&quot;, &quot;seconds&quot;, &quot;mins&quot;, &quot;minutes&quot;,&quot;hours&quot;, and &quot;ticks&quot;.
To aggregate based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregateQuotes_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregateQuotes_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time, by default: <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregateQuotes_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time, by default: <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregateQuotes_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output &quot;BID&quot; and &quot;OFR&quot; columns are constructed using previous tick aggregation.
</p>
<p>The variables &quot;BIDSIZ&quot; and &quot;OFRSIZ&quot; are aggregated by taking the sum of the respective inputs over each interval.
</p>
<p>The timestamps of the new time series are the closing times of the intervals. 
</p>
<p>Please note: Returned objects always contain the first observation (i.e. opening quotes,...).
</p>


<h3>Value</h3>

<p>A <code>data.table</code> or an <code>xts</code> object containing the aggregated quote data.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Aggregate quote data to the 30 second frequency
qDataAggregated &lt;- aggregateQuotes(sampleQData, alignBy = "seconds", alignPeriod = 30)
qDataAggregated # Show the aggregated data
</code></pre>

<hr>
<h2 id='aggregateTrades'>Aggregate a <code>data.table</code> or <code>xts</code> object containing trades dataÂ´</h2><span id='topic+aggregateTrades'></span>

<h3>Description</h3>

<p>Aggregate tick-by-tick trade data and return a time series as a <code>data.table</code> or <code>xts</code> object where first observation is always the opening price
and subsequent observations are the closing prices over the interval. This function accepts arbitrary number of symbols over an arbitrary number of days.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregateTrades(
  tData,
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregateTrades_+3A_tdata">tData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object to be aggregated, containing the intraday price series of a stock for possibly multiple days.</p>
</td></tr>
<tr><td><code id="aggregateTrades_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. Possible values are: <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code>.
To aggregate based on a 5 minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregateTrades_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5 minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregateTrades_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time, by default: <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregateTrades_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time, by default: <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="aggregateTrades_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The time stamps of the new time series are the closing times and/or days of the intervals. 
</p>
<p>The output <code>"PRICE"</code> column is constructed using previous tick aggregation.
</p>
<p>The variable <code>"SIZE"</code> is aggregated by taking the sum over each interval.
</p>
<p>The variable <code>"VWPRICE"</code> is the aggregated price weighted by volume.
</p>
<p>The time stamps of the new time series are the closing times of the intervals. 
</p>
<p>In case of previous tick aggregation or <code>alignBy = "seconds"/"minutes"/"hours"</code>,
the element of the returned series with e.g. time stamp 09:35:00 contains 
the last observation up to that point, including the value at 09:35:00 itself.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> or <code>xts</code> object containing the aggregated time series.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Aggregate trade data to 5 minute frequency
tDataAggregated &lt;- aggregateTrades(sampleTData, alignBy = "minutes", alignPeriod = 5)
tDataAggregated
</code></pre>

<hr>
<h2 id='aggregateTS'>Aggregate a time series</h2><span id='topic+aggregateTS'></span>

<h3>Description</h3>

<p>Aggregate a time series as <code>xts</code> or <code>data.table</code> object. 
It can handle irregularly spaced time series and returns a regularly spaced one.
Use univariate time series as input for this function and check out <code><a href="#topic+aggregateTrades">aggregateTrades</a></code>
and <code><a href="#topic+aggregateQuotes">aggregateQuotes</a></code> to aggregate Trade or Quote data objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregateTS(
  ts,
  FUN = "previoustick",
  alignBy = "minutes",
  alignPeriod = 1,
  weights = NULL,
  dropna = FALSE,
  tz = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregateTS_+3A_ts">ts</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> object to aggregate.</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_fun">FUN</code></td>
<td>
<p>function to apply over each interval. By default, previous tick aggregation is done. 
Alternatively one can set e.g. FUN = &quot;mean&quot;.
In case weights are supplied, this argument is ignored and a weighted average is taken.</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. Possible values are: &quot;secs&quot;, &quot;seconds&quot;, &quot;mins&quot;, &quot;minutes&quot;, &quot;hours&quot;, &quot;days&quot;, &quot;weeks&quot;, &quot;ticks&quot;.</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_weights">weights</code></td>
<td>
<p>By default, no weighting scheme is used. 
When you assign an <code>xts</code> object with weights to this argument, a weighted mean is taken over each interval. 
Of course, the weights should have the same time stamps as the supplied time series.</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_dropna">dropna</code></td>
<td>
<p>boolean, which determines whether empty intervals should be dropped.
By default, an NA is returned in case an interval is empty, except when the user opts
for previous tick aggregation, by setting <code>FUN = "previoustick"</code> (default).</p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_tz">tz</code></td>
<td>
<p>character denoting which timezone the output should be in. Defaults to <code>NULL</code></p>
</td></tr>
<tr><td><code id="aggregateTS_+3A_...">...</code></td>
<td>
<p>extra parameters passed on to <code>FUN</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The time stamps of the new time series are the closing times and/or days of the intervals. 
For example, for a weekly aggregation the new time stamp is the last day in that particular week (namely Sunday).
</p>
<p>In case of previous tick aggregation, 
for <code>alignBy</code> is either <code>"seconds"</code> <code>"minutes"</code>, or <code>"hours"</code>,
the element of the returned series with e.g. timestamp 09:35:00 contains 
the last observation up to that point, including the value at 09:35:00 itself.
</p>
<p>Please note: In case an interval is empty, by default an NA is returned.. In case e.g. previous 
tick aggregation it makes sense to fill these NAs by the function <code>na.locf</code>
(last observation carried forward) from the <span class="pkg">zoo</span> package.
</p>
<p>In case <code>alignBy = "ticks"</code>, the sampling is done such the sampling starts on the first tick and the last tick is always included.
For example, if 14 observations are made on one day, and these are 1, 2, 3, ... 14.
Then, with <code>alignBy = "ticks"</code> and <code>alignPeriod = 3</code>, the output will be 1, 4, 7, 10, 13, 14.
</p>


<h3>Value</h3>

<p>An <code>xts</code> object containing the aggregated time series.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample price data
## Not run: 
library(xts)
ts &lt;- as.xts(sampleTData[, list(DT, PRICE, SIZE)])

# Previous tick aggregation to the 5-minute sampling frequency:
tsagg5min &lt;- aggregateTS(ts, alignBy = "minutes", alignPeriod = 5)
head(tsagg5min)
# Previous tick aggregation to the 30-second sampling frequency:
tsagg30sec &lt;- aggregateTS(ts, alignBy = "seconds", alignPeriod = 30)
tail(tsagg30sec)
tsagg3ticks &lt;- aggregateTS(ts, alignBy = "ticks", alignPeriod = 3)

## End(Not run)


</code></pre>

<hr>
<h2 id='AJjumpTest'>Ait-Sahalia and Jacod (2009) tests for the presence of jumps in the price series.</h2><span id='topic+AJjumpTest'></span>

<h3>Description</h3>

<p>This test examines the presence of jumps in highfrequency price series. It is based on the theory of Ait-Sahalia and Jacod (2009). 
It consists in comparing the multi-power variation of equi-spaced returns computed at a fast time scale (<code class="reqn">h</code>), <code class="reqn">r_{t,i}</code> (<code class="reqn">i=1, \ldots,N</code>) and those computed at the slower time scale (<code class="reqn">kh</code>), <code class="reqn">y_{t,i}</code>(<code class="reqn">i=1, \ldots ,\mbox{N/k}</code>).
</p>
<p>They found that the limit (for <code class="reqn">N</code> <code class="reqn">\to</code> <code class="reqn">\infty</code> ) of the realized power variation is invariant for different sampling scales and that their ratio is <code class="reqn">1</code> in case of jumps and <code class="reqn">\mbox{k}^{p/2}-1</code> if no jumps.
Therefore the AJ test detects the presence of jump using the ratio of realized power variation sampled from two scales. The null hypothesis is no jumps.
</p>
<p>The function returns three outcomes: 1.z-test value 2.critical value under confidence level of <code class="reqn">95\%</code> and 3. <code class="reqn">p</code>-value.
</p>
<p>Assume there is <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>. Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i=1, \ldots,N</code>) in period <code class="reqn">t</code>.
</p>
<p>And there is <code class="reqn">N/k</code> equispaced returns in period <code class="reqn">t</code>. Let <code class="reqn">y_{t,i}</code> be a return (with <code class="reqn">i=1, \ldots ,\mbox{N/k}</code>) in period <code class="reqn">t</code>.
</p>
<p>Then the AJjumpTest is given by: 
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{AJjumpTest}_{t,N}= \frac{S_t(p,k,h)-k^{p/2-1}}{\sqrt{V_{t,N}}}
   </code>
</p>

<p>in which, 
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{S}_t(p,k,h)= \frac{PV_{t,M}(p,kh)}{PV_{t,M}(p,h)}
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mbox{PV}_{t,N}(p,kh)= \sum_{i=1}^{N/k}{|y_{t,i}|^p}
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mbox{PV}_{t,N}(p,h)= \sum_{i=1}^{N}{|r_{t,i}|^p}
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mbox{V}_{t,N}= \frac{N(p,k) A_{t,N(2p)}}{N A_{t,N(p)}}
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mbox{N}(p,k)= \left(\frac{1}{\mu_p^2}\right)(k^{p-2}(1+k))\mu_{2p} + k^{p-2}(k-1) \mu_p^2 - 2k^{p/2-1}\mu_{k,p}
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mbox{A}_{t,n(2p)}= \frac{(1/N)^{(1-p/2)}}{\mu_p} \sum_{i=1}^{N}{|r_{t,i}|^p} \ \ \mbox{for} \ \ |r_j|&lt; \alpha(1/N)^w
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \mu_{k,p}=  E(|U|^p  |U+\sqrt{k-1}V|^p)
 </code>
</p>
 
<p><code class="reqn">U, V</code>: independent standard normal random variables; <code class="reqn">h=1/N</code>; <code class="reqn">p, k, \alpha, w</code>: parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AJjumpTest(
  pData,
  p = 4,
  k = 2,
  alignBy = NULL,
  alignPeriod = NULL,
  alphaMultiplier = 4,
  alpha = 0.975,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AJjumpTest_+3A_pdata">pData</code></td>
<td>
<p>either an <code>xts</code> or a <code>data.table</code> containing the prices of a single asset, possibly over multiple days.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_p">p</code></td>
<td>
<p>can be chosen among 2 or 3 or 4. The author suggests 4. 4 by default.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_k">k</code></td>
<td>
<p>can be chosen among 2 or 3 or 4. The author suggests 2. 2 by default.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code>
To aggregate based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to &quot;minutes&quot;.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5 minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_alphamultiplier">alphaMultiplier</code></td>
<td>
<p>alpha multiplier</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_alpha">alpha</code></td>
<td>
<p>numeric of length one with the significance level to use for the jump test(s). Defaults to 0.975.</p>
</td></tr>
<tr><td><code id="AJjumpTest_+3A_...">...</code></td>
<td>
<p>used internally</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The theoretical framework underlying jump test is that the logarithmic price process <code class="reqn">X_t</code> belongs to the class of Brownian semimartingales, which can be written as:
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{X}_{t}=  \int_{0}^{t} a_udu + \int_{0}^{t}\sigma_{u}dW_{u} + Z_t
   </code>
</p>

<p>where <code class="reqn">a</code> is the drift term, <code class="reqn">\sigma</code> denotes the spot volatility process, <code class="reqn">W</code> is a standard Brownian motion and <code class="reqn">Z</code> is a jump process defined by:
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{Z}_{t}=  \sum_{j=1}^{N_t}k_j
   </code>
</p>

<p>where <code class="reqn">k_j</code> are nonzero random variables. The counting process can be either finite or infinite for finite or infinite activity jumps.
</p>
<p>Using the convergence properties of power variation and its dependence on the time scale on which it is measured, 
Ait-Sahalia and Jacod (2009) define a new variable which converges to 1 in the presence of jumps in the underlying return series, 
or to another deterministic and known number in the absence of jumps (Theodosiou and Zikes, 2009).
</p>


<h3>Value</h3>

<p>a list or <code>xts</code> in depending on whether input prices span more than one day.
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Ait-Sahalia, Y. and Jacod, J. (2009). Testing for jumps in a discretely observed process. The Annals of Statistics, 37(1), 184-222.
</p>
<p>Theodosiou, M., &amp; Zikes, F. (2009). A comprehensive comparison of alternative tests for jumps in asset prices. Unpublished manuscript, Graduate School of Business, Imperial College London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

jt &lt;- AJjumpTest(sampleTData[, list(DT, PRICE)], p = 2, k = 3, 
                 alignBy = "seconds", alignPeriod = 5, makeReturns = TRUE)

</code></pre>

<hr>
<h2 id='autoSelectExchangeQuotes'>Retain only data from the stock exchange with the highest volume</h2><span id='topic+autoSelectExchangeQuotes'></span>

<h3>Description</h3>

<p>Filters raw quote data and return only data that stems from the exchange with the highest
value for the sum of <code>"BIDSIZ"</code> and <code>"OFRSIZ"</code>, i.e. the highest quote volume.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoSelectExchangeQuotes(qData, printExchange = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoSelectExchangeQuotes_+3A_qdata">qData</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object with at least a column <code>"EX"</code>, indicating the exchange symbol 
and columns <code>"BIDSIZ"</code> and <code>"OFRSIZ"</code>, indicating 
the volume available at the bid and ask respectively.</p>
</td></tr>
<tr><td><code id="autoSelectExchangeQuotes_+3A_printexchange">printExchange</code></td>
<td>
<p>indicates whether the chosen exchange is printed on the console, default is <code>TRUE</code>.
The possible exchanges are:
</p>

<ul>
<li><p> A: AMEX
</p>
</li>
<li><p> N: NYSE
</p>
</li>
<li><p> B: Boston
</p>
</li>
<li><p> P: Arca
</p>
</li>
<li><p> C: NSX
</p>
</li>
<li><p> T/Q: NASDAQ
</p>
</li>
<li><p> D: NASD ADF and TRF
</p>
</li>
<li><p> X: Philadelphia
</p>
</li>
<li><p> I: ISE
</p>
</li>
<li><p> M: Chicago
</p>
</li>
<li><p> W: CBOE
</p>
</li>
<li><p> Z: BATS
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.table</code> or <code>xts</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>autoSelectExchangeQuotes(sampleQDataRaw)

</code></pre>

<hr>
<h2 id='autoSelectExchangeTrades'>Retain only data from the stock exchange with the highest trading volume</h2><span id='topic+autoSelectExchangeTrades'></span>

<h3>Description</h3>

<p>Filters raw trade data and return only data that stems from the exchange with the highest
value for the variable <code>"SIZE"</code>, i.e. the highest trade volume.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoSelectExchangeTrades(tData, printExchange = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoSelectExchangeTrades_+3A_tdata">tData</code></td>
<td>
<p>an <code>xts</code> object with at least a column <code>"EX"</code> 
indicating the exchange symbol and <code>"SIZE"</code>
indicating the trade volume.</p>
</td></tr>
<tr><td><code id="autoSelectExchangeTrades_+3A_printexchange">printExchange</code></td>
<td>
<p>indicates whether the chosen exchange is printed on the console, default is <code>TRUE</code>.
The possible exchanges are:
</p>

<ul>
<li><p> A: AMEX
</p>
</li>
<li><p> N: NYSE
</p>
</li>
<li><p> B: Boston
</p>
</li>
<li><p> P: Arca
</p>
</li>
<li><p> C: NSX
</p>
</li>
<li><p> T/Q: NASDAQ
</p>
</li>
<li><p> D: NASD ADF and TRF
</p>
</li>
<li><p> X: Philadelphia
</p>
</li>
<li><p> I: ISE
</p>
</li>
<li><p> M: Chicago
</p>
</li>
<li><p> W: CBOE
</p>
</li>
<li><p> Z: BATS
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.table</code> or <code>xts</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>autoSelectExchangeTrades(sampleTDataRaw)

</code></pre>

<hr>
<h2 id='Bj'>Internal HEAVY functions</h2><span id='topic+Bj'></span>

<h3>Description</h3>

<p>Internal HEAVY functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bj(j, parVarEq, parRMEq)
</code></pre>

<hr>
<h2 id='BNSjumpTest'>Barndorff-Nielsen and Shephard (2006) tests for the presence of jumps in the price series.</h2><span id='topic+BNSjumpTest'></span>

<h3>Description</h3>

<p>This test examines the presence of jumps in highfrequency price series. It is based on theory of Barndorff-Nielsen and Shephard (2006). 
The null hypothesis is that there are no jumps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BNSjumpTest(
  rData,
  IVestimator = "BV",
  IQestimator = "TP",
  type = "linear",
  logTransform = FALSE,
  max = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  alpha = 0.975
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BNSjumpTest_+3A_rdata">rData</code></td>
<td>
<p>either an <code>xts</code> or a <code>data.table</code> containing the log-returns or prices of a single asset, possibly over multiple days-</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_ivestimator">IVestimator</code></td>
<td>
<p>can be chosen among jump robust integrated variance estimators: 
<code><a href="#topic+rBPCov">rBPCov</a></code>, <code><a href="#topic+rMinRVar">rMinRVar</a></code>, <code><a href="#topic+rMedRVar">rMedRVar</a></code>, <code><a href="#topic+rOWCov">rOWCov</a></code> and corrected threshold bipower variation (<code><a href="#topic+rThresholdCov">rThresholdCov</a></code>). 
If <code><a href="#topic+rThresholdCov">rThresholdCov</a></code> is chosen, an argument of <code>startV</code>, start point of auxiliary estimators in threshold estimation can be included. <code><a href="#topic+rBPCov">rBPCov</a></code> by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_iqestimator">IQestimator</code></td>
<td>
<p>can be chosen among jump robust integrated quarticity estimators: <code><a href="#topic+rTPQuar">rTPQuar</a></code>, <code><a href="#topic+rQPVar">rQPVar</a></code>, <code><a href="#topic+rMinRQuar">rMinRQuar</a></code> and <code><a href="#topic+rMedRQuar">rMedRQuar</a></code>. 
<code><a href="#topic+rTPQuar">rTPQuar</a></code> by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_type">type</code></td>
<td>
<p>a method of BNS testing: can be linear or ratio. Linear by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_logtransform">logTransform</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>QVestimator</code> and <code>IVestimator</code> are in logarithm form. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_max">max</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when max adjustment in SE. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code>
To aggregate based on a 5 minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5 minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>pData</code> contains prices. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="BNSjumpTest_+3A_alpha">alpha</code></td>
<td>
<p>numeric of length one with the significance level to use for the jump test(s). Defaults to 0.975.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume there is <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>. 
Assume the Realized variance (RV), IVestimator and IQestimator are based on <code class="reqn">N</code> equi-spaced returns. 
</p>
<p>Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i = 1, \ldots, N</code>) in period <code class="reqn">t</code>. 
</p>
<p>Then the BNSjumpTest is given by
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{BNSjumpTest}= \frac{\code{RV} - \code{IVestimator}}{\sqrt{(\theta-2)\frac{1}{N} {\code{IQestimator}}}}.
</code>
</p>

<p>The options for <code>IVestimator</code> and <code>IQestimator</code> are listed above. <code class="reqn">\theta</code> depends on the chosen <code>IVestimator</code> (Huang and Tauchen, 2005).
</p>
<p>The theoretical framework underlying the jump test is that the logarithmic price process <code class="reqn">X_t</code> belongs to the class of Brownian semimartingales, which can be written as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{X}_{t}=  \int_{0}^{t} a_u \ du + \int_{0}^{t}\sigma_{u} \ dW_{u} + Z_t
</code>
</p>

<p>where <code class="reqn">a</code> is the drift term, <code class="reqn">\sigma</code> denotes the spot volatility process, <code class="reqn">W</code> is a standard Brownian motion and <code class="reqn">Z</code> is a jump process defined by:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{Z}_{t}=  \sum_{j=1}^{N_t}k_j
</code>
</p>

<p>where <code class="reqn">k_j</code> are nonzero random variables. The counting process can be either finite or infinite for finite or infinite activity jumps.
</p>
<p>Since the realized volatility converges to the sum of integrated variance and jump variation, while the robust <code>IVestimator</code> converges to the integrated variance, 
it follows that the difference between <code>RV</code> and the <code>IVestimator</code> captures the jump part only, and this observation underlines the BNS test for jumps (Theodosiou and Zikes, 2009).
</p>


<h3>Value</h3>

<p>a list or <code>xts</code> (depending on whether input prices span more than one day)
with the following values:
</p>

<ul>
<li> <p><code class="reqn">z</code>-test value.
</p>
</li>
<li><p> critical value (with confidence level of 95%).
</p>
</li>
<li> <p><code class="reqn">p</code>-value of the test. 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., and Shephard, N. (2006). Econometrics of testing for jumps in financial economics using bipower variation. <em>Journal of Financial Econometrics</em>, 4, 1-30. 
</p>
<p>Corsi, F., Pirino, D., and Reno, R. (2010). Threshold bipower variation and the impact of jumps on volatility forecasting. <em>Journal of Econometrics</em>, 159, 276-288.
</p>
<p>Huang, X., and Tauchen, G. (2005). The relative contribution of jumps to total price variance. <em>Journal of Financial Econometrics</em>, 3, 456-499.
</p>
<p>Theodosiou, M., and Zikes, F. (2009). A comprehensive comparison of alternative tests for jumps in asset prices. Unpublished manuscript, Graduate School of Business, Imperial College London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bns &lt;- BNSjumpTest(sampleTData[, list(DT, PRICE)], IVestimator= "rMinRVar",
                   IQestimator = "rMedRQuar", type= "linear", makeReturns = TRUE)
bns

</code></pre>

<hr>
<h2 id='businessTimeAggregation'>Business time aggregation</h2><span id='topic+businessTimeAggregation'></span>

<h3>Description</h3>

<p>Time series aggregation based on 'business time' statistics. Instead of equidistant sampling based on time during a trading day, business time sampling creates measures and samples equidistantly using these instead.
For example when sampling based on volume, business time aggregation will result in a time series that has an equal amount of volume between each observation (if possible).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>businessTimeAggregation(
  pData,
  measure = "volume",
  obs = 390,
  bandwidth = 0.075,
  tz = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="businessTimeAggregation_+3A_pdata">pData</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing data to aggregate.</p>
</td></tr>
<tr><td><code id="businessTimeAggregation_+3A_measure">measure</code></td>
<td>
<p>character denoting which measure to use. Valid options are <code>"intensity"</code>, <code>"vol"</code>, and <code>"volume"</code>, 
denoting the trade intensity process of Oomen (2005), volatility, and volume, respectively. Default is <code>"volume"</code>.</p>
</td></tr>
<tr><td><code id="businessTimeAggregation_+3A_obs">obs</code></td>
<td>
<p>integer valued numeric of length 1 denoting how many observations is wanted after the aggregation procedure.</p>
</td></tr>
<tr><td><code id="businessTimeAggregation_+3A_bandwidth">bandwidth</code></td>
<td>
<p>numeric of length one, denoting which bandwidth parameter to use in the trade intensity process estimation of Oomen (2005).</p>
</td></tr>
<tr><td><code id="businessTimeAggregation_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>.</p>
</td></tr>
<tr><td><code id="businessTimeAggregation_+3A_...">...</code></td>
<td>
<p>extra arguments passed on to <code><a href="#topic+spotVol">spotVol</a></code> when measure is <code>"vol"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing <code>"pData"</code> which is the aggregated data and a list containing the intensity process, split up day by day.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>References</h3>

<p>Dong, Y., and Tse, Y. K. (2017). Business time sampling scheme with applications to testing semi-martingale hypothesis and estimating integrated volatility. <em>Econometrics</em>, 5, 51.
</p>
<p>Oomen, R. C. A. (2006). Properties of realized variance under alternative sampling schemes. <em>Journal of Business &amp; Economic Statistics</em>, 24, 219-237
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


pData &lt;- sampleTData[,list(DT, PRICE, SIZE)]
# Aggregate based on the trade intensity measure. Getting 390 observations.
agged &lt;- businessTimeAggregation(pData, measure = "intensity", obs = 390, bandwidth = 0.075)
# Plot the trade intensity measure
plot.ts(agged$intensityProcess$`2018-01-02`)
rCov(agged$pData[, list(DT, PRICE)], makeReturns = TRUE)
rCov(pData[,list(DT, PRICE)], makeReturns = TRUE, alignBy = "minutes", alignPeriod = 1)

# Aggregate based on the volume measure. Getting 78 observations.
agged &lt;- businessTimeAggregation(pData, measure = "volume", obs = 78)
rCov(agged$pData[,list(DT, PRICE)], makeReturns = TRUE)
rCov(pData[,list(DT, PRICE)], makeReturns = TRUE, alignBy = "minutes", alignPeriod = 5)

</code></pre>

<hr>
<h2 id='cholCovrMRCov'>#' @keywords internal
zgamma &lt;- function (x, y, gamma_power) 
if (x^2 &lt; y) 
out &lt;- abs(x)^gamma_power
 else 
if (gamma_power == 1) 
out &lt;- 1.094 * sqrt(y)

if (gamma_power == 2) 
out &lt;- 1.207 * y

if (gamma_power == 4/3) 
out &lt;- 1.129 * y^(2/3)


return(out)
</h2><span id='topic+cholCovrMRCov'></span>

<h3>Description</h3>

<p>#' @keywords internal
zgamma &lt;- function (x, y, gamma_power) 
if (x^2 &lt; y) 
out &lt;- abs(x)^gamma_power
 else 
if (gamma_power == 1) 
out &lt;- 1.094 * sqrt(y)

if (gamma_power == 2) 
out &lt;- 1.207 * y

if (gamma_power == 4/3) 
out &lt;- 1.129 * y^(2/3)


</p>
<p>return(out)

</p>


<h3>Usage</h3>

<pre><code class='language-R'>cholCovrMRCov(returns, delta = 0.1, theta = 1)
</code></pre>

<hr>
<h2 id='driftBursts'>Inference on drift burst hypothesis</h2><span id='topic+driftBursts'></span>

<h3>Description</h3>

<p>Calculates the test-statistic for the drift burst hypothesis
</p>
<p>Let the efficient log-price be defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    dX_{t} = \mu_{t}dt + \sigma_{t}dW_{t} + dJ_{t},
</code>
</p>

<p>where <code class="reqn">\mu_{t}</code>, <code class="reqn">\sigma_{t}</code>, and <code class="reqn">J_{t}</code> are the spot drift, the spot volatility, and a jump process respectively.
However, due to microstructure noise, the observed log-price is 
</p>
<p style="text-align: center;"><code class="reqn">
    Y_{t} = X_{t} + \varepsilon_{t}
</code>
</p>

<p>In order robustify the results to the presence of market microstructure noise, the pre-averaged returns are used:
</p>
<p style="text-align: center;"><code class="reqn">
    \Delta_{i}^{n}\overline{Y} = \sum_{j=1}^{k_{n}-1}g_{j}^{n}\Delta_{i+j}^{n}Y,
</code>
</p>

<p>where <code class="reqn">g(\cdot)</code> is a weighting function, <code class="reqn">min(x, 1-x)</code>, and <code class="reqn">k_{n}</code> is the pre-averaging horizon.
</p>
<p>The test statistic for the Drift Burst Hypothesis can then be calculated as
</p>
<p style="text-align: center;"><code class="reqn">
    \bar{T}_{t}^{n} = \sqrt{\frac{h_{n}}{K_{2}}}\frac{\hat{\bar{\mu}}_{t}^{n}}{\sqrt{\hat{\bar{\sigma}}_{t}^{n}}},
</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\bar{\mu}}_{t}^{n} = \frac{1}{h_{n}}\sum_{i=1}^{n-k_{n}+2}K\left(\frac{t_{i-1}-t}{h_{n}}\right)\Delta_{i-1}^{n}\overline{Y},
</code>
</p>

<p>and
</p>
<p><code class="reqn">
    \hat{\bar{\sigma}}_{t}^{n} = \frac{1}{h_{n}'}\bigg[\sum_{i=1}^{n-k_{n}+2}\left(K\left(\frac{t_{i-1}-t}{h'_{n}}\right)\Delta_{i-1}^{n}\overline{Y}\right)^{2} \\
    + 2\sum_{L=1}^{L_{n}}\omega\left(\frac{L}{L_{n}}\right)\sum_{i=1}^{n-k_{n}-L+2}K\left(\frac{t_{i-1}-t}{h_{n}'}\right)K\left(\frac{t_{i+L-1}-t}{h_{n}'}\right)\Delta_{i-1}^{n}\overline{Y}\Delta_{i-1+L}^{n}\overline{Y}\bigg],
</code>
</p>
<p>where <code class="reqn">\omega(\cdot)</code> is a smooth kernel function, in this case the Parzen kernel. <code class="reqn">L_{n}</code> is the lag length for adjusting for auto-correlation and <code class="reqn">K(\cdot)</code>
is a kernel weighting function, which in this case is the left-sided exponential kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>driftBursts(
  pData,
  testTimes = seq(34260, 57600, 60),
  preAverage = 5,
  ACLag = -1L,
  meanBandwidth = 300L,
  varianceBandwidth = 900L,
  parallelize = FALSE,
  nCores = NA,
  warnings = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="driftBursts_+3A_pdata">pData</code></td>
<td>
<p>Either a <code>data.table</code> or an <code>xts</code> object. If pData is a data.table, columns DT and PRICE must be present, containing timestamps of the trades and the price of the 
trades (in levels) respectively. If pData is an <code>xts</code> object and the number of columns is greater than one, PRICE must be present.</p>
</td></tr>
<tr><td><code id="driftBursts_+3A_testtimes">testTimes</code></td>
<td>
<p>A <code>numeric</code> containing the times at which to calculate the tests. The standard of <code>seq(34260, 57600, 60)</code> 
denotes calculating the test-statistic once per minute, i.e. 390 times for a typical 6.5 hour trading day from 9:31:00 to 16:00:00. See details.
Additionally, <code>testTimes</code> can be set to 'all' where the test statistic will be calculated on each tick more than 5 seconds after opening</p>
</td></tr>
<tr><td><code id="driftBursts_+3A_preaverage">preAverage</code></td>
<td>
<p>A positive <code>integer</code> denoting the length of pre-averaging window for the log-prices. Default is <code>5</code></p>
</td></tr>
<tr><td><code id="driftBursts_+3A_aclag">ACLag</code></td>
<td>
<p>A positive <code>integer</code> greater than 1 denoting how many lags are to be used for the HAC estimator of the variance - the default
of <code>-1</code> denotes using an automatic lag selection algorithm for each iteration. Default is <code>-1L</code></p>
</td></tr>
<tr><td><code id="driftBursts_+3A_meanbandwidth">meanBandwidth</code></td>
<td>
<p>An <code>integer</code> denoting the bandwidth for the left-sided exponential kernel for the mean. Default is <code>300L</code></p>
</td></tr>
<tr><td><code id="driftBursts_+3A_variancebandwidth">varianceBandwidth</code></td>
<td>
<p>An <code>integer</code> denoting the bandwidth for the left-sided exponential kernel for the variance. Default is <code>900L</code></p>
</td></tr>
<tr><td><code id="driftBursts_+3A_parallelize">parallelize</code></td>
<td>
<p>A <code>logical</code> to determine whether to parallelize the underlying C++ code (Using OpenMP). Default is <code>FALSE</code>. 
Note that the parallelized code is not interruptable, while the non-parallel code is interruptable and it's checked every 100 iterations.</p>
</td></tr>
<tr><td><code id="driftBursts_+3A_ncores">nCores</code></td>
<td>
<p>An <code>integer</code> denoting the number of cores to use for calculating the code when parallelized. 
If this argument is not provided, sequential evaluation will be used even though <code>parallelize</code> is TRUE. Default is <code>NA</code></p>
</td></tr>
<tr><td><code id="driftBursts_+3A_warnings">warnings</code></td>
<td>
<p>A <code>logical</code> denoting whether warnings should be shown. Default is <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the <code>testTimes</code> vector contains instructions to test before the first trade, or more than 15 minutes after the last trade, these entries will be deleted, as not doing so may cause crashes.
The test statistic is unstable before <code>max(meanBandwidth , varianceBandwidth)</code> seconds has passed.
The lags from the Newey-West algorithm is increased by <code>2 * (preAveage-1)</code> due to the pre-averaging we know at least this many lags should be corrected for.
The maximum of 20 lags is also increased by this factor for the same reason.
</p>


<h3>Value</h3>

<p>An object of class <code>DBH</code> and <code>list</code> containing the series of the drift burst hypothesis test-statistic as well as the estimated spot drift and variance series. 
The list also contains some information such as the variance and mean bandwidths along with the pre-averaging setting and the amount of observations. 
Additionally, the list will contain information on whether testing happened for all <code>testTimes</code> entries.
Objects of class <code>DBH</code> has the methods <code><a href="#topic+print.DBH">print.DBH</a></code>, <code><a href="#topic+plot.DBH">plot.DBH</a></code>, and <code><a href="#topic+getCriticalValues.DBH">getCriticalValues.DBH</a></code> which prints, plots, and
retrieves critical values for the test described in appendix B of Christensen, Oomen, and Reno (2020).
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>References</h3>

<p>Christensen, K., Oomen, R., and Reno, R. (2020) The drift burst hypothesis. Journal of Econometrics. Forthcoming.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Usage with data.table object
dat &lt;- sampleTData[as.Date(DT) == "2018-01-02"]
# Testing every 60 seconds after 09:45:00
DBH1 &lt;- driftBursts(dat, testTimes = seq(35100, 57600, 60), preAverage = 2, ACLag = -1L,
                    meanBandwidth = 300L, varianceBandwidth = 900L)
print(DBH1)

plot(DBH1, pData = dat)
# Usage with xts object (1 column)
library("xts")
dat &lt;- xts(sampleTData[as.Date(DT) == "2018-01-03"]$PRICE, 
           order.by = sampleTData[as.Date(DT) == "2018-01-03"]$DT)
# Testing every 60 seconds after 09:45:00
DBH2 &lt;- driftBursts(dat, testTimes = seq(35100, 57600, 60), preAverage = 2, ACLag = -1L,
                    meanBandwidth = 300L, varianceBandwidth = 900L)
plot(DBH2, pData = dat)

## Not run:  
# This block takes some time
dat &lt;- xts(sampleTDataEurope$PRICE, 
           order.by = sampleTDataEurope$DT)
# Testing every 60 seconds after 09:00:00
system.time({DBH4 &lt;- driftBursts(dat, testTimes = seq(32400 + 900, 63000, 60), preAverage = 2, 
             ACLag = -1L, meanBandwidth = 300L, varianceBandwidth = 900L)})

system.time({DBH4 &lt;- driftBursts(dat, testTimes = seq(32400 + 900, 63000, 60), preAverage = 2, 
                                 ACLag = -1L, meanBandwidth = 300L, varianceBandwidth = 900L,
                                 parallelize = TRUE, nCores = 8)})
plot(DBH4, pData = dat)

# The print method for DBH objects takes an argument alpha that determines the confidence level
# of the test performed
print(DBH4, alpha = 0.99)
# Additionally, criticalValue can be passed directly
print(DBH4, criticalValue = 3)
max(abs(DBH4$tStat)) &gt; getCriticalValues(DBH4, 0.99)$quantile

## End(Not run)

</code></pre>

<hr>
<h2 id='exchangeHoursOnly'>Extract data from an <code>xts</code> object for the exchange hours only</h2><span id='topic+exchangeHoursOnly'></span>

<h3>Description</h3>

<p>Filter raw trade data such and return only data between market close and market open. 
By default, <code>marketOpen = "09:30:00"</code> and <code>marketClose = "16:00:00"</code> (see Brownlees and Gallo (2006) for more information on good choices for these arguments).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exchangeHoursOnly(
  data,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exchangeHoursOnly_+3A_data">data</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object containing the time series data. 
Multiple days of input are allowed.</p>
</td></tr>
<tr><td><code id="exchangeHoursOnly_+3A_marketopen">marketOpen</code></td>
<td>
<p>character in the format of <code>"HH:MM:SS"</code>,
specifying the opening time of the exchange(s).</p>
</td></tr>
<tr><td><code id="exchangeHoursOnly_+3A_marketclose">marketClose</code></td>
<td>
<p>character in the format of <code>"HH:MM:SS"</code>,
specifying the closing time of the exchange(s).</p>
</td></tr>
<tr><td><code id="exchangeHoursOnly_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Brownlees, C. T. and Gallo, G. M. (2006). Financial econometric analysis at ultra-high frequency: Data handling concerns. Computational Statistics &amp; Data Analysis, 51, pages 2232-2245.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exchangeHoursOnly(sampleTDataRaw)
</code></pre>

<hr>
<h2 id='gatherPrices'>Make TAQ format</h2><span id='topic+gatherPrices'></span>

<h3>Description</h3>

<p>Convenience function to gather data from one <code>xts</code> or <code>data.table</code> 
with at least <code>"DT"</code>, and d columns containing price data to a <code>"DT"</code>, <code>"SYMBOL"</code>, and <code>"PRICE"</code>
column. This function the opposite of <code><a href="#topic+spreadPrices">spreadPrices</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gatherPrices(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gatherPrices_+3A_data">data</code></td>
<td>
<p>An <code>xts</code> or a <code>data.table</code> object with at least <code>"DT"</code> and d columns with price data with their names corresponding to the respective symbols.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.table</code> with columns <code>DT</code>, <code>SYMBOL</code>, and <code>PRICE</code>
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spreadPrices">spreadPrices</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(data.table)
data1 &lt;- copy(sampleTData)[,  `:=`(PRICE = PRICE * runif(.N, min = 0.99, max = 1.01),
                                               DT = DT + runif(.N, 0.01, 0.02))]
data2 &lt;- copy(sampleTData)[, SYMBOL := 'XYZ']
dat1 &lt;- rbind(data1[, list(DT, SYMBOL, PRICE)], data2[, list(DT, SYMBOL, PRICE)])
setkeyv(dat1, c("DT", "SYMBOL"))
dat1
dat &lt;- spreadPrices(dat1) # Easy to use for realized measures
dat
dat &lt;- gatherPrices(dat)
dat
all.equal(dat1, dat) # We have changed to RM format and back.

## End(Not run)
</code></pre>

<hr>
<h2 id='getAlphaVantageData'>Get high frequency data from Alpha Vantage</h2><span id='topic+getAlphaVantageData'></span>

<h3>Description</h3>

<p>Function to retrieve high frequency data from Alpha Vantage - wrapper around quantmod's getSymbols.av function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getAlphaVantageData(
  symbols = NULL,
  interval = "5min",
  outputType = "xts",
  apiKey = NULL,
  doSleep = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getAlphaVantageData_+3A_symbols">symbols</code></td>
<td>
<p>character vector with the symbols to import.</p>
</td></tr>
<tr><td><code id="getAlphaVantageData_+3A_interval">interval</code></td>
<td>
<p>the sampling interval of the data retrieved. Should be one of one of &quot;1min&quot;, &quot;5min&quot;, &quot;15min&quot;, &quot;30min&quot;, or &quot;60min&quot;</p>
</td></tr>
<tr><td><code id="getAlphaVantageData_+3A_outputtype">outputType</code></td>
<td>
<p>string either <code>"xts"</code> or <code>"DT"</code> to denote the type of output wanted. <code>"xts"</code> will yield an xts object, <code>"DT"</code> will yield a data.table object.</p>
</td></tr>
<tr><td><code id="getAlphaVantageData_+3A_apikey">apiKey</code></td>
<td>
<p>string with the api key provided by Alpha Vantage.</p>
</td></tr>
<tr><td><code id="getAlphaVantageData_+3A_dosleep">doSleep</code></td>
<td>
<p>logical when the length of symbols &gt; 5 the function will sleep for 12 seconds by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>doSleep</code> argument is set to true as default because Alpha Vantage has a limit of five calls per minute. 
The function does not try to extract when the last API call was made which means that if
you made successive calls to get 3 symbols in rapid succession, the function may not retrieve all the data.
</p>


<h3>Value</h3>

<p>An object of type <code>xts</code> or <code>data.table</code> in case the length of symbols is 1. If the length of symbols &gt; 1 the <code>xts</code> and 
<code>data.table</code> objects will be put into a list.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup (wrapper only) Paul Teetor (for quantmod's getSymbols.av)
</p>


<h3>See Also</h3>

<p>The getSymbols.av function in the quantmod package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Get data for SPY at an interval of 1 minute in the standard xts format.
data &lt;- getAlphaVantageData(symbols = "SPY", apiKey = "yourKey", interval = "1min")

# Get data for 3M and Goldman Sachs  at a 5 minute interval in the data.table format. 
# The data.tables will be put in a list.
data &lt;- getAlphaVantageData(symbols = c("MMM", "GS"), interval = "5min", 
                  outputType = "DT", apiKey = 'yourKey')

# Get data for JPM and Citicorp at a 15 minute interval in the xts format. 
# The xts objects will be put in a list.
data &lt;- getAlphaVantageData(symbols = c("JPM", "C"), interval = "15min", 
                  outputType = "xts", apiKey = "yourKey")

## End(Not run)

</code></pre>

<hr>
<h2 id='getCriticalValues'>Get critical value for the drift burst hypothesis t-statistic</h2><span id='topic+getCriticalValues'></span><span id='topic+getCriticalValues.DBH'></span>

<h3>Description</h3>

<p>Method for DBH objects to calculate the critical value for the presence of a burst of drift.
The critical value is that of the test described in appendix B in Christensen Oomen Reno
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getCriticalValues(x, alpha = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getCriticalValues_+3A_x">x</code></td>
<td>
<p>object of class <code>DBH</code></p>
</td></tr>
<tr><td><code id="getCriticalValues_+3A_alpha">alpha</code></td>
<td>
<p>numeric denoting the confidence level for the critical value. Possible values are <code>c(0.9 0.95 0.99 0.995 0.999 0.9999)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>References</h3>

<p>Christensen, K., Oomen, R., and Reno, R. (2020) The drift burst hypothesis. Journal of Econometrics. Forthcoming.
</p>

<hr>
<h2 id='getLiquidityMeasures'>Compute Liquidity Measure</h2><span id='topic+getLiquidityMeasures'></span>

<h3>Description</h3>

<p>Function returns an <code>xts</code> or <code>data.table</code> object containing 23 liquidity measures. Please see details below.
</p>
<p>Note that this assumes a regular time grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getLiquidityMeasures(tqData, win = 300)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getLiquidityMeasures_+3A_tqdata">tqData</code></td>
<td>
<p>A <code>data.table</code> or <code>xts</code> object as in the <span class="pkg">highfrequency</span> merged
trades and quotes data.</p>
</td></tr>
<tr><td><code id="getLiquidityMeasures_+3A_win">win</code></td>
<td>
<p>A windows length for the forward-prices used for &lsquo;realized&rsquo;
spread</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NOTE: <code>xts</code> or <code>data.table</code> should only contain one day of observations
Some markets have publish information about whether it was a buyer or a seller who initiated the trade. 
This information can be passed in a column <code>DIRECTION</code> this column must only have 1 or -1 as values.
</p>
<p>The respective liquidity measures are defined as follows:
</p>

<ul>
<li><p>effectiveSpread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{effective spread}_t =  2*D_t*(\mbox{PRICE}_{t} - \frac{(\mbox{BID}_{t}+\mbox{OFR}_{t})}{2}),
    </code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) (see Boehmer (2005), Bessembinder (2003)). 
Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).
 
</p>
</li>
<li><p>realizedSpread: realized spread 
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{realized spread}_t =  2*D_t*(\mbox{PRICE}_{t} - \frac{(\mbox{BID}_{t+300}+\mbox{OFR}_{t+300})}{2}),
   </code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) (see Boehmer (2005), Bessembinder (2003)). 
Note that the time indication of <code class="reqn">\mbox{BID}</code> and <code class="reqn">\mbox{OFR}</code> refers 
to the registered time of the quote in seconds.

</p>
</li>
<li><p>valueTrade: trade value 
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{trade value}_t =  \mbox{SIZE}_{t}*\mbox{PRICE}_{t}.
   </code>
</p>

 
</li>
<li><p>signedValueTrade: signed trade value
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{signed trade value}_t =  D_t * (\mbox{SIZE}_{t}*\mbox{PRICE}_{t}),</code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) 
(see Boehmer (2005), Bessembinder (2003)).
  
</p>
</li>
<li><p>depthImbalanceDifference: depth imbalance (as a difference)
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{depth imbalance (as difference)}_t =  \frac{D_t *(\mbox{OFRSIZ}_{t}-\mbox{BIDSIZ}_{t})}{(\mbox{OFRSIZ}_{t}+\mbox{BIDSIZ}_{t})},
   </code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) (see Boehmer (2005), Bessembinder (2003)). 
Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).
 
</p>
</li>
<li><p>depthImbalanceRatio: depth imbalance (as ratio)
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{depth imbalance (as ratio)}_t =  (\frac{\mbox{OFRSIZ}_{t}}{\mbox{BIDSIZ}_{t}})^{D_t},
   </code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) (see Boehmer (2005), Bessembinder (2003)). 
Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).
 
</p>
</li>
<li><p>proportionalEffectiveSpread: proportional effective spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{proportional effective spread}_t =  \frac{\mbox{effective spread}_t}{(\mbox{OFR}_{t}+\mbox{BID}_{t})/2}
   </code>
</p>

<p>(Venkataraman, 2001).
</p>
<p>Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).
 
</p>
</li>
<li><p>proportionalRealizedSpread: proportional realized spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{proportional realized spread}_t =  \frac{\mbox{realized spread}_t}{(\mbox{OFR}_{t}+\mbox{BID}_{t})/2}
   </code>
</p>

<p>(Venkataraman, 2001).
</p>
<p>Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered 
 
</p>
</li>
<li><p>priceImpact: price impact
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{price impact}_t =  \frac{\mbox{effective spread}_t - \mbox{realized spread}_t}{2}
   </code>
</p>

<p>(see Boehmer (2005), Bessembinder (2003)). 

</p>
</li>
<li><p>proportionalPriceImpact: proportional price impact
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{proportional price impact}_t =  \frac{\frac{(\mbox{effective spread}_t - \mbox{realized spread}_t)}{2}}{\frac{\mbox{OFR}_{t}+\mbox{BID}_{t}}{2}}
   </code>
</p>

<p>(Venkataraman, 2001).
Note that the input of this function consists of the matched trades and
quotes, so this is where the time indication refers to (and thus not to the
registered quote timestamp).
  
</p>
</li>
<li><p>halfTradedSpread: half traded spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{half traded spread}_t =  D_t*(\mbox{PRICE}_{t} - \frac{(\mbox{BID}_{t}+\mbox{OFR}_{t})}{2}),
   </code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell) (see Boehmer (2005), Bessembinder (2003)). 
Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).
 
</p>
</li>
<li><p>proportionalHalfTradedSpread: proportional half traded spread 
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{proportional half traded spread}_t =  \frac{\mbox{half traded spread}_t}{\frac{\mbox{OFR}_{t}+\mbox{BID}_{t}}{2}}.
   </code>
</p>

<p>Note that the input of this function consists of the matched trades and quotes,
so this is were the time indication refers to (and thus not to the registered quote timestamp).

</p>
</li>
<li><p>squaredLogReturn: squared log return on trade prices
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{squared log return on Trade prices}_t =  (\log(\mbox{PRICE}_{t})-\log(\mbox{PRICE}_{t-1}))^2.
   </code>
</p>

 
</li>
<li><p>absLogReturn: absolute log return on trade prices
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{absolute log return on Trade prices}_t =  |\log(\mbox{PRICE}_{t})-\log(\mbox{PRICE}_{t-1})|.
   </code>
</p>

 
</li>
<li><p>quotedSpread: quoted spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{quoted spread}_t =  \mbox{OFR}_{t}-\mbox{BID}_{t}
   </code>
</p>

<p>Note that the input of this function consists of the matched trades and
quotes, so this is where the time indication refers to (and thus not to the
registered quote timestamp).
 
</p>
</li>
<li><p>proportionalQuotedSpread: proportional quoted spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{proportional quoted spread}_t =  \frac{\mbox{quoted spread}_t}{\frac{\mbox{OFR}_{t}+\mbox{BID}_{t}}{2}}
   </code>
</p>

<p>(Venkataraman, 2001).
Note that the input of this function consists of the matched trades and
quotes, so this is where the time indication refers to (and thus not to the
registered quote timestamp).
  
</p>
</li>
<li><p>logQuotedSpread: log quoted spread
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{log quoted spread}_t =  \log(\frac{\mbox{OFR}_{t}}{\mbox{BID}_{t}})
   </code>
</p>

<p>(Hasbrouck and Seppi, 2001).
Note that the input of this function consists of the matched trades and
quotes, so this is where the time indication refers to (and thus not to the
registered quote timestamp).
</p>

</li>
<li><p>logQuotedSize: log quoted size
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{log quoted size}_t =  \log(\mbox{OFRSIZ}_{t})+\log(\mbox{BIDSIZ}_{t})
   </code>
</p>

<p>(Hasbrouck and Seppi, 2001).
Note that the input of this function consists of the matched trades and
quotes, so this is where the time indication refers to (and thus not to the
registered quote timestamp).
 
</p>
</li>
<li><p>quotedSlope: quoted slope
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{quoted slope}_t =  \frac{\mbox{quoted spread}_t}{\mbox{log quoted size}_t}
   </code>
</p>

<p>(Hasbrouck and Seppi, 2001).

</p>
</li>
<li><p>logQSlope: log quoted slope
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{log quoted slope}_t =  \frac{\mbox{log quoted spread}_t}{\mbox{log quoted size}_t}.
   </code>
</p>

 
</li>
<li><p>midQuoteSquaredReturn: midquote squared return
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{midquote squared return}_t =  (\log(\mbox{midquote}_{t})-\log(\mbox{midquote}_{t-1}))^2,
   </code>
</p>

<p>where <code class="reqn">\mbox{midquote}_{t} = \frac{\mbox{BID}_{t} + \mbox{OFR}_{t}}{2}</code>.

</p>
</li>
<li><p>midQuoteAbsReturn: midquote absolute return
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{midquote absolute return}_t =  |\log(\mbox{midquote}_{t})-\log(\mbox{midquote}_{t-1})|,
   </code>
</p>

<p>where <code class="reqn">\mbox{midquote}_{t} = \frac{\mbox{BID}_{t} + \mbox{OFR}_{t}}{2}</code>.

</p>
</li>
<li><p>signedTradeSize: signed trade size
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{signed trade size}_t =  D_t * \mbox{SIZE}_{t},</code>
</p>

<p>where <code class="reqn">D_t</code> is 1 (-1) if <code class="reqn">trade_t</code> was buy (sell).

</p>
</li></ul>



<h3>Value</h3>

<p>A modified (enlarged) <code>xts</code> or <code>data.table</code> with the new measures.
</p>


<h3>References</h3>

<p>Bessembinder, H. (2003). Issues in assessing trade execution costs. <em>Journal of Financial Markets</em>, 223-257.
</p>
<p>Boehmer, E. (2005). Dimensions of execution quality: Recent evidence for US equity markets. <em>Journal of Financial Economics</em>, 78, 553-582.
</p>
<p>Hasbrouck, J. and Seppi, D. J. (2001). Common factors in prices, order flows and liquidity. <em>Journal of Financial Economics</em>, 59, 383-411.
</p>
<p>Venkataraman, K. (2001). Automated versus floor trading: An analysis of execution costs on the Paris and New York exchanges. <em>The Journal of Finance</em>, 56, 1445-1485.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tqData &lt;- matchTradesQuotes(sampleTData[as.Date(DT) == "2018-01-02"], 
                            sampleQData[as.Date(DT) == "2018-01-02"])
res &lt;- getLiquidityMeasures(tqData)
res
</code></pre>

<hr>
<h2 id='getTradeDirection'>Get trade direction</h2><span id='topic+getTradeDirection'></span>

<h3>Description</h3>

<p>Function returns a vector with the inferred trade direction which is 
determined using the Lee and Ready algorithm (Lee and Ready, 1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTradeDirection(tqData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTradeDirection_+3A_tqdata">tqData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object, containing joined trades and quotes (e.g. using <code><a href="#topic+matchTradesQuotes">matchTradesQuotes</a></code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NOTE: By convention the first observation is always marked as a buy.
</p>


<h3>Value</h3>

<p>A vector which has values 1 or (-1) if the inferred trade direction
is buy or sell respectively.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup. Special thanks to Dirk Eddelbuettel.
</p>


<h3>References</h3>

<p>Lee, C. M. C. and Ready, M. J. (1991). Inferring trade direction from intraday data. <em>Journal of Finance</em>, 46, 733-746.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate matched trades and quote data set
tqData &lt;- matchTradesQuotes(sampleTData[as.Date(DT) == "2018-01-02"], 
                            sampleQData[as.Date(DT) == "2018-01-02"])
directions &lt;- getTradeDirection(tqData)
head(directions)

</code></pre>

<hr>
<h2 id='HARmodel'>Heterogeneous autoregressive (HAR) model for realized volatility model estimation</h2><span id='topic+HARmodel'></span>

<h3>Description</h3>

<p>Function returns the estimates for the heterogeneous autoregressive model (HAR)
for realized volatility  discussed in Andersen et al. (2007) and Corsi (2009).
This model is mainly used to forecast the next day's volatility based on the high-frequency returns of the past.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HARmodel(
  data,
  periods = c(1, 5, 22),
  periodsJ = c(1, 5, 22),
  periodsQ = c(1),
  leverage = NULL,
  RVest = c("rCov", "rBPCov", "rQuar"),
  type = "HAR",
  inputType = "RM",
  jumpTest = "ABDJumptest",
  alpha = 0.05,
  h = 1,
  transform = NULL,
  externalRegressor = NULL,
  periodsExternal = c(1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HARmodel_+3A_data">data</code></td>
<td>
<p>an <code>xts</code> object containing either: intraday (log-)returns or realized measures already computed from such returns. 
In case more than one realized measure is needed, the object should have the as many columns as realized measures needed. 
The first column should always be the realized variance proxy. 
In case type is either <code>"HARQJ"</code> or <code>"CHARQ"</code> the order should be 
<code>"RV"</code>, <code>"BPV"</code>, <code>"RQ"</code>, or the relevant proxies.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_periods">periods</code></td>
<td>
<p>a vector of integers indicating over how days the realized measures in the model should be aggregated. 
By default  <code>periods = c(1,5,22)</code>, which corresponds to one day, one week and one month respectively. This default is in line with Andersen et al. (2007).</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_periodsj">periodsJ</code></td>
<td>
<p>a vector of integers indicating over what time periods the jump components in the model should be aggregated. 
By default <code>periodsJ = c(1,5,22)</code>, which corresponds to one day, one week and one month respectively.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_periodsq">periodsQ</code></td>
<td>
<p>a vector of integers indicating over what time periods the realized quarticity in the model should be aggregated. 
By default <code>periodsQ = c(1,5,22)</code>, which corresponds to one day, one week and one month respectively.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_leverage">leverage</code></td>
<td>
<p>a vector of integers indicating over what periods the negative returns should be aggregated.
See Corsi and Reno (2012) for more information. By default <code>leverage = NULL</code> and the model assumes the absence of a  leverage effect. 
Set <code>leverage = c(1,5,22)</code> to mimic the analysis in Corsi and Reno (2012).</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_rvest">RVest</code></td>
<td>
<p>a character vector with one, two, or three elements. The first element always refers to the name of the function to estimate the daily integrated variance (non-jump-robust).
The second and third element depends on which type of model is estimated: 
If <code>type = "HARJ"</code>, <code>type = "HARCJ"</code>, <code>type = "HARQJ"</code> the second element refers to the name of the function to estimate the continuous component of daily volatility (jump robust). 
If type = &quot;HARQ&quot;, the second element refers to the name of the function used to estimate the integrated quarticity.
If <code>type = "HARQJ"</code> the third element always refers to the name of the function used to estimate integrated quarticity.
By default <code>RVest = c("rCov","rBPCov","rQuar")</code>, i.e. using the realized volatility, realized bi-power variance, and realized quarticity.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_type">type</code></td>
<td>
<p>a string referring to the type of HAR model you would like to estimate. By default <code>type = "HAR"</code>, the most basic model. 
Other valid options for type are <code>"HARJ"</code>, <code>"HARCJ"</code>, <code>"HARQ"</code>, <code>"HARQJ"</code>, <code>"CHAR"</code>, or <code>"CHARQ"</code>.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_inputtype">inputType</code></td>
<td>
<p>a string denoting if the input data consists of realized measures or high-frequency returns. 
Default &quot;RM&quot; is the only way to denote realized measures and everything else denotes returns.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_jumptest">jumpTest</code></td>
<td>
<p>the function name of a function used to test whether the test statistic which determines whether the jump variability is significant that day. By default <code>jumpTest = "ABDJumptest"</code>, hence using the test statistic in Equation or Equation (18) of Andersen et al. (2007).
It is also possible to provide pre-computed test statistics for jump tests by setting <code>jumpTest</code> to <code>"testStat"</code>. These test statistics should still be passed as the third column.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_alpha">alpha</code></td>
<td>
<p>a real indicating the confidence level used in testing for jumps. By default <code>alpha = 0.05</code>.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_h">h</code></td>
<td>
<p>an integer indicating the number over how many days the dependent variable should be aggregated.
By default, <code>h = 1</code>, i.e. no aggregation takes place, you just model the daily realized volatility.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_transform">transform</code></td>
<td>
<p>optionally a string referring to a function that transforms both the dependent and explanatory variables in the model. By default <code>transform = NULL</code>, so no transformation is done. Typical other choices in this context would be &quot;log&quot; or &quot;sqrt&quot;.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_externalregressor">externalRegressor</code></td>
<td>
<p>an <code>xts</code> object of same number of rows as <code>data</code>, and one column. This is used as an external regressor. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_periodsexternal">periodsExternal</code></td>
<td>
<p>a vector of integers indicating over how days <code>externalRegressor</code> should be aggregated.</p>
</td></tr>
<tr><td><code id="HARmodel_+3A_...">...</code></td>
<td>
<p>extra arguments for jump test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic specification in Corsi (2009) is as follows. 
Let <code class="reqn">RV_{t}</code> be the realized variances at day <code class="reqn">t</code> and <code class="reqn">RV_{t-k:t}</code> the average
realized variance in between <code class="reqn">t-k</code> and <code class="reqn">t</code>, <code class="reqn">k \geq 0</code>. 
</p>
<p>The dynamics of the model are given by
</p>
<p style="text-align: center;"><code class="reqn">
RV_{t+1} = \beta_0 + \beta_1 \ RV_{t} + \beta_2 \ RV_{t-4:t} + \beta_3 \ RV_{t-21:t} + \varepsilon_{t+1}
</code>
</p>

<p>which is estimated by ordinary least squares under the assumption that at time <code class="reqn">t</code>,
the conditional mean of  <code class="reqn">\varepsilon_{t+1}</code> is equal to zero.
</p>
<p>For other specifications, please refer to the cited papers.
</p>
<p>The standard errors reporting in the <code>print</code> and <code>summary</code> methods are Newey-West standard errors calculated with the <span class="pkg">sandwich</span> package.
</p>


<h3>Value</h3>

<p>The function outputs an object of class <code>HARmodel</code> and <code><a href="stats.html#topic+lm">lm</a></code> (so <code>HARmodel</code> is  a subclass of <code><a href="stats.html#topic+lm">lm</a></code>). Objects
of class <code>HARmodel</code> has the following methods <code><a href="#topic+plot.HARmodel">plot.HARmodel</a></code>, <code><a href="#topic+predict.HARmodel">predict.HARmodel</a></code>, <code><a href="#topic+print.HARmodel">print.HARmodel</a></code>, and <code><a href="#topic+summary.HARmodel">summary.HARmodel</a></code>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Bollerslev, T., and Diebold, F. (2007). Roughing it up: Including jump components in the measurement, modelling and forecasting of return volatility. <em>The Review of Economics and Statistics</em>, 89, 701-720.
</p>
<p>Corsi, F. (2009). A simple approximate long memory model of realized volatility. <em>Journal of Financial Econometrics</em>, 7, 174-196.
</p>
<p>Corsi, F. and Reno R. (2012). Discrete-time volatility forecasting with persistent leverage effect and the link with continuous-time volatility modeling. <em>Journal of Business &amp; Economic Statistics</em>, 30, 368-380.
</p>
<p>Bollerslev, T., Patton, A., and Quaedvlieg, R. (2016).  Exploiting the errors: A simple approach for improved volatility forecasting, <em>Journal of Econometrics</em>, 192, 1-18.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Example 1: HAR
# Forecasting daily Realized volatility for the S&amp;P 500 using the basic HARmodel: HAR
library(xts)
RVSPY &lt;- as.xts(SPYRM$RV5, order.by = SPYRM$DT)

x &lt;- HARmodel(data = RVSPY , periods = c(1,5,22), RVest = c("rCov"),
              type = "HAR", h = 1, transform = NULL, inputType = "RM")
class(x)
x
summary(x)
plot(x)
predict(x)


# Example 2: HARQ
# Get the highfrequency returns
dat &lt;- as.xts(sampleOneMinuteData[, makeReturns(STOCK), by = list(DATE = as.Date(DT))])
x &lt;- HARmodel(dat, periods = c(1,5,10), periodsJ = c(1,5,10),
            periodsQ = c(1), RVest = c("rCov", "rQuar"),
              type="HARQ", inputType = "returns")
# Estimate the HAR model of type HARQ
class(x)
x
# plot(x)
# predict(x)

# Example 3: HARQJ with already computed realized measures
dat &lt;- SPYRM[, list(DT, RV5, BPV5, RQ5)]
x &lt;- HARmodel(as.xts(dat), periods = c(1,5,22), periodsJ = c(1),
              periodsQ = c(1), type = "HARQJ")
# Estimate the HAR model of type HARQJ
class(x)
x
# plot(x)
predict(x)

# Example 4: CHAR with already computed realized measures
dat &lt;- SPYRM[, list(DT, RV5, BPV5)]

x &lt;- HARmodel(as.xts(dat), periods = c(1, 5, 22), type = "CHAR")
# Estimate the HAR model of type CHAR
class(x)
x
# plot(x)
predict(x)

# Example 5: CHARQ with already computed realized measures
dat &lt;- SPYRM[, list(DT, RV5, BPV5, RQ5)]

x &lt;- HARmodel(as.xts(dat), periods = c(1,5,22), periodsQ = c(1), type = "CHARQ")
# Estimate the HAR model of type CHARQ
class(x)
x
# plot(x)
predict(x)

# Example 6: HARCJ with pre-computed test-statistics
## BNSJumptest manually calculated.
testStats &lt;- sqrt(390) * (SPYRM$RV1 - SPYRM$BPV1)/sqrt((pi^2/4+pi-3 - 2) * SPYRM$medRQ1)
model &lt;- HARmodel(cbind(as.xts(SPYRM[, list(DT, RV5, BPV5)]), testStats), type = "HARCJ")


</code></pre>

<hr>
<h2 id='HEAVYmodel'>HEAVY model estimation</h2><span id='topic+HEAVYmodel'></span>

<h3>Description</h3>

<p>This function calculates the High frEquency bAsed VolatilitY (HEAVY) model proposed in Shephard and Sheppard (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HEAVYmodel(data, startingValues = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HEAVYmodel_+3A_data">data</code></td>
<td>
<p>an <code>xts</code> object where the first column is a vector of returns 
and the second column is a vector of realized stock market variation</p>
</td></tr>
<tr><td><code id="HEAVYmodel_+3A_startingvalues">startingValues</code></td>
<td>
<p>a vector of alternative starting values: first three arguments for variance equation and last three arguments for measurement equation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">r_{t}</code> and <code class="reqn">RM_{t}</code> be series of demeaned returns and realized measures of
daily stock price variation. The HEAVY model is a two-component model.
We assume <code class="reqn">r_{t} = h_{t}^{1/2} Z_{t}</code> where <code class="reqn">Z_t</code> is an i.i.d. zero-mean 
and unit-variance innovation term. The dynamics of the HEAVY model are given by
</p>
<p style="text-align: center;"><code class="reqn">
   h_{t} = \omega + \alpha RM_{t-1} + \beta h_{t-1}
 </code>
</p>

<p>and 
</p>
<p style="text-align: center;"><code class="reqn">
   \mu_{t} = \omega_{R} + \alpha_{R} RM_{t-1} + \beta_{R} \mu_{t-1}.
 </code>
</p>

<p>The two equations are estimated separately as mentioned in Shephard and Sheppard (2010).
We report robust standard errors based on the matrix-product of inverted Hessians and
the outer product of gradients.
</p>
<p>Note that we always demean the returns in the data input as we don't include a constant in the mean equation.
</p>


<h3>Value</h3>

<p>The function outputs an object of class <code>HEAVYmodel</code>, a list containing
</p>

<ul>
<li><p> coefficients = estimated coefficients.
</p>
</li>
<li><p> se = robust standard errors based on inverted Hessian matrix.
</p>
</li>
<li><p> residuals = the residuals in the return equation.
</p>
</li>
<li><p> llh = the two-component log-likelihood values.
</p>
</li>
<li><p> varCondVariances = conditional variances in the variance equation.
</p>
</li>
<li><p> RMCondVariances = conditional variances in the RM equation.
</p>
</li>
<li><p> data = the input data.
</p>
</li></ul>

<p>The class HEAVYmodel has the following methods: plot.HEAVYmodel, predict.HEAVYmodel, 
print.HEAVYmodel, and summary.HEAVYmodel.
</p>


<h3>Author(s)</h3>

<p>Onno Kleen and Emil Sjorup.
</p>


<h3>References</h3>

<p>Shephard, N. and Sheppard, K. (2010). Realising the future: Forecasting with high frequency based volatility (HEAVY) models. Journal of Applied Econometrics 25, 197&ndash;231.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.HEAVYmodel">predict.HEAVYmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Calculate returns in percentages
logReturns &lt;- 100 * makeReturns(SPYRM$CLOSE)[-1]

# Combine both returns and realized measures into one xts
# Due to return calculation, the first observation is missing
dataSPY &lt;- xts::xts(cbind(logReturns, SPYRM$BPV5[-1] * 10000), order.by = SPYRM$DT[-1])

# Fit the HEAVY model
fittedHEAVY &lt;- HEAVYmodel(dataSPY)

# Examine the estimated coefficients and robust standard errors
fittedHEAVY

# Calculate iterative multi-step-ahead forecasts
predict(fittedHEAVY, stepsAhead = 12)

</code></pre>

<hr>
<h2 id='highfrequency-package'>highfrequency: Tools for Highfrequency Data Analysis</h2><span id='topic+highfrequency'></span><span id='topic+highfrequency-package'></span>

<h3>Description</h3>

<p>The <span class="pkg">highfrequency</span> package provides numerous tools for analyzing high-frequency financial data, including functionality to:
</p>

<ul>
<li><p> Clean, handle, and manage high frequency trades and quotes data.
</p>
</li>
<li><p> Calculate liquidity measures
</p>
</li>
<li><p> Calculate (multivariate) realized measures of the distribution of high-frequency returns
</p>
</li>
<li><p> Estimate models for realized measures of volatility and the corresponding forecasts
</p>
</li>
<li><p> Detect jumps in prices
</p>
</li>
<li><p> Analyze market microstructure noise in asset prices
</p>
</li>
<li><p> Estimate spot volatility and drift as well as analyze intraday periodicity of spot volatility
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Kris Boudt, Jonathan Cornelissen, Onno Kleen, Scott Payseur, Emil Sjoerup
Maintainer: Kris Boudt &lt;Kris.Boudt@ugent.be&gt;
</p>
<p>Contributors: Giang Nguyen
</p>
<p>Thanks: We would like to thank Brian Peterson, Chris Blakely, Dirk Eddelbuettel, Maarten Schermer, and Eric Zivot
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/jonathancornelissen/highfrequency">https://github.com/jonathancornelissen/highfrequency</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/jonathancornelissen/highfrequency/issues">https://github.com/jonathancornelissen/highfrequency/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ICov'>Estimators of the integrated covariance</h2><span id='topic+ICov'></span>

<h3>Description</h3>

<p>This documentation page functions as a point of reference to quickly look up the estimators of the integrated covariance provided in the <span class="pkg">highfrequency</span> package.
</p>
<p>The implemented estimators are:
</p>
<p>Realized covariance <code><a href="#topic+rCov">rCov</a></code>
</p>
<p>Realized bipower covariance <code><a href="#topic+rBPCov">rBPCov</a></code>
</p>
<p>Hayashi-Yoshida realized covariance <code><a href="#topic+rHYCov">rHYCov</a></code>
</p>
<p>Realized kernel covariance <code><a href="#topic+rKernelCov">rKernelCov</a></code>
</p>
<p>Realized outlyingness-weighted covariance <code><a href="#topic+rOWCov">rOWCov</a></code>
</p>
<p>Realized threshold covariance <code><a href="#topic+rThresholdCov">rThresholdCov</a></code>
</p>
<p>Realized two-scale covariance <code><a href="#topic+rTSCov">rTSCov</a></code>
</p>
<p>Robust realized two-scale covariance <code><a href="#topic+rRTSCov">rRTSCov</a></code>
</p>
<p>Subsampled realized covariance <code><a href="#topic+rAVGCov">rAVGCov</a></code>
</p>
<p>Realized semi-covariance <code><a href="#topic+rSemiCov">rSemiCov</a></code>
</p>
<p>Modulated Realized covariance <code><a href="#topic+rMRCov">rMRCov</a></code>
</p>
<p>Realized Cholesky covariance <code><a href="#topic+rCholCov">rCholCov</a></code>
</p>
<p>Beta-adjusted realized covariance <code><a href="#topic+rBACov">rBACov</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>

<hr>
<h2 id='intradayJumpTest'>Intraday jump tests</h2><span id='topic+intradayJumpTest'></span>

<h3>Description</h3>

<p>This function can be used to  test for jumps in intraday price paths.
</p>
<p>The tests are of the form <code class="reqn">L(t) = (R(t) - mu(t))/sigma(t)</code>.
</p>
<p>See <code><a href="#topic+spotVol">spotVol</a></code> and <code><a href="#topic+spotDrift">spotDrift</a></code> for Estimators for <code class="reqn">\sigma(t)</code> and <code class="reqn">\mu(t)</code>, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intradayJumpTest(
  pData,
  volEstimator = "RM",
  driftEstimator = "none",
  alpha = 0.95,
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL,
  n = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intradayJumpTest_+3A_pdata">pData</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> of the price data in levels. This data can (and should in some cases) be tick-level data. The data can span more than one day. Should only contain a sinlge <code>SYMBOL</code></p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_volestimator">volEstimator</code></td>
<td>
<p>character denoting which volatility estimator to use for the tests. See <code><a href="#topic+spotVol">spotVol</a></code>. Default = <code>"RM"</code> denoting realized measures.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_driftestimator">driftEstimator</code></td>
<td>
<p>character denoting which drift estimator to use for the tests. See <code><a href="#topic+spotDrift">spotDrift</a></code>. Default = <code>"none"</code> denoting no drift estimation.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_alpha">alpha</code></td>
<td>
<p>numeric of length one determining what confidence level to use when constructing the critical values.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to &quot;minutes&quot;.
<code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time. This should be in the time zone
specified by <code>tz</code>. By default, <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time. This should be in the time zone
specified by <code>tz</code>. By default, <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_n">n</code></td>
<td>
<p>number of observation to use in the calculation of the critical values of the test statistic. If this is left as <code>NULL</code> we fall back to the total number of observations in the sample.</p>
</td></tr>
<tr><td><code id="intradayJumpTest_+3A_...">...</code></td>
<td>
<p>extra arguments passed on to <code><a href="#topic+spotVol">spotVol</a></code> for the volatility estimation, and to <code><a href="#topic+spotDrift">spotDrift</a></code>.
</p>
<p>The null hypothesis of the tests in this function is that there are no jumps in the price series</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>References</h3>

<p>Christensen, K., Oomen, R. C. A., Podolskij, M. (2014): Fact or Friction: Jumps at ultra high frequency. <em>Journal of Financial Economics</em>, 144, 576-599
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# We can easily make a Lee-Mykland jump test.
LMtest &lt;- intradayJumpTest(pData = sampleTData[, list(DT, PRICE)], 
                           volEstimator = "RM", driftEstimator = "none",
                           RM = "rBPCov", lookBackPeriod = 20,
                           alignBy = "minutes", alignPeriod = 5, marketOpen = "09:30:00", 
                           marketClose = "16:00:00")
plot(LMtest)

# We can just as easily use the pre-averaged version from the "Fact or Friction" paper
FoFtest &lt;- intradayJumpTest(pData = sampleTData[, list(DT, PRICE)], 
                            volEstimator = "PARM", driftEstimator = "none",
                            RM = "rBPCov", lookBackPeriod = 20, theta = 1.2,
                            marketOpen = "09:30:00", marketClose = "16:00:00")
plot(FoFtest)


## End(Not run)
</code></pre>

<hr>
<h2 id='IVar'>Estimators of the integrated variance</h2><span id='topic+IVar'></span>

<h3>Description</h3>

<p>This documentation page functions as a point of reference to quickly look up the estimators of the integrated variance provided in the <span class="pkg">highfrequency</span> package.
</p>
<p>The implemented estimators are:
Realized Variance <code><a href="#topic+rRVar">rRVar</a></code>
</p>
<p>Median realized variance <code><a href="#topic+rMedRVar">rMedRVar</a></code>
</p>
<p>Minimum realized variance <code><a href="#topic+rMinRVar">rMinRVar</a></code>
</p>
<p>Realized quadpower variance <code><a href="#topic+rQPVar">rQPVar</a></code>
</p>
<p>Realized multipower variance <code><a href="#topic+rMPVar">rMPVar</a></code>
</p>
<p>Realized semivariance <code><a href="#topic+rSVar">rSVar</a></code> 
</p>
<p>Note that almost all estimators in the list in <code><a href="#topic+ICov">ICov</a></code> also work yield estimates of the integrated variance on the diagonals.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>

<hr>
<h2 id='IVinference'>Function returns the value, the standard error and the confidence band of the integrated variance (IV) estimator.</h2><span id='topic+IVinference'></span>

<h3>Description</h3>

<p>This function supplies information about standard error and confidence band of integrated variance (IV) estimators under Brownian semimartingales model such as: bipower variation, rMinRV, rMedRV. 
Depending on users' choices of estimator (integrated variance (IVestimator), integrated quarticity (IQestimator)) and confidence level, the function returns the result.(Barndorff (2002))
Function returns three outcomes: 1.value of IV estimator 2.standard error of IV estimator and 3.confidence band of IV estimator. 
</p>
<p>Assume there is <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>.
</p>
<p>Then the IVinference is given by: 
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{standard error}= \frac{1}{\sqrt{N}} *sd
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mbox{confidence band}= \hat{IV} \pm cv*se
</code>
</p>

<p>in which,
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{sd}= \sqrt{\theta \times \hat{IQ}} 
</code>
</p>

<p><code class="reqn">cv:</code> critical value. 
</p>
<p><code class="reqn">se:</code> standard error.
</p>
<p><code class="reqn">\theta:</code> depending on IQestimator, <code class="reqn">\theta</code> can take different value (Andersen et al. (2012)). 
</p>
<p><code class="reqn">\hat{IQ}</code> integrated quarticity estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IVinference(
  rData,
  IVestimator = "RV",
  IQestimator = "rQuar",
  confidence = 0.95,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IVinference_+3A_rdata">rData</code></td>
<td>
<p><code>xts</code> object containing all returns in period t for one asset.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_ivestimator">IVestimator</code></td>
<td>
<p>can be chosen among integrated variance estimators: RV, BV, rMinRV or rMedRV. RV by default.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_iqestimator">IQestimator</code></td>
<td>
<p>can be chosen among integrated quarticity estimators: rQuar, realized tri-power quarticity (TPQ), quad-power quarticity (QPQ), rMinRQuar or rMedRQuar. TPQ by default.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_confidence">confidence</code></td>
<td>
<p>confidence level set by users. 0.95 by default.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="IVinference_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="IVinference_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The theoretical framework is the logarithmic price process <code class="reqn">X_t</code> belongs to the class of Brownian semimartingales, which can be written as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{X}_{t}=  \int_{0}^{t} a_udu + \int_{0}^{t}\sigma_{u}dW_{u}
</code>
</p>

<p>where <code class="reqn">a</code> is the drift term, <code class="reqn">\sigma</code> denotes the spot vivInferenceolatility process, <code class="reqn">W</code> is a standard Brownian motion (assume that there are no jumps).
</p>


<h3>Value</h3>

<p>list
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen and Kris Boudt
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>
<p>Barndorff-Nielsen, O. E. (2002). Econometric analysis of realized volatility and its use in estimating stochastic volatility models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 64, 253-280.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("xts") # This function only accepts xts data currently
ivInf &lt;- IVinference(as.xts(sampleTData[, list(DT, PRICE)]), IVestimator= "rMinRV",
                     IQestimator = "rMedRQ", confidence = 0.95, makeReturns = TRUE)
ivInf

## End(Not run)
            
</code></pre>

<hr>
<h2 id='JOjumpTest'>Jiang and Oomen (2008) tests for the presence of jumps in the price series.</h2><span id='topic+JOjumpTest'></span>

<h3>Description</h3>

<p>This test examines the jump in highfrequency data. It is based on theory of Jiang and Oomen (JO). They found that the difference of simple return and logarithmic return can capture one half of integrated variance if there is no jump in the underlying sample path. The null hypothesis is no jumps.
</p>
<p>Function returns three outcomes: 1.z-test value 2.critical value under confidence level of <code class="reqn">95\%</code> and 3.p-value.  
</p>
<p>Assume there is <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>.
</p>
<p>Let <code class="reqn">r_{t,i}</code> be a logarithmic return (with <code class="reqn">i=1, \ldots,N</code>) in period <code class="reqn">t</code>.
</p>
<p>Let <code class="reqn">R_{t,i}</code> be a simple return (with <code class="reqn">i=1, \ldots,N</code>) in period <code class="reqn">t</code>.
</p>
<p>Then the JOjumpTest is given by: 
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{JOjumpTest}_{t,N}= \frac{N BV_{t}}{\sqrt{\Omega_{SwV}} \left(1-\frac{RV_{t}}{SwV_{t}} \right)}
   </code>
</p>

<p>in which, 
<code class="reqn">BV</code>: bipower variance;
<code class="reqn">RV</code>: realized variance (defined by Andersen et al. (2012));
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{SwV}_{t}=2 \sum_{i=1}^{N}(R_{t,i}-r_{t,i})
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \Omega_{SwV}= \frac{\mu_6}{9} \frac{{N^3}{\mu_{6/p}^{-p}}}{N-p-1} \sum_{i=0}^{N-p}\prod_{k=1}^{p}|r_{t,i+k}|^{6/p}
 </code>
</p>
 
<p style="text-align: center;"><code class="reqn">
   \mu_{p}= \mbox{E}[|\mbox{U}|^{p}] = 2^{p/2} \frac{\Gamma(1/2(p+1))}{\Gamma(1/2)}
   % \mbox{E}[|\mbox{U}|^p]=
 </code>
</p>


<p><code class="reqn">U</code>: independent standard normal random variables
</p>
<p>p: parameter (power).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JOjumpTest(
  pData,
  power = 4,
  alignBy = NULL,
  alignPeriod = NULL,
  alpha = 0.975,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="JOjumpTest_+3A_pdata">pData</code></td>
<td>
<p>a zoo/xts object containing all prices in period t for one asset.</p>
</td></tr>
<tr><td><code id="JOjumpTest_+3A_power">power</code></td>
<td>
<p>can be chosen among 4 or 6. 4 by default.</p>
</td></tr>
<tr><td><code id="JOjumpTest_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="JOjumpTest_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to &quot;minutes&quot;.</p>
</td></tr>
<tr><td><code id="JOjumpTest_+3A_alpha">alpha</code></td>
<td>
<p>numeric of length one with the significance level to use for the jump test(s). Defaults to 0.975.</p>
</td></tr>
<tr><td><code id="JOjumpTest_+3A_...">...</code></td>
<td>
<p>Used internally, do not set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The theoretical framework underlying jump test is that the logarithmic price process <code class="reqn">X_t</code> belongs to the class of Brownian semimartingales, which can be written as:
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{X}_{t}=  \int_{0}^{t} a_udu + \int_{0}^{t}\sigma_{u}dW_{u} + Z_t
   </code>
</p>

<p>where <code class="reqn">a</code> is the drift term, <code class="reqn">\sigma</code> denotes the spot volatility process, <code class="reqn">W</code> is a standard Brownian motion and <code class="reqn">Z</code> is a jump process defined by:
</p>
<p style="text-align: center;"><code class="reqn">
     \mbox{Z}_{t}=  \sum_{j=1}^{N_t}k_j
   </code>
</p>

<p>where <code class="reqn">k_j</code> are nonzero random variables. The counting process can be either finite or infinite for finite or infinite activity jumps.
</p>
<p>The the Jiang and Ooment test is that in the absence of jumps, the accumulated difference between the simple returns and log returns captures half of the integrated variance. (Theodosiou and Zikes, 2009).
If this difference is too great, the null hypothesis of no jumps is rejected.
</p>


<h3>Value</h3>

<p>list
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75- 93.
</p>
<p>Jiang, J. G., and Oomen, R. C. A (2008). Testing for jumps when asset prices are observed with noise- a &quot;swap variance&quot; approach. <em>Journal of Econometrics</em>, 144, 352-370.
</p>
<p>Theodosiou, M., Zikes, F. (2009). A comprehensive comparison of alternative tests for jumps in asset prices. Unpublished manuscript, Graduate School of Business, Imperial College London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>joDT &lt;- JOjumpTest(sampleTData[, list(DT, PRICE)])
</code></pre>

<hr>
<h2 id='knChooseReMeDI'>ReMeDI tuning parameter</h2><span id='topic+knChooseReMeDI'></span>

<h3>Description</h3>

<p>Function to choose the tuning parameter, kn in ReMeDI estimation. 
The optimal parameter <code>kn</code> is the smallest value that where the criterion:
</p>
<p style="text-align: center;"><code class="reqn">
    SqErr(k_{n})^{n}_{t} = \left(\hat{R}^{n,k_{n}}_{t,0} - \hat{R}^{n,k_{n}}_{t,1} - \hat{R}^{n,k_{n}}_{t,2} + \hat{R}^{n,k_{n}}_{t,3} - \hat{R}^{n, k_{n}}_{t,l}\right)^{2}
</code>
</p>

<p>is perceived to be zero. The tuning parameter <code>tol</code> can be set to choose the tolerance of the perception of 'close to zero', a higher tolerance will lead to a higher optimal value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knChooseReMeDI(
  pData,
  knMax = 10,
  tol = 0.05,
  size = 3,
  lower = 2,
  upper = 5,
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knChooseReMeDI_+3A_pdata">pData</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing the log-prices of the asset.</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_knmax">knMax</code></td>
<td>
<p>max value of <code>kn</code> to be considered.</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_tol">tol</code></td>
<td>
<p>tolerance for the minimizing value. If <code>tol</code> is high, the algorithm will choose a lower optimal value.</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_size">size</code></td>
<td>
<p>size of the local window.</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_lower">lower</code></td>
<td>
<p>lower boundary for the method if it fails to find an optimal value. If this is the case, the best kn between lower and upper is returned</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_upper">upper</code></td>
<td>
<p>upper boundary for the method if it fails to find an optimal value. If this is the case, the best kn between lower and upper is returned</p>
</td></tr>
<tr><td><code id="knChooseReMeDI_+3A_plot">plot</code></td>
<td>
<p>logical whether to plot the errors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the algorithm B.2 in the appendix of the Li and Linton (2019) working paper.
</p>


<h3>Value</h3>

<p>integer containing the optimal kn
</p>


<h3>Note</h3>

<p>We Thank Merrick Li for contributing his Matlab code for this estimator.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>References</h3>

<p>Li, M. and Linton, O. (2019). A ReMeDI for microstructure noise. Cambridge Working Papers in Economics 1908.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
optimalKn &lt;- knChooseReMeDI(sampleTData[as.Date(DT) == "2018-01-02",],
                            knMax = 10, tol = 0.05, size = 3,
                            lower = 2, upper = 5, plot = TRUE)
optimalKn
## Not run: 
# We can also have a much larger search-space
optimalKn &lt;- knChooseReMeDI(sampleTDataEurope,
                            knMax = 50, tol = 0.05,
                            size = 3, lower = 2, upper = 5, plot = TRUE)
optimalKn

## End(Not run)

</code></pre>

<hr>
<h2 id='leadLag'>Lead-Lag estimation</h2><span id='topic+leadLag'></span>

<h3>Description</h3>

<p>Function that estimates whether one series leads (or lags) another.
</p>
<p>Let <code class="reqn">X_{t}</code> and <code class="reqn">Y_{t}</code> be two observed price over the time interval <code class="reqn">[0,1]</code>.
</p>
<p>For every integer <code class="reqn">k \in \cal{Z}</code>, we form the shifted time series
</p>
<p style="text-align: center;"><code class="reqn">
   Y_{\left(k+i\right)/n},  \quad i = 1, 2, \dots
</code>
</p>

<p><code class="reqn">H=\left(\underline{H},\overline{H}\right]</code> is an interval for <code class="reqn">\vartheta\in\Theta</code>, define the shift interval <code class="reqn">H_{\vartheta}=H+\vartheta=\left(\underline{H}+\vartheta,\overline{H}+\vartheta\right]</code> then let
</p>
<p style="text-align: center;"><code class="reqn">
    X\left(H\right)_{t}=\int_{0}^{t}1_{H}\left(s\right)\textrm{d}X_{s}
</code>
</p>

<p>Which will be abbreviated:
</p>
<p style="text-align: center;"><code class="reqn">
    X\left(H\right)=X\left(H\right)_{T+\delta}=\int_{0}^{T+\delta}1_{H}\left(s\right)\textrm{d}X_{s}
</code>
</p>

<p>Then the shifted HY contrast function is:
</p>
<p style="text-align: center;"><code class="reqn">
    \tilde{\vartheta}\rightarrow U^{n}\left(\tilde{\vartheta}\right)= \\
    1_{\tilde{\vartheta}\geq0}\sum_{I\in{\cal{I}},J\in{\cal{J}},\overline{I}\leq T}X\left(I\right)Y\left(J\right)1_{\left\{ I\cap J_{-\tilde{\vartheta}}\neq\emptyset\right\}} \\
    +1_{\tilde{\vartheta}&lt;0}\sum_{I\in{\cal{I}},J\in{\cal{J}},\overline{J}\leq T}X\left(I\right)Y\left(Y\right)1_{\left\{ J\cap I_{\tilde{\vartheta}}\neq\emptyset\right\} }
</code>
</p>

<p>This contrast function is then calculated for all the lags passed in the argument <code>lags</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leadLag(
  price1 = NULL,
  price2 = NULL,
  lags = NULL,
  resolution = "seconds",
  normalize = TRUE,
  parallelize = FALSE,
  nCores = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leadLag_+3A_price1">price1</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing prices in levels, in case of data.table,
use a column DT to denote the date-time in POSIXct format, and a column PRICE to denote the price</p>
</td></tr>
<tr><td><code id="leadLag_+3A_price2">price2</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing prices in levels, in case of data.table,
use a column DT to denote the date-time in POSIXct format, and a column PRICE to denote the price</p>
</td></tr>
<tr><td><code id="leadLag_+3A_lags">lags</code></td>
<td>
<p>a numeric denoting which lags (in units of <code>resolution</code>) should be tested as leading or lagging</p>
</td></tr>
<tr><td><code id="leadLag_+3A_resolution">resolution</code></td>
<td>
<p>the resolution at which the lags is measured. 
The default is &quot;seconds&quot;, use this argument to gain 1000 times resolution by setting it to either &quot;ms&quot;, &quot;milliseconds&quot;, or &quot;milli-seconds&quot;.</p>
</td></tr>
<tr><td><code id="leadLag_+3A_normalize">normalize</code></td>
<td>
<p>logical denoting whether the contrasts should be normalized by the product of the L2 norms of both the prices. Default = TRUE. 
This does not change the value of the lead-lag-ratio.</p>
</td></tr>
<tr><td><code id="leadLag_+3A_parallelize">parallelize</code></td>
<td>
<p>logical denoting whether to use a parallelized version of the C++ code (parallelized using OPENMP). Default = FALSE</p>
</td></tr>
<tr><td><code id="leadLag_+3A_ncores">nCores</code></td>
<td>
<p>integer valued numeric denoting how many cores to use for the lead-lag estimation procedure in case parallelize is TRUE. 
Default is NA, which does not parallelize the code.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The lead-lag-ratio (LLR) can be used to see if one asset leads the other. If LLR &lt; 1, then price1 MAY be leading price2 and vice versa if LLR &gt; 1.
</p>


<h3>Value</h3>

<p>A list with class <code>leadLag</code> which contains <code>contrasts</code>, <code>lead-lag-ratio</code>, and <code>lags</code>, denoting the estimated values for each lag calculated,
the lead-lag-ratio, and the tested lags respectively.
</p>


<h3>References</h3>

<p>Hoffmann, M., Rosenbaum, M., and Yoshida, N. (2013). Estimation of the lead-lag parameter from non-synchronous data. <em>Bernoulli</em>, 19, 1-37.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Toy example to show the usage
# Spread prices
spread &lt;- spreadPrices(sampleMultiTradeData[SYMBOL %in% c("ETF", "AAA")])
# Use lead-lag estimator
llEmpirical &lt;- leadLag(spread[!is.na(AAA), list(DT, PRICE = AAA)], 
                       spread[!is.na(ETF), list(DT, PRICE = ETF)], seq(-15,15))
plot(llEmpirical)

## End(Not run)

</code></pre>

<hr>
<h2 id='listAvailableKernels'>Available kernels</h2><span id='topic+listAvailableKernels'></span>

<h3>Description</h3>

<p>Returns a vector of the available kernels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listAvailableKernels()
</code></pre>


<h3>Details</h3>

<p>The available kernels are: 
</p>

<ul>
<li><p> Rectangular: <code class="reqn">K(x) = 1</code>.
</p>
</li>
<li><p> Bartlett: <code class="reqn">K(x) = 1 - x</code>.
</p>
</li>
<li><p> Second-order: <code class="reqn">K(x) = 1 - 2x - x^2</code>.
</p>
</li>
<li><p> Epanechnikov: <code class="reqn">K(x) = 1 - x^2</code>.
</p>
</li>
<li><p> Cubic: <code class="reqn">K(x) = 1 - 3 x^2 + 2 x^3</code>.
</p>
</li>
<li><p> Fifth: <code class="reqn">K(x) = 1 - 10 x^3 + 15 x^4 - 6 x^5</code>.
</p>
</li>
<li><p> Sixth: <code class="reqn">K(x) = 1 - 15 x^4 + 24 x^5 - 10 x^6</code>
</p>
</li>
<li><p> Seventh: <code class="reqn">K(x) = 1 - 21 x^5 + 35 x^6 - 15 x^7</code>.
</p>
</li>
<li><p> Eighth: <code class="reqn">K(x) = 1 - 28 x^6 + 48 x^7 - 21 x^8</code>.
</p>
</li>
<li><p> Parzen: <code class="reqn">K(x) = 1- 6 x^2 + 6 x^3</code> if <code class="reqn">k \leq 0.5</code> and <code class="reqn">K(x) = 2 (1-x)^3</code> if <code class="reqn">k &gt; 0.5</code>.
</p>
</li>
<li><p> TukeyHanning: <code class="reqn">K(x) = 1 + \sin(\pi/2 - \pi \cdot x))/2</code>.
</p>
</li>
<li><p> ModifiedTukeyHanning: <code class="reqn">K(x) = (1 - \sin(\pi/2 - \pi \ (1 - x)^2 ) / 2</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a character vector.
</p>


<h3>Author(s)</h3>

<p>Scott Payseur.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Hansen, P. R., Lunde, A., and Shephard, N. (2008). Designing realized kernels to measure the ex post variation of equity prices in the presence of noise. <em>Econometrica</em>, 76, 1481-1536.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>listAvailableKernels
</code></pre>

<hr>
<h2 id='listCholCovEstimators'>Utility function listing the available estimators for the CholCov estimation</h2><span id='topic+listCholCovEstimators'></span>

<h3>Description</h3>

<p>Utility function listing the available estimators for the CholCov estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listCholCovEstimators()
</code></pre>


<h3>Value</h3>

<p>This function returns a character vector containing the available estimators.
</p>

<hr>
<h2 id='makeOHLCV'>Make Open-High-Low-Close-Volume bars</h2><span id='topic+makeOHLCV'></span>

<h3>Description</h3>

<p>This function makes OHLC-V bars at arbitrary intervals. If the SIZE column is not present in the input, no volume column is created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeOHLCV(pData, alignBy = "minutes", alignPeriod = 5, tz = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeOHLCV_+3A_pdata">pData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object to make the bars out of, 
containing the intraday price series of possibly multiple stocks for possibly multiple days.</p>
</td></tr>
<tr><td><code id="makeOHLCV_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code>, and <code>"ticks"</code>.
To aggregate based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="makeOHLCV_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="makeOHLCV_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
With the non-disk functionality, we attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
minuteBars &lt;- makeOHLCV(sampleTDataEurope, alignBy = "minutes", alignPeriod = 1)
# We can use the quantmod package's chartSeries function to plot the ohlcv data
quantmod::chartSeries(minuteBars)

minuteBars &lt;- makeOHLCV(sampleTDataEurope[,], alignBy = "minutes", alignPeriod = 1)
# Again we plot the series with chartSeries
quantmod::chartSeries(minuteBars)

# We can also handle data across multiple days.
fiveMinuteBars &lt;- makeOHLCV(sampleTData)
# Again we plot the series with chartSeries
quantmod::chartSeries(fiveMinuteBars)

# We can use arbitrary alignPeriod, here we choose pi
bars &lt;- makeOHLCV(sampleTDataEurope, alignBy = "seconds", alignPeriod = pi)
# Again we plot the series with chartSeries
quantmod::chartSeries(bars)

## End(Not run)
</code></pre>

<hr>
<h2 id='makePsd'>Returns the positive semidefinite projection of a symmetric matrix using the eigenvalue method</h2><span id='topic+makePsd'></span>

<h3>Description</h3>

<p>Function returns the positive semidefinite projection of a symmetric matrix using the eigenvalue method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makePsd(S, method = "covariance")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makePsd_+3A_s">S</code></td>
<td>
<p>a non-PSD matrix.</p>
</td></tr>
<tr><td><code id="makePsd_+3A_method">method</code></td>
<td>
<p>character, indicating whether the negative eigenvalues of the correlation or covariance should be replaced by zero. Possible values are &quot;covariance&quot; and &quot;correlation&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We use the eigenvalue method to transform <code class="reqn">S</code> into a positive
semidefinite covariance matrix (see, e.g., Barndorff-Nielsen and Shephard, 2004, and Rousseeuw and Molenberghs, 1993).  Let <code class="reqn">\Gamma</code> be the
orthogonal matrix consisting of the <code class="reqn">p</code> eigenvectors of <code class="reqn">S</code>. Denote
<code class="reqn">\lambda_1^+,\ldots,\lambda_p^+</code> its <code class="reqn">p</code> eigenvalues, whereby the negative eigenvalues have been replaced by zeroes.
Under this approach, the positive semi-definite
projection of <code class="reqn">S</code> is <code class="reqn"> S^+ = \Gamma' \mbox{diag}(\lambda_1^+,\ldots,\lambda_p^+) \Gamma</code>. 
</p>
<p>If method = &quot;correlation&quot;, the eigenvalues of the correlation matrix corresponding to the matrix <code class="reqn">S</code> are 
transformed, see Fan et al. (2010).
</p>


<h3>Value</h3>

<p>A matrix containing the positive semi definite matrix.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E. and Shephard, N. (2004). Measuring the impact of jumps in multivariate price processes using bipower covariation. Discussion paper, Nuffield College, Oxford University.
</p>
<p>Fan, J., Li, Y., and Yu, K. (2012). Vast volatility matrix estimation using high frequency data for portfolio selection. <em>Journal of the American Statistical Association</em>, 107,  412-428
</p>
<p>Rousseeuw, P. and Molenberghs, G. (1993). Transformation of non positive semidefinite correlation matrices. <em>Communications in Statistics - Theory and Methods</em>, 22, 965-984.
</p>

<hr>
<h2 id='makeReturns'>Compute log returns</h2><span id='topic+makeReturns'></span>

<h3>Description</h3>

<p>Convenience function to calculate log-returns, also used extensively internally.
Accepts <code>xts</code> and <code>matrix</code>-like objects. If you use this with a <code>data.table</code> object, remember to not pass the <code>DT</code> column.
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{log return}_t =  (\log(\mbox{PRICE}_{t})-\log(\mbox{PRICE}_{t-1})).
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>makeReturns(ts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeReturns_+3A_ts">ts</code></td>
<td>
<p>a possibly multivariate matrix-like object containing prices in levels. If <code>ts</code> is an <code>xts</code> object, we return an <code>xts</code> object. Other types will result in a <code>matrix</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: the first (row of) observation(s) is set to zero.
</p>


<h3>Value</h3>

<p>Depending on input, either a <code>matrix</code> or an <code>xts</code> object containing the log returns.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup
</p>

<hr>
<h2 id='makeRMFormat'>DEPRECATED 
use <code><a href="#topic+spreadPrices">spreadPrices</a></code></h2><span id='topic+makeRMFormat'></span>

<h3>Description</h3>

<p>DEPRECATED 
use <code><a href="#topic+spreadPrices">spreadPrices</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeRMFormat(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeRMFormat_+3A_data">data</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
</table>

<hr>
<h2 id='matchTradesQuotes'>Match trade and quote data</h2><span id='topic+matchTradesQuotes'></span>

<h3>Description</h3>

<p>Match the trades and quotes of the input data. All trades are retained and the latest bids and offers are retained,
while 'old' quotes are discarded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matchTradesQuotes(
  tData,
  qData,
  lagQuotes = 0,
  BFM = FALSE,
  backwardsWindow = 3600,
  forwardsWindow = 0.5,
  plot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matchTradesQuotes_+3A_tdata">tData</code></td>
<td>
<p><code>data.table</code> or xts-object containing the trade data possibly with multiple symbols and over multiple days possible</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_qdata">qData</code></td>
<td>
<p><code>data.table</code> or xts-object containing the quote data possibly with multiple symbols and over multiple days possible</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_lagquotes">lagQuotes</code></td>
<td>
<p>numeric, number of seconds the quotes are registered faster than
the trades (should be round and positive). Default is 0. For older datasets, i.e. before 2010, it may be a good idea to set this to e.g. 2. See Vergote (2005)</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_bfm">BFM</code></td>
<td>
<p>a logical determining whether to conduct 'Backwards - Forwards matching' of trades and quotes.
The algorithm tries to match trades that fall outside the bid - ask and first tries to match a small window forwards and if this fails, it tries to match backwards in a bigger window.
The small window is a tolerance for inaccuracies in the timestamps of bids and asks. The backwards window allow for matching of late reported trades. I.e. block trades.</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_backwardswindow">backwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the backwards window used when <code>BFM = TRUE</code>. Default is 3600, corresponding to one hour.</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_forwardswindow">forwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the forwards window used when <code>BFM = TRUE</code>. Default is 0.5, corresponding to one half second.</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_plot">plot</code></td>
<td>
<p>a logical denoting whether to visualize the forwards, backwards, and unmatched trades in a plot.</p>
</td></tr>
<tr><td><code id="matchTradesQuotes_+3A_...">...</code></td>
<td>
<p>used internally. Don't set this parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the input data type, we return either a <code>data.table</code> or an <code>xts</code> object containing the matched trade and quote data.
When using the BFM algorithm, a report of the matched and unmatched trades are also returned (This is omitted when we call this function from the <code><a href="#topic+tradesCleanupUsingQuotes">tradesCleanupUsingQuotes</a></code> function).
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Vergote, O. (2005). How to match trades and quotes for NYSE stocks? K.U.Leuven working paper.
</p>
<p>Christensen, K., Oomen, R. C. A., Podolskij, M. (2014): Fact or Friction: Jumps at ultra high frequency. <em>Journal of Financial Economics</em>, 144, 576-599
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Multi-day input allowed
tqData &lt;- matchTradesQuotes(sampleTData, sampleQData)
# Show output
tqData
</code></pre>

<hr>
<h2 id='MDtest'># Difference of medians test
# See Fried (2012)
# Returns TRUE if H0 is rejected
# importFrom stats density
# keywords internal
DMtest &lt;- function(x, y, alpha = 0.005) 
m &lt;- length(x)
n &lt;- length(y)
xmed &lt;- median(x)
ymed &lt;- median(y)
xcor &lt;- x - xmed
ycor &lt;- y - ymed
delta1 &lt;- ymed - xmed
out &lt;- density(c(xcor, ycor), kernel = &quot;epanechnikov&quot;)
fmed &lt;- as.numeric(BMS::quantile.density(out, probs = 0.5))
fmedvalue &lt;- (out$y[max(which(out$x &lt; fmed))] +
out$y[max(which(out$x &lt; fmed))+1])/2
test &lt;- sqrt((m*n)/(m + n))*2*fmedvalue*delta1
return(abs(test) &gt; qnorm(1-alpha/2))
</h2><span id='topic+MDtest'></span>

<h3>Description</h3>

<p># Difference of medians test
# See Fried (2012)
# Returns TRUE if H0 is rejected
# importFrom stats density
# keywords internal
DMtest &lt;- function(x, y, alpha = 0.005) 
m &lt;- length(x)
n &lt;- length(y)
xmed &lt;- median(x)
ymed &lt;- median(y)
xcor &lt;- x - xmed
ycor &lt;- y - ymed
delta1 &lt;- ymed - xmed
out &lt;- density(c(xcor, ycor), kernel = &quot;epanechnikov&quot;)
fmed &lt;- as.numeric(BMS::quantile.density(out, probs = 0.5))
fmedvalue &lt;- (out$y[max(which(out$x &lt; fmed))] +
out$y[max(which(out$x &lt; fmed))+1])/2
test &lt;- sqrt((m*n)/(m + n))*2*fmedvalue*delta1
return(abs(test) &gt; qnorm(1-alpha/2))

</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDtest(x, y, alpha = 0.005, type = "MDa")
</code></pre>

<hr>
<h2 id='mergeQuotesSameTimestamp'>Merge multiple quote entries with the same time stamp</h2><span id='topic+mergeQuotesSameTimestamp'></span>

<h3>Description</h3>

<p>Merge quote entries that have the same time stamp to a single one and returns an <code>xts</code> or a <code>data.table</code> object
with unique time stamps only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeQuotesSameTimestamp(qData, selection = "median")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeQuotesSameTimestamp_+3A_qdata">qData</code></td>
<td>
<p>an <code>xts</code> object or <code>data.table</code> containing the time series data, with 
at least two columns named <code>BID</code> and <code>OFR</code> indicating the bid and ask price 
as well as two columns <code>BIDSIZ</code>, <code>OFRSIZ</code> indicating the number of round lots available at these 
prices. For <code>data.table</code> an additional column <code>DT</code> is necessary that stores the date/time information.</p>
</td></tr>
<tr><td><code id="mergeQuotesSameTimestamp_+3A_selection">selection</code></td>
<td>
<p>indicates how the bid and ask price for a certain time stamp
should be calculated in case of multiple observation for a certain time
stamp. By default, <code>selection = "median"</code>, and the median price is taken. Alternatively:
</p>

<ul>
<li> <p><code>selection = "max.volume"</code>: use the (bid/ask) price of the entry with
largest (bid/ask) volume.
</p>
</li>
<li> <p><code>selection = "weighted.average"</code>: take the weighted average of all bid (ask) prices,
weighted by &quot;BIDSIZ&quot; (&quot;OFRSIZ&quot;).
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the input data type, we return either a <code>data.table</code> or an <code>xts</code> object containing the quote data which has been cleaned.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>

<hr>
<h2 id='mergeTradesSameTimestamp'>Merge multiple transactions with the same time stamp</h2><span id='topic+mergeTradesSameTimestamp'></span>

<h3>Description</h3>

<p>Merge trade entries that have the same time stamp to a single one and returns an <code>xts</code> or a <code>data.table</code> object
with unique time stamps only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeTradesSameTimestamp(tData, selection = "median")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeTradesSameTimestamp_+3A_tdata">tData</code></td>
<td>
<p>an <code>xts</code> object containing the time series data, with 
one column named <code>PRICE</code> indicating the transaction price 
and one column <code>SIZE</code> indicating the number of shares traded.</p>
</td></tr>
<tr><td><code id="mergeTradesSameTimestamp_+3A_selection">selection</code></td>
<td>
<p>indicates how the price for a certain time stamp
should be calculated in case of multiple observation for a certain time
stamp. By default, <code>selection = "median"</code>, and the median price is taken. Alternatively:
</p>

<ul>
<li> <p><code>selection = "max.volume"</code>: use the price of the transaction with
largest volume.
</p>
</li>
<li> <p><code>selection = "weighted.average"</code>: take the weighted average of all prices.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.table</code> or <code>xts</code> object depending on input.
</p>


<h3>Note</h3>

<p>previously this function returned the mean of the size of the merged trades (pre version 0.7 and when not using max.volume as the criterion), now it returns the sum.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>

<hr>
<h2 id='mukp'>to use when p,k different from range [4,6]</h2><span id='topic+mukp'></span>

<h3>Description</h3>

<p>to use when p,k different from range [4,6]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mukp(p, k, t = 1e+06)
</code></pre>

<hr>
<h2 id='noZeroPrices'>Delete the observations where the price is zero</h2><span id='topic+noZeroPrices'></span>

<h3>Description</h3>

<p>Function deletes the observations where the price is zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noZeroPrices(tData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noZeroPrices_+3A_tdata">tData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object at least containing a column <code>PRICE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen and Kris Boudt.
</p>

<hr>
<h2 id='noZeroQuotes'>Delete the observations where the bid or ask is zero</h2><span id='topic+noZeroQuotes'></span>

<h3>Description</h3>

<p>Function deletes the observations where the bid or ask is zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noZeroQuotes(qData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noZeroQuotes_+3A_qdata">qData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object at least containing the columns <code>BID</code> and <code>OFR</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xts</code> object or <code>data.table</code> depending on type of input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen and Kris Boudt.
</p>

<hr>
<h2 id='plot.DBH'>Plotting method for <code>DBH</code> objects</h2><span id='topic+plot.DBH'></span>

<h3>Description</h3>

<p>Plotting method for <code>DBH</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DBH'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.DBH_+3A_x">x</code></td>
<td>
<p>an object of class <code>DBH</code></p>
</td></tr>
<tr><td><code id="plot.DBH_+3A_...">...</code></td>
<td>
<p>optional arguments, see details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plotting method has the following optional parameters:
</p>

<ul>
<li><p><code>pData</code> A <code>data.table</code> or an <code>xts</code> object, containing the prices and timestamps of the data used to calculate the test statistic.
If specified, and <code>which = "tStat"</code>, the price will be shown on the right y-axis along with the test statistic
</p>
</li>
<li><p><code>which</code> A string denoting which of four plots to make. <code>"tStat"</code> denotes plotting the test statistic. <code>"sigma"</code> denotes plotting the
estimated volatility process. <code>"mu"</code> denotes plotting the estimated drift process. If <code>which = c("sigma", "mu")</code> or <code>which = c("mu", "sigma")</code>,
both the drift and volatility processes are plotted. CaPiTAlizAtIOn doesn't matter
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Testing every 60 seconds after 09:15:00
DBH &lt;- driftBursts(sampleTDataEurope, testTimes = seq(32400 + 900, 63000, 60), preAverage = 2, 
                    ACLag = -1L, meanBandwidth = 300L, varianceBandwidth = 900L)
plot(DBH)
plot(DBH, pData = sampleTDataEurope)
plot(DBH, which = "sigma")
plot(DBH, which = "mu")
plot(DBH, which = c("sigma", "mu"))

</code></pre>

<hr>
<h2 id='plot.HARmodel'>Plotting method for HARmodel objects</h2><span id='topic+plot.HARmodel'></span>

<h3>Description</h3>

<p>Plotting method for HARmodel objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HARmodel'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.HARmodel_+3A_x">x</code></td>
<td>
<p>an object of class <code>HARmodel</code></p>
</td></tr>
<tr><td><code id="plot.HARmodel_+3A_...">...</code></td>
<td>
<p>extra arguments, see details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plotting method has the following optional parameter:
</p>

<ul>
<li><p><code>legend.loc</code> A string denoting the location of the legend passed on to <code>addLegend</code> of the <span class="pkg">xts</span> package
</p>
</li></ul>


<hr>
<h2 id='plot.HEAVYmodel'>Plotting method for HEAVYmodel objects</h2><span id='topic+plot.HEAVYmodel'></span>

<h3>Description</h3>

<p>Plotting method for HEAVYmodel objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HEAVYmodel'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.HEAVYmodel_+3A_x">x</code></td>
<td>
<p>an object of class <code>HEAVYmodel</code>.</p>
</td></tr>
<tr><td><code id="plot.HEAVYmodel_+3A_...">...</code></td>
<td>
<p>extra arguments, see details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plotting method has the following optional parameter:
</p>

<ul>
<li><p><code>legend.loc</code> A string denoting the location of the legend passed on to <code>addLegend</code> of the <span class="pkg">xts</span> package
</p>
</li>
<li><p><code>type</code> A string denoting the type of lot to be made. If <code>type</code> is <code>"condVar"</code> the fitted values of the conditional variance of the returns
is shown. If <code>type</code> is different from <code>"condVar"</code>, the fitted values of the realized measure is shown. Default is <code>"condVar"</code>
</p>
</li></ul>


<hr>
<h2 id='plotTQData'>Plot Trade and Quote data</h2><span id='topic+plotTQData'></span>

<h3>Description</h3>

<p>Plot trade and quote data, trades are marked by crosses, and quotes are plotted as boxes denoting the bid-offer spread for all the quotes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTQData(
  tData,
  qData = NULL,
  xLim = NULL,
  tradeCol = "black",
  quoteCol = "darkgray",
  format = "%H:%M:%S",
  axisCol = "black",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotTQData_+3A_tdata">tData</code></td>
<td>
<p>cleaned trades data</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_qdata">qData</code></td>
<td>
<p>cleaned quotes data</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_xlim">xLim</code></td>
<td>
<p>timestamps for the start and the end of the plots.</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_tradecol">tradeCol</code></td>
<td>
<p>color in which to paint the trade crosses.</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_quotecol">quoteCol</code></td>
<td>
<p>color in which to fill out the bid-offer spread.</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_format">format</code></td>
<td>
<p>format string to pass to <code>axis.POSIXct</code> when creating the timestamps on the x axis.
If you are plotting a very short time interval, use <code>"%H:%M:%OS"</code> to get fractional seconds on the time axis.</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_axiscol">axisCol</code></td>
<td>
<p>string to denote which color to use for the x axis</p>
</td></tr>
<tr><td><code id="plotTQData_+3A_...">...</code></td>
<td>
<p>passed to <code>plot</code> and <code>points</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cleanedQuotes = quotesCleanup(qDataRaw = sampleQDataRaw, report = FALSE, printExchange = FALSE)
cleanedTrades &lt;- tradesCleanupUsingQuotes(
        tData = tradesCleanup(tDataRaw = sampleTDataRaw, report = FALSE, printExchange = FALSE),
        qData = quotesCleanup(qDataRaw = sampleQDataRaw, report = FALSE, printExchange = FALSE)
        )[as.Date(DT) == "2018-01-03"]
xLim &lt;- range(as.POSIXct(c("2018-01-03 15:30:00", "2018-01-03 16:00:00"), tz = "EST"))
plotTQData(cleanedTrades, cleanedQuotes, xLim = xLim, 
           main = "Raw trade and quote data from NYSE TAQ")

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.HARmodel'>Predict method for objects of type <code>HARmodel</code></h2><span id='topic+predict.HARmodel'></span>

<h3>Description</h3>

<p>Predict method for objects of type <code>HARmodel</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HARmodel'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.HARmodel_+3A_object">object</code></td>
<td>
<p>an object of class <code>HARmodel</code></p>
</td></tr>
<tr><td><code id="predict.HARmodel_+3A_...">...</code></td>
<td>
<p>extra arguments. See details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The print method has the following optional parameters:
</p>

<ul>
<li><p><code>newdata</code> new data to use for forecasting
</p>
</li>
<li><p><code>warnings</code> A logical denoting whether to display warnings, detault is <code>TRUE</code>
</p>
</li>
<li><p><code>backtransform</code> A string. If the model is estimated with transformation this parameter can be set to transform the prediction back into variance
The possible values are <code>"simple"</code> which means inverse of transformation, i.e. <code>exp</code> when log-transformation is applied. If using log transformation,
the option <code>"parametric"</code> can also be used to transform back. The parametric method adds a correction that stems from using the log-transformation 
</p>
</li></ul>


<hr>
<h2 id='predict.HEAVYmodel'>Iterative multi-step-ahead forecasting for HEAVY models</h2><span id='topic+predict.HEAVYmodel'></span>

<h3>Description</h3>

<p>Calculates forecasts for <code class="reqn">h_{T+k}</code>, where <code class="reqn">T</code> denotes the end of the estimation
period for fitting the HEAVYmodel and <code class="reqn">k = 1, \dots, \code{stepsAhead}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HEAVYmodel'
predict(object, stepsAhead = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.HEAVYmodel_+3A_object">object</code></td>
<td>
<p>an object of class HEAVYmodel.</p>
</td></tr>
<tr><td><code id="predict.HEAVYmodel_+3A_stepsahead">stepsAhead</code></td>
<td>
<p>the number of days iterative forecasts are calculated for (default 10).</p>
</td></tr>
<tr><td><code id="predict.HEAVYmodel_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='print.DBH'>Printing method for <code>DBH</code> objects</h2><span id='topic+print.DBH'></span>

<h3>Description</h3>

<p>Printing method for <code>DBH</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DBH'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.DBH_+3A_x">x</code></td>
<td>
<p>an object of class <code>DBH</code></p>
</td></tr>
<tr><td><code id="print.DBH_+3A_...">...</code></td>
<td>
<p>optional arguments, see details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The print method has the following optional parameters:
</p>

<ul>
<li><p><code>criticalValue</code> A numeric denoting a custom critical value of the test.
</p>
</li>
<li><p><code>alpha</code> A numeric denoting the confidence level of the test. The alpha value is passed on to <code><a href="#topic+getCriticalValues">getCriticalValues</a></code>.
The default value is 0.95
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
DBH &lt;- driftBursts(sampleTDataEurope, testTimes = seq(32400 + 900, 63000, 300), preAverage = 2, 
                   ACLag = -1L, meanBandwidth = 300L, varianceBandwidth = 900L)
print(DBH)
print(DBH, criticalValue = 1) # This value doesn't make sense - don't actually use it!
print(DBH, alpha = 0.95) # 5% confidence level - this is the standard
print(DBH, alpha = 0.99) # 1% confidence level

## End(Not run)

</code></pre>

<hr>
<h2 id='print.HARmodel'>Printing method for <code>HARmodel</code> objects</h2><span id='topic+print.HARmodel'></span>

<h3>Description</h3>

<p>Printing method for <code>HARmodel</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HARmodel'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.HARmodel_+3A_x">x</code></td>
<td>
<p>object of type <code>HARmodel</code></p>
</td></tr>
<tr><td><code id="print.HARmodel_+3A_...">...</code></td>
<td>
<p>extra options</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The printing method has the extra option <code>digits</code> which can be used to set the number of digits for printing pass <code>lag</code> to determine the maximum order of the Newey West estimator. Default is <code>22</code>
</p>

<hr>
<h2 id='quotesCleanup'>Cleans quote data</h2><span id='topic+quotesCleanup'></span>

<h3>Description</h3>

<p>This is a wrapper function for cleaning the quote data in the entire folder <code>dataSource</code>. 
The result is saved in the folder <code>dataDestination</code>. 
</p>
<p>In case you supply the argument <code>qDataRaw</code>, the on-disk functionality is ignored
and the function returns the cleaned quotes as <code>xts</code> or <code>data.table</code> object (see examples).
</p>
<p>The following cleaning functions are performed sequentially:
<code><a href="#topic+noZeroQuotes">noZeroQuotes</a></code>, <code><a href="#topic+exchangeHoursOnly">exchangeHoursOnly</a></code>, <code><a href="#topic+autoSelectExchangeQuotes">autoSelectExchangeQuotes</a></code> or <code><a href="#topic+selectExchange">selectExchange</a></code>, <code><a href="#topic+rmNegativeSpread">rmNegativeSpread</a></code>, <code><a href="#topic+rmLargeSpread">rmLargeSpread</a></code>
<code><a href="#topic+mergeQuotesSameTimestamp">mergeQuotesSameTimestamp</a></code>, <code><a href="#topic+rmOutliersQuotes">rmOutliersQuotes</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quotesCleanup(
  dataSource = NULL,
  dataDestination = NULL,
  exchanges = "auto",
  qDataRaw = NULL,
  report = TRUE,
  selection = "median",
  maxi = 50,
  window = 50,
  type = "standard",
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  rmoutliersmaxi = 10,
  printExchange = TRUE,
  saveAsXTS = FALSE,
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quotesCleanup_+3A_datasource">dataSource</code></td>
<td>
<p>character indicating the folder in which the original data is stored.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_datadestination">dataDestination</code></td>
<td>
<p>character indicating the folder in which the cleaned data is stored.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_exchanges">exchanges</code></td>
<td>
<p>vector of stock exchange symbols for all data in dataSource, 
e.g. <code>exchanges = c("T","N")</code> retrieves all stock market data from both NYSE and NASDAQ.
The possible exchange symbols are:
</p>

<ul>
<li><p> A: AMEX
</p>
</li>
<li><p> N: NYSE
</p>
</li>
<li><p> B: Boston
</p>
</li>
<li><p> P: Arca
</p>
</li>
<li><p> C: NSX
</p>
</li>
<li><p> T/Q: NASDAQ
</p>
</li>
<li><p> D: NASD ADF and TRF
</p>
</li>
<li><p> X: Philadelphia
</p>
</li>
<li><p> I: ISE
</p>
</li>
<li><p> M: Chicago
</p>
</li>
<li><p> W: CBOE
</p>
</li>
<li><p> Z: BATS
</p>
</li></ul>
<p>. The default value is <code>"auto"</code> which automatically selects the exchange for the stocks and days independently using the <code><a href="#topic+autoSelectExchangeQuotes">autoSelectExchangeQuotes</a></code></p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_qdataraw">qDataRaw</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> object containing raw quote data, possibly for multiple symbols over multiple days. This argument is <code>NULL</code> by default. 
Enabling it means the arguments <code>dataSource</code> and <code>dataDestination</code> will be ignored. (only advisable for small chunks of data)</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_report">report</code></td>
<td>
<p>boolean and <code>TRUE</code> by default. In case it is true and we don't use the on-disk functionality, the function returns (also) a vector indicating how many quotes were deleted by each cleaning step.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_selection">selection</code></td>
<td>
<p>argument to be passed on to the cleaning routine <code><a href="#topic+mergeQuotesSameTimestamp">mergeQuotesSameTimestamp</a></code>. The default is <code>"median"</code>.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_maxi">maxi</code></td>
<td>
<p>spreads which are greater than median spreads of the day times <code>maxi</code> are excluded.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_window">window</code></td>
<td>
<p>argument to be passed on to the cleaning routine <code><a href="#topic+rmOutliersQuotes">rmOutliersQuotes</a></code>.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_type">type</code></td>
<td>
<p>argument to be passed on to the cleaning routine <code><a href="#topic+rmOutliersQuotes">rmOutliersQuotes</a></code>.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_marketopen">marketOpen</code></td>
<td>
<p>passed to <code><a href="#topic+exchangeHoursOnly">exchangeHoursOnly</a></code>. A character in the format of <code>"HH:MM:SS"</code>,
specifying the starting hour, minute and second of an exchange.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_marketclose">marketClose</code></td>
<td>
<p>passed to <code><a href="#topic+exchangeHoursOnly">exchangeHoursOnly</a></code>. A character in the format of <code>"HH:MM:SS"</code>,
specifying the closing hour, minute and second of an exchange.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_rmoutliersmaxi">rmoutliersmaxi</code></td>
<td>
<p>argument to be passed on to the cleaning routine <code><a href="#topic+rmOutliersQuotes">rmOutliersQuotes</a></code>.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_printexchange">printExchange</code></td>
<td>
<p>Argument passed to <code><a href="#topic+autoSelectExchangeQuotes">autoSelectExchangeQuotes</a></code> indicates whether the chosen exchange is printed on the console, 
default is <code>TRUE</code>. This is only used when <code>exchanges</code> is <code>"auto"</code></p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_saveasxts">saveAsXTS</code></td>
<td>
<p>indicates whether data should be saved in <code>xts</code> format instead of <code>data.table</code> when using on-disk functionality. <code>FALSE</code> by default, which means we save as <code>data.table</code>.</p>
</td></tr>
<tr><td><code id="quotesCleanup_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. With the non-disk functionality, we attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>. 
In the on-disk functionality, if <code>tz</code> is not specified, the timezone used will be the system default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the on-disk functionality with .csv.zip files which is the standard from the WRDS database
will write temporary files on your machine - we try to clean up after it, but cannot guarantee that 
there won't be files that slip through the crack if the permission settings on your machine does not match 
ours.
</p>
<p>If the input <code>data.table</code> does not contain a <code>DT</code> column but it does contain <code>DATE</code> and <code>TIME_M</code> columns, we create the <code>DT</code> column by REFERENCE, altering the <code>data.table</code> that may be in the user's environment!
</p>


<h3>Value</h3>

<p>The function converts every (compressed) csv (or rds) file in <code>dataSource</code> into multiple <code>xts</code> or <code>data.table</code> files.
</p>
<p>In <code>dataDestination</code>, there will be one folder for each symbol containing .rds files with cleaned data stored either in <code>data.table</code> or <code>xts</code> format.
</p>
<p>In case you supply the argument <code>qDataRaw</code>, the on-disk functionality is ignored
and the function returns a list with the cleaned quotes as an <code>xts</code> or <code>data.table</code> object depending on input (see examples).
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Hansen, P. R., Lunde, A., and Shephard, N. (2009). Realized kernels in practice: Trades and quotes. Econometrics Journal 12, C1-C32.
</p>
<p>Brownlees, C.T. and Gallo, G.M. (2006). Financial econometric analysis at ultra-high frequency: Data handling concerns. Computational Statistics &amp; Data Analysis, 51, pages 2232-2245.
</p>
<p>Falkenberry, T.N. (2002). High frequency data filtering. Unpublished technical report.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Consider you have raw quote data for 1 stock for 2 days
head(sampleQDataRaw)
dim(sampleQDataRaw)
qDataAfterCleaning &lt;- quotesCleanup(qDataRaw = sampleQDataRaw, exchanges = "N")
qDataAfterCleaning$report
dim(qDataAfterCleaning$qData)

# In case you have more data it is advised to use the on-disk functionality
# via "dataSource" and "dataDestination" arguments

</code></pre>

<hr>
<h2 id='rankJumpTest'>Rank jump test</h2><span id='topic+rankJumpTest'></span>

<h3>Description</h3>

<p>Calculate the rank jump test of Li et al. (2019).
The procedure tests for the rank of the jump matrix at simultaneous jump events in market returns as well as individual assets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankJumpTest(
  marketPrice,
  stockPrices,
  alpha = c(5, 3),
  coarseFreq = 10,
  localWindow = 30,
  rank = 1,
  BoxCox = 1,
  quantiles = c(0.9, 0.95, 0.99),
  nBoot = 1000,
  dontTestAtBoundaries = TRUE,
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rankJumpTest_+3A_marketprice">marketPrice</code></td>
<td>
<p>data.table or <code>xts</code>containing the market prices in levels</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_stockprices">stockPrices</code></td>
<td>
<p>list containing the individual stock prices in either data.table or <code>xts</code>format. The format should be the the same as <code>marketPrice</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_alpha">alpha</code></td>
<td>
<p>significance level (in standard deviations) to use for the jump detections. Default is <code>c(5,3)</code> for 5 and 3 in the market and stocks respectively.</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_coarsefreq">coarseFreq</code></td>
<td>
<p>numeric denoting the coarse sampling frequency. Default is <code>10</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_localwindow">localWindow</code></td>
<td>
<p>numeric denoting the local window for the bootstrap algorithm. Default is <code>30</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_rank">rank</code></td>
<td>
<p>rank of the jump matrix under the null hypothesis. Default is <code>1</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_boxcox">BoxCox</code></td>
<td>
<p>numeric of exponents for the Box-Cox transformation, default is <code>1</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_quantiles">quantiles</code></td>
<td>
<p>numeric denoting which quantiles of the bootstrapped critical values to return and compare against. Default is <code>c(0.9, 0.95, 0.99)</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_nboot">nBoot</code></td>
<td>
<p>numeric denoting how many replications to be used for the bootstrap algorithm. Default is <code>1000</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_donttestatboundaries">dontTestAtBoundaries</code></td>
<td>
<p>logical determining whether to exclude data across different days. Default is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. Possible values are: &quot;secs&quot;, &quot;seconds&quot;, &quot;mins&quot;, &quot;minutes&quot;,&quot;hours&quot;, and &quot;ticks&quot;.
To aggregate based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5 minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time, by default: <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time, by default: <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="rankJumpTest_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let the jump times be defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    {\cal I}_{n} = \left\{ i:\left|\Delta_{i}^{n}Z\right|&gt;u_{n}\right\} 
</code>
</p>

<p>Then the estimated jump matrix is:
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\boldsymbol{J}_{n}}=\left[\Delta_{i,k}^{n}\boldsymbol{X}\right]_{i\in{\cal I}_{n}}
</code>
</p>
   
<p>Let <code class="reqn">\hat{\lambda}_{n,1}^{2}\geq\hat{\lambda}_{n,2}^{2}\geq\cdots\geq\hat{\lambda}_{n,d}^{2}</code> be the ordered eigenvalues of <code class="reqn">\hat{\boldsymbol{J}}_{n}\hat{\boldsymbol{J}}_{n}^{\prime}</code>, then test statistic is
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{S}_{n,t}=\sum_{j=r+1}^{d}\hat{\lambda}_{n,j}^{2}.
</code>
</p>

<p>The critical values are computed by applying a bootstrapping method 
</p>
<p>The singular value decomposition of the jump matrix <code class="reqn">\hat{\boldsymbol{J}}_{n}</code> is:
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\boldsymbol{J}}=\hat{\boldsymbol{U}}_{n}\hat{\boldsymbol{D}}_{n}\hat{\boldsymbol{V}}_{n}^{\prime}
</code>
</p>

<p>then <code class="reqn">\hat{\boldsymbol{U}}_{n}=\left[\hat{\boldsymbol{U}}_{1n}:\hat{\boldsymbol{U}}_{2n}\right]</code> and <code class="reqn">\hat{\boldsymbol{V}}_{n}=\left[\hat{\boldsymbol{V}}_{1n}:\hat{\boldsymbol{V}}_{2n}\right]</code>
</p>
<p><code class="reqn">\boldsymbol{\upsilon}_{n}=\left(\upsilon_{j,n}\right)_{1\leq j\leq d}</code> such that <code class="reqn">\upsilon_{j,n}\asymp\Delta_{n}^{\varpi} for \varpi\in\left(0,1/2\right)</code> which is used to trim jumps. The bootstrapping method is calculated by the following algorithm
</p>

<ul>
<li>
<p>Step 1.
</p>
<p>For each <code class="reqn">i\in{\cal I}_{n}</code>, draw <code class="reqn">\kappa_{i}^{\star}\sim\textrm{Uniform}\left[0,1\right]</code> and draw with equal probability,
</p>
<p style="text-align: center;"><code class="reqn">
    \boldsymbol{\xi}_{n,i-}^{\star} \textrm{from}\left\{ \min\left(\max\left(\Delta_{i-j}^{n}\boldsymbol{X},-\boldsymbol{\upsilon}_{n}\right),\boldsymbol{\upsilon}_{n}\right):1\leq j\leq k_{n}\right\}, 
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \boldsymbol{\xi}_{n,i+}^{\star} \textrm{from}\left\{ \min\left(\max\left(\Delta_{i+j}^{n}\boldsymbol{X},-\boldsymbol{\upsilon}_{n}\right),\boldsymbol{\upsilon}_{n}\right):1\leq j\leq k_{n}\right\},
</code>
</p>

<p>and set <code class="reqn">\boldsymbol{\zeta}_{n,i}^{\star}=\sqrt{\kappa_{i}^{\star}}\boldsymbol{\xi}_{n,i-}^{\star}+\sqrt{k-\kappa_{i}^{\star}}\boldsymbol{\xi}_{n,i+}^{\star}</code> and <code class="reqn">\boldsymbol{\zeta}_{n}^{\star}=\left[\boldsymbol{\zeta}_{n,i}^{\star}\right]_{i\in{\cal I}_{n}}</code>
</p>

</li>
<li>
<p>Step 2.
</p>
<p>Repeat 1 for a large number of iterations. Set <code class="reqn">c\upsilon_{n,\alpha}</code> as as the <code class="reqn">1-\alpha</code> quantile of <code class="reqn">\left\Vert \hat{\boldsymbol{U}}_{2n}^{\prime}\boldsymbol{\xi}_{n}^{\star}\hat{\boldsymbol{V}}_{2n}\right\Vert ^{2}</code> in the simulated sample.

</p>
</li></ul>



<h3>Value</h3>

<p>A list containing <code>criticalValues</code> which are the bootstrapped critical values, <code>testStatistic</code> the test statistic of the jump test, <code>dimensions</code> which are the dimensions of the jump matrix
<code>marketJumpDetections</code> the jumps detected in the market prices, <code>stockJumpDetections</code> the co-jumps detected in the individual stock prices, and <code>jumpIndices</code> which are the indices of the detected jumps.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup, based on Matlab code provided by Li et al. (2019)
</p>


<h3>References</h3>

<p>Li, j., Todorov, V., Tauchen, G., and Lin, H. (2019). Rank Tests at Jump Events. <em>Journal of Business &amp; Economic Statistics</em>, 37, 312-321.
</p>

<hr>
<h2 id='rAVGCov'>Realized covariances via subsample averaging</h2><span id='topic+rAVGCov'></span>

<h3>Description</h3>

<p>Calculates realized variances via averaging across partially
overlapping grids, first introduced by Zhang et al. (2005).
This estimator is basically an average across different <code><a href="#topic+rCov">rCov</a></code> estimates that start at
different points in time, see details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rAVGCov(
  rData,
  cor = FALSE,
  alignBy = "minutes",
  alignPeriod = 5,
  k = 1,
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rAVGCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_k">k</code></td>
<td>
<p>numeric denoting which horizon to use for the subsambles. This can be a fraction as long as <code class="reqn">k</code> is a divisor of <code>alignPeriod</code> default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rAVGCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that in period <code class="reqn">t</code>, there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{i,t}</code> 
and let <code class="reqn">\Delta</code> be equal to <code>alignPeriod</code>. For <code class="reqn">\ i \geq \Delta</code>, 
we define the subsampled <code class="reqn">\Delta</code>-period return as
</p>
<p style="text-align: center;"><code class="reqn">
\tilde r_{t,i} = \sum_{k = 0}^{\Delta - 1} r_{t,i-k}, .
</code>
</p>

<p>Now define <code class="reqn">N^*(j) = N/\Delta</code> if <code class="reqn">j = 0</code> and  <code class="reqn">N^*(j) = N/\Delta - 1</code> otherwise.
The <code class="reqn">j</code>-th component of the <code>rAVGCov</code> estimator is given by
</p>
<p style="text-align: center;"><code class="reqn">
RV_t^j = \sum_{i = 1}^{N^*(j)} \tilde r_{t, j + i \cdot \Delta}^2.
</code>
</p>

<p>Now taking the average across the different <code class="reqn">RV_t^j, \ j = 0, \dots, \Delta-1,</code> defines the <code>rAVGCov</code> estimator.
The multivariate version follows analogously.
</p>
<p>Note that Liu et al. (2015) show that <code>rAVGCov</code> is not only theoretically but also empirically a more reliable estimator than rCov.
</p>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an <code class="reqn">N</code> by <code class="reqn">N</code> matrix is returned. If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing <code class="reqn">N</code> by <code class="reqn">N</code> matrices. 
Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Scott Payseur, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Liu, L. Y., Patton, A. J., Sheppard, K. (2015). Does anything beat 5-minute RV? A comparison of realized measures across multiple asset classes. <em>Journal of Econometrics</em>, 187, 293-311.
</p>
<p>Zhang, L., Mykland, P. A. , and Ait-Sahalia, Y. (2005). A tale of two time scales: Determining integrated volatility with noisy high-frequency data. <em>Journal of the American Statistical Association</em>, 100, 1394-1411.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Average subsampled realized variance/covariance aligned at one minute returns at
# 5 sub-grids (5 minutes).

# Univariate subsampled realized variance
rvAvgSub &lt;- rAVGCov(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
                    makeReturns = TRUE)
rvAvgSub

# Multivariate subsampled realized variance
rvAvgCovSub &lt;- rAVGCov(rData = sampleOneMinuteData[1:391], makeReturns = TRUE)
rvAvgCovSub
</code></pre>

<hr>
<h2 id='rBACov'>rBACov</h2><span id='topic+rBACov'></span>

<h3>Description</h3>

<p>The Beta Adjusted Covariance (BAC) equals the pre-estimator plus a minimal adjustment matrix such that the covariance-implied stock-ETF beta equals a target beta.
</p>
<p>The BAC estimator works by applying a minimum correction factor to a pre-estimated covariance matrix such that a target beta derived from the ETF is reached.
</p>
<p>Let 
</p>
<p style="text-align: center;"><code class="reqn">
    \bar{\beta}
</code>
</p>
 
<p>denote the implied beta derived from the pre-estimator, and
</p>
<p style="text-align: center;"><code class="reqn">
    \beta_{\bullet}
</code>
</p>

<p>denote the target beta, then the correction factor is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">
 L\left(\bar{\beta}-\beta_{\bullet}\right),
</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">
    L=\left(I_{d^{2}}-\frac{1}{2}{\cal Q}\right)\bar{W}^{\prime}\left(I_{d^{2}}\left(\sum_{k=1}^{d}\frac{\sum_{k=1}^{n_{k}}\left(w_{t_{m-1}^{k}}^{k}\right)^{2}}{n_{k}}\right)-\frac{\bar{W}{\cal Q}\bar{W}^{\prime}}{2}\right)^{-1},
</code>
</p>

<p>where <code class="reqn">d</code> is the number of assets in the ETF, and <code class="reqn">n_{k}</code> is the number of trades in the <code class="reqn">k</code>th asset, and
</p>
<p style="text-align: center;"><code class="reqn">
    \bar{W}^{k}=\left(0_{\left(k-1\right)d}^{\prime},\frac{1}{n_{1}}\sum_{m=1}^{n_{1}}w_{t_{m-1}^{1}}^{1},\dots,\frac{1}{n_{d}}\sum_{m=1}^{n_{d}}w_{t_{m-1}^{d}}^{d},0_{\left(d-k\right)d}^{\prime}\right),
</code>
</p>

<p>where <code class="reqn">w_{t_{m-1}^{k}}^{k}</code> is the weight of the <code class="reqn">k</code>th asset in the ETF. 
</p>
<p>and 
</p>
<p style="text-align: center;"><code class="reqn">
     {\cal Q}^{\left(i-1\right)d+j}
</code>
</p>
 
<p>is defined by the following two cases:
</p>
<p><code class="reqn">
    \left(0_{\left(i-1\right)d+j-1}^{\prime},1,0_{\left(d-i+1\right)d-j}^{\prime}\right)+\left(0_{\left(j-1\right)d+i-1}^{\prime},-1,0_{\left(d-j+1\right)d-i}^{\prime}\right) \quad \textrm{if }i\neq j;
</code>
</p>
<p><code class="reqn">
    0_{d^{2}}^{\prime} \quad \textrm{otherwise}.
</code>
</p>
<p><code class="reqn">\bar{W}^k</code> has dimensions <code class="reqn">d \times d^2</code> and <code class="reqn">{\cal Q}^{\left(i-1\right)d+j}</code> has dimensions <code class="reqn">d^2 \times d^2</code>.
</p>
<p>The Beta-Adjusted Covariance is then 
</p>
<p style="text-align: center;"><code class="reqn">
\Sigma^{\textrm{BAC}} = \Sigma - L\left(\bar{\beta}-\beta_{\bullet}\right),
</code>
</p>

<p>where <code class="reqn">\Sigma</code> is the pre-estimated covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rBACov(
  pData,
  shares,
  outstanding,
  nonEquity,
  ETFNAME = "ETF",
  unrestricted = TRUE,
  targetBeta = c("HY", "VAB", "expert"),
  expertBeta = NULL,
  preEstimator = "rCov",
  noiseRobustEstimator = rTSCov,
  noiseCorrection = FALSE,
  returnL = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rBACov_+3A_pdata">pData</code></td>
<td>
<p>a named list. Each list-item contains an <code>xts</code> or <code>data.table</code> object with the intraday price data of an ETF and it's component stocks. <code>xts</code> objects are turned into <code>data.table</code>s</p>
</td></tr>
<tr><td><code id="rBACov_+3A_shares">shares</code></td>
<td>
<p>a <code>numeric</code> with length corresponding to the number of component stocks in the ETF. The entries are the stock holdings of the ETF in the corresponding stock. The order of these entries should correspond to the order the stocks are listed in the <code>list</code> passed in the <code>pData</code> argument.</p>
</td></tr>
<tr><td><code id="rBACov_+3A_outstanding">outstanding</code></td>
<td>
<p>number of shares outstanding of the ETF</p>
</td></tr>
<tr><td><code id="rBACov_+3A_nonequity">nonEquity</code></td>
<td>
<p>aggregated value of the additional components (like cash, money-market funds, bonds, etc.) of the ETF which are not included in the components in <code>pData</code>.</p>
</td></tr>
<tr><td><code id="rBACov_+3A_etfname">ETFNAME</code></td>
<td>
<p>a <code>character</code> denoting which entry in the <code>pData</code> list is the ETF. Default is <code>"ETF"</code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_unrestricted">unrestricted</code></td>
<td>
<p>a <code>logical</code> denoting whether to use the unrestricted estimator, which is an extension that also affects the diagonal. Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_targetbeta">targetBeta</code></td>
<td>
<p>a <code>character</code>, one of <code>c("HY", "VAB", "expert")</code> (default) denoting which target beta to use, only the first entry will be used. A value <code>"HY"</code> means using the Hayashi-Yoshida estimator to estimate the
empirical beta. A value of <code>"VAB"</code> denotes using the variance adjusted beta. A value of <code>"expert"</code> denotes using a user-supplied target beta, which can be supplied in the <code>expertBeta</code> argument.</p>
</td></tr>
<tr><td><code id="rBACov_+3A_expertbeta">expertBeta</code></td>
<td>
<p>a <code>numeric</code> containing the user supplied expert beta used when <code>targetBeta</code> is <code>"expert"</code>. The <code>expertBeta</code> must be of length equal to the number of assets in the ETF. Default is <code>NULL</code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_preestimator">preEstimator</code></td>
<td>
<p>a <code>function</code> which estimates the integrated covariance matrix. Default is <code><a href="#topic+rCov">rCov</a></code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_noiserobustestimator">noiseRobustEstimator</code></td>
<td>
<p>a <code>function</code> which estimates the integrated (co)variance and is robust to microstructure noise (only the diagonal will be estimated).
This function is only used when <code>noiseCorrection</code> is <code>TRUE</code>. Default is <code><a href="#topic+rTSCov">rTSCov</a></code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_noisecorrection">noiseCorrection</code></td>
<td>
<p>a <code>logical</code> which denotes whether to use the extension of the estimator which corrects for microstructure noise by using the <code>noiseRobustEstimator</code> function. Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_returnl">returnL</code></td>
<td>
<p>a <code>logical</code> which denotes whether to return the <code>L</code> matrix. Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="rBACov_+3A_...">...</code></td>
<td>
<p>extra arguments passed to <code>preEstimator</code> and <code>noiseRobustEstimator</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Emil Sjoerup, (Kris Boudt and Kirill Dragun for the Python version)
</p>


<h3>References</h3>

<p>Boudt, K., Dragun, K., Omauri, S., and Vanduffel, S. (2021) Beta-Adjusted Covariance Estimation (working paper).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Since we don't have any data in this package that is of the required format we must simulate it.
library(xts)
library(highfrequency)
# The mvtnorm package is needed for this example
# Please install this package before running this example
library("mvtnorm")
# Set the seed for replication
set.seed(123)
iT &lt;- 23400 # Number of observations
# Simulate returns
rets &lt;- rmvnorm(iT * 3 + 1, mean = rep(0,4), 
                sigma = matrix(c(0.1, -0.03 , 0.02, 0.04,
                                -0.03, 0.05, -0.03, 0.02,
                                 0.02, -0.03, 0.05, -0.03,  
                                 0.04, 0.02, -0.03, 0.08), ncol = 4))
# We assume that the assets don't trade in a synchronous manner
w1 &lt;- rets[sort(sample(1:nrow(rets), size = nrow(rets) * 0.5)), 1]
w2 &lt;- rets[sort(sample(1:nrow(rets), size = nrow(rets) * 0.75)), 2]
w3 &lt;- rets[sort(sample(1:nrow(rets), size = nrow(rets) * 0.65)), 3]
w4 &lt;- rets[sort(sample(1:nrow(rets), size = nrow(rets) * 0.8)), 4]
w5 &lt;- rnorm(nrow(rets) * 0.9, mean = 0, sd = 0.005)
timestamps1 &lt;- seq(34200, 57600, length.out = length(w1))
timestamps2 &lt;- seq(34200, 57600, length.out = length(w2))
timestamps3 &lt;- seq(34200, 57600, length.out = length(w3))
timestamps4 &lt;- seq(34200, 57600, length.out = length(w4))
timestamps4 &lt;- seq(34200, 57600, length.out = length(w4))
timestamps5 &lt;- seq(34200, 57600, length.out = length(w5))

w1 &lt;- xts(w1 * c(0,sqrt(diff(timestamps1) / (max(timestamps1) - min(timestamps1)))),
          as.POSIXct(timestamps1, origin = "1970-01-01"), tzone = "UTC")
w2 &lt;- xts(w2 * c(0,sqrt(diff(timestamps2) / (max(timestamps2) - min(timestamps2)))),
          as.POSIXct(timestamps2, origin = "1970-01-01"), tzone = "UTC")
w3 &lt;- xts(w3 * c(0,sqrt(diff(timestamps3) / (max(timestamps3) - min(timestamps3)))),
          as.POSIXct(timestamps3, origin = "1970-01-01"), tzone = "UTC")
w4 &lt;- xts(w4 * c(0,sqrt(diff(timestamps4) / (max(timestamps4) - min(timestamps4)))),
          as.POSIXct(timestamps4, origin = "1970-01-01"), tzone = "UTC")
w5 &lt;- xts(w5 * c(0,sqrt(diff(timestamps5) / (max(timestamps5) - min(timestamps5)))),
          as.POSIXct(timestamps5, origin = "1970-01-01"), tzone = "UTC")

p1  &lt;- exp(cumsum(w1))
p2  &lt;- exp(cumsum(w2))
p3  &lt;- exp(cumsum(w3))
p4  &lt;- exp(cumsum(w4))

weights &lt;- runif(4) * 1:4
weights &lt;- weights / sum(weights)
p5 &lt;- xts(rowSums(cbind(w1 * weights[1], w2 * weights[2], w3 * weights[3], w4 * weights[4]),
                   na.rm = TRUE),
                   index(cbind(p1, p2, p3, p4)))
p5 &lt;- xts(cumsum(rowSums(cbind(p5, w5), na.rm = TRUE)), index(cbind(p5, w5)))

p5 &lt;- exp(p5[sort(sample(1:length(p5), size = nrow(rets) * 0.9))])


BAC &lt;- rBACov(pData = list(
                     "ETF" = p5, "STOCK 1" = p1, "STOCK 2" = p2, "STOCK 3" = p3, "STOCK 4" = p4
                   ), shares = 1:4, outstanding = 1, nonEquity = 0, ETFNAME = "ETF", 
                   unrestricted = FALSE, preEstimator = "rCov", noiseCorrection = FALSE, 
                   returnL = FALSE, K = 2, J = 1)

# Noise robust version of the estimator
noiseRobustBAC &lt;- rBACov(pData = list(
                     "ETF" = p5, "STOCK 1" = p1, "STOCK 2" = p2, "STOCK 3" = p3, "STOCK 4" = p4
                   ), shares = 1:4, outstanding = 1, nonEquity = 0, ETFNAME = "ETF", 
                   unrestricted = FALSE, preEstimator = "rCov", noiseCorrection = TRUE, 
                   noiseRobustEstimator = rHYCov, returnL = FALSE, K = 2, J = 1)

# Use the Variance Adjusted Beta method
# Also use a different pre-estimator.
VABBAC &lt;- rBACov(pData = list(
                     "ETF" = p5, "STOCK 1" = p1, "STOCK 2" = p2, "STOCK 3" = p3, "STOCK 4" = p4
                   ), shares = 1:4, outstanding = 1, nonEquity = 0, ETFNAME = "ETF", 
                   unrestricted = FALSE, targetBeta = "VAB", preEstimator = "rHYov", 
                   noiseCorrection = FALSE, returnL = FALSE, Lin = FALSE, L = 0, K = 2, J = 1)                    
                   

## End(Not run)
</code></pre>

<hr>
<h2 id='rBeta'>Realized beta</h2><span id='topic+rBeta'></span>

<h3>Description</h3>

<p>Depending on users' choices of estimator (realized covariance (RCOVestimator) and realized variance (RVestimator)), 
the function returns the realized beta, defined as the ratio between both.
</p>
<p>The realized beta is given by
</p>
<p style="text-align: center;"><code class="reqn">
\beta_{jm} = \frac {RCOVestimator_{jm}}{RVestimator_{m}}
</code>
</p>

<p>in which
</p>
<p><code class="reqn">RCOVestimator:</code> Realized covariance of asset j and market index <code class="reqn">m</code>.
</p>
<p><code class="reqn">RVestimator:</code> Realized variance of market index <code class="reqn">m</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rBeta(
  rData,
  rIndex,
  RCOVestimator = "rCov",
  RVestimator = "rRVar",
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rBeta_+3A_rdata">rData</code></td>
<td>
<p>a <code>xts</code> object containing all returns in period t for one asset.</p>
</td></tr>
<tr><td><code id="rBeta_+3A_rindex">rIndex</code></td>
<td>
<p>a <code>xts</code> object containing return in period t for an index.</p>
</td></tr>
<tr><td><code id="rBeta_+3A_rcovestimator">RCOVestimator</code></td>
<td>
<p>can be chosen among realized covariance estimators: <code>"rCov"</code>, <code>"rAVGCov"</code>, <code>"rBPCov"</code>, <code>"rHYCov"</code>, <code>"rKernelCov"</code>, <code>"rOWCov"</code>, <code>"rRTSCov"</code>, <code>"rThresholdCov"</code> and <code>"rTSCov"</code> <code>"rCov"</code> by default.</p>
</td></tr>
<tr><td><code id="rBeta_+3A_rvestimator">RVestimator</code></td>
<td>
<p>can be chosen among realized variance estimators: <code>"rRVar"</code>, <code>"rMinRVar"</code> and <code>"rMedRVar"</code>. <code>"rRVar"</code> by default. 
In case of missing <code>RVestimator</code>, <code>RCOVestimator</code> function applying for <code>rIndex</code> will be used.</p>
</td></tr>
<tr><td><code id="rBeta_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rBeta_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>RCOVestimator</code> and <code>RVestimator</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose there are <code class="reqn">N</code> equispaced returns on day <code class="reqn">t</code> for the asset <code class="reqn">j</code> and the index <code class="reqn">m</code>. 
Denote <code class="reqn">r_{(j)i,t}</code>, <code class="reqn">r_{(m)i,t}</code> as the <code class="reqn">i</code>th return on day <code class="reqn">t</code> for asset <code class="reqn">j</code> and index <code class="reqn">m</code> (with <code class="reqn">i=1, \ldots,N</code>).
</p>
<p>By default, the RCov is used and the realized beta coefficient is computed as:
</p>
<p style="text-align: center;"><code class="reqn">
\hat{\beta}_{(jm)t}= \frac{\sum_{i=1}^{N} r_{(j)i,t} r_{(m)i,t}}{\sum_{i=1}^{N} r_{(m)i,t}^2}.
</code>
</p>

<p>Note: The function does not support to calculate betas across multiple days.
</p>


<h3>Value</h3>

<p>numeric
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E. and Shephard, N. (2004). Econometric analysis of realized covariation: high frequency based covariance, regression, and correlation in
financial economics. <em>Econometrica</em>, 72, 885-925.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("xts")
a &lt;- as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, MARKET)])
b &lt;-  as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, STOCK)])
rBeta(a, b, RCOVestimator = "rBPCov", RVestimator = "rMinRVar", makeReturns = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='rBPCov'>Realized bipower covariance</h2><span id='topic+rBPCov'></span>

<h3>Description</h3>

<p>Calculate the Realized BiPower Covariance (rBPCov),
defined in Barndorff-Nielsen and Shephard (2004).
</p>
<p>Let <code class="reqn">r_{t,i}</code> be an intraday <code class="reqn">N x 1</code> return vector and <code class="reqn">i=1,...,M</code>
the number of intraday returns.
</p>
<p>The rBPCov is defined as the process whose value at time <code class="reqn">t</code>
is the <code class="reqn">N</code>-dimensional square matrix with <code class="reqn">k,q</code>-th element equal to
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{rBPCov}[k,q]_t = \frac{\pi}{8} \bigg( \sum_{i=2}^{M}
                                              \left|
                                                r_{(k)t,i} + r_{(q)t,i} \right| \ \left| r_{(k)t,i-1} + r_{(q)t,i-1} \right|   \\
                                              - \left| r_{(k)t,i}  - r_{(q)t,i} \right| \ \left|
                                                r_{(k)t,i-1} - r_{(q)t,i-1} \right|  \bigg),
 </code>
</p>

<p>where <code class="reqn">r_{(k)t,i}</code> is the
<code class="reqn">k</code>-th component of the return vector <code class="reqn">r_{i,t}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rBPCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  makePsd = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rBPCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days</p>
</td></tr>
<tr><td><code id="rBPCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rBPCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rBPCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. E.g. to aggregate
based on a 5-minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="rBPCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rBPCov_+3A_makepsd">makePsd</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, the positive definite version of rBPCov is returned. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rBPCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an <code class="reqn">N</code> by <code class="reqn">N</code> matrix is returned. If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing <code class="reqn">N</code> by <code class="reqn">N</code> matrices. Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., and Shephard, N. (2004). Measuring the impact of jumps in multivariate price processes using bipower covariation. Discussion paper, Nuffield College, Oxford University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Realized Bipower Variance/Covariance for a price series aligned
# at 5 minutes.

# Univariate:
rbpv &lt;- rBPCov(rData = sampleTData[, list(DT, PRICE)], alignBy ="minutes",
               alignPeriod = 5, makeReturns = TRUE)
# Multivariate:
rbpc &lt;- rBPCov(rData = sampleOneMinuteData, makeReturns = TRUE, makePsd = TRUE)
rbpc
</code></pre>

<hr>
<h2 id='RBPCov_bi'># Check data:
#' @keywords internal
rdatacheck &lt;- function (rData, multi = FALSE) 
if ((dim(rData)[2] &lt; 2) &amp; (multi)) 
stop(&quot;Your rData object should have at least 2 columns&quot;)

</h2><span id='topic+RBPCov_bi'></span>

<h3>Description</h3>

<p># Check data:
#' @keywords internal
rdatacheck &lt;- function (rData, multi = FALSE) 
if ((dim(rData)[2] &lt; 2) &amp; (multi)) 
stop(&quot;Your rData object should have at least 2 columns&quot;)


</p>


<h3>Usage</h3>

<pre><code class='language-R'>RBPCov_bi(ts1, ts2)
</code></pre>

<hr>
<h2 id='rCholCov'>CholCov estimator</h2><span id='topic+rCholCov'></span>

<h3>Description</h3>

<p>Positive semi-definite covariance estimation using the CholCov algorithm.
The algorithm estimates the integrated covariance matrix by sequentially adding series and using 'refreshTime' to synchronize the observations.
This is done in order of liquidity, which means that the algorithm uses more data points than most other estimation techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rCholCov(
  pData,
  IVest = "rMRCov",
  COVest = "rMRCov",
  criterion = "squared duration",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rCholCov_+3A_pdata">pData</code></td>
<td>
<p>a list. Each list-item i contains an <code>xts</code> object with the intraday price data (in levels)
of stock <code class="reqn">i</code> for day <code class="reqn">t</code>. The order of the data does not matter as it will be sorted according to the criterion specified in the <code>criterion</code> argument</p>
</td></tr>
<tr><td><code id="rCholCov_+3A_ivest">IVest</code></td>
<td>
<p>integrated variance estimator, default is <code>"rMRCov"</code>. For a list of implemented estimators, use <code>listCholCovEstimators()</code>.</p>
</td></tr>
<tr><td><code id="rCholCov_+3A_covest">COVest</code></td>
<td>
<p>covariance estimator, default is <code>"rMRCov"</code>. For a list of implemented estimators, use <code>listCholCovEstimators()</code>.</p>
</td></tr>
<tr><td><code id="rCholCov_+3A_criterion">criterion</code></td>
<td>
<p>criterion to use for sorting the data according to liquidity. 
Possible values are <code>"squared duration"</code>, <code>"duration"</code>, <code>"count"</code>, defaults to <code>"squared duration"</code>.</p>
</td></tr>
<tr><td><code id="rCholCov_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code>IVest</code> and <code>COVest</code>. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Additional arguments for <code>IVest</code> and <code>COVest</code> should be passed in the ... argument.
For the <code>rMRCov</code> estimator, which is the default, the <code>theta</code> and <code>delta</code> parameters can be set. These default to 1 and 0.1 respectively.
</p>
<p>The CholCov estimation algorithm is useful for estimating covariances of <code class="reqn">d</code> series that are sampled asynchronously and with different liquidities.
The CholCov estimation algorithm is as follows:
</p>

<ul>
<li><p> First sort the series in terms of decreasing liquidity according to a liquidity criterion, such that series <code class="reqn">1</code> is the most liquid, and series <code class="reqn">d</code> the least.
</p>
</li>
<li><p> Step 1:
</p>
<p>Apply refresh-time on <code class="reqn">{a} = \{1\}</code> to obtain the grid <code class="reqn">\tau^{a}</code>. 
</p>
<p>Estimate <code class="reqn">\hat{g}_{11}</code> using an IV estimator on  <code class="reqn">f_{\tau^{a}_j}^{(1)}= \hat{u}_{\tau^{a}_j}^{(1)}</code>.
</p>
</li>
<li><p> Step 2:
</p>
<p>Apply refresh-time on <code class="reqn">{b} = \{1,2\}</code> to obtain the grid <code class="reqn">\tau^{b}</code>. 
</p>
<p>Estimate <code class="reqn">\hat{h}^{b}_{21}</code> as the realized beta between <code class="reqn">f_{\tau^{b}_j}^{(1)}</code> and <code class="reqn">\hat{u}_{\tau^{b}_j}^{(2)}</code>. Set <code class="reqn">\hat{h}_{21}=\hat{h}^{b}_{21}</code>.
</p>
<p>Estimate <code class="reqn">\hat{g}_{22}</code> using an IV estimator on  <code class="reqn">f_{\tau^{b}_j}^{(2)}= \hat{u}_{\tau^{b}_j}^{(2)}-\hat{h}_{21}f_{\tau^{b}_j}^{(1)}</code>. 
</p>
</li>
<li><p>  Step 3:
</p>
<p>Apply refresh-time on <code class="reqn">{c} = \{1,3\}</code> to obtain the grid <code class="reqn">\tau^{c}</code>. 
</p>
<p>Estimate <code class="reqn">\hat{h}^{c}_{31}</code> as the realized beta between <code class="reqn">f_{\tau^{c}_j}^{(1)}</code> and <code class="reqn">\hat{u}_{\tau^{c}_j}^{(3)}</code>. Set <code class="reqn">\hat{h}_{31}= \hat{h}^{c}_{31}</code>.
</p>
<p>Apply refresh-time on <code class="reqn">{d} = \{1,2,3\}</code> to obtain the grid <code class="reqn">\tau^{d}</code>.
</p>
<p>Re-estimate <code class="reqn">\hat{h}_{21}^{d}</code> at the new grid, such that the projections <code class="reqn">f_{\tau^{d}_j}^{(1)}</code> and <code class="reqn">f_{\tau^{d}_j}^{(2)}</code> are orthogonal.
</p>
<p>Estimate <code class="reqn">\hat{h}^{d}_{32}</code> as the realized beta between <code class="reqn">f_{\tau^{d}_j}^{(2)}</code> and <code class="reqn">\hat{u}_{\tau^{d}_j}^{(3)}</code>. Set <code class="reqn">\hat{h}_{32} = \hat{h}^{d}_{32}</code>. 
</p>
<p>Estimate <code class="reqn">\hat{g}_{33}</code> using an IV estimator on <code class="reqn">f_{\tau^{d}_j}^{(3)}= \hat{u}_{\tau^{d}_j}^{(3)}-\hat{h}_{32}f_{\tau^{d}_j}^{(2)} -\hat{h}_{31}f_{\tau^{d}_j}^{(1)}</code>. 
</p>
</li>
<li><p> Step 4 to d:
</p>
<p>Continue in the same fashion by sampling over <code class="reqn">{1,...,k,l}</code> to estimate <code class="reqn">h_{lk}</code> using the smallest possible set. 
</p>
<p>Re-estimate the <code class="reqn">h_{nm}</code> with <code class="reqn">m&lt;n\leq k</code> at every new grid to obtain orthogonal projections. 
</p>
<p>Estimate the <code class="reqn">g_{kk}</code> as the IV of projections based on the final estimates, <code class="reqn">\hat{h}</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a list containing the covariance matrix <code>"CholCov"</code>, and the Cholesky decomposition <code>"L"</code> and <code>"G"</code> such that <code class="reqn">\code{L} \times \code{G} \times \code{L}' = \code{CholCov}</code>.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup
</p>


<h3>References</h3>

<p>Boudt, K., Laurent, S., Lunde, A., Quaedvlieg, R., and Sauri, O. (2017). Positive semidefinite integrated covariance estimation, factorizations and asynchronicity. <em>Journal of Econometrics</em>, 196, 347-367.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>

<hr>
<h2 id='rCov'>Realized covariance</h2><span id='topic+rCov'></span>

<h3>Description</h3>

<p>Function returns the Realized Covariation (rCov).
Let <code class="reqn">r_{t,i}</code> be an intraday <code class="reqn">N \times M</code> return vector and <code class="reqn">i=1,...,M</code>
the number of intraday returns.
</p>
<p>Then, the rCov is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mbox{rCov}_{t}=\sum_{i=1}^{M}r_{t,i}r'_{t,i}.
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an <code class="reqn">N \times N</code> matrix is returned. If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing N by N matrices. Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Realized Variance/Covariance for prices aligned at 5 minutes.

# Univariate:
rv = rCov(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
          alignPeriod = 5, makeReturns = TRUE)
rv

# Multivariate:
rc = rCov(rData = sampleOneMinuteData, makeReturns = TRUE)
rc
</code></pre>

<hr>
<h2 id='refreshTime'>Synchronize (multiple) irregular timeseries by refresh time</h2><span id='topic+refreshTime'></span>

<h3>Description</h3>

<p>This function implements the refresh time synchronization scheme proposed by Harris et al. (1995). 
It picks the so-called refresh times at which all assets have traded at least once since the last refresh time point. 
For example, the first refresh time corresponds to the first time at which all stocks have traded.
The subsequent refresh time is defined as the first time when all stocks have traded again.
This process is repeated until the end of one time series is reached.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>refreshTime(pData, sort = FALSE, criterion = "squared duration")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="refreshTime_+3A_pdata">pData</code></td>
<td>
<p>a list. Each list-item contains an <code>xts</code> or a <code>data.table</code> object (with first column DT (datetime)) containing the original time series (one day only and typically a price series).</p>
</td></tr>
<tr><td><code id="refreshTime_+3A_sort">sort</code></td>
<td>
<p>logical determining whether to sort the index based on a criterion (will only sort descending; i.e., most liquid first). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="refreshTime_+3A_criterion">criterion</code></td>
<td>
<p>character determining which criterion used. Currently supports <code>"squared duration"</code> and <code>"duration"</code>. Default is <code>"squared duration"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>xts</code> or <code>data.table</code> object containing the synchronized time series - depending on the input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Harris, F., T. McInish, Shoesmith, G., and Wood, R. (1995). Cointegration, error correction, and price discovery on informationally linked security markets. <em>Journal of Financial and Quantitative Analysis</em>, 30, 563-581.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Suppose irregular timepoints:
start &lt;- as.POSIXct("2010-01-01 09:30:00")
ta &lt;- start + c(1,2,4,5,9)
tb &lt;- start + c(1,3,6,7,8,9,10,11)

# Yielding the following timeseries:
a &lt;- xts::as.xts(1:length(ta), order.by = ta)
b &lt;- xts::as.xts(1:length(tb), order.by = tb)

# Calculate the synchronized timeseries:
refreshTime(list(a,b))

</code></pre>

<hr>
<h2 id='ReMeDI'>ReMeDI</h2><span id='topic+ReMeDI'></span>

<h3>Description</h3>

<p>This function estimates the auto-covariance of market-microstructure noise
</p>
<p>Let the observed price <code class="reqn">Y_{t}</code> be given as <code class="reqn">Y_{t} = X_{t} + \varepsilon_{t}</code>, where <code class="reqn">X_{t}</code> is the efficient price and <code class="reqn">\varepsilon_t</code> is the market microstructure noise
</p>
<p>The estimator of the <code class="reqn">l</code>'th lag of the market microstructure is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{R}^{n}_{t,l} = \frac{1}{n_{t}} \sum_{i=2k_{n}}^{n_{t}-k_{n}-l} \left(Y_{i+l}^n - Y_{i+l+k_{n}}^{n} \right) \left(Y_{i}^n - Y_{i- 2k_{n}}^{n} \right),
</code>
</p>

<p>where <code class="reqn">k_{n}</code> is a tuning parameter. In the function <code><a href="#topic+knChooseReMeDI">knChooseReMeDI</a></code>, we provide a function to estimate the optimal <code class="reqn">k_{n}</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReMeDI(pData, kn = 1, lags = 1, makeCorrelation = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReMeDI_+3A_pdata">pData</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing the log-prices of the asset</p>
</td></tr>
<tr><td><code id="ReMeDI_+3A_kn">kn</code></td>
<td>
<p>numeric of length 1 determining the tuning parameter kn this controls the lengths of the non-overlapping interval in the ReMeDI estimation</p>
</td></tr>
<tr><td><code id="ReMeDI_+3A_lags">lags</code></td>
<td>
<p>numeric containing integer values indicating the lags for which to estimate the (co)variance</p>
</td></tr>
<tr><td><code id="ReMeDI_+3A_makecorrelation">makeCorrelation</code></td>
<td>
<p>logical indicating whether to transform the autocovariances into autocorrelations. 
The estimate of variance is imprecise and thus, constructing the correlation like this may show correlations that fall outside <code class="reqn">(-1,1)</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>We Thank Merrick Li for contributing his Matlab code for this estimator.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>References</h3>

<p>Li, M. and Linton, O. (2021). A ReMeDI for microstructure noise. Econometrica, forthcoming
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

remed &lt;- ReMeDI(sampleTData[as.Date(DT) == "2018-01-02", ], kn = 2, lags = 1:8)
# We can also use the algorithm for choosing the kn tuning parameter
optimalKn &lt;- knChooseReMeDI(sampleTData[as.Date(DT) == "2018-01-02",],
                            knMax = 10, tol = 0.05, size = 3,
                            lower = 2, upper = 5, plot = TRUE)
optimalKn
remed &lt;- ReMeDI(sampleTData[as.Date(DT) == "2018-01-02", ], kn = optimalKn, lags = 1:8)

</code></pre>

<hr>
<h2 id='ReMeDIAsymptoticVariance'>Asymptotic variance of ReMeDI estimator</h2><span id='topic+ReMeDIAsymptoticVariance'></span>

<h3>Description</h3>

<p>Estimates the asymptotic variance of the ReMeDI estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReMeDIAsymptoticVariance(pData, kn, lags, phi, i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReMeDIAsymptoticVariance_+3A_pdata">pData</code></td>
<td>
<p><code>xts</code> or <code>data.table</code> containing the log-prices of the asset</p>
</td></tr>
<tr><td><code id="ReMeDIAsymptoticVariance_+3A_kn">kn</code></td>
<td>
<p>numerical value determining the tuning parameter kn this controls the lengths of the non-overlapping interval in the ReMeDI estimation</p>
</td></tr>
<tr><td><code id="ReMeDIAsymptoticVariance_+3A_lags">lags</code></td>
<td>
<p>numeric containing integer values indicating the lags for which to estimate the (co)variance</p>
</td></tr>
<tr><td><code id="ReMeDIAsymptoticVariance_+3A_phi">phi</code></td>
<td>
<p>tuning parameter phi</p>
</td></tr>
<tr><td><code id="ReMeDIAsymptoticVariance_+3A_i">i</code></td>
<td>
<p>tuning parameter i</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some notation is needed for the estimator of the asymptotic covariance of the ReMeDI estimator.
Let
</p>
<p style="text-align: center;"><code class="reqn">
    \delta\left(n, i\right) = t_{i}^{n}-t_{t-1}^{n}, i\geq 1,
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \hat{\delta}_{t}^{n}=\left(\frac{k_{n}\delta\left(n,i+1+k_{n}\right)-t_{i+2+2k_{n}}^{n}+t_{i+2+k_{n}}^{n}}{\left(t_{i+k_{n}}^{n}-t_{i}^{n}\right)\vee\phi_{n}}\right)^{2},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(1\right)_{t}^{n}=\sum_{i=0}^{n_{t}-\omega\left(1\right)_{n}}\hat{\delta}_{i}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(2,\boldsymbol{j}\right)_{t}^{n}=\sum_{i=0}^{n_{t}-\omega\left(2\right)_{n}}\hat{\delta}_{i}^{n}\Delta_{\boldsymbol{j}}\left(Y\right)_{i+\omega\left(2\right)_{2}^{n}}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(3,\boldsymbol{j},\boldsymbol{j}'\right)_{t}^{n}=\sum_{i=0}^{n_{t}-\omega\left(3\right)_{n}}\hat{\delta}_{i}^{n}\Delta_{\boldsymbol{j}}\left(Y\right)_{i+\omega\left(3\right)_{2}^{n}}^{n}\Delta_{\boldsymbol{j}'}\left(Y\right)_{i+\omega\left(3\right)_{3}^{n}}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(4;\boldsymbol{j},\boldsymbol{j}'\right)_{t}^{n}=-\sum_{i=2^{q-1}k_{n}}^{n_{t}-\omega\left(4\right)_{n}}\Delta_{\boldsymbol{j}}\left(Y\right)\Delta_{\boldsymbol{j}^{\prime}}\left(Y\right)_{i+\omega\left(3\right)_{3}^{n}}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(5,k;\boldsymbol{j},\boldsymbol{j}'\right)_{t}^{n}=\sum_{Q_{q}\in\mathcal{Q}_{q}}\sum_{i=2^{e\left(Q_{q}\right)}k_{n}}^{n_{t}-\omega\left(5\right)_{n}}\Delta_{\boldsymbol{j}_{Q_{q}\oplus\left(\boldsymbol{j}\prime_{Q_{q'}}\left(+k\right)\right)}}\left(Y\right)_{i}^{n}\prod_{\ell:l_{\ell}\in Q_{q}^{c}}\Delta_{\left(j_{l_{\ell}},j\prime_{l_{\ell}}+k\right)\left(Y\right)_{i+\omega\left(5\right)_{\ell+1}^{n}\prime}},
</code>
</p>

<p><code class="reqn">
    U\left(6,k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)=\sum_{j_{l}\in\boldsymbol{j},j_{l^{\prime}}^{\prime}\in\boldsymbol{j}^{\prime}}\sum_{i=2k_{n}}^{n_{t}-\omega\left(6\right)n}\Delta_{\left(j_{l},j_{l^{\prime}}^{\prime}+k\right)}\left(Y\right)_{i}^{n}\Delta_{\boldsymbol{j}_{-l}}\left(Y\right)_{i+\omega\left(6\right)_{2}^{n}}^{n}\Delta_{\boldsymbol{j}_{-l^{\prime}}^{\prime}}\left(Y\right)_{i+\omega\left(6\right)_{3}^{n}}^{n} \\
    -\sum_{j_{l}\in\boldsymbol{j}}\sum_{i=2^{q}k_{n}}^{n_{t}-\omega^{\prime}\left(6\right)_{n}}\Delta_{\left\{ j_{l}\right\} \oplus\boldsymbol{j}^{\prime}\left(+k\right)}\left(Y\right)_{i}^{n}\Delta_{\boldsymbol{j}-l}\left(Y\right)_{i+\omega^{\prime}\left(6\right)_{2}^{n}}^{n} \\
    -\sum_{j_{l^{\prime}\in\boldsymbol{j}^{\prime}}^{\prime}}\sum_{i=2^{q}k_{n}}^{n_{t}-\omega^{\prime\prime}\left(6\right)n}\Delta_{\left\{ j_{l^{\prime}}^{\prime}+k\right\} \oplus\boldsymbol{j}}\left(Y\right)_{i}^{n}\Delta_{\boldsymbol{j}_{-l^{\prime}}^{\prime}}\left(Y\right)_{i+\omega^{\prime\prime}\left(6\right)_{2}^{n}\prime}^{n},
</code>
</p>
<p style="text-align: center;"><code class="reqn">
    U\left(7,k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=ReMeDI\left(\boldsymbol{j}\oplus\boldsymbol{j}^{\prime}\left(+k\right)\right)_{t}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    U\left(k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=\sum_{\ell=5}^{7}U\left(\ell,k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n},
</code>
</p>
 
<p style="text-align: center;"><code class="reqn">
    U\left(k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=\sum_{\ell=5}^{7}U\left(\ell,k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n},
</code>
</p>

<p>Where the indices are given by:
</p>
<p style="text-align: center;"><code class="reqn">
    \omega\left(1\right)_{n}=2+2k_{n},\ \omega\left(2\right)_{2}^{n}=2+\left(3+2^{q-1}\right)k_{n},\ \omega\left(2\right)_{n}=\omega\left(2\right)_{2}^{n}+j_{1}+k_{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega\left(3\right)_{2}^{n}=2+\left(3+2^{q-1}\right)k_{n},\ \omega\left(3\right)_{3}^{n}=2+\left(5+2^{q-1}+2^{q^{\prime}-1}\right)k_{n}+j_{1},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega\left(3\right)_{n}=\omega\left(3\right)_{3}^{n}+j_{1}^{\prime}+k_{n},\ \omega\left(4\right)_{2}^{n}=2k_{n}+q_{n}^{\prime}+j_{1},\ \omega\left(4\right)_{n}=\omega\left(4\right)_{2}^{n}+j_{1}^{\prime}+k_{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    e\left(Q_{q}\right)=\left(2\left|Q_{q}\right|+q^{\prime}-q-1\right)\vee1,\ \omega\left(5\right)_{\ell+1}^{n}=4\ell k_{n}+\sum_{\ell^{\prime}=1}^{\ell}j_{l_{\ell^{\prime}}}\vee\left(j_{l_{\ell}}^{\prime}+k\right)\textrm{for}\ell\geq 1,
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega\left(5\right)_{n}=\omega\left(5\right)_{\left|Q_{q}^{c}\right|+1}^{n}+j_{l_{\left|Q_{q}^{c}\right|}}\vee\left(j_{l_{\left|Q_{q}^{c}\right|}}+k\right)+k_{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega\left(6\right)_{2}^{n}=\left(2^{q-2}+2\right)k_{n}+j_{\ell}\vee\left(j_{\ell^{\prime}}^{\prime}+k\right),\ \omega\left(6\right)_{3}^{n}=\left(2^{q-2}+2^{q^{\prime}-2}+2\right)k_{n}+j_{1}+j_{\ell}\vee\left(j_{\ell}^{\prime}+k\right),
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega^{\prime}\left(6\right)_{2}^{n}=\left(2^{q-2}+2\right)k_{n}+j_{\ell}\vee\left(j_{1}^{\prime}+k\right),\ \omega^{\prime\prime}\left(6\right)_{2}^{n}=\left(2^{q^{\prime}-2}+1\right)k_{n}+\left(j_{\ell^{\prime}}^{\prime}+k\right)\vee j_{1},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \omega\left(6\right)_{n}=\omega\left(6\right)_{3}^{n}+j^{\prime}+k_{n},\ \omega^{\prime}\left(6\right)_{n}=\omega^{\prime}\left(6\right)_{2}^{n}+j_{1}+k_{n},\ \omega^{\prime\prime}\left(6\right)_{n}=\omega^{\prime\prime}\left(6\right)_{2}^{n}j_{1}^{\prime}+k_{n},
</code>
</p>

<p>The asymptotic variance estimator is then given by
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\sigma}\left(\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=\frac{1}{n_{t}}\sum_{\ell=1}^{3}\hat{\sigma}_{\ell}\left(\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n},
</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\sigma}_{1}\left(\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=U\left(0;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)+\sum_{k=1}^{i_{n}}\left(U\left(k;\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}\right)+\left(2i_{n}+1\right)U\left(4;\boldsymbol{j},\boldsymbol{j}\right)_{t}^{n},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \hat{\sigma}_{2}\left(\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=U\left(3;\boldsymbol{j},\boldsymbol{j}^{\prime}\right),
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    \hat{\sigma}_{3}\left(\boldsymbol{j},\boldsymbol{j}^{\prime}\right)_{t}^{n}=\frac{1}{n_{t}^{2}}\textrm{ReMeDI}\left(Y,\boldsymbol{j}\right)_{t}^{n}\textrm{ReMeDI}\left(Y,\boldsymbol{j}^{\prime}\right)_{t}^{n}U\left(1\right)_{t}^{n}\\,
</code>
</p>

<p style="text-align: center;"><code class="reqn">
    -\frac{1}{n_{t}}\left(\textrm{ReMeDI}\left(Y,\boldsymbol{j}\right)_{t}^{n}U\left(2,\boldsymbol{j}^{\prime}\right)_{t}^{n}+\textrm{ReMeDI}\left(Y,\boldsymbol{j}^{\prime}\right)_{t}^{n}U\left(2,\boldsymbol{j}\right)_{t}^{n}\right),
</code>
</p>



<h3>Value</h3>

<p>a list with components <code>ReMeDI</code> and <code>asympVar</code> containing the ReMeDI estimation and it's asymptotic variance respectively
</p>


<h3>Note</h3>

<p>We Thank Merrick Li for contributing his Matlab code for this estimator.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

kn &lt;- knChooseReMeDI(sampleTDataEurope[, list(DT, PRICE)])

remedi &lt;- ReMeDI(sampleTDataEurope[, list(DT, PRICE)], kn = kn, lags = 0:15)

asympVar &lt;- ReMeDIAsymptoticVariance(sampleTDataEurope[, list(DT, PRICE)], 
                                     kn = kn, lags = 0:15, phi = 0.9, i = 2)

</code></pre>

<hr>
<h2 id='rHYCov'>Hayashi-Yoshida covariance</h2><span id='topic+rHYCov'></span>

<h3>Description</h3>

<p>Calculates the Hayashi-Yoshida Covariance estimator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rHYCov(
  rData,
  cor = FALSE,
  period = 1,
  alignBy = "seconds",
  alignPeriod = 1,
  makeReturns = FALSE,
  makePsd = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rHYCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_period">period</code></td>
<td>
<p>Sampling period</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rHYCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_makepsd">makePsd</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, the positive definite version of rHYCov is returned. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rHYCov_+3A_...">...</code></td>
<td>
<p>used internally. Do not set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Payseur and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Hayashi, T. and Yoshida, N. (2005). On covariance estimation of non-synchronously observed diffusion processes. <em>Bernoulli</em>, 11, 359-379.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("xts")
hy &lt;- rHYCov(rData = as.xts(sampleOneMinuteData)["2001-08-05"],
             period = 5, alignBy = "minutes", alignPeriod = 5, makeReturns = TRUE)
</code></pre>

<hr>
<h2 id='rKernelCov'>Realized kernel estimator</h2><span id='topic+rKernelCov'></span>

<h3>Description</h3>

<p>Realized covariance calculation using a kernel estimator. 
The different types of kernels available can be found using <code><a href="#topic+listAvailableKernels">listAvailableKernels</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rKernelCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  kernelType = "rectangular",
  kernelParam = 1,
  kernelDOFadj = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rKernelCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. 
<code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. 
For example, to aggregate based on a 5-minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_kerneltype">kernelType</code></td>
<td>
<p>Kernel name.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_kernelparam">kernelParam</code></td>
<td>
<p>Kernel parameter.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_kerneldofadj">kernelDOFadj</code></td>
<td>
<p>Kernel degree of freedom adjustment.</p>
</td></tr>
<tr><td><code id="rKernelCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">r_{t,i}</code> be <code class="reqn">N</code> returns in period <code class="reqn">t</code>, <code class="reqn">i = 1, \ldots, N</code>. The returns or prices 
do not have to be equidistant. The kernel estimator for <code class="reqn">H = \code{kernelParam}</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
\gamma_0 + 2 \sum_{h = 1}^H k \left(\frac{h-1}{H}\right) \gamma_h,
</code>
</p>

<p>where <code class="reqn">k(x)</code> is the chosen kernel function and 
</p>
<p style="text-align: center;"><code class="reqn">
\gamma_h = \sum_{i = h}^N r_{t,i} \times r_{t,i-h}
</code>
</p>

<p>is the empirical autocovariance function. The multivariate version employs the cross-covariances instead.
</p>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an <code class="reqn">N</code> by <code class="reqn">N</code> matrix is returned. 
If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing <code class="reqn">N</code> by <code class="reqn">N</code> matrices. 
Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Scott Payseur, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Hansen, P. R., Lunde, A., and Shephard, N. (2008). Designing realized kernels to measure the ex post variation of equity prices in the presence of noise. <em>Econometrica</em>, 76, 1481-1536.
</p>
<p>Hansen, P. and Lunde, A. (2006). Realized variance and market microstructure noise. <em>Journal of Business and Economic Statistics</em>, 24, 127-218.
</p>
<p>Zhou., B. (1996). High-frequency data and volatility in foreign-exchange rates. <em>Journal of Business &amp; Economic Statistics</em>, 14, 45-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Univariate:
rvKernel &lt;- rKernelCov(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
                       alignPeriod = 5, makeReturns = TRUE)
rvKernel

# Multivariate:
rcKernel &lt;- rKernelCov(rData = sampleOneMinuteData, makeReturns = TRUE)
rcKernel
</code></pre>

<hr>
<h2 id='rKurt'>Realized kurtosis of highfrequency return series.</h2><span id='topic+rKurt'></span>

<h3>Description</h3>

<p>Calculate the realized kurtosis as defined in Amaya et al. (2015).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>. Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i=1, \ldots,N</code>) in period <code class="reqn">t</code>.
Then, <code>rKurt</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{rKurt}_{t} = \frac{N \sum_{i=1}^{N}(r_{t,i})^4}{\left( \sum_{i=1}^N r_{t,i}^2 \right)^2}.
  </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rKurt(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rKurt_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rKurt_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rKurt_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rKurt_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Amaya, D., Christoffersen, P., Jacobs, K., and Vasquez, A. (2015). Does realized skewness and kurtosis predict the cross-section of equity returns? <em>Journal of Financial Economics</em>, 118, 135-167.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rk &lt;- rKurt(sampleTData[, list(DT, PRICE)], alignBy = "minutes",
            alignPeriod = 5, makeReturns = TRUE)
rk
</code></pre>

<hr>
<h2 id='rMedRQ'>DEPRECATED</h2><span id='topic+rMedRQ'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMedRQuar">rMedRQuar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMedRQ(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMedRQ_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRQuar">rMedRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMedRQ_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRQuar">rMedRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMedRQ_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRQuar">rMedRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMedRQ_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRQuar">rMedRQuar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rMedRQuar'>An estimator of integrated quarticity from applying the median operator on blocks of three returns</h2><span id='topic+rMedRQuar'></span>

<h3>Description</h3>

<p>Calculate the rMedRQ, defined in Andersen et al. (2012). Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>. 
Then, the rMedRQ is given by
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{rMedRQ}_{t}=\frac{3\pi N}{9\pi +72 - 52\sqrt{3}} \left(\frac{N}{N-2}\right) \sum_{i=2}^{N-1} \mbox{med}(|r_{t,i-1}|, |r_{t,i}|, |r_{t,i+1}|)^4.
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rMedRQuar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMedRQuar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rMedRQuar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rMedRQuar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod</code> to 5 and <code>alignBy</code> to <code>"minutes"</code>.</p>
</td></tr>
<tr><td><code id="rMedRQuar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rq &lt;- rMedRQuar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
            alignPeriod = 5, makeReturns = TRUE)
rq
</code></pre>

<hr>
<h2 id='rMedRV'>DEPRECATED</h2><span id='topic+rMedRV'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMedRVar">rMedRVar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMedRV(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMedRV_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRVar">rMedRVar</a></code></p>
</td></tr>
<tr><td><code id="rMedRV_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRVar">rMedRVar</a></code></p>
</td></tr>
<tr><td><code id="rMedRV_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRVar">rMedRVar</a></code></p>
</td></tr>
<tr><td><code id="rMedRV_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMedRVar">rMedRVar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rMedRVar'>rMedRVar</h2><span id='topic+rMedRVar'></span>

<h3>Description</h3>

<p>Calculate the rMedRVar, defined in Andersen et al. (2012). 
Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i=1,\ldots,M</code>) in period <code class="reqn">t</code>.
Then, the rMedRVar is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mbox{rMedRVar}_{t}=\frac{\pi}{6-4\sqrt{3}+\pi}\left(\frac{M}{M-2}\right) \sum_{i=2}^{M-1} \mbox{med}(|r_{t,i-1}|,|r_{t,i}|, |r_{t,i+1}|)^2
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rMedRVar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMedRVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days</p>
</td></tr>
<tr><td><code id="rMedRVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rMedRVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rMedRVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rMedRVar_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The rMedRVar belongs to the class of realized volatility measures in this package
that use the series of high-frequency returns <code class="reqn">r_{t,i}</code> of a day <code class="reqn">t</code>
to produce an ex post estimate of the realized volatility of that day <code class="reqn">t</code>.
rMedRVar is designed to be robust to price jumps.
The difference between RV and rMedRVar is an estimate of the realized jump
variability. Disentangling the continuous and jump components in RV
can lead to more precise volatility forecasts,
as shown in Andersen et al. (2012)
</p>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>medrv &lt;- rMedRVar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
               alignPeriod = 5, makeReturns = TRUE)
medrv
</code></pre>

<hr>
<h2 id='rMinRQ'>DEPRECATED</h2><span id='topic+rMinRQ'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMinRQuar">rMinRQuar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMinRQ(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMinRQ_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRQuar">rMinRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMinRQ_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRQuar">rMinRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMinRQ_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRQuar">rMinRQuar</a></code></p>
</td></tr>
<tr><td><code id="rMinRQ_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRQuar">rMinRQuar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rMinRQuar'>An estimator of integrated quarticity from applying the minimum operator on blocks of two returns</h2><span id='topic+rMinRQuar'></span>

<h3>Description</h3>

<p>Calculate the rMinRQuar, defined in Andersen et al. (2012).
Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>.
Then, the rMinRQuar is given by
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{rMinRQuar}_{t}=\frac{\pi N}{3 \pi - 8} \left(\frac{N}{N-1}\right) \sum_{i=1}^{N-1} \mbox{min}(|r_{t,i}| ,|r_{t,i+1}|)^4
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rMinRQuar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMinRQuar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days</p>
</td></tr>
<tr><td><code id="rMinRQuar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rMinRQuar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rMinRQuar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rq &lt;- rMinRQuar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
            alignPeriod = 5, makeReturns = TRUE)
rq
</code></pre>

<hr>
<h2 id='rMinRV'>DEPRECATED</h2><span id='topic+rMinRV'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMinRVar">rMinRVar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMinRV(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMinRV_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRVar">rMinRVar</a></code></p>
</td></tr>
<tr><td><code id="rMinRV_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRVar">rMinRVar</a></code></p>
</td></tr>
<tr><td><code id="rMinRV_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRVar">rMinRVar</a></code></p>
</td></tr>
<tr><td><code id="rMinRV_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMinRVar">rMinRVar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rMinRVar'>rMinRVar</h2><span id='topic+rMinRVar'></span>

<h3>Description</h3>

<p>Calculate the rMinRVar, defined in Andersen et al. (2009).
Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i=1,\ldots,M</code>) in period <code class="reqn">t</code>.
Then, the rMinRVar is given by
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{rMinRVar}_{t}=\frac{\pi}{\pi - 2}\left(\frac{M}{M-1}\right) \sum_{i=1}^{M-1} \mbox{min}(|r_{t,i}| ,|r_{t,i+1}|)^2
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rMinRVar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMinRVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rMinRVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rMinRVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rMinRVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rMinRVar_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>minrv &lt;- rMinRVar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
               alignPeriod = 5, makeReturns = TRUE)
minrv
</code></pre>

<hr>
<h2 id='rmLargeSpread'>Delete entries for which the spread is more than <code>maxi</code> times the median spread</h2><span id='topic+rmLargeSpread'></span>

<h3>Description</h3>

<p>Function deletes entries for which the spread is more than <code>"maxi"</code> times the median
spread on that day.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmLargeSpread(qData, maxi = 50, tz = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmLargeSpread_+3A_qdata">qData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object at least containing the columns <code>"BID"</code> and <code>"OFR"</code>.</p>
</td></tr>
<tr><td><code id="rmLargeSpread_+3A_maxi">maxi</code></td>
<td>
<p>an integer. By default <code>maxi = "50"</code>, which means that entries are deleted 
if the spread is more than 50 times the median spread on that day.</p>
</td></tr>
<tr><td><code id="rmLargeSpread_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. With the non-disk functionality, we attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>. 
In the on-disk functionality, if tz is not specified, the timezone used will be the system default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>

<hr>
<h2 id='rmNegativeSpread'>Delete entries for which the spread is negative</h2><span id='topic+rmNegativeSpread'></span>

<h3>Description</h3>

<p>Function deletes entries for which the spread is negative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmNegativeSpread(qData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmNegativeSpread_+3A_qdata">qData</code></td>
<td>
<p>an <code>xts</code> object at least containing the columns &quot;BID&quot; and &quot;OFR&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.table</code> or <code>xts</code> object
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt and Onno Kleen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rmNegativeSpread(sampleQDataRaw)

</code></pre>

<hr>
<h2 id='rmOutliersQuotes'>Remove outliers in quotes</h2><span id='topic+rmOutliersQuotes'></span>

<h3>Description</h3>

<p>Delete entries for which the mid-quote is outlying with respect to surrounding entries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmOutliersQuotes(qData, maxi = 10, window = 50, type = "advanced", tz = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmOutliersQuotes_+3A_qdata">qData</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object at least containing the columns <code>"BID"</code> and <code>"OFR"</code>.</p>
</td></tr>
<tr><td><code id="rmOutliersQuotes_+3A_maxi">maxi</code></td>
<td>
<p>an integer, indicating the maximum number of median absolute deviations allowed.</p>
</td></tr>
<tr><td><code id="rmOutliersQuotes_+3A_window">window</code></td>
<td>
<p>an integer, indicating the time window for which the &quot;outlyingness&quot; is considered.</p>
</td></tr>
<tr><td><code id="rmOutliersQuotes_+3A_type">type</code></td>
<td>
<p>should be <code>"standard"</code> or <code>"advanced"</code> (see details).</p>
</td></tr>
<tr><td><code id="rmOutliersQuotes_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
With the non-disk functionality, we attempt to extract the timezone from the <code>DT</code> column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If <code>type = "standard"</code>: Function deletes entries for which the mid-quote deviated by more than &quot;maxi&quot;
median absolute deviations from a rolling centered median (excluding
the observation under consideration) of window observations.
</p>
</li>
<li><p> If <code>type = "advanced"</code>:  Function deletes entries for which the mid-quote deviates by more than &quot;maxi&quot;
median absolute deviations from the value closest to the mid-quote of
these three options:
</p>

<ol>
<li><p> Rolling centered median (excluding the observation under consideration)
</p>
</li>
<li><p> Rolling median of the following window of observations
</p>
</li>
<li><p> Rolling median of the previous window of observations
</p>
</li></ol>

<p>The advantage of this procedure compared to the &quot;standard&quot; proposed
by Barndorff-Nielsen et al. (2010) is that it will not incorrectly remove
large price jumps. Therefore this procedure has been set as the default
for removing outliers. 
</p>
<p>Note that the median absolute deviation is taken over the entire
day. In case it is zero (which can happen if mid-quotes don't change much), 
the median absolute deviation is taken over a subsample without constant mid-quotes.
</p>
</li></ul>



<h3>Value</h3>

<p><code>xts</code> object or <code>data.table</code> depending on type of input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen and Kris Boudt.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., P. R. Hansen, A. Lunde, and N. Shephard (2009). Realized kernels in practice: Trades and quotes. <em>Econometrics Journal</em>, 12, C1-C32.
</p>
<p>Brownlees, C.T., and Gallo, G.M. (2006). Financial econometric analysis at ultra-high frequency: Data handling concerns. <em>Computational Statistics &amp; Data Analysis</em>, 51, 2232-2245.
</p>

<hr>
<h2 id='rmOutliersTrades'>Remove outliers in trades without using quote data</h2><span id='topic+rmOutliersTrades'></span>

<h3>Description</h3>

<p>Delete entries for which the price is outlying with respect to surrounding entries. 
In comparison to <a href="#topic+tradesCleanupUsingQuotes">tradesCleanupUsingQuotes</a>, this function doesn't need quote data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmOutliersTrades(pData, maxi = 10, window = 50, type = "advanced", tz = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmOutliersTrades_+3A_pdata">pData</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object at least containing the column <code>"PRICE"</code>.</p>
</td></tr>
<tr><td><code id="rmOutliersTrades_+3A_maxi">maxi</code></td>
<td>
<p>an integer, indicating the maximum number of median absolute deviations allowed.</p>
</td></tr>
<tr><td><code id="rmOutliersTrades_+3A_window">window</code></td>
<td>
<p>an integer, indicating the time window for which the &quot;outlyingness&quot; is considered.</p>
</td></tr>
<tr><td><code id="rmOutliersTrades_+3A_type">type</code></td>
<td>
<p>should be <code>"standard"</code> or <code>"advanced"</code> (see details).</p>
</td></tr>
<tr><td><code id="rmOutliersTrades_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
With the non-disk functionality, we attempt to extract the timezone from the <code>DT</code> column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If <code>type = "standard"</code>: Function deletes entries for which the price deviated by more than &quot;maxi&quot;
median absolute deviations from a rolling centered median (excluding
the observation under consideration) of window observations.
</p>
</li>
<li><p> If <code>type = "advanced"</code>:  Function deletes entries for which the price deviates by more than &quot;maxi&quot;
median absolute deviations from the value closest to the price of
these three options:
</p>

<ol>
<li><p> Rolling centered median (excluding the observation under consideration)
</p>
</li>
<li><p> Rolling median of the following window of observations
</p>
</li>
<li><p> Rolling median of the previous window of observations
</p>
</li></ol>

<p>The advantage of this procedure compared to the &quot;standard&quot; proposed
by Barndorff-Nielsen et al. (2010, footnote 8) is that it will not incorrectly remove
large price jumps. Therefore this procedure has been set as the default
for removing outliers. 
</p>
<p>Note that the median absolute deviation is taken over the entire
day. In case it is zero (which can happen if prices don't change much), 
the median absolute deviation is taken over a subsample without constant prices.
</p>
</li></ul>



<h3>Value</h3>

<p><code>xts</code> object or <code>data.table</code> depending on type of input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Onno Kleen.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., P. R. Hansen, A. Lunde, and N. Shephard (2009). Realized kernels in practice: Trades and quotes. <em>Econometrics Journal</em>, 12, C1-C32.
</p>

<hr>
<h2 id='rMPV'>DEPRECATED</h2><span id='topic+rMPV'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMPVar">rMPVar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMPV(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMPV_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="rMPV_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="rMPV_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="rMPV_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
</table>

<hr>
<h2 id='rMPVar'>Realized multipower variation</h2><span id='topic+rMPVar'></span>

<h3>Description</h3>

<p>Calculate the Realized Multipower Variation rMPVar, defined in Andersen et al. (2012).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>. Then, the rMPVar is given by
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{rMPVar}_{N}(m,p)= d_{m,p} \frac{N^{p/2}}{N-m+1} \sum_{i=1}^{N-m+1}|r_{t,i}|^{p/m} \ldots |r_{t,i+m-1}|^{p/m}
  </code>
</p>

<p>in which
</p>
<p><code class="reqn">d_{m,p} = \mu_{p/m}^{-m}</code>:
</p>
<p><code class="reqn">m</code>: the window size of return blocks;
</p>
<p><code class="reqn">p</code>: the power of the variation;
</p>
<p>and <code class="reqn">m</code> &gt; <code class="reqn">p/2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMPVar(
  rData,
  m = 2,
  p = 2,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMPVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rMPVar_+3A_m">m</code></td>
<td>
<p>the window size of return blocks. 2 by default.</p>
</td></tr>
<tr><td><code id="rMPVar_+3A_p">p</code></td>
<td>
<p>the power of the variation. 2 by default.</p>
</td></tr>
<tr><td><code id="rMPVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rMPVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rMPVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rMPVar_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mpv &lt;- rMPVar(sampleTData[, list(DT, PRICE)], m = 2, p = 3, alignBy = "minutes",
            alignPeriod = 5, makeReturns = TRUE)
mpv
</code></pre>

<hr>
<h2 id='rMRC'>DEPRECATED rMRC</h2><span id='topic+rMRC'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMRC(pData, pairwise = FALSE, makePsd = FALSE, theta = 0.8, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMRC_+3A_pdata">pData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code></p>
</td></tr>
<tr><td><code id="rMRC_+3A_pairwise">pairwise</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code></p>
</td></tr>
<tr><td><code id="rMRC_+3A_makepsd">makePsd</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code></p>
</td></tr>
<tr><td><code id="rMRC_+3A_theta">theta</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code></p>
</td></tr>
<tr><td><code id="rMRC_+3A_...">...</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rMRCov">rMRCov</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rMRCov'>Modulated realized covariance</h2><span id='topic+rMRCov'></span>

<h3>Description</h3>

<p>Calculate univariate or multivariate pre-averaged estimator, as defined in Hautsch and Podolskij (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMRCov(
  pData,
  pairwise = FALSE,
  makePsd = FALSE,
  theta = 0.8,
  crossAssetNoiseCorrection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMRCov_+3A_pdata">pData</code></td>
<td>
<p>a list. Each list-item contains an <code>xts</code> or <code>data.table</code> object with the intraday price data of a stock.</p>
</td></tr>
<tr><td><code id="rMRCov_+3A_pairwise">pairwise</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when refresh times are based on pairs of assets. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rMRCov_+3A_makepsd">makePsd</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, the positive definite version of rMRCov is returned. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rMRCov_+3A_theta">theta</code></td>
<td>
<p>a <code>numeric</code> controlling the preaveraging horizon. Detaults to <code>0.8</code> as recommended by Hautsch and Podolskij (2013)</p>
</td></tr>
<tr><td><code id="rMRCov_+3A_crossassetnoisecorrection">crossAssetNoiseCorrection</code></td>
<td>
<p>a <code>logical</code> denoting whether to apply the bias correction term on the off-diagonals (covariance) terms. 
We set this to <code>FALSE</code> by default as noise is typically seen as independent across assets.</p>
</td></tr>
<tr><td><code id="rMRCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In practice, market microstructure noise leads to a departure from the pure semimartingale model. We consider the process <code class="reqn">Y</code> in period <code class="reqn">\tau</code>:
</p>
<p style="text-align: center;"><code class="reqn">
      \mbox{Y}_{\tau} = X_{\tau} + \epsilon_{\tau},
    </code>
</p>

<p>where the observed <code class="reqn">d</code> dimensional log-prices are the sum of underlying Brownian semimartingale process <code class="reqn">X</code> and a noise term <code class="reqn">\epsilon_{\tau}</code>.
</p>
<p><code class="reqn">\epsilon_{\tau}</code> is an <em>i.i.d.</em> process with <code class="reqn">X</code>.
</p>
<p>It is intuitive that under mean zero <em>i.i.d.</em> microstructure noise some form of smoothing of the observed log-price should tend to diminish the impact of the noise.
Effectively, we are going to approximate a continuous function by an average of observations of <code class="reqn">Y</code> in a neighborhood, the noise being averaged away.
</p>
<p>Assume there is <code class="reqn">N</code> equispaced returns in period <code class="reqn">\tau</code> of a list (after refreshing data). 
Let <code class="reqn">r_{\tau_i}</code> be a return (with <code class="reqn">i=1, \ldots,N</code>) of an asset in period <code class="reqn">\tau</code>. Assume there is <code class="reqn">d</code> assets.
</p>
<p>In order to define the univariate pre-averaging estimator, we first define the pre-averaged returns as
</p>
<p style="text-align: center;"><code class="reqn">
    \bar{r}_{\tau_j}^{(k)}= \sum_{h=1}^{k_N-1}g\left(\frac{h}{k_N}\right)r_{\tau_{j+h}}^{(k)}
  </code>
</p>

<p>where g is a non-zero real-valued function <code class="reqn">g:[0,1]</code> <code class="reqn">\rightarrow</code> <code class="reqn">R</code> given by <code class="reqn">g(x)</code> = <code class="reqn">\min(x,1-x)</code>. <code class="reqn">k_N</code> is a sequence of integers satisfying  <code class="reqn">\mbox{k}_{N} = \lfloor\theta N^{1/2}\rfloor</code>.
We use <code class="reqn">\theta = 0.8</code> as recommended in Hautsch and Podolskij (2013). The pre-averaged returns are simply a weighted average over the returns in a local window.
This averaging diminishes the influence of the noise. The order of the window size <code class="reqn">k_n</code> is chosen to lead to optimal convergence rates.
The pre-averaging estimator is then simply the analogue of the realized variance but based on pre-averaged returns and an additional term to remove bias due to noise
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{C}= \frac{N^{-1/2}}{\theta \psi_2}\sum_{i=0}^{N-k_N+1}  (\bar{r}_{\tau_i})^2-\frac{\psi_1^{k_N}N^{-1}}{2\theta^2\psi_2^{k_N}}\sum_{i=0}^{N}r_{\tau_i}^2
  </code>
</p>

<p>with
</p>
<p style="text-align: center;"><code class="reqn">
    \psi_1^{k_N}= k_N \sum_{j=1}^{k_N}\left(g\left(\frac{j+1}{k_N}\right)-g\left(\frac{j}{k_N}\right)\right)^2,\quad
  </code>
</p>

<p style="text-align: center;"><code class="reqn">
    \psi_2^{k_N}= \frac{1}{k_N}\sum_{j=1}^{k_N-1}g^2\left(\frac{j}{k_N}\right).
  </code>
</p>

<p style="text-align: center;"><code class="reqn">
    \psi_2= \frac{1}{12}
  </code>
</p>

<p>The multivariate counterpart is very similar. The estimator is called the Modulated Realized Covariance (rMRCov) and is defined as
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{MRC}= \frac{N}{N-k_N+2}\frac{1}{\psi_2k_N}\sum_{i=0}^{N-k_N+1}\bar{\boldsymbol{r}}_{\tau_i}\cdot \bar{\boldsymbol{r}}'_{\tau_i} -\frac{\psi_1^{k_N}}{\theta^2\psi_2^{k_N}}\hat{\Psi}
</code>
</p>

<p>where <code class="reqn">\hat{\Psi}_N = \frac{1}{2N}\sum_{i=1}^N \boldsymbol{r}_{\tau_i}(\boldsymbol{r}_{\tau_i})'</code>. It is a bias correction to make it consistent.
However, due to this correction, the estimator is not ensured PSD.
An alternative is to slightly enlarge the bandwidth such that <code class="reqn">\mbox{k}_{N} = \lfloor\theta N^{1/2+\delta}\rfloor</code>. <code class="reqn">\delta = 0.1</code> results in a consistent estimate without the bias correction and a PSD estimate, in which case:
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{MRC}^{\delta}= \frac{N}{N-k_N+2}\frac{1}{\psi_2k_N}\sum_{i=0}^{N-k_N+1}\bar{\boldsymbol{r}}_i\cdot \bar{\boldsymbol{r}}'_i
</code>
</p>



<h3>Value</h3>

<p>A <code class="reqn">d \times d</code> covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Hautsch, N., and Podolskij, M. (2013). Preaveraging-based estimation of quadratic variation in the presence of noise and jumps: theory, implementation, and empirical Evidence. <em>Journal of Business &amp; Economic Statistics</em>, 31, 165-183.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("xts")
# Note that this ought to be tick-by-tick data and this example is only to show the usage.
a &lt;- list(as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, MARKET)]),
          as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, STOCK)]))
rMRCov(a, pairwise = TRUE, makePsd = TRUE)


# We can also add use data.tables and use a named list to convey asset names
a &lt;- list(foo = sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, MARKET)],
          bar = sampleOneMinuteData[as.Date(DT) == "2001-08-04", list(DT, STOCK)])
rMRCov(a, pairwise = TRUE, makePsd = TRUE)


## End(Not run)
</code></pre>

<hr>
<h2 id='rmTradeOutliersUsingQuotes'>Delete transactions with unlikely transaction prices</h2><span id='topic+rmTradeOutliersUsingQuotes'></span>

<h3>Description</h3>

<p>Function deletes entries with prices that are above the ask plus the bid-ask spread.
Similar for entries with prices below the bid minus the bid-ask spread.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmTradeOutliersUsingQuotes(
  tData,
  qData,
  lagQuotes = 0,
  nSpreads = 1,
  BFM = FALSE,
  backwardsWindow = 3600,
  forwardsWindow = 0.5,
  plot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_tdata">tData</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object containing the time series data, with at least the column <code>"PRICE"</code>, containing the transaction price.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_qdata">qData</code></td>
<td>
<p>a <code>data.table</code> or <code>xts</code> object containing the time series data with at least the columns <code>"BID"</code> and <code>"OFR"</code>, containing the bid and ask prices.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_lagquotes">lagQuotes</code></td>
<td>
<p>numeric, number of seconds the quotes are registered faster than
the trades (should be round and positive). Default is 0. For older datasets, i.e. before 2010, it may be a good idea to set this to e.g. 2. See Vergote (2005)</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_nspreads">nSpreads</code></td>
<td>
<p>numeric of length 1 denotes how far above the offer and below bid we allow outliers to be. Trades are filtered out if they are MORE THAN nSpread * spread above (below) the offer (bid)</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_bfm">BFM</code></td>
<td>
<p>a logical determining whether to conduct 'Backwards - Forwards matching' of trades and quotes.
The algorithm tries to match trades that fall outside the bid - ask and first tries to match a small window forwards and if this fails, it tries to match backwards in a bigger window.
The small window is a tolerance for inaccuracies in the timestamps of bids and asks. The backwards window allow for matching of late reported trades, i.e. block trades.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_backwardswindow">backwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the backwards window. Default is 3600, corresponding to one hour.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_forwardswindow">forwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the forwards window. Default is 0.5, corresponding to one half second.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_plot">plot</code></td>
<td>
<p>a logical denoting whether to visualize the forwards, backwards, and unmatched trades in a plot.</p>
</td></tr>
<tr><td><code id="rmTradeOutliersUsingQuotes_+3A_...">...</code></td>
<td>
<p>used internally</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: in order to work correctly, the input data of this function should be
cleaned trade (tData) and quote (qData) data respectively.
In older high frequency datasets the trades frequently lag the quotes. In newer datasets this tends to happen 
only during extreme market activity when exchange networks are at maximum capacity.
</p>


<h3>Value</h3>

<p><code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Vergote, O. (2005). How to match trades and quotes for NYSE stocks? K.U.Leuven working paper.
</p>
<p>Christensen, K., Oomen, R. C. A., Podolskij, M. (2014): Fact or Friction: Jumps at ultra high frequency. <em>Journal of Financial Economics</em>, 144, 576-599
</p>

<hr>
<h2 id='rOWCov'>Realized outlyingness weighted covariance</h2><span id='topic+rOWCov'></span>

<h3>Description</h3>

<p>Calculate the Realized Outlyingness Weighted Covariance (rOWCov), defined in Boudt et al. (2008).
</p>
<p>Let <code class="reqn">r_{t,i}</code>, for <code class="reqn">i = 1,..., M</code> be a sample
of <code class="reqn">M</code> high-frequency <code class="reqn">(N \times 1)</code> return vectors and <code class="reqn">d_{t,i}</code>
their outlyingness given by the squared Mahalanobis distance between
the return vector and zero in terms of the reweighted MCD covariance
estimate based on these returns.
</p>
<p>Then, the rOWCov is given by
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{rOWCov}_{t}=c_{w}\frac{\sum_{i=1}^{M}w(d_{t,i})r_{t,i}r'_{t,i}}{\frac{1}{M}\sum_{i=1}^{M}w(d_{t,i})},
</code>
</p>

<p>The weight  <code class="reqn">w_{i,\Delta}</code> is one if the multivariate jump test statistic for <code class="reqn">r_{i,\Delta}</code> in Boudt et al. (2008) is less
than the 99.9% percentile of the chi-square distribution with <code class="reqn">N</code> degrees of freedom and zero otherwise.
The scalar <code class="reqn">c_{w}</code> is a correction factor ensuring consistency of the rOWCov for the Integrated Covariance,
under the Brownian Semimartingale with Finite Activity Jumps model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rOWCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  seasadjR = NULL,
  wFunction = "HR",
  alphaMCD = 0.75,
  alpha = 0.001,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rOWCov_+3A_rdata">rData</code></td>
<td>
<p>a <code class="reqn">(M x N)</code> <code>xts</code> object containing the <code class="reqn">N</code>
return series over period <code class="reqn">t</code>, with <code class="reqn">M</code> observations during <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. 
<code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rOWCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_seasadjr">seasadjR</code></td>
<td>
<p>a <code class="reqn">(M x N)</code> <code>xts</code> object containing
the seasonally adjusted returns. This is an optional argument.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_wfunction">wFunction</code></td>
<td>
<p>determines whether
a zero-one weight function (one if no jump is detected based on <code class="reqn">d_{t,i}</code> and 0 otherwise)
or
Soft Rejection (&quot;SR&quot;) weight function is to be used.
By default a zero-one weight function (wFunction = &quot;HR&quot;) is used.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_alphamcd">alphaMCD</code></td>
<td>
<p>a numeric parameter, controlling the size of
the subsets over which the determinant is minimized.
Allowed values are between 0.5 and 1 and
the default is 0.75. See Boudt et al. (2008) or the <code>covMcd</code> function in the
<span class="pkg"><a href="robustbase.html#topic+covMcd">robustbase</a></span> package.</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_alpha">alpha</code></td>
<td>
<p>is a parameter between 0 and 0.5,
that determines the rejection threshold value
(see Boudt et al. (2008) for details).</p>
</td></tr>
<tr><td><code id="rOWCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Advantages of the rOWCov compared to the <code><a href="#topic+rBPCov">rBPCov</a></code> include a higher statistical efficiency, positive semi-definiteness and affine equi-variance.
However, the rOWCov suffers from a curse of dimensionality.
The rOWCov gives a zero weight to a return vector
if at least one of the components is affected by a jump.
In the case of independent jump occurrences, the average proportion of observations
with at least one component being affected by jumps increases fast with the dimension
of the series. This means that a potentially large proportion of the returns receives
a zero weight, due to which the rOWCov can have a low finite sample efficiency in higher dimensions.
</p>


<h3>Value</h3>

<p>an <code class="reqn">N \times N</code> matrix
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Boudt, K., Croux, C., and Laurent, S. (2008). Outlyingness weighted covariation. <em>Journal of Financial Econometrics</em>, 9, 657â684.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("xts")
# Realized Outlyingness Weighted Variance/Covariance for prices aligned
# at 1 minutes.

# Univariate:
row &lt;- rOWCov(rData = as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04",
                                                 list(DT, MARKET)]), makeReturns = TRUE)
row

# Multivariate:
rowc &lt;- rOWCov(rData = as.xts(sampleOneMinuteData[as.Date(DT) == "2001-08-04",]),
               makeReturns = TRUE)
rowc

## End(Not run)
</code></pre>

<hr>
<h2 id='rQPVar'>Realized quad-power variation of intraday returns</h2><span id='topic+rQPVar'></span>

<h3>Description</h3>

<p>Calculate the realized quad-power variation, defined in Andersen et al. (2012).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>. Then, the rQPVar is given by
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{rQPVar}_{t}=N*\frac{N}{N-3} \left(\frac{\pi^2}{4} \right)^{-4} \mbox({|r_{t,i}|} {|r_{t,i-1}|} {|r_{t,i-2}|} {|r_{t,i-3}|})
 </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rQPVar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rQPVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days</p>
</td></tr>
<tr><td><code id="rQPVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rQPVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rQPVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rQPVar_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
qpv &lt;- rQPVar(rData= sampleTData[, list(DT, PRICE)], alignBy= "minutes",
              alignPeriod =5, makeReturns= TRUE)
qpv

</code></pre>

<hr>
<h2 id='rQuar'>Realized quarticity</h2><span id='topic+rQuar'></span>

<h3>Description</h3>

<p>Calculate the realized quarticity (rQuar), defined in Andersen et al. (2012).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>.
</p>
<p>Then, the rQuar is given by
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{rQuar}_{t}=\frac{N}{3} \sum_{i=1}^{N} \mbox(r_{t,i}^4)
 </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rQuar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rQuar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rQuar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rQuar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rQuar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rq &lt;- rQuar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
            alignPeriod = 5, makeReturns = TRUE)
rq
</code></pre>

<hr>
<h2 id='rRTSCov'>Robust two time scale covariance estimation</h2><span id='topic+rRTSCov'></span>

<h3>Description</h3>

<p>Calculate the robust two time scale covariance matrix proposed in Boudt and Zhang (2010).
Unlike the <code><a href="#topic+rOWCov">rOWCov</a></code>, but similarly to the <code><a href="#topic+rThresholdCov">rThresholdCov</a></code>, the <code>rRTSCov</code> uses univariate jump detection rules
to truncate the effect of jumps on the covariance
estimate. By the use of two time scales, this covariance estimate
is not only robust to price jumps, but also to microstructure noise and non-synchronic trading.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rRTSCov(
  pData,
  cor = FALSE,
  startIV = NULL,
  noisevar = NULL,
  K = 300,
  J = 1,
  KCov = NULL,
  JCov = NULL,
  KVar = NULL,
  JVar = NULL,
  eta = 9,
  makePsd = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rRTSCov_+3A_pdata">pData</code></td>
<td>
<p>a list. Each list-item i contains an <code>xts</code> object with the intraday price data
of stock <code class="reqn">i</code> for day <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_startiv">startIV</code></td>
<td>
<p>vector containing the first step estimates of the integrated variance of the assets, needed in the truncation. Is <code>NULL</code> by default.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_noisevar">noisevar</code></td>
<td>
<p>vector containing the estimates of the noise variance of the assets, needed in the truncation. Is <code>NULL</code> by default.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_k">K</code></td>
<td>
<p>positive integer, slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_j">J</code></td>
<td>
<p>positive integer, fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_kcov">KCov</code></td>
<td>
<p>positive integer, for the extradiagonal covariance elements the slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_jcov">JCov</code></td>
<td>
<p>positive integer, for the extradiagonal covariance elements the fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_kvar">KVar</code></td>
<td>
<p>vector of positive integers, for the diagonal variance elements the slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_jvar">JVar</code></td>
<td>
<p>vector of positive integers, for the diagonal variance elements the fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_eta">eta</code></td>
<td>
<p>positive real number, squared standardized high-frequency returns that exceed eta are detected as jumps.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_makepsd">makePsd</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, the positive definite version of rRTSCov is returned. 
<code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rRTSCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The rRTSCov requires the tick-by-tick transaction prices. 
(Co)variances are then computed using log-returns calculated on a rolling basis
on stock prices that are <code class="reqn">K</code> (slow time scale) and <code class="reqn">J</code> (fast time scale) steps apart.
</p>
<p>The diagonal elements of the rRTSCov matrix are the variances, computed for log-price series <code class="reqn">X</code> with <code class="reqn">n</code> price observations
at times <code class="reqn">  \tau_1,\tau_2,\ldots,\tau_n</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
(1-\frac{\overline{n}_K}{\overline{n}_J})^{-1}(\{X,X\}_T^{(K)^{*}}-\frac{\overline{n}_K}{\overline{n}_J}\{X,X\}_T^{(J)^{*}}),
</code>
</p>

<p>where <code class="reqn">\overline{n}_K=(n-K+1)/K</code>,  <code class="reqn">\overline{n}_J=(n-J+1)/J</code> and
</p>
<p style="text-align: center;"><code class="reqn">\{X,X\}_T^{(K)^{*}} =\frac{c_\eta^{*}}{K}\frac{\sum_{i=1}^{n-K+1}(X_{t_{i+K}}-X_{t_i})^2I_X^K(i;\eta)}{\frac{1}{n-K+1}\sum_{i=1}^{n-K+1}I_X^K(i;\eta)}.</code>
</p>

<p>The constant  <code class="reqn">c_\eta</code> adjusts for the bias due to the thresholding  and <code class="reqn">I_{X}^K(i;\eta)</code> is a jump indicator function
that is one if
</p>
<p style="text-align: center;"><code class="reqn"> \frac{(X_{t_{i+K}}-X_{t_{i}})^2}{(\int_{t_{i}}^{t_{i+K}} \sigma^2_sds +2\sigma_{\varepsilon_{\mbox{\tiny X}}}^2)}  \ \ \leq  \ \    \eta </code>
</p>

<p>and zero otherwise.  The elements in the denominator are the integrated variance (estimated recursively) and noise variance (estimated by the method in Zhang et al, 2005).
</p>
<p>The extradiagonal elements of the rRTSCov are the covariances.
For their calculation, the data is first synchronized by the refresh time method proposed by Harris et al (1995).
It uses the function <code><a href="#topic+refreshTime">refreshTime</a></code> to collect first the so-called refresh times at which all assets have traded at least once
since the last refresh time point. Suppose we have two log-price series:  <code class="reqn">X</code> and <code class="reqn">Y</code>. Let <code class="reqn"> \Gamma =\{ \tau_1,\tau_2,\ldots,\tau_{N^{\mbox{\tiny X}}_{\mbox{\tiny T}}}\}</code> and
<code class="reqn">\Theta=\{\theta_1,\theta_2,\ldots,\theta_{N^{\mbox{\tiny Y}}_{\mbox{\tiny T}}}\}</code>
be the set of transaction times of these assets.
The first refresh time corresponds to the first time at which both stocks have traded, i.e.
<code class="reqn">\phi_1=\max(\tau_1,\theta_1)</code>. The subsequent refresh time is defined as the first time when both stocks have again traded, i.e.
<code class="reqn">\phi_{j+1}=\max(\tau_{N^{\mbox{\tiny{X}}}_{\phi_j}+1},\theta_{N^{\mbox{\tiny{Y}}}_{\phi_j}+1})</code>. The
complete refresh time sample grid is
<code class="reqn">\Phi=\{\phi_1,\phi_2,...,\phi_{M_N+1}\}</code>, where <code class="reqn">M_N</code> is the total number of paired returns.  The
sampling points of asset <code class="reqn">X</code> and <code class="reqn">Y</code> are defined to be
<code class="reqn">t_i=\max\{\tau\in\Gamma:\tau\leq \phi_i\}</code> and
<code class="reqn">s_i=\max\{\theta\in\Theta:\theta\leq \phi_i\}</code>.
</p>
<p>Given these refresh times, the covariance is computed as follows:
</p>
<p style="text-align: center;"><code class="reqn">
c_{N}( \{X,Y\}^{(K)}_T-\frac{\overline{n}_K}{\overline{n}_J}\{X,Y\}^{(J)}_T ),
</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\{X,Y\}^{(K)}_T =\frac{1}{K} \frac{\sum_{i=1}^{M_N-K+1}c_i (X_{t_{i+K}}-X_{t_{i}})(Y_{s_{i+K}}-Y_{s_{i}})I_{X}^K(i;\eta)
I_{Y}^K(i;\eta)}{\frac{1}{M_N-K+1}\sum_{i=1}^{M_N-K+1}{I_X^K(i;\eta)I_Y^K(i;\eta)}},</code>
</p>

<p>with  <code class="reqn">I_{X}^K(i;\eta)</code> the same jump indicator function as for the variance and <code class="reqn">c_N</code> a constant to adjust for the bias due to the thresholding.
</p>
<p>Unfortunately, the rRTSCov is not always positive semidefinite.
By setting the argument <code>makePsd = TRUE</code>, the function  <code><a href="#topic+makePsd">makePsd</a></code> is used to return a positive semidefinite
matrix. This function replaces the negative eigenvalues with zeroes.
</p>


<h3>Value</h3>

<p>an <code class="reqn">N \times N</code> matrix
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Boudt K. and Zhang, J. 2010. Jump robust two time scale covariance estimation and realized volatility budgets. Mimeo.
</p>
<p>Harris, F., McInish, T., Shoesmith, G., and Wood, R. (1995). Cointegration, error correction, and price discovery on informationally linked security markets. <em>Journal of Financial and Quantitative Analysis</em>, 30, 563-581.
</p>
<p>Zhang, L., Mykland, P. A., and Ait-Sahalia, Y. (2005). A tale of two time scales: Determining integrated volatility with noisy high-frequency data. <em>Journal of the American Statistical Association</em>, 100, 1394-1411.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(xts)
set.seed(123)
start &lt;- strptime("1970-01-01", format = "%Y-%m-%d", tz = "UTC")
timestamps &lt;- start + seq(34200, 57600, length.out = 23401)

dat &lt;- cbind(rnorm(23401) * sqrt(1/23401), rnorm(23401) * sqrt(1/23401))

dat &lt;- exp(cumsum(xts(dat, timestamps)))
price1 &lt;- dat[,1]
price2 &lt;- dat[,2]
rcRTS &lt;- rRTSCov(pData = list(price1, price2))
# Note: List of prices as input
rcRTS

## End(Not run)
</code></pre>

<hr>
<h2 id='rRVar'>An estimator of realized variance.</h2><span id='topic+rRVar'></span>

<h3>Description</h3>

<p>Calculates the daily Realized Variance.
Let <code class="reqn">r_{t,i}</code> be an intraday return vector with <code class="reqn">i=1,...,M</code> number of intraday returns.
</p>
<p>Then, the realized variance is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mbox{RVar}_{t}=\sum_{i=1}^{M}r_{t,i}^{2}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rRVar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rRVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rRVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rRVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rRVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rRVar_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rv &lt;- rRVar(sampleOneMinuteData, makeReturns = TRUE)
plot(rv[, DT], rv[, MARKET], xlab = "Date", ylab = "Realized Variance", type = "l")
</code></pre>

<hr>
<h2 id='rSemiCov'>Realized semicovariance</h2><span id='topic+rSemiCov'></span>

<h3>Description</h3>

<p>Calculate the Realized Semicovariances (rSemiCov).
Let <code class="reqn"> r_{t,i} </code> be an intraday <code class="reqn">M x N</code> return matrix and <code class="reqn">i=1,...,M</code>
the number of intraday returns. Then, let <code class="reqn">r_{t,i}^{+} = max(r_{t,i},0)</code> and <code class="reqn">r_{t,i}^{-} = min(r_{t,i},0)</code>.
</p>
<p>Then, the realized semicovariance is given by the following three matrices:
</p>
<p style="text-align: center;"><code class="reqn">
 \mbox{pos}_t =\sum_{i=1}^{M} r^{+}_{t,i} r^{+'}_{t,i}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
 \mbox{neg}_t =\sum_{i=1}^{M} r^{-}_{t,i} r^{-'}_{t,i}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
 \mbox{mixed}_t =\sum_{i=1}^{M} (r^{+}_{t,i} r^{-'}_{t,i} + r^{-}_{t,i} r^{+'}_{t,i})
</code>
</p>

<p>The mixed covariance matrix will have 0 on the diagonal.
From these three matrices, the realized covariance can be constructed as <code class="reqn">pos + neg + mixed</code>.
The concordant semicovariance matrix is <code class="reqn">pos + neg</code>.
The off-diagonals of the concordant matrix is always positive, while for the mixed matrix, it is always negative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rSemiCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rSemiCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rSemiCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rSemiCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rSemiCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rSemiCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case that cor is <code>TRUE</code>, the mixed matrix will be an <code class="reqn">N \times N</code> matrix filled with NA as mapping the mixed covariance matrix into correlation space is impossible due to the 0-diagonal.
</p>


<h3>Value</h3>

<p>In case the data consists of one day a list of five <code class="reqn">N \times N</code> matrices are returned. These matrices are named <code>mixed</code>, <code>positive</code>, <code>negative</code>, <code>concordant</code>, and <code>rCov</code>.
The latter matrix corresponds to the realized covariance estimator and is thus named like the function <code><a href="#topic+rCov">rCov</a></code>.
In case the data spans more than one day, the list for each day will be put into another list named according to the date of the estimates.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>References</h3>

<p>Bollerslev, T., Li, J., Patton, A. J., and Quaedvlieg, R. (2020). Realized semicovariances. <em>Econometrica</em>, 88, 1515-1551.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Realized semi-variance/semi-covariance for prices aligned
# at 5 minutes.

# Univariate:
rSVar = rSemiCov(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
                   alignPeriod = 5, makeReturns = TRUE)
rSVar
## Not run: 
library("xts")
# Multivariate multi day:
rSC &lt;- rSemiCov(sampleOneMinuteData, makeReturns = TRUE) # rSC is a list of lists
# We extract the covariance between stock 1 and stock 2 for all three covariances.
mixed &lt;- sapply(rSC, function(x) x[["mixed"]][1,2])
neg &lt;- sapply(rSC, function(x) x[["negative"]][1,2])
pos &lt;- sapply(rSC, function(x) x[["positive"]][1,2])
covariances &lt;- xts(cbind(mixed, neg, pos), as.Date(names(rSC)))
colnames(covariances) &lt;- c("mixed", "neg", "pos")
# We make a quick plot of the different covariances
plot(covariances)
addLegend(lty = 1) # Add legend so we can distinguish the series.

## End(Not run)

</code></pre>

<hr>
<h2 id='rSkew'>Realized skewness</h2><span id='topic+rSkew'></span>

<h3>Description</h3>

<p>Calculate the realized skewness, defined in Amaya et al. (2015).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns in period <code class="reqn">t</code>. Let <code class="reqn">r_{t,i}</code> be a return (with <code class="reqn">i=1, \ldots,N</code>) in period <code class="reqn">t</code>. Then, <code>rSkew</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
    \mbox{rSkew}_{t}= \frac{\sqrt{N} \sum_{i=1}^{N}(r_{t,i})^3}{\left(\sum r_{i,t}^2\right)^{3/2}}.
  </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rSkew(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rSkew_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rSkew_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rSkew_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rSkew_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Amaya, D., Christoffersen, P., Jacobs, K., and Vasquez, A. (2015). Does realized skewness and kurtosis predict the cross-section of equity returns? <em>Journal of Financial Economics</em>, 118, 135-167.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rs &lt;- rSkew(sampleTData[, list(DT, PRICE)],alignBy ="minutes", alignPeriod =5,
            makeReturns = TRUE)
rs
</code></pre>

<hr>
<h2 id='rSV'>DEPRECATED</h2><span id='topic+rSV'></span>

<h3>Description</h3>

<p>DEPRECATED USE <code><a href="#topic+rSVar">rSVar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rSV(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rSV_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rSVar">rSVar</a></code></p>
</td></tr>
<tr><td><code id="rSV_+3A_alignby">alignBy</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rSVar">rSVar</a></code></p>
</td></tr>
<tr><td><code id="rSV_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rSVar">rSVar</a></code></p>
</td></tr>
<tr><td><code id="rSV_+3A_makereturns">makeReturns</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rSVar">rSVar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='rSVar'>Realized semivariance of highfrequency return series</h2><span id='topic+rSVar'></span>

<h3>Description</h3>

<p>Calculate the realized semivariances, defined in Barndorff-Nielsen et al. (2008).
</p>
<p>Function returns two outcomes: 
</p>

<ol>
<li><p> Downside realized semivariance 
</p>
</li>
<li><p> Upside realized semivariance.
</p>
</li></ol>

<p>Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>.
</p>
<p>Then, the <code>rSVar</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{rSVardownside}_{t}= \sum_{i=1}^{N} (r_{t,i})^2  \ \times \ I [ r_{t,i} &lt; 0]
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  \mbox{rSVarupside}_{t}= \sum_{i=1}^{N} (r_{t,i})^2 \ \times \ I [ r_{t,i} &gt; 0]
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rSVar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rSVar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rSVar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rSVar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example to aggregate.
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rSVar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. 
<code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rSVar_+3A_...">...</code></td>
<td>
<p>used internally</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two entries, the realized positive and negative semivariances
</p>


<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Kinnebrock, S., and Shephard N. (2010). <em>Measuring downside risk: realised semivariance</em>. In: Volatility and Time Series Econometrics: Essays in Honor of Robert F. Engle,
(Edited by Bollerslev, T., Russell, J., and Watson, M.), 117-136. Oxford University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IVar">IVar</a></code> for a list of implemented estimators of the integrated variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sv &lt;- rSVar(sampleTData[, list(DT, PRICE)], alignBy = "minutes",
          alignPeriod = 5, makeReturns = TRUE)
sv
</code></pre>

<hr>
<h2 id='rThresholdCov'>Threshold Covariance</h2><span id='topic+rThresholdCov'></span>

<h3>Description</h3>

<p>Calculate the threshold covariance matrix proposed in Gobbi and Mancini (2009).
Unlike the <code><a href="#topic+rOWCov">rOWCov</a></code>, the rThresholdCov uses univariate jump detection rules to truncate the effect of jumps on the covariance
estimate. As such, it remains feasible in high dimensions, but it is less robust to small cojumps.
</p>
<p>Let <code class="reqn">r_{t,i}</code> be an intraday <code class="reqn">N x 1</code> return vector of <code class="reqn">N</code> assets where <code class="reqn">i=1,...,M</code> and
<code class="reqn">M</code> being the number of intraday returns.
</p>
<p>Then, the <code class="reqn">k,q</code>-th element of the threshold covariance matrix is defined as
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{thresholdcov}[k,q]_{t} = \sum_{i=1}^{M} r_{(k)t,i} 1_{\{r_{(k)t,i}^2 \leq TR_{M}\}}  \ \ r_{(q)t,i} 1_{\{r_{(q)t,i}^2 \leq TR_{M}\}},
</code>
</p>

<p>with the threshold value <code class="reqn">TR_{M}</code> set to <code class="reqn">9 \Delta^{-1}</code> times the daily realized bi-power variation of asset <code class="reqn">k</code>,
as suggested in Jacod and Todorov (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rThresholdCov(
  rData,
  cor = FALSE,
  alignBy = NULL,
  alignPeriod = NULL,
  makeReturns = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rThresholdCov_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rThresholdCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rThresholdCov_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="rThresholdCov_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rThresholdCov_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. 
<code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rThresholdCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an <code class="reqn">N \times N</code> matrix is returned. If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing <code class="reqn">N \times N</code> matrices. Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. and Shephard, N. (2004). Measuring the impact of jumps in multivariate price processes using bipower covariation. Discussion paper, Nuffield College, Oxford University.
</p>
<p>Jacod, J. and Todorov, V. (2009). Testing for common arrival of jumps in discretely-observed multidimensional processes. <em>Annals of Statistics</em>, 37, 1792-1838.
</p>
<p>Mancini, C. and Gobbi, F. (2012). Identifying the Brownian covariation from the co-jumps given discrete observations. <em>Econometric Theory</em>, 28, 249-273.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Realized threshold  Variance/Covariance:
# Multivariate:
## Not run: 
library("xts")
set.seed(123)
start &lt;- strptime("1970-01-01", format = "%Y-%m-%d", tz = "UTC")
timestamps &lt;- start + seq(34200, 57600, length.out = 23401)

dat &lt;- cbind(rnorm(23401) * sqrt(1/23401), rnorm(23401) * sqrt(1/23401))

dat &lt;- exp(cumsum(xts(dat, timestamps)))
rcThreshold &lt;- rThresholdCov(dat, alignBy = "minutes", alignPeriod = 1, makeReturns = TRUE)
rcThreshold

## End(Not run)
</code></pre>

<hr>
<h2 id='rTPQuar'>Realized tri-power quarticity</h2><span id='topic+rTPQuar'></span>

<h3>Description</h3>

<p>Calculate the rTPQuar, defined in Andersen et al. (2012).
</p>
<p>Assume there are <code class="reqn">N</code> equispaced returns <code class="reqn">r_{t,i}</code> in period <code class="reqn">t</code>, <code class="reqn">i=1, \ldots,N</code>. Then, the rTPQuar is given by
</p>
<p style="text-align: center;"><code class="reqn">
   \mbox{rTPQuar}_{t}=N\frac{N}{N-2} \left(\frac{\Gamma \left(0.5\right)}{ 2^{2/3}\Gamma \left(7/6\right)} \right)^{3} \sum_{i=3}^{N} \mbox({|r_{t,i}|}^{4/3} {|r_{t,i-1}|}^{4/3} {|r_{t,i-2}|}^{4/3})
 </code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>rTPQuar(rData, alignBy = NULL, alignPeriod = NULL, makeReturns = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rTPQuar_+3A_rdata">rData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing returns or prices, possibly for multiple assets over multiple days.</p>
</td></tr>
<tr><td><code id="rTPQuar_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code>.</p>
</td></tr>
<tr><td><code id="rTPQuar_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive numeric, indicating the number of periods to aggregate over. For example, to aggregate
based on a 5-minute frequency, set <code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="rTPQuar_+3A_makereturns">makeReturns</code></td>
<td>
<p>boolean, should be <code>TRUE</code> when <code>rData</code> contains prices instead of returns. <code>FALSE</code> by default.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> In case the input is an <code>xts</code> object with data from one day, a numeric of the same length as the number of assets.
</p>
</li>
<li><p> If the input data spans multiple days and is in <code>xts</code> format, an <code>xts</code> will be returned.
</p>
</li>
<li><p> If the input data is a <code>data.table</code> object, the function returns a <code>data.table</code> with the same column names as the input data, containing the date and the realized measures.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Giang Nguyen, Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G., Dobrev, D., and Schaumburg, E. (2012). Jump-robust volatility estimation using nearest neighbor truncation. <em>Journal of Econometrics</em>, 169, 75-93.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tpq &lt;- rTPQuar(rData = sampleTData[, list(DT, PRICE)], alignBy = "minutes",
              alignPeriod = 5, makeReturns = TRUE)
tpq

</code></pre>

<hr>
<h2 id='rTSCov'>Two time scale covariance estimation</h2><span id='topic+rTSCov'></span>

<h3>Description</h3>

<p>Calculate the two time scale covariance matrix proposed in Zhang et al. (2005) and Zhang (2010).
By the use of two time scales, this covariance estimate
is robust to microstructure noise and non-synchronic trading.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rTSCov(
  pData,
  cor = FALSE,
  K = 300,
  J = 1,
  KCov = NULL,
  JCov = NULL,
  KVar = NULL,
  JVar = NULL,
  makePsd = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rTSCov_+3A_pdata">pData</code></td>
<td>
<p>a list. Each list-item i contains an <code>xts</code> object with the intraday price data
of stock <code class="reqn">i</code> for day <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_cor">cor</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, and the input data is multivariate, the correlation is returned instead of the covariance matrix. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_k">K</code></td>
<td>
<p>positive integer, slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_j">J</code></td>
<td>
<p>positive integer, fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_kcov">KCov</code></td>
<td>
<p>positive integer, for the extradiagonal covariance elements the slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_jcov">JCov</code></td>
<td>
<p>positive integer, for the extradiagonal covariance elements the fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_kvar">KVar</code></td>
<td>
<p>vector of positive integers, for the diagonal variance elements the slow time scale returns are computed on prices that are <code>K</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_jvar">JVar</code></td>
<td>
<p>vector of positive integers, for the diagonal variance elements the fast time scale returns are computed on prices that are <code>J</code> steps apart.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_makepsd">makePsd</code></td>
<td>
<p>boolean, in case it is <code>TRUE</code>, the positive definite version of <code>rTSCov</code> is returned. <code>FALSE</code> by default.</p>
</td></tr>
<tr><td><code id="rTSCov_+3A_...">...</code></td>
<td>
<p>used internally, do not change.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The rTSCov requires the tick-by-tick transaction prices. (Co)variances are then computed using log-returns calculated on a rolling basis
on stock prices that are <code class="reqn">K</code> (slow time scale) and <code class="reqn">J</code> (fast time scale) steps apart.
</p>
<p>The diagonal elements of the rTSCov matrix are the variances, computed for log-price series <code class="reqn">X</code> with <code class="reqn">n</code> price observations
at times <code class="reqn">  \tau_1,\tau_2,\ldots,\tau_n</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">(1-\frac{\overline{n}_K}{\overline{n}_J})^{-1}([X,X]_T^{(K)}-
       \frac{\overline{n}_K}{\overline{n}_J}[X,X]_T^{(J))}</code>
</p>

<p>where <code class="reqn">\overline{n}_K=(n-K+1)/K</code>,  <code class="reqn">\overline{n}_J=(n-J+1)/J</code> and
</p>
<p style="text-align: center;"><code class="reqn">[X,X]_T^{(K)} =\frac{1}{K}\sum_{i=1}^{n-K+1}(X_{t_{i+K}}-X_{t_i})^2.</code>
</p>

<p>The extradiagonal elements of the rTSCov are the covariances.
For their calculation, the data is first synchronized by the refresh time method proposed by Harris et al (1995).
It uses the function <code><a href="#topic+refreshTime">refreshTime</a></code> to collect first the so-called refresh times at which all assets have traded at least once
since the last refresh time point. Suppose we have two log-price series:  <code class="reqn">X</code> and <code class="reqn">Y</code>. Let <code class="reqn"> \Gamma =\{ \tau_1,\tau_2,\ldots,\tau_{N^{\mbox{\tiny X}}_{\mbox{\tiny T}}}\}</code> and
<code class="reqn">\Theta=\{\theta_1,\theta_2,\ldots,\theta_{N^{\mbox{\tiny Y}}_{\mbox{\tiny T}}}\}</code>
be the set of transaction times of these assets.
The first refresh time corresponds to the first time at which both stocks have traded, i.e.
<code class="reqn">\phi_1=\max(\tau_1,\theta_1)</code>. The subsequent refresh time is defined as the first time when both stocks have again traded, i.e.
<code class="reqn">\phi_{j+1}=\max(\tau_{N^{\mbox{\tiny{X}}}_{\phi_j}+1},\theta_{N^{\mbox{\tiny{Y}}}_{\phi_j}+1})</code>. The
complete refresh time sample grid is
<code class="reqn">\Phi=\{\phi_1,\phi_2,...,\phi_{M_N+1}\}</code>, where <code class="reqn">M_N</code> is the total number of paired returns.  The
sampling points of asset <code class="reqn">X</code> and <code class="reqn">Y</code> are defined to be
<code class="reqn">t_i=\max\{\tau\in\Gamma:\tau\leq \phi_i\}</code> and
<code class="reqn">s_i=\max\{\theta\in\Theta:\theta\leq \phi_i\}</code>.
</p>
<p>Given these refresh times, the covariance is computed as follows:
</p>
<p style="text-align: center;"><code class="reqn">
c_{N}( [X,Y]^{(K)}_T-\frac{\overline{n}_K}{\overline{n}_J}[X,Y]^{(J)}_T ),
</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">[X,Y]^{(K)}_T =\frac{1}{K} \sum_{i=1}^{M_N-K+1} (X_{t_{i+K}}-X_{t_{i}})(Y_{s_{i+K}}-Y_{s_{i}}).</code>
</p>

<p>Unfortunately, the rTSCov is not always positive semidefinite.
By setting the argument makePsd = TRUE, the function <code><a href="#topic+makePsd">makePsd</a></code> is used to return a positive semidefinite
matrix. This function replaces the negative eigenvalues with zeroes.
</p>


<h3>Value</h3>

<p>in case the input is and contains data from one day, an N by N matrix is returned. If the data is a univariate <code>xts</code> object with multiple days, an <code>xts</code> is returned.
If the data is multivariate and contains multiple days (<code>xts</code> or <code>data.table</code>), the function returns a list containing N by N matrices. Each item in the list has a name which corresponds to the date for the matrix.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Harris, F., McInish, T., Shoesmith, G., and Wood, R. (1995). Cointegration, error correction, and price discovery on informationally linked security markets. <em>Journal of Financial and Quantitative Analysis</em>, 30, 563-581.
</p>
<p>Zhang, L., Mykland, P. A., and Ait-Sahalia, Y. (2005). A tale of two time scales: Determining integrated volatility with noisy high-frequency data. <em>Journal of the American Statistical Association</em>, 100, 1394-1411.
</p>
<p>Zhang, L. (2011). Estimating covariation: Epps effect, microstructure noise. <em>Journal of Econometrics</em>, 160, 33-47.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ICov">ICov</a></code> for a list of implemented estimators of the integrated covariance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Robust Realized two timescales Variance/Covariance
# Multivariate:
## Not run: 
library(xts)
set.seed(123)
start &lt;- strptime("1970-01-01", format = "%Y-%m-%d", tz = "UTC")
timestamps &lt;- start + seq(34200, 57600, length.out = 23401)

dat &lt;- cbind(rnorm(23401) * sqrt(1/23401), rnorm(23401) * sqrt(1/23401))

dat &lt;- exp(cumsum(xts(dat, timestamps)))
price1 &lt;- dat[,1]
price2 &lt;- dat[,2]
rcovts &lt;- rTSCov(pData = list(price1, price2))
# Note: List of prices as input
rcovts

## End(Not run)

</code></pre>

<hr>
<h2 id='RV'>DEPRECATED
DEPRECATED USE <code><a href="#topic+rRVar">rRVar</a></code></h2><span id='topic+RV'></span>

<h3>Description</h3>

<p>DEPRECATED
DEPRECATED USE <code><a href="#topic+rRVar">rRVar</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RV(rData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RV_+3A_rdata">rData</code></td>
<td>
<p>DEPRECATED USE <code><a href="#topic+rRVar">rRVar</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='salesCondition'><a href="#topic+salesCondition">salesCondition</a> is deprecated. Use <a href="#topic+tradesCondition">tradesCondition</a> instead.</h2><span id='topic+salesCondition'></span>

<h3>Description</h3>

<p><a href="#topic+salesCondition">salesCondition</a> is deprecated. Use <a href="#topic+tradesCondition">tradesCondition</a> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>salesCondition(
  tData,
  validConds = c("", "@", "E", "@E", "F", "FI", "@F", "@FI", "I", "@I")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="salesCondition_+3A_tdata">tData</code></td>
<td>
<p><a href="#topic+salesCondition">salesCondition</a> is deprecated. Use <a href="#topic+tradesCondition">tradesCondition</a> instead.</p>
</td></tr>
<tr><td><code id="salesCondition_+3A_validconds">validConds</code></td>
<td>
<p><a href="#topic+salesCondition">salesCondition</a> is deprecated. Use <a href="#topic+tradesCondition">tradesCondition</a> instead.</p>
</td></tr>
</table>

<hr>
<h2 id='sampleMultiTradeData'>Multivariate tick by tick data</h2><span id='topic+sampleMultiTradeData'></span>

<h3>Description</h3>

<p>Cleaned Tick by tick data for a sector ETF, called <code>ETF</code> and two stock components of that ETF,
these stocks are named <code>AAA</code> and <code>BBB</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleMultiTradeData
</code></pre>


<h3>Format</h3>

<p>A <code>data.table</code> object
</p>

<hr>
<h2 id='sampleOneMinuteData'>One minute data</h2><span id='topic+sampleOneMinuteData'></span>

<h3>Description</h3>

<p>One minute data price of one stock and a market proxy. This is data from the US market.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleOneMinuteData
</code></pre>


<h3>Format</h3>

<p>A <code>data.table</code> object
</p>

<hr>
<h2 id='sampleQData'>Sample of cleaned quotes for stock XXX for 2 days measured in microseconds</h2><span id='topic+sampleQData'></span>

<h3>Description</h3>

<p>A <code>data.table</code> object containing the quotes for the pseudonymized stock XXX for 2 days. This is the cleaned version of the data sample <code><a href="#topic+sampleQDataRaw">sampleQDataRaw</a></code>, using <code>quotesCleanup</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleQData
</code></pre>


<h3>Format</h3>

<p>data.table object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# The code to create the sampleQData dataset from raw data is
sampleQData &lt;- quotesCleanup(qDataRaw = sampleQDataRaw,
                                         exchanges = "N", type = "standard", report = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='sampleQDataRaw'>Sample of raw quotes for stock XXX for 2 days measured in microseconds</h2><span id='topic+sampleQDataRaw'></span>

<h3>Description</h3>

<p>A <code>data.table</code> object containing the raw quotes the pseudonymized stock XXX for 2 days, in the typical NYSE TAQ database format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleQDataRaw
</code></pre>


<h3>Format</h3>

<p>data.table object
</p>

<hr>
<h2 id='sampleTData'>Sample of cleaned trades for stock XXX for 2 days</h2><span id='topic+sampleTData'></span>

<h3>Description</h3>

<p>A <code>data.table</code> object containing the trades for the pseudonymized stock XXX for 2 days, in the typical NYSE TAQ database format.
This is the cleaned version of the data sample <code><a href="#topic+sampleTDataRaw">sampleTDataRaw</a></code>, using <code><a href="#topic+tradesCleanupUsingQuotes">tradesCleanupUsingQuotes</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleTData
</code></pre>


<h3>Format</h3>

<p>A data.table object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# The code to create the sampleTData dataset from raw data is
sampleQData &lt;- quotesCleanup(qDataRaw = sampleQDataRaw,
                                         exchanges = "N", type = "standard", report = FALSE)

tradesAfterFirstCleaning &lt;- tradesCleanup(tDataRaw = sampleTDataRaw, 
                                          exchanges = "N", report = FALSE)

sampleTData &lt;- tradesCleanupUsingQuotes(
  tData = tradesAfterFirstCleaning,
  qData = sampleQData,
  lagQuotes = 0)[, c("DT", "EX", "SYMBOL", "PRICE", "SIZE")]
# Only some columns are included. These are the ones that were historically included.

# For most applications, we recommend aggregating the data at a high frequency
# For example, every second.
aggregated &lt;- aggregatePrice(sampleTData[, list(DT, PRICE)],
              alignBy = "seconds", alignPeriod = 1)
acf(diff(aggregated[as.Date(DT) == "2018-01-02", PRICE]))
acf(diff(aggregated[as.Date(DT) == "2018-01-03", PRICE]))

signature &lt;- function(x, q){
res &lt;- x[, (rCov(diff(log(PRICE), lag = q, differences = 1))/q), by = as.Date(DT)]
return(res[[2]])
}
rvAgg &lt;- matrix(nrow = 100, ncol = 2)
for(i in 1:100) rvAgg[i, ] &lt;- signature(aggregated, i)
plot(rvAgg[,1], type = "l")
plot(rvAgg[,2], type = "l")

## End(Not run)

</code></pre>

<hr>
<h2 id='sampleTDataEurope'>European data</h2><span id='topic+sampleTDataEurope'></span>

<h3>Description</h3>

<p>Trade data of one stock on one day in the European stock market.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleTDataEurope
</code></pre>


<h3>Format</h3>

<p>A <code>data.table</code> object
</p>

<hr>
<h2 id='sampleTDataRaw'>Sample of raw trades for stock XXX for 2 days</h2><span id='topic+sampleTDataRaw'></span>

<h3>Description</h3>

<p>An imaginary <code>data.table</code> object containing the raw trades the pseudonymized stock XXX for 2 days, in the typical NYSE TAQ database format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleTDataRaw
</code></pre>


<h3>Format</h3>

<p>A data.table object.
</p>

<hr>
<h2 id='selectExchange'>Retain only data from a single stock exchange</h2><span id='topic+selectExchange'></span>

<h3>Description</h3>

<p>Filter raw trade data to only contain specified exchanges
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectExchange(data, exch = "N")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectExchange_+3A_data">data</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing the time series data. 
The object should have a column &quot;EX&quot;, indicating the exchange by its symbol.</p>
</td></tr>
<tr><td><code id="selectExchange_+3A_exch">exch</code></td>
<td>
<p>The (vector of) symbol(s) of the stock exchange(s) that should be selected.
By default the NYSE is chosen (<code>exch = "N"</code>). Other exchange symbols are:
</p>

<ul>
<li><p> A: AMEX
</p>
</li>
<li><p> N: NYSE
</p>
</li>
<li><p> B: Boston
</p>
</li>
<li><p> P: Arca
</p>
</li>
<li><p> C: NSX
</p>
</li>
<li><p> T/Q: NASDAQ
</p>
</li>
<li><p> D: NASD ADF and TRF
</p>
</li>
<li><p> X: Philadelphia
</p>
</li>
<li><p> I: ISE
</p>
</li>
<li><p> M: Chicago
</p>
</li>
<li><p> W: CBOE
</p>
</li>
<li><p> Z: BATS
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>

<hr>
<h2 id='spotDrift'>Spot Drift Estimation</h2><span id='topic+spotDrift'></span>

<h3>Description</h3>

<p>Function used to estimate the spot drift of intraday (tick) stock prices/returns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spotDrift(
  data,
  method = "mean",
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spotDrift_+3A_data">data</code></td>
<td>
<p>Can be one of two input types, <code>xts</code> or <code>data.table</code>. It is assumed that the input comprises prices in levels.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_method">method</code></td>
<td>
<p>Which method to be used to estimate the spot-drift. Currently, three methods are available, 
rolling mean and median as well as the kernel method of Christensen et al. (2018).
The kernel is a left hand exponential kernel that will weigh newer observations more heavily than older observations.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="spotDrift_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>How often should the estimation take place? If <code>alignPeriod</code> is 5 the estimation will be done every fifth unit of <code>alignBy</code>.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_marketopen">marketOpen</code></td>
<td>
<p>Opening time of the market, standard is &quot;09:30:00&quot;.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_marketclose">marketClose</code></td>
<td>
<p>Closing time of the market, standard is &quot;16:00:00&quot;.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
We attempt to extract the timezone from the <code>DT</code> column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>.</p>
</td></tr>
<tr><td><code id="spotDrift_+3A_...">...</code></td>
<td>
<p>Additional arguments for the individual methods. See &lsquo;Details&rsquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additional arguments for the mean and median methods are: 
</p>

<ul>
<li> <p><code>periods</code> for the rolling window length which is 5 by default.
</p>
</li>
<li> <p><code>align</code> controls the alignment. The default is <code>"right"</code>. 
</p>
</li></ul>

<p>For the kernel mean estimator, the arguments <code>meanBandwidth</code> can be used to control the bandwidth of the 
drift estimator and the <code>preAverage</code> argument, which can be used to control the pre-averaging horizon. 
These arguments default to 300 and 5 respectively.
</p>
<p>The following estimation methods can be specified in <code>method</code>:
</p>
<p><strong>Rolling window mean (<code>"mean"</code>)</strong>
</p>
<p>Estimates the spot drift by applying a rolling mean over returns.
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\mu_{t}} = \sum_{t = k}^{T} \textrm{mean} \left(r_{t-k : t} \right),
</code>
</p>

<p>where <code class="reqn">k</code> is the argument <code>periods</code>.
Parameters:
</p>

<ul>
<li><p><code>periods</code> how big the window for the estimation should be. The estimator will have <code>periods</code> <code>NA</code>s at the beginning of each trading day.
</p>
</li>
<li><p><code>align</code> alignment method for returns. Defaults to <code>"left"</code>, which includes only past data, but other choices, <code>"center"</code> and <code>"right"</code> are available.
Warning: These values includes future data.
</p>
</li></ul>

<p>Outputs:
</p>

<ul>
<li><p><code>mu</code> a matrix containing the spot drift estimates
</p>
</li></ul>

<p><strong>Rolling window median (<code>"median"</code>)</strong>
</p>
<p>Estimates the spot drift by applying a rolling mean over returns.
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\mu_{t}} = \sum_{t = k}^{T} \textrm{median} \left(r_{t-k : t} \right),
</code>
</p>

<p>where <code class="reqn">k</code> is the argument <code>periods</code>.
Parameters:
</p>

<ul>
<li><p><code>periods</code> How big the window for the estimation should be. The estimator will have <code>periods</code> <code>NA</code>s at the beginning of each trading day.
</p>
</li>
<li><p><code>align</code> Alignment method for returns. Defaults to <code>"left"</code>, which includes only past data, but other choices, <code>"center"</code> and <code>"right"</code> are available.
These values includes FUTURE DATA, so beware!
</p>
</li></ul>

<p>Outputs:
</p>

<ul>
<li><p><code>mu</code> a matrix containing the spot drift estimates
</p>
</li></ul>

<p><strong>kernel spot drift estimator (<code>"kernel"</code>)</strong>
</p>
<p style="text-align: center;"><code class="reqn">
    dX_{t} = \mu_{t}dt + \sigma_{t}dW_{t} + dJ_{t},
</code>
</p>

<p>where <code class="reqn">\mu_{t}</code>, <code class="reqn">\sigma_{t}</code>, and <code class="reqn">J_{t}</code> are the spot drift, the spot volatility, and a jump process respectively.
However, due to microstructure noise, the observed log-price is 
</p>
<p style="text-align: center;"><code class="reqn">
    Y_{t} = X_{t} + \varepsilon_{t}
</code>
</p>

<p>In order robustify the results to the presence of market microstructure noise, the pre-averaged returns are used:
</p>
<p style="text-align: center;"><code class="reqn">
    \Delta_{i}^{n}\overline{Y} = \sum_{j=1}^{k_{n}-1}g_{j}^{n}\Delta_{i+j}^{n}Y,
</code>
</p>

<p>where <code class="reqn">g(\cdot)</code> is a weighting function, <code class="reqn">min(x, 1-x)</code>, and <code class="reqn">k_{n}</code> is the pre-averaging horizon.
The spot drift estimator is then:
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\bar{\mu}}_{t}^{n} = \sum_{i=1}^{n-k_{n}+2}K\left(\frac{t_{i-1}-t}{h_{n}}\right)\Delta_{i-1}^{n}\overline{Y},
</code>
</p>

<p>The kernel estimation method has the following parameters:
</p>

<ul>
<li><p><code>preAverage</code> a positive <code>integer</code> denoting the length of pre-averaging window for the log-prices. Default is 5
</p>
</li>
<li><p><code>meanBandwidth</code> an <code>integer</code> denoting the bandwidth for the left-sided exponential kernel for the mean. Default is <code>300L</code>
</p>
</li></ul>

<p>Outputs:
</p>

<ul>
<li><p><code>mu</code> a matrix containing the spot drift estimates
</p>
</li></ul>



<h3>Value</h3>

<p>An object of class <code>"spotDrift"</code> containing at least the estimated spot drift process. 
Input on what this class should contain and methods for it is welcome.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>References</h3>

<p>Christensen, K., Oomen, R., and Reno, R. (2020) The drift burst hypothesis. Journal of Econometrics. Forthcoming.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Rolling mean and median estimators for 2 days
meandrift &lt;- spotDrift(data = sampleTData, alignPeriod = 1)
mediandrift &lt;- spotDrift(data = sampleTData, method = "median", 
                         alignBy = "seconds", alignPeriod = 30, tz = "EST")
plot(meandrift)
plot(mediandrift)
## Not run: 
# Example 2: Kernel based estimator for one day with data.table format
price &lt;- sampleTData[as.Date(DT) == "2018-01-02", list(DT, PRICE)]
kerneldrift &lt;- spotDrift(sampleTDataEurope, method = "driftKernel",
                         alignBy = "minutes", alignPeriod = 1)
plot(kerneldrift)

## End(Not run)

</code></pre>

<hr>
<h2 id='spotVol'>Spot volatility estimation</h2><span id='topic+spotVol'></span>

<h3>Description</h3>

<p>Estimates a wide variety of spot volatility estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spotVol(
  data,
  method = "detPer",
  alignBy = "minutes",
  alignPeriod = 5,
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  tz = "GMT",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spotVol_+3A_data">data</code></td>
<td>
<p>Can be one of two input types, <code>xts</code> or <code>data.table</code>. 
It is assumed that the input comprises prices in levels. Irregularly spaced
observations are allowed. They will be aggregated to the level specified by
parameters <code>alignBy</code> and <code>alignPeriod</code>.</p>
</td></tr>
<tr><td><code id="spotVol_+3A_method">method</code></td>
<td>
<p>specifies which method will be used to estimate the spot
volatility. Valid options are <code>"detPer"</code>, <code>"stochPer"</code> <code>"kernel"</code> <code>"piecewise"</code> <code>"garch"</code>, <code>"RM"</code> ,<code>"PARM"</code>
See &lsquo;Details&rsquo; below for explanation and parameters to use in each of the methods.</p>
</td></tr>
<tr><td><code id="spotVol_+3A_alignby">alignBy</code></td>
<td>
<p>character, indicating the time scale in which <code>alignPeriod</code> is expressed. 
Possible values are: <code>"ticks"</code>, <code>"secs"</code>, <code>"seconds"</code>, <code>"mins"</code>, <code>"minutes"</code>, <code>"hours"</code></p>
</td></tr>
<tr><td><code id="spotVol_+3A_alignperiod">alignPeriod</code></td>
<td>
<p>positive integer, indicating the number of periods to aggregate
over. For example, to aggregate an <code>xts</code> object to the 5-minute frequency, set
<code>alignPeriod = 5</code> and <code>alignBy = "minutes"</code>.</p>
</td></tr>
<tr><td><code id="spotVol_+3A_marketopen">marketOpen</code></td>
<td>
<p>the market opening time. This should be in the time zone
specified by <code>tz</code>. By default, <code>marketOpen = "09:30:00"</code>.</p>
</td></tr>
<tr><td><code id="spotVol_+3A_marketclose">marketClose</code></td>
<td>
<p>the market closing time. This should be in the time zone
specified by <code>tz</code>. By default, <code>marketClose = "16:00:00"</code>.</p>
</td></tr>
<tr><td><code id="spotVol_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
We attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code></p>
</td></tr>
<tr><td><code id="spotVol_+3A_...">...</code></td>
<td>
<p>method-specific parameters (see &lsquo;Details&rsquo; below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following estimation methods can be specified in <code>method</code>:
</p>
<p><strong>Deterministic periodicity method (<code>"detPer"</code>)</strong>
</p>
<p>Parameters:
</p>

<ul>
<li> <p><code>dailyVol</code> A string specifying the estimation method for the daily component <code class="reqn">s_t</code>.
Possible values are <code>"rBPCov", "rRVar", "rMedRVar"</code>. <code>"rBPCov"</code> by default.
</p>
</li>
<li> <p><code>periodicVol</code> A string specifying the estimation method for the component of intraday volatility,
that depends in a deterministic way on the intraday time at which the return is observed.
Possible values are <code>"SD", "WSD", "TML", "OLS"</code>. See Boudt et al. (2011) for details. Default = <code>"TML"</code>.
</p>
</li>
<li> <p><code>P1</code> A positive integer corresponding to the number of cosine terms used in the flexible Fourier
specification of the periodicity function, see Andersen et al. (1997) for details. Default = 5.
</p>
</li>
<li> <p><code>P2</code> Same as <code>P1</code>, but for the sine terms. Default = 5. 
</p>
</li>
<li> <p><code>dummies</code> Boolean: in case it is <code>TRUE</code>, the parametric estimator of periodic standard deviation
specifies the periodicity function as the sum of dummy variables corresponding to each intraday period.
If it is <code>FALSE</code>, the parametric estimator uses the flexible Fourier specification. Default is <code>FALSE</code>. 
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li> <p><code>spot</code>
</p>
</li>
<li> <p><code>daily</code>
</p>
</li>
<li> <p><code>periodic</code>
</p>
</li></ul>

<p>Let there be <code class="reqn">T</code> days of <code class="reqn">N</code> equally-spaced log-returns <code class="reqn">r_{i,t}</code>, 
<code class="reqn">i = 1, \dots, N</code> and <code class="reqn">i = 1, \dots, T</code>.
In case of <code>method = "detPer"</code>, the returns are modeled as
</p>
<p style="text-align: center;"><code class="reqn">
r_{i,t} = f_i s_t u_{i,t}
</code>
</p>

<p>with independent <code class="reqn">u_{i,t} \sim \mathcal{N}(0,1)</code>.
The spot volatility is decomposed into a deterministic periodic factor
<code class="reqn">f_{i}</code> (identical for every day in the sample) and a daily factor
<code class="reqn">s_{t}</code> (identical for all observations within a day). 
Both components are then estimated separately, see Taylor and Xu (1997)
and Andersen and Bollerslev (1997). The jump robust versions by Boudt et al.
(2011) have also been implemented.
</p>
<p>If <code>periodicVol = "SD"</code>, we have
</p>
<p style="text-align: center;"><code class="reqn">
\hat f_i^{SD} = \frac{SD_i}{\sqrt{\frac{1}{\lfloor{\lambda / \Delta}\rfloor} \sum_{j = 1}^N SD_j^2}}
</code>
</p>

<p>with <code class="reqn">\Delta = 1 / N</code>, cross-daily averages <code class="reqn">SD_i = \sqrt{1/T \sum_{i = t}^T r_{i,t}^2}</code>, 
and <code class="reqn">\lambda</code> being the length of the intraday time intervals.
</p>
<p>If <code>periodicVol = "WSD"</code>, we have another nonparametric estimator that is robust to jumps in contrast to
<code>periodicVol = "SD"</code>. The definition of this estimator can be found in Boudt et al. (2011, Eqs. 2.9-2.12).
</p>
<p>The estimates when <code>periodicVol = "OLS"</code> and <code>periodicVol = "TML"</code> are based on the regression equation
</p>
<p style="text-align: center;"><code class="reqn">
\log \left| 1/T \sum_{t = 1}^T r_{i,t} \right| - c = \log f_i + \varepsilon_i
</code>
</p>

<p>with <em>i.i.d.</em> zero-mean error term <code class="reqn">\varepsilon_i</code> and <code class="reqn">c = -0.63518</code>. 
<code>periodicVol = "OLS"</code> employs ordinary-least-squares estimation and 
<code>periodicVol = "TML"</code> truncated maximum-likelihood estimation (see Boudt et al., 2011, Section 2.2, for further details).
</p>
<p><strong>Stochastic periodicity method (<code>"stochPer"</code>)</strong>
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>P1</code>: A positive integer corresponding to the number of cosine terms used in the flexible Fourier
specification of the periodicity function. Default = 5. 
</p>
</li>
<li><p><code>P2</code>: Same as <code>P1</code>, but for the sine terms. Default = 5.
</p>
</li>
<li><p><code>init</code>: A named list of initial values to be used in the optimization routine (<code>"BFGS"</code> in <code>optim</code>).
Default = <code>list(sigma = 0.03, sigma_mu = 0.005, sigma_h = 0.005, sigma_k = 0.05,
  phi = 0.2, rho = 0.98, mu = c(2, -0.5), delta_c = rep(0, max(1,P1)),
delta_s = rep(0, max(1,P2)))</code>. 
The naming of the parameters follows Beltratti and Morana (2001), the corresponding model equations are listed below.
<code>init</code> can contain any number of these parameters.
For parameters not specified in <code>init</code>, the default initial value will be used.
</p>
</li>
<li><p><code>control</code>: A list of options to be passed down to <code>optim</code>.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>par</code>
</p>
</li></ul>

<p>This method by Beltratti and Morana (2001) assumes the periodicity factor to
be stochastic. The spot volatility estimation is split into four components:
a random walk, an autoregressive process, a stochastic cyclical process and
a deterministic cyclical process. The model is estimated using a
quasi-maximum likelihood method based on the Kalman Filter. The package
<code>FKF</code> is used to apply the Kalman filter. In addition to
the spot volatility estimates, all parameter estimates are returned.
</p>
<p>The model for the intraday change in the return series is given by
</p>
<p style="text-align: center;"><code class="reqn">
r_{t,n} = \sigma_{t,n} \varepsilon_{t,n}, \ t = 1, \dots, T; \ n = 1, \dots, N,
</code>
</p>

<p>where <code class="reqn">\sigma_{t,n}</code> is the conditional standard deviation of the <code class="reqn">n</code>-th interval
of day <code class="reqn">t</code> and <code class="reqn">\varepsilon_{t,n}</code> is a <em>i.i.d.</em> mean-zero unit-variance process.
The conditional standard deviations are modeled as
</p>
<p style="text-align: center;"><code class="reqn">
\sigma_{t,n} = \sigma \exp \left(\frac{\mu_{t,n} + h_{t,n} + c_{t,n}}{2} \right)
</code>
</p>

<p>with <code class="reqn">\sigma</code> being a scaling factor and <code class="reqn">\mu_{t,n}</code> is the non-stationary volatility
component
</p>
<p style="text-align: center;"><code class="reqn">
\mu_{t,n} = \mu_{t,n-1} + \xi_{t,n}
</code>
</p>

<p>with independent <code class="reqn">\xi_{t,n} \sim \mathcal{N}(0,\sigma_\xi^2)</code>. 
<code class="reqn">h_{t,n}</code> is the stochastic stationary acyclical volatility component
</p>
<p style="text-align: center;"><code class="reqn">
h_{t,n} = \phi h_{t,n-1} + \nu_{t,n} 
</code>
</p>

<p>with independent <code class="reqn">\eta_{t,n} \sim \mathcal{N}(0,\sigma_\eta^2)</code> and <code class="reqn">| \phi | \leq 1</code>.
The cyclical component is separated in two components:
</p>
<p style="text-align: center;"><code class="reqn">
c_{t,n} = c_{1,t,n} + c_{2,t,n}
</code>
</p>
 
<p>The first component is written in state-space form, 
</p>
<p style="text-align: center;"><code class="reqn">
\left( \begin{array}{r}
c_{1,t,n} \\ c_{1,t,n}^*
\end{array}\right) =
\rho 
\left(\begin{array}{rr}
\cos \lambda &amp; \sin \lambda \\ -\sin \lambda &amp; \cos \lambda
\end{array}\right)
\left(\begin{array}{r}
c_{1,t,n - 1} \\ c_{1,t,n-1}^*
\end{array}\right)
+
\left(\begin{array}{r}
\kappa_{1,t,n} \\ \kappa_{1,t,n}^*
\end{array}\right)
</code>
</p>

<p>with <code class="reqn">0 \leq \rho \leq 1</code> and <code class="reqn">\kappa_{1,t,n}, \kappa_{1,t,n}^*</code> are 
mutually independent zero-mean normal random variables with variance <code class="reqn">\sigma_\kappa^2</code>.
All other parameters and the process <code class="reqn">c_{1,t,n}^*</code> in the state-space representation 
are only of instrumental use and are not part of
the return value which is why we won't introduce them in detail
in this vignette; see Beltratti and Morana (2001, pp. 208-209) for more information.
</p>
<p>The second component is given by
</p>
<p style="text-align: center;"><code class="reqn">
c_{2,t,n} = \mu_1 n_1 + \mu_2 n_2 + \sum_{p = 2}^P (\delta_{cp} \cos(p\lambda) + \delta_{sp} \sin (p \lambda n))
</code>
</p>

<p>with <code class="reqn">n_1 = 2n / (N+1)</code> and <code class="reqn">n_2 = 6n^2 / (N+1) / (N+2)</code>. 
</p>
<p><strong>Nonparametric filtering (<code>"kernel"</code>)</strong>
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>type</code> String specifying the type of kernel to be used. Options
include <code>"gaussian", "epanechnikov", "beta"</code>. Default = <code>"gaussian"</code>.
</p>
</li>
<li><p><code>h</code> Scalar or vector specifying bandwidth(s) to be used in kernel.
If <code>h</code> is a scalar, it will be assumed equal throughout the sample. If
it is a vector, it should contain bandwidths for each day. If left empty,
it will be estimated. Default = <code>NULL</code>.
</p>
</li>
<li><p><code>est</code> String specifying the bandwidth estimation method. Possible
values include <code>"cv", "quarticity"</code>. Method <code>"cv"</code> equals
cross-validation, which chooses the bandwidth that minimizes the Integrated
Square Error. <code>"quarticity"</code> multiplies the simple plug-in estimator
by a factor based on the daily quarticity of the returns. <code>est</code> is
obsolete if <code>h</code> has already been specified by the user.
<code>"cv"</code> by default.
</p>
</li>
<li><p><code>lower</code> Lower bound to be used in bandwidth optimization routine,
when using cross-validation method. Default is <code class="reqn">0.1n^{-0.2}</code>.
</p>
</li>
<li><p><code>upper</code> Upper bound to be used in bandwidth optimization routine,
when using cross-validation method. Default is <code class="reqn">n^{-0.2}</code>.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>par</code>
</p>
</li></ul>

<p>This method by Kristensen (2010) filters the spot volatility in a
nonparametric way by applying kernel weights to the standard realized
volatility estimator. Different kernels and bandwidths can
be used to focus on specific characteristics of the volatility process.
</p>
<p>Estimation results heavily depend on the bandwidth parameter <code class="reqn">h</code>, so it
is important that this parameter is well chosen. However, it is difficult to
come up with a method that determines the optimal bandwidth for any kind of
data or kernel that can be used. Although some estimation methods are
provided, it is advised that you specify <code class="reqn">h</code> yourself, or make sure that
the estimation results are appropriate.
</p>
<p>One way to estimate <code class="reqn">h</code>, is by using cross-validation. For each day in
the sample, <code class="reqn">h</code> is chosen as to minimize the Integrated Square Error,
which is a function of <code class="reqn">h</code>. However, this function often has multiple
local minima, or no minima at all (<code class="reqn">h \rightarrow \infty</code>). To ensure a reasonable
optimum is reached, strict boundaries have to be imposed on <code class="reqn">h</code>. These
can be specified by <code>lower</code> and <code>upper</code>, which by default are
<code class="reqn">0.1n^{-0.2}</code> and <code class="reqn">n^{-0.2}</code> respectively, where <code class="reqn">n</code> is the
number of observations in a day.
</p>
<p>When using the method <code>"kernel"</code>, in addition to the spot volatility
estimates, all used values of the bandwidth <code class="reqn">h</code> are returned.
</p>
<p>A formal definition of the estimator is too extensive for the context of this vignette.
Please refer to Kristensen (2010) for more detailed information. Our parameter
names are aligned with this reference.
</p>
<p><strong>Piecewise constant volatility (<code>"piecewise"</code>)</strong>
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>type</code> string specifying the type of test to be used. Options
include <code>"MDa", "MDb", "DM"</code>. See Fried (2012) for details. Default = <code>"MDa"</code>.
</p>
</li>
<li><p><code>m</code> number of observations to include in reference window.
Default = <code>40</code>.
</p>
</li>
<li><p><code>n</code> number of observations to include in test window.
Default = <code>20</code>.
</p>
</li>
<li><p><code>alpha</code> significance level to be used in tests. Note that the test
will be executed many times (roughly equal to the total number of
observations), so it is advised to use a small value for <code>alpha</code>, to
avoid a lot of false positives. Default = <code>0.005</code>.
</p>
</li>
<li><p><code>volEst</code> string specifying the realized volatility estimator to be
used in local windows. Possible values are <code>"rBPCov", "rRVar", "rMedRVar"</code>.
Default = <code>"rBPCov"</code>.
</p>
</li>
<li><p><code>online</code> boolean indicating whether estimations at a certain point
<code class="reqn">t</code> should be done online (using only information available at
<code class="reqn">t-1</code>), or ex post (using all observations between two change points).
Default = <code>TRUE</code>.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>cp</code>
</p>
</li></ul>

<p>This nonparametric method by Fried (2012) is a two-step approach and 
assumes the volatility to be
piecewise constant over local windows. Robust two-sample tests are applied to
detect changes in variability between subsequent windows. The spot volatility
can then be estimated by evaluating regular realized volatility estimators
within each local window.
<code>"MDa", "MDb"</code> refer to different test statistics, see Section 2.2 in Fried (2012).
</p>
<p>Along with the spot volatility estimates, this method will return the
detected change points in the volatility level. When plotting a
<code>spotVol</code> object containing <code>cp</code>, these change points will be
visualized.
</p>
<p><strong>GARCH models with intraday seasonality  (<code>"garch"</code>)</strong>
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>model</code> string specifying the type of test to be used. Options
include <code>"sGARCH", "eGARCH"</code>. See <code>ugarchspec</code> in the
<code>rugarch</code> package. Default = <code>"eGARCH"</code>.
</p>
</li>
<li><p><code>garchorder</code> numeric value of length 2, containing the order of
the GARCH model to be estimated. Default = <code>c(1,1)</code>.
</p>
</li>
<li><p><code>dist</code> string specifying the distribution to be assumed on the
innovations. See <code>distribution.model</code> in <code>ugarchspec</code> for
possible options. Default = <code>"norm"</code>.
</p>
</li>
<li><p><code>solver.control</code> list containing solver options.
See <code>ugarchfit</code> for possible values. Default = <code>list()</code>.
</p>
</li>
<li><p><code>P1</code> a positive integer corresponding to the number of cosine
terms used in the flexible Fourier specification of the periodicity function.
Default = 5.
</p>
</li>
<li><p><code>P2</code> same as <code>P1</code>, but for the sinus terms. Default = 5.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>ugarchfit</code>
</p>
</li></ul>

<p>Along with the spot volatility estimates, this method will return the
<code>ugarchfit</code> object used by the <code>rugarch</code> package.
</p>
<p>In this model, daily returns <code class="reqn">r_t</code> based on intraday observations <code class="reqn">r_{i,t}, i = 1, \dots, N</code>
are modeled as
</p>
<p style="text-align: center;"><code class="reqn">
r_t = \sum_{i = 1}^N r_{i,t} = \sigma_t \frac{1}{\sqrt{N}} \sum_{i = 1}^N s_i Z_{i,t}.
</code>
</p>

<p>with <code class="reqn">\sigma_t &gt; 0</code>, intraday seasonality <code class="reqn">s_i</code> &gt; 0, and <code class="reqn">Z_{i,t}</code> being 
a zero-mean unit-variance error term.  
</p>
<p>The overall approach is as in Appendix B of Andersen and Bollerslev (1997).
This method generates the external regressors <code class="reqn">s_i</code> needed to model the intraday
seasonality with a flexible Fourier form (Andersen and Bollerslev, 1997, Eqs. A.1-A.4). 
The <code>rugarch</code> package is then employed to estimate the specified intraday GARCH(1,1) model 
on the residuals <code class="reqn">r_{i,t} / s_i</code>.
</p>
<p><strong>Realized Measures (<code>"RM"</code>)</strong>
</p>
<p>This estimator takes trailing rolling window observations of intraday returns to estimate the spot volatility.
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>RM</code> string denoting which realized measure to use to estimate the local volatility. 
Possible values are: <code>"rBPCov", "rMedRVar", "rMinRVar", "rCov", "rRVar"</code>.
Default = <code>"rBPCov"</code>.
</p>
</li>
<li><p><code>lookBackPeriod</code> positive integer denoting the amount of sub-sampled returns to use 
for the estimation of the local volatility. Default is <code>10</code>.
</p>
</li>
<li><p><code>dontIncludeLast</code> logical indicating whether to omit the last return in the calculation of the local volatility.
This is done in Lee-Mykland (2008) to produce jump-robust estimates of spot volatility. 
Setting this to <code>TRUE</code> will then use <code>lookBackPeriod - 1</code> returns in the construction of the realized measures. Default = <code>FALSE</code>.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>RM</code>
</p>
</li>
<li><p><code>lookBackPeriod</code>
</p>
</li></ul>

<p>This method returns the estimates of the spot volatility, a string containing the realized measure used, and the lookBackPeriod.
</p>
<p><strong>(Non-overlapping) Pre-Averaged Realized Measures (<code>"PARM"</code>)</strong>
</p>
<p>This estimator takes rolling historical window observations of intraday returns to estimate the spot volatility 
as in the option <code>"RM"</code> but adds return pre-averaging of the realized measures. 
For a description of return pre-averaging see the details on <code><a href="#topic+spotDrift">spotDrift</a>.</code>
</p>
<p>Parameters:
</p>

<ul>
<li><p><code>RM</code> String denoting which realized measure to use to estimate the local volatility.
Possible values are: <code>"rBPCov", "rMedRVar", "rMinRVar", "rCov", and "rRVar"</code>. Default = <code>"rBPCov"</code>.
</p>
</li>
<li><p><code>lookBackPeriod</code> positive integer denoting the amount of sub-sampled returns to use for the estimation of the local volatility. Default = 50.
</p>
</li></ul>

<p>Outputs (see &lsquo;Value&rsquo; for a full description of each component):
</p>

<ul>
<li><p><code>spot</code>
</p>
</li>
<li><p><code>RM</code>
</p>
</li>
<li><p><code>lookBackPeriod</code>
</p>
</li>
<li><p><code>kn</code>
</p>
</li></ul>



<h3>Value</h3>

<p>A <code>spotVol</code> object, which is a list containing one or more of the
following outputs, depending on the method used:
</p>
 
<ul>
<li> <p><code>spot</code>
</p>
<p>An <code>xts</code> or <code>matrix</code> object (depending on the input) containing
spot volatility estimates <code class="reqn">\sigma_{t,i}</code>, reported for each interval
<code class="reqn">i</code> between <code>marketOpen</code> and <code>marketClose</code> for every day
<code class="reqn">t</code> in <code>data</code>. The length of the intervals is specified by <code>alignPeriod</code>
and <code>alignBy</code>. Methods that provide this output: All.
</p>
<p><code>daily</code>
An <code>xts</code> or <code>numeric</code> object (depending on the input) containing
estimates of the daily volatility levels for each day <code class="reqn">t</code> in <code>data</code>,
if the used method decomposed spot volatility into a daily and an intraday
component. Methods that provide this output: <code>"detPer"</code>.
</p>
</li>
<li> <p><code>periodic</code>
</p>
<p>An <code>xts</code> or <code>numeric</code> object (depending on the input) containing
estimates of the intraday periodicity factor for each day interval <code class="reqn">i</code>
between <code>marketOpen</code> and <code>marketClose</code>, if the spot volatility was
decomposed into a daily and an intraday component. If the output is in
<code>xts</code> format, this periodicity factor will be dated to the first day of
the input data, but it is identical for each day in the sample. Methods that
provide this output: <code>"detPer"</code>.
</p>
</li>
<li> <p><code>par</code>
</p>
<p>A named list containing parameter estimates, for methods that estimate one
or more parameters. Methods that provide this output:
<code>"stochper", "kernel"</code>.
</p>
</li>
<li> <p><code>cp</code>
</p>
<p>A vector containing the change points in the volatility, i.e. the observation
indices after which the volatility level changed, according to the applied
tests. The vector starts with a 0. Methods that provide this output:
<code>"piecewise"</code>.
</p>
</li>
<li> <p><code>ugarchfit</code>
</p>
<p>A <code>ugarchfit</code> object, as used by the <code>rugarch</code>
package, containing all output from fitting the GARCH model to the data.
Methods that provide this output: <code>"garch"</code>.
</p>
<p>The <code>spotVol</code> function offers several methods to estimate spot
volatility and its intraday seasonality, using high-frequency data. It
returns an object of class <code>spotVol</code>, which can contain various outputs,
depending on the method used. See &lsquo;Details&rsquo; for a description of each method.
In any case, the output will contain the spot volatility estimates.
</p>
<p>The input can consist of price data or return data, either tick by tick or
sampled at set intervals. The data will be converted to equispaced
high-frequency returns <code class="reqn">r_{t,i}</code> (read: the <code class="reqn">i</code>-th return on day
<code class="reqn">t</code>).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Andersen, T. G. and Bollerslev, T. (1997). Intraday periodicity and volatility persistence in financial markets. <em>Journal of Empirical Finance</em>, 4, 115-158.
</p>
<p>Beltratti, A. and Morana, C. (2001). Deterministic and stochastic methods for estimation of intraday seasonal components with high frequency data. <em>Economic Notes</em>, 30, 205-234.
</p>
<p>Boudt K., Croux C., and Laurent S. (2011). Robust estimation of intraweek periodicity in volatility and jump detection. <em>Journal of Empirical Finance</em>, 18, 353-367.
</p>
<p>Fried, R. (2012). On the online estimation of local constant volatilities. <em>Computational Statistics and Data Analysis</em>, 56, 3080-3090.
</p>
<p>Kristensen, D. (2010). Nonparametric filtering of the realized spot volatility: A kernel-based approach. <em>Econometric Theory</em>, 26, 60-93.
</p>
<p>Taylor, S. J. and Xu, X. (1997). The incremental volatility information in one million foreign exchange quotations. <em>Journal of Empirical Finance</em>, 4, 317-340.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
init &lt;- list(sigma = 0.03, sigma_mu = 0.005, sigma_h = 0.007,
             sigma_k = 0.06, phi = 0.194, rho = 0.986, mu = c(1.87,-0.42),
             delta_c = c(0.25, -0.05, -0.2, 0.13, 0.02),
             delta_s = c(-1.2, 0.11, 0.26, -0.03, 0.08))

# Next method will take around 370 iterations
vol1 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)], method = "stochPer", init = init)
plot(vol1$spot[1:780])
legend("topright", c("stochPer"), col = c("black"), lty=1)
## End(Not run)

# Various kernel estimates
## Not run: 
h1 &lt;- bw.nrd0((1:nrow(sampleOneMinuteData[, list(DT, PRICE = MARKET)]))*60)
vol2 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)],
                method = "kernel", h = h1)
vol3 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)], 
                method = "kernel", est = "quarticity")
vol4 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)],
                method = "kernel", est = "cv")
plot(cbind(vol2$spot, vol3$spot, vol4$spot))
xts::addLegend("topright", c("h = simple estimate", "h = quarticity corrected",
                     "h = crossvalidated"), col = 1:3, lty=1)

## End(Not run)

# Piecewise constant volatility
## Not run: 
vol5 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)], 
                method = "piecewise", m = 200, n  = 100, online = FALSE)
plot(vol5)
## End(Not run)

# Compare regular GARCH(1,1) model to eGARCH, both with external regressors
## Not run: 
vol6 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)], method = "garch", model = "sGARCH")
vol7 &lt;- spotVol(sampleOneMinuteData[, list(DT, PRICE = MARKET)], method = "garch", model = "eGARCH")
plot(as.numeric(t(vol6$spot)), type = "l")
lines(as.numeric(t(vol7$spot)), col = "red")
legend("topleft", c("GARCH", "eGARCH"), col = c("black", "red"), lty = 1)

## End(Not run)

## Not run: 
# Compare realized measure spot vol estimation to pre-averaged version
vol8 &lt;- spotVol(sampleTDataEurope[, list(DT, PRICE)], method = "RM", marketOpen = "09:00:00",
                marketClose = "17:30:00", tz = "UTC", alignPeriod = 1, alignBy = "mins",
                lookBackPeriod = 10)
vol9 &lt;- spotVol(sampleTDataEurope[, list(DT, PRICE)], method = "PARM", marketOpen = "09:00:00",
                marketClose = "17:30:00", tz = "UTC", lookBackPeriod = 10)
plot(zoo::na.locf(cbind(vol8$spot, vol9$spot)))

## End(Not run)
</code></pre>

<hr>
<h2 id='spreadPrices'>Convert to format for realized measures</h2><span id='topic+spreadPrices'></span>

<h3>Description</h3>

<p>Convenience function to split data from one <code>xts</code> or <code>data.table</code> 
with at least <code>"DT"</code>, <code>"SYMBOL"</code>, and <code>"PRICE"</code> columns to a format 
that can be used in the functions for calculation of realized measures. 
This is the opposite of <code><a href="#topic+gatherPrices">gatherPrices</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spreadPrices(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spreadPrices_+3A_data">data</code></td>
<td>
<p>An <code>xts</code> or a <code>data.table</code> object with at least <code>"DT"</code>, 
<code>"SYMBOL"</code>, and <code>"PRICE"</code> columns. This data should already be cleaned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>xts</code> or a <code>data.table</code> object with columns <code>"DT"</code> and 
a column named after each unique entrance in the <code>"SYMBOL"</code> column of the input. 
These columns contain the price of the associated symbol. We drop all other columns, e.g. <code>SIZE</code>.
</p>


<h3>Author(s)</h3>

<p>Emil Sjoerup.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gatherPrices">gatherPrices</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(data.table)
data1 &lt;- copy(sampleTData)[,  `:=`(PRICE = PRICE * runif(.N, min = 0.99, max = 1.01),
                                               DT = DT + runif(.N, 0.01, 0.02))]
data2 &lt;- copy(sampleTData)[, SYMBOL := 'XYZ']

dat &lt;- rbind(data1, data2)
setkey(dat, "DT")
dat &lt;- spreadPrices(dat)

rCov(dat, alignBy = 'minutes', alignPeriod = 5, makeReturns = TRUE, cor = TRUE) 

## End(Not run)
</code></pre>

<hr>
<h2 id='SPYRM'>SPY realized measures</h2><span id='topic+SPYRM'></span>

<h3>Description</h3>

<p>Realized measures for the SPY ETF calculated at 1 and 5 minute sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPYRM
</code></pre>


<h3>Format</h3>

<p>A <code>data.table</code> object
</p>


<h3>Note</h3>

<p>The CLOSE column is NOT the official close price, but simply the last recorded price of the day. Thus, this may be slightly different from other sources.
</p>

<hr>
<h2 id='summary.HARmodel'>Summary for <code>HARmodel</code> objects</h2><span id='topic+summary.HARmodel'></span>

<h3>Description</h3>

<p>Summary for <code>HARmodel</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HARmodel'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.HARmodel_+3A_object">object</code></td>
<td>
<p>An object of class <code>HARmodel</code></p>
</td></tr>
<tr><td><code id="summary.HARmodel_+3A_...">...</code></td>
<td>
<p>pass <code>lag</code> to determine the maximum order of the Newey West estimator. Default is <code>22</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified <code>summary.lm</code>
</p>

<hr>
<h2 id='tradesCleanup'>Cleans trade data</h2><span id='topic+tradesCleanup'></span>

<h3>Description</h3>

<p>This is a wrapper function for cleaning the trade data of all stock data inside the folder dataSource. 
The result is saved in the folder dataDestination. 
</p>
<p>In case you supply the argument <code>rawtData</code>, the on-disk functionality is ignored. The function returns a vector
indicating how many trades were removed at each cleaning step in this case.
and the function returns an <code>xts</code> or <code>data.table</code> object.
</p>
<p>The following cleaning functions are performed sequentially:
<code><a href="#topic+noZeroPrices">noZeroPrices</a></code>, <code><a href="#topic+autoSelectExchangeTrades">autoSelectExchangeTrades</a></code> or <code><a href="#topic+selectExchange">selectExchange</a></code>, <code><a href="#topic+tradesCondition">tradesCondition</a></code>, and
<code><a href="#topic+mergeTradesSameTimestamp">mergeTradesSameTimestamp</a></code>.
</p>
<p>Since the function <code><a href="#topic+rmTradeOutliersUsingQuotes">rmTradeOutliersUsingQuotes</a></code>
also requires cleaned quote data as input, it is not incorporated here and
there is a separate wrapper called <code><a href="#topic+tradesCleanupUsingQuotes">tradesCleanupUsingQuotes</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tradesCleanup(
  dataSource = NULL,
  dataDestination = NULL,
  exchanges = "auto",
  tDataRaw = NULL,
  report = TRUE,
  selection = "median",
  validConds = c("", "@", "E", "@E", "F", "FI", "@F", "@FI", "I", "@I"),
  marketOpen = "09:30:00",
  marketClose = "16:00:00",
  printExchange = TRUE,
  saveAsXTS = FALSE,
  tz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tradesCleanup_+3A_datasource">dataSource</code></td>
<td>
<p>character indicating the folder in which the original data is stored.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_datadestination">dataDestination</code></td>
<td>
<p>character indicating the folder in which the cleaned data is stored.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_exchanges">exchanges</code></td>
<td>
<p>vector of stock exchange symbols for all data in <code>dataSource</code>, 
e.g. <code>exchanges = c("T","N")</code> retrieves all stock market data from both NYSE and NASDAQ.
The possible exchange symbols are:
</p>

<ul>
<li><p> A: AMEX
</p>
</li>
<li><p> N: NYSE
</p>
</li>
<li><p> B: Boston
</p>
</li>
<li><p> P: Arca
</p>
</li>
<li><p> C: NSX
</p>
</li>
<li><p> T/Q: NASDAQ
</p>
</li>
<li><p> D: NASD ADF and TRF
</p>
</li>
<li><p> X: Philadelphia
</p>
</li>
<li><p> I: ISE
</p>
</li>
<li><p> M: Chicago
</p>
</li>
<li><p> W: CBOE
</p>
</li>
<li><p> Z: BATS
</p>
</li></ul>
<p> The default value is <code>"auto"</code> which automatically selects the exchange for the stocks and days independently using the <code><a href="#topic+autoSelectExchangeTrades">autoSelectExchangeTrades</a></code></p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_tdataraw">tDataRaw</code></td>
<td>
<p><code>xts</code> object containing raw trade data. This argument is <code>NULL</code> by default. Enabling it means the arguments
<code>from</code>, <code>to</code>, <code>dataSource</code> and <code>dataDestination</code> will be ignored (only advisable for small chunks of data).</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_report">report</code></td>
<td>
<p>boolean and <code>TRUE</code> by default. In case it is true the function returns (also) a vector indicating how many trades remained after each cleaning step.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_selection">selection</code></td>
<td>
<p>argument to be passed on to the cleaning routine <code><a href="#topic+mergeTradesSameTimestamp">mergeTradesSameTimestamp</a></code>. The default is &quot;median&quot;.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_validconds">validConds</code></td>
<td>
<p>character vector containing valid sales conditions. Passed through to <code><a href="#topic+tradesCondition">tradesCondition</a></code>.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_marketopen">marketOpen</code></td>
<td>
<p>character in the format of <code>"HH:MM:SS"</code>,
specifying the opening time of the exchange(s).</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_marketclose">marketClose</code></td>
<td>
<p>character in the format of <code>"HH:MM:SS"</code>,
specifying the closing time of the exchange(s).</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_printexchange">printExchange</code></td>
<td>
<p>Argument passed to <code><a href="#topic+autoSelectExchangeTrades">autoSelectExchangeTrades</a></code> indicates whether the chosen exchange is printed on the console, 
default is TRUE. This is only used when <code>exchanges</code> is <code>"auto"</code></p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_saveasxts">saveAsXTS</code></td>
<td>
<p>indicates whether data should be saved in <code>xts</code> format instead of <code>data.table</code> when using on-disk functionality. FALSE by default.</p>
</td></tr>
<tr><td><code id="tradesCleanup_+3A_tz">tz</code></td>
<td>
<p>fallback time zone used in case we we are unable to identify the timezone of the data, by default: <code>tz = NULL</code>. 
With the non-disk functionality, we attempt to extract the timezone from the DT column (or index) of the data, which may fail. 
In case of failure we use <code>tz</code> if specified, and if it is not specified, we use <code>"UTC"</code>. 
In the on-disk functionality, if <code>tz</code> is not specified, the timezone used will be the system default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the on-disk functionality with .csv.zip files from the WRDS database
will write temporary files on your machine in order to unzip the files - we try to clean up after it,
but cannot guarantee that there won't be files that slip through the crack if the permission settings on your machine does not match 
ours.
</p>
<p>If the input <code>data.table</code> does not contain a DT column but it does contain DATE and TIME_M columns, we create the DT column by REFERENCE, altering the <code>data.table</code> that may be in the user's environment.
</p>


<h3>Value</h3>

<p>For each day an <code>xts</code> or <code>data.table</code> object is saved into the folder of that date, containing the cleaned data.
This procedure is performed for each stock in <code>"ticker"</code>.
The function returns a vector indicating how many trades remained after each cleaning step.
</p>
<p>In case you supply the argument <code>rawtData</code>, the on-disk functionality is ignored
and the function returns a list with the cleaned trades as <code>xts</code> object (see examples).
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Hansen, P. R., Lunde, A., and Shephard, N. (2009). Realized kernels in practice: Trades and quotes. <em>Econometrics Journal</em>, 12, C1-C32.
</p>
<p>Brownlees, C.T. and Gallo, G.M. (2006). Financial econometric analysis at ultra-high frequency: Data handling concerns. <em>Computational Statistics &amp; Data Analysis</em>, 51, 2232-2245.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Consider you have raw trade data for 1 stock for 2 days 
head(sampleTDataRaw)
dim(sampleTDataRaw)
tDataAfterFirstCleaning &lt;- tradesCleanup(tDataRaw = sampleTDataRaw, 
                                         exchanges = list("N"))
tDataAfterFirstCleaning$report
dim(tDataAfterFirstCleaning$tData)
# In case you have more data it is advised to use the on-disk functionality
# via "dataSource" and "dataDestination" arguments

</code></pre>

<hr>
<h2 id='tradesCleanupUsingQuotes'>Perform a final cleaning procedure on trade data</h2><span id='topic+tradesCleanupUsingQuotes'></span>

<h3>Description</h3>

<p>Function performs cleaning procedure <code><a href="#topic+rmTradeOutliersUsingQuotes">rmTradeOutliersUsingQuotes</a></code> 
for the trades of all stocks data in &quot;dataDestination&quot;. 
Note that preferably the input data for this function 
is trade and quote data cleaned by respectively e.g. <code><a href="#topic+tradesCleanup">tradesCleanup</a></code>
and <code><a href="#topic+quotesCleanup">quotesCleanup</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tradesCleanupUsingQuotes(
  tradeDataSource = NULL,
  quoteDataSource = NULL,
  dataDestination = NULL,
  tData = NULL,
  qData = NULL,
  lagQuotes = 0,
  nSpreads = 1,
  BFM = FALSE,
  backwardsWindow = 3600,
  forwardsWindow = 0.5,
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_tradedatasource">tradeDataSource</code></td>
<td>
<p>character indicating the folder in which the original trade data is stored.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_quotedatasource">quoteDataSource</code></td>
<td>
<p>character indicating the folder in which the original quote data is stored.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_datadestination">dataDestination</code></td>
<td>
<p>character indicating the folder in which the cleaned data is stored, folder of <code>dataSource</code> by default.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_tdata">tData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object containing trade data cleaned by <code><a href="#topic+tradesCleanup">tradesCleanup</a></code>. This argument is <code>NULL</code> by default. Enabling it, means the arguments
<code>from</code>, <code>to</code>, <code>dataSource</code> and <code>dataDestination</code> will be ignored (only advisable for small chunks of data).</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_qdata">qData</code></td>
<td>
<p><code>data.table</code> or <code>xts</code> object containing cleaned quote data. This argument is NULL by default. Enabling it means the arguments
<code>from</code>, <code>to</code>, <code>dataSource</code>, <code>dataDestination</code> will be ignored (only advisable for small chunks of data).</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_lagquotes">lagQuotes</code></td>
<td>
<p>numeric, number of seconds the quotes are registered faster than
the trades (should be round and positive). Default is 0. For older datasets, i.e. before 2010, it may be a good idea to set this to, e.g., 2 (see, Vergote, 2005).</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_nspreads">nSpreads</code></td>
<td>
<p>numeric of length 1 denotes how far above the offer and below bid we allow outliers to be. Trades are filtered out if they are MORE THAN nSpread * spread above (below) the offer (bid)</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_bfm">BFM</code></td>
<td>
<p>a logical determining whether to conduct &quot;Backwards - Forwards matching&quot; of trades and quotes.
The algorithm tries to match trades that fall outside the bid - ask and first tries to match a small window forwards and if this fails, it tries to match backwards in a bigger window.
The small window is a tolerance for inaccuracies in the timestamps of bids and asks. The backwards window allow for matching of late reported trades, i.e. block trades.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_backwardswindow">backwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the backwards window used when <code>BFM = TRUE</code>. Default is 3600, corresponding to one hour.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_forwardswindow">forwardsWindow</code></td>
<td>
<p>a numeric denoting the length of the forwards window used when <code>BFM = TRUE</code>. Default is 0.5, corresponding to one half second.</p>
</td></tr>
<tr><td><code id="tradesCleanupUsingQuotes_+3A_plot">plot</code></td>
<td>
<p>a logical denoting whether to visualize the forwards, backwards, and unmatched trades in a plot. Passed on to <code><a href="#topic+rmTradeOutliersUsingQuotes">rmTradeOutliersUsingQuotes</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case you supply the arguments <code>tData</code> and <code>qData</code>, the on-disk functionality is ignored
and the function returns cleaned trades as a <code>data.table</code> or <code>xts</code> object (see examples).
</p>
<p>When using the on-disk functionality and tradeDataSource and quoteDataSource are the same, the quote files are all files in the folder that contains 'quote', and the rest are treated as containing trade data.
</p>


<h3>Value</h3>

<p>For each day an <code>xts</code> object is saved into the folder of that date, containing the cleaned data.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>


<h3>References</h3>

<p>Barndorff-Nielsen, O. E., Hansen, P. R., Lunde, A., and Shephard, N. (2009). Realized kernels in practice: Trades and quotes. <em>Econometrics Journal</em>, 12, C1-C32.
</p>
<p>Brownlees, C.T., and Gallo, G.M. (2006). Financial econometric analysis at ultra-high frequency: Data handling concerns. <em>Computational Statistics &amp; Data Analysis</em>, 51, 2232-2245.
</p>
<p>Christensen, K., Oomen, R. C. A., Podolskij, M. (2014): Fact or Friction: Jumps at ultra high frequency. <em>Journal of Financial Economics</em>, 144, 576-599
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Consider you have raw trade data for 1 stock for 2 days 
## Not run: 
tDataAfterFirstCleaning &lt;- tradesCleanup(tDataRaw = sampleTDataRaw, 
                                          exchanges = "N", report = FALSE)
qData &lt;- quotesCleanup(qDataRaw = sampleQDataRaw, 
                       exchanges = "N", report = FALSE)
dim(tDataAfterFirstCleaning)
tDataAfterFinalCleaning &lt;- 
  tradesCleanupUsingQuotes(qData = qData[as.Date(DT) == "2018-01-02"],
                           tData = tDataAfterFirstCleaning[as.Date(DT) == "2018-01-02"])
dim(tDataAfterFinalCleaning)

## End(Not run)
# In case you have more data it is advised to use the on-disk functionality
# via the "tradeDataSource", "quoteDataSource", and "dataDestination" arguments
</code></pre>

<hr>
<h2 id='tradesCondition'>Delete entries with abnormal trades condition.</h2><span id='topic+tradesCondition'></span>

<h3>Description</h3>

<p>Delete entries with abnormal trades condition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tradesCondition(
  tData,
  validConds = c("", "@", "E", "@E", "F", "FI", "@F", "@FI", "I", "@I")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tradesCondition_+3A_tdata">tData</code></td>
<td>
<p>an <code>xts</code> or <code>data.table</code> object containing the time series data, with 
one column named <code>"COND"</code> indicating the Sale Condition.</p>
</td></tr>
<tr><td><code id="tradesCondition_+3A_validconds">validConds</code></td>
<td>
<p>a character vector containing valid sales conditions defaults to <br />
<code>c('', '@', 'E', '@E', 'F', 'FI', '@F', '@FI', 'I', '@I')</code>. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To get more information on the sales conditions, see the NYSE documentation. Section about Daily TAQ Trades File.
The current version (as of May 2020) can be found online at <a href="https://www.nyse.com/publicdocs/nyse/data/Daily_TAQ_Client_Spec_v3.3.pdf">NYSE's webpage</a>
</p>


<h3>Value</h3>

<p><code>xts</code> or <code>data.table</code> object depending on input.
</p>


<h3>Note</h3>

<p>Some CSV readers and the WRDS API parses empty strings as NAs. We transform <code>NA</code> values in COND to <code>""</code>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Cornelissen, Kris Boudt, Onno Kleen, and Emil Sjoerup.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
