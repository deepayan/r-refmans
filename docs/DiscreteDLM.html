<!DOCTYPE html><html lang="en"><head><title>Help for package DiscreteDLM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DiscreteDLM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ALD'><p>The Asymmetric Laplacian Distribution</p></a></li>
<li><a href='#dataframe_DLM'><p>Creating a DLM-Ready Dataframe</p></a></li>
<li><a href='#DiscreteDLM-overview'>
<p>Fitting Discrete Distributed Lag Models with Variable Selection via MCMC</p></a></li>
<li><a href='#NB_MCMC'><p>Negative Binomial Regression via MCMC</p></a></li>
<li><a href='#plot.MCMC_DLM'><p>Visualises Results from MCMC_DLM Fits</p></a></li>
<li><a href='#QB_MCMC'><p>Binary Quantile Regression via MCMC</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Bayesian Distributed Lag Model Fitting for Binary and Count
Response Data</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Dempsey &lt;daniel.dempsey0@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for fitting Bayesian Distributed Lag Models (DLMs) to longitudinal response data that is a count or binary. Count data is fit using negative binomial regression and binary is fit using quantile regression. The contribution of the lags are fit via b-splines. In addition, infers the predictor inclusion uncertainty. Multimomial models are not supported. Based on Dempsey and Wyse (2025) &lt;<a href="https://doi.org/10.48550%2FarXiv.2403.03646">doi:10.48550/arXiv.2403.03646</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>statmod, BayesLogit, dlnm, splines, dplyr, ggplot2, ggridges,
reshape2</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/DanDempsey/DiscreteDLM">https://github.com/DanDempsey/DiscreteDLM</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/DanDempsey/DiscreteDLM/issues">https://github.com/DanDempsey/DiscreteDLM/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-06 01:49:36 UTC; bluff</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Dempsey <a href="https://orcid.org/0000-0003-0899-5412"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jason Wyse <a href="https://orcid.org/0000-0003-1391-7371"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Christian E. Galarza [ctb] (Author of the ald package, which this
    package derives some code from (see the COPYRIGHT file in the inst
    folder).),
  Victor H. Lachos [ctb] (Author of the ald package, which this package
    derives some code from (see the COPYRIGHT file in the inst
    folder).)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-06 19:40:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='ALD'>The Asymmetric Laplacian Distribution</h2><span id='topic+pALD'></span><span id='topic+qALD'></span><span id='topic+rALD'></span><span id='topic+dALD'></span><span id='topic+rTALD'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random generation for the Asymmetric Laplacian Distribution (ALD). Also contains random number generation for a truncated ALD. Our code here is heavily derived from the &quot;ald&quot; package by Galarza and Lachos (2021), adapted to vectorize the mu parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dALD(x, mu = 0, sigma = 1, p = 0.5)
pALD(q, mu = 0, sigma = 1, p = 0.5)
qALD(prob, mu = 0, sigma = 1, p = 0.5)
rALD(n, mu = 0, sigma = 1, p = 0.5)
rTALD(n, upper_tail, mu = 0, sigma = 1, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ALD_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters.</p>
</td></tr>
<tr><td><code id="ALD_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="ALD_+3A_p">p</code></td>
<td>
<p>ALD skew parameter.</p>
</td></tr>
<tr><td><code id="ALD_+3A_prob">prob</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="ALD_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="ALD_+3A_x">x</code>, <code id="ALD_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="ALD_+3A_upper_tail">upper_tail</code></td>
<td>
<p>Logical denoting what portion of the distribution is retained. If TRUE, only positive values are sampled, and negative if FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are based on the three parameter ALD:
</p>
<p style="text-align: center;"><code class="reqn">f(x|\mu, \sigma, p) = \frac{p(1-p)}{\sigma}exp\left(-\rho_p(\frac{x-\mu}{\sigma})\right)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\rho_p(z) = z(p - I_{z&lt;0})</code>
</p>

<p>These functions differ than the ones provided in the <em>ald</em> package by vectorising the mu parameter. In addition we have implemented a function to sample from the truncated ALD (rTALD), which is needed for Bayesian quantile regression. Note that truncation is fixed at zero; the user only decides whether the negative or positive axis is discarded.
</p>


<h3>Value</h3>

<p>Each function returns a vector. dALD, pALD and qALD returns the PDF, CDF and inverse CDF of the ALD respectively. rALD returns a random sample from the ALD distribution, and rTALD returns a random sample from the ALD truncated at 0.
</p>


<h3>Author(s)</h3>

<p>Daniel Dempsey (<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>). Original code from the <em>ald</em> package were written by Christian E. Galarza and Victor H. Lachos.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed( 100 )

### Vectorised input
random_ALD &lt;- rALD( 1000, mu = runif(1000, -100, 100) )
plot( random_ALD )

### Truncated version
trunc_random_ALD &lt;- rTALD( 1000, TRUE, mu = 2, sigma = 3, p = 0.75 )
plot( dALD(sort(trunc_random_ALD), mu = 2, sigma = 3, p = 0.75) )

</code></pre>

<hr>
<h2 id='dataframe_DLM'>Creating a DLM-Ready Dataframe</h2><span id='topic+dataframe_DLM'></span>

<h3>Description</h3>

<p>Transforms a dataframe to prepare it for distributed lag modelling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataframe_DLM(X, lag, dynamic_vars = NULL, arglag = list(fun = "bs"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dataframe_DLM_+3A_x">X</code></td>
<td>
<p>A dataframe, or something that can be coerced into one.</p>
</td></tr>
<tr><td><code id="dataframe_DLM_+3A_lag">lag</code></td>
<td>
<p>Lag length of the dynamic variable. See <code><a href="dlnm.html#topic+crossbasis">crossbasis</a></code> for more details.</p>
</td></tr>
<tr><td><code id="dataframe_DLM_+3A_dynamic_vars">dynamic_vars</code></td>
<td>
<p>The column name or indices that correspond to the longitudinal variables. If left missing, the dataset is not altered in any way and a warning is supplied.</p>
</td></tr>
<tr><td><code id="dataframe_DLM_+3A_arglag">arglag</code></td>
<td>
<p>A list that is passed into <code><a href="dlnm.html#topic+onebasis">onebasis</a></code> for generating a basis matrix for the lag space. See <code><a href="dlnm.html#topic+crossbasis">crossbasis</a></code>.</p>
</td></tr>
<tr><td><code id="dataframe_DLM_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed into the <code><a href="dlnm.html#topic+crossbasis">crossbasis</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of this function is to streamline the preperation of the data for distributed lag modelling in the case where every dynamic variable is handled the same way.
</p>
<p>If no dynamic variables are given, the input dataframe is returned unaltered with a warning. Otherwise, the function returns a <em>dataframe_DLM</em> object, which is essentially treated like a standard dataframe but contains extra information to simplify the process of distributed lag modelling when using the <em>NB_MCMC</em> or <em>QB_MCMC</em> functions.
</p>


<h3>Value</h3>

<p>The input is returned unchanged if no dynamic variables are given. Otherwise a <em>dataframe_DLM</em> object is returned. See details.
</p>


<h3>Author(s)</h3>

<p>Daniel Dempsey (<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- dplyr::select( dlnm::chicagoNMMAPS, c('cvd', 'dow', 'temp', 'dptp', 'o3') )
X &lt;- na.omit( X )
arglag &lt;- list( fun = 'bs', df = 4 )
DLM_dat &lt;- dataframe_DLM( X, lag = 40, dynamic_vars =  c('temp', 'dptp', 'o3'), arglag = arglag )
</code></pre>

<hr>
<h2 id='DiscreteDLM-overview'>
Fitting Discrete Distributed Lag Models with Variable Selection via MCMC
</h2><span id='topic+DiscreteDLM-overview'></span><span id='topic+DiscreteDLM'></span>

<h3>Description</h3>

<p>DiscreteDLM contains functionality for fitting and visualising Bayesian Distributed Lag Models (DLMs) when the response variable is either count or binary. More specifically, this package contains an implementation of Bayesian quantile binary regression (for binary response data) and negative-binomial regression (for count response data). Additionally, these functions implement variable uncertainty inference for each predictor. Multinomial regression is not implemented.
</p>
<p>While these functions can be applied to any standard regression problem, the package was set up with DLMs in mind; the functions therein allow for an easy-to-use pipeline to go from defining the DLM, fitting the DLM, and visualising the results.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> DiscreteDLM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.9.8</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-11-06</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=3)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Daniel Dempsey &lt;<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>&gt;, with contributions and supervision by Jason Wyse &lt;<a href="mailto:wyseja@tcd.ie">wyseja@tcd.ie</a>&gt;
</p>


<h3>References</h3>

<p>Daniel Dempsey and Jason Wyse. 2025. &quot;Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary Quantile and Count Data Regressions.&quot; https://doi.org/10.48550/arXiv.2403.03646.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dataframe_DLM">dataframe_DLM</a></code>,<code><a href="#topic+NB_MCMC">NB_MCMC</a></code>,<code><a href="#topic+QB_MCMC">QB_MCMC</a></code>
</p>

<hr>
<h2 id='NB_MCMC'>Negative Binomial Regression via MCMC</h2><span id='topic+NB_MCMC'></span>

<h3>Description</h3>

<p>Fits a negative binomial regression model using Gibbs sampling and performs Bayesian variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NB_MCMC(
  formula,
  data = NULL,
  nsamp = 1000,
  nburn = 1000,
  thin = 1,
  standardize = TRUE,
  prior_beta_mu = 0,
  prior_beta_sigma = 100,
  prior_gamma_p = 0.5,
  prior_xi_shape = 2,
  prior_xi_scale = 1/50,
  init_beta = 0,
  init_gamma = FALSE,
  init_xi = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NB_MCMC_+3A_formula">formula</code></td>
<td>
<p>Formula object to set the symbolic description of the model to be fitted.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_data">data</code></td>
<td>
<p>Optional dataframe. Ideally this should be a <code><a href="#topic+dataframe_DLM">dataframe_DLM</a></code> object if doing distributed lag modelling.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_nsamp">nsamp</code></td>
<td>
<p>The desired sample size from the posterior. Set to 5000 by default.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_nburn">nburn</code></td>
<td>
<p>The number of iterations of the MCMC to be discarded as burn-in. Set to 5000 by default.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_thin">thin</code></td>
<td>
<p>Thinning factor of the MCMC chain after burn-in. Set to 1 by default.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_standardize">standardize</code></td>
<td>
<p>Logical indicating if the data should be standardised prior to fitting the model to zero mean and unit variance. If there is no intercept, then only scaling is applied (with a warning). The posterior sample of beta is transformed back to the original scale upon completion of the algorithm.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_prior_beta_mu">prior_beta_mu</code></td>
<td>
<p>Mean of the Gaussian prior for the regression coefficients, beta. Either a vector of length equal to the number of predictors or a single numeric to represent a constant vector.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_prior_beta_sigma">prior_beta_sigma</code></td>
<td>
<p>Covariance matrix of the Gaussian prior for the regression coefficients, beta. Either a square matrix of dimension equal to the number of predictors or a single numeric to represent an isotropic covariance matrix.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_prior_gamma_p">prior_gamma_p</code></td>
<td>
<p>Probability parameter of the Bernoulli prior for the predictor inclusion parameter, gamma. Either a vector of length equal to the number of predictors or a single numeric, to represent that all predictors have the same prior probability of inclusion.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_prior_xi_shape">prior_xi_shape</code></td>
<td>
<p>Shape parameter of the Gamma distribution prior for the negative binomial stopping parameter, xi.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_prior_xi_scale">prior_xi_scale</code></td>
<td>
<p>Scale parameter of the Gamma distribution prior for the negative binomial stopping parameter, xi.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_init_beta">init_beta</code></td>
<td>
<p>Initial MCMC values for the beta parameters. Either a vector of length equal to the number of predictors or a single numeric representing the same starting value for each beta component.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_init_gamma">init_gamma</code></td>
<td>
<p>Initial MCMC values for the gamma parameters. Either a vector of length equal to the number of predictors or a single numeric representing the same starting value for each gamma component.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_init_xi">init_xi</code></td>
<td>
<p>Initial MCMC value for xi.</p>
</td></tr>
<tr><td><code id="NB_MCMC_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating if a progress report should be printed to the console during the run.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits a negative binomial regression model via MCMC. Latent variable representation allows for Gibbs sampling of the parameters; see Pillow and Scott (2012) and Zhou et. al. (2012). The algorithm also includes predictor inclusion uncertainty inference (inferred via a Metroplis step) adapted from Holmes and Held (2006).
</p>
<p>The parameters of interest for this model are the regression slopes (beta), the binary predictor inclusion indicator (gamma) and the negative binomial stopping parameter (xi). For further details, see Dempsey and Wyse (2024).
</p>
<p>Note that when setting initial values and priors for gamma, the intercept must be included and therefore its initial value and prior are forced to be equal to 1. If an intercept is not used, then the first variable is forced to be included instead.
</p>


<h3>Value</h3>

<p>An &quot;MCMC_DLM&quot; object; a list containing the MCMC-derived posterior sample, as well as the data that were used.
</p>


<h3>Author(s)</h3>

<p>Daniel Dempsey (<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>)
</p>


<h3>References</h3>

<p>Chris C. Holmes and Leonhard Held. 2006. &quot;Bayesian auxiliary variable models for binary and multinomial regression.&quot; Bayesian Analysis 1(1): 145-168.
</p>
<p>Daniel Dempsey and Jason Wyse. 2025. &quot;Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary Quantile and Count Data Regressions.&quot; https://doi.org/10.48550/arXiv.2403.03646
</p>
<p>Jonathan Pillow and James Scott. 2012. &quot;Fully Bayesian inference for neural models with negative-binomial spiking.&quot; Advances in neural information processing systems 25.
</p>
<p>Mingyuan Zhou, Lingbo Li, David Dunson and Lawrence Carin. 2012. &quot;Lognormal and gamma mixed negative binomial regression.&quot; Proceedings of the... International Conference on Machine Learning. International Conference on Machine Learning. Vol. 2012. NIH Public Access.
</p>


<h3>See Also</h3>

<p><a href="#topic+QB_MCMC">QB_MCMC</a>, <a href="#topic+dataframe_DLM">dataframe_DLM</a>, <a href="#topic+plot.MCMC_DLM">plot.MCMC_DLM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Set up data
X &lt;- dplyr::select( dlnm::chicagoNMMAPS, c('cvd', 'dow', 'temp', 'dptp', 'o3') )
X &lt;- na.omit( X )
arglag &lt;- list( fun = 'bs', df = 4 )
DLM_dat &lt;- dataframe_DLM( X, lag = 40, dynamic_vars =  c('temp', 'dptp', 'o3'), arglag = arglag )

### Fit model
# NOTE: Only using 100 total iterations for illustration purposes!
myfit &lt;- NB_MCMC( cvd ~ ., data = DLM_dat, nsamp = 50, nburn = 50 )
summary( myfit )
</code></pre>

<hr>
<h2 id='plot.MCMC_DLM'>Visualises Results from MCMC_DLM Fits</h2><span id='topic+plot.MCMC_DLM'></span>

<h3>Description</h3>

<p>Provides some ggplot visualisations of the posterior from MCMC_DLM objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MCMC_DLM'
plot(x, type = "beta", include_intercept = FALSE, print_output = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.MCMC_DLM_+3A_x">x</code></td>
<td>
<p>An MCMC_DLM object.</p>
</td></tr>
<tr><td><code id="plot.MCMC_DLM_+3A_type">type</code></td>
<td>
<p>One of &quot;beta&quot;, &quot;gamma&quot;, &quot;xi&quot;, or &quot;lags&quot;. Partial matching is supported. See Details below.</p>
</td></tr>
<tr><td><code id="plot.MCMC_DLM_+3A_include_intercept">include_intercept</code></td>
<td>
<p>Logical indicating if the intercept should be included in the graphs.</p>
</td></tr>
<tr><td><code id="plot.MCMC_DLM_+3A_print_output">print_output</code></td>
<td>
<p>Logical indicating whether or not the ggplot objects should be printed to the screen.</p>
</td></tr>
<tr><td><code id="plot.MCMC_DLM_+3A_...">...</code></td>
<td>
<p>Extra arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The type of plot generated is determined by the <em>type</em> argument. The options are:
</p>

<ul>
<li> <p><em>beta</em>: A ridgeplot of the regression slopes,
</p>
</li>
<li> <p><em>gamma</em>: A bar chart of the mean probability of inclusion,
</p>
</li>
<li> <p><em>xi</em>: A kernel density estimate plot of the negative binomial stopping parameter (of course, this only works for negative binomial regression fits),
</p>
</li>
<li> <p><em>lags</em>: A collection of plots (supplied in a list), each containing a line chart of the lag-response for each dynamic variable.
</p>
</li></ul>



<h3>Value</h3>

<p>A ggplot object, except when type = 'lags', in which case it will be a list of ggplot objects. Will display the ggplot visual if print_output = TRUE.
</p>


<h3>Author(s)</h3>

<p>Daniel Dempsey (<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>)
</p>

<hr>
<h2 id='QB_MCMC'>Binary Quantile Regression via MCMC</h2><span id='topic+QB_MCMC'></span>

<h3>Description</h3>

<p>Fits a quantile regression model to a binary response dataset using Gibbs sampling and performs Bayesian variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QB_MCMC(
  formula,
  data = NULL,
  quantile = 0.5,
  nsamp = 1000,
  nburn = 1000,
  thin = 1,
  standardize = TRUE,
  prior_beta_mu = 0,
  prior_beta_sigma = 100,
  prior_gamma_p = 0.5,
  init_beta = 0,
  init_gamma = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="QB_MCMC_+3A_formula">formula</code></td>
<td>
<p>Formula object to set the symbolic description of the model to be fitted.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_data">data</code></td>
<td>
<p>Optional dataframe. Ideally this should be a <code><a href="#topic+dataframe_DLM">dataframe_DLM</a></code> object if doing distributed lag modelling.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_quantile">quantile</code></td>
<td>
<p>The chosen quantile for regression.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_nsamp">nsamp</code></td>
<td>
<p>The desired sample size from the posterior. Set to 5000 by default.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_nburn">nburn</code></td>
<td>
<p>The number of iterations of the MCMC to be discarded as burn-in. Set to 5000 by default.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_thin">thin</code></td>
<td>
<p>Thinning factor of the MCMC chain after burn-in. Set to 1 by default (no values discarded after burn-in).</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_standardize">standardize</code></td>
<td>
<p>Logical indicating if the data should be standardised prior to fitting the model to zero mean and unit variance. If there is no intercept, then only scaling is applied (with a warning). The posterior sample of beta is transformed back to the original scale upon completion of the algorithm.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_prior_beta_mu">prior_beta_mu</code></td>
<td>
<p>Mean of the Gaussian prior for the regression coefficients, beta. Either a vector of length equal to the number of predictors or a single numeric to represent a constant vector.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_prior_beta_sigma">prior_beta_sigma</code></td>
<td>
<p>Covariance matrix of the Gaussian prior for the regression coefficients, beta. Either a square matrix of dimension equal to the number of predictors or a single numeric to represent an isotropic covariance matrix.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_prior_gamma_p">prior_gamma_p</code></td>
<td>
<p>Probability parameter of the Bernoulli prior for the predictor inclusion parameter, gamma. Either a vector of length equal to the number of predictors or a single numeric, to represent that all predictors have the same prior probability of inclusion.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_init_beta">init_beta</code></td>
<td>
<p>Initial MCMC values for the beta parameters. Either a vector of length equal to the number of predictors or a single numeric representing the same starting value for each beta component.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_init_gamma">init_gamma</code></td>
<td>
<p>Initial MCMC values for the gamma parameters. Either a vector of length equal to the number of predictors or a single numeric representing the same starting value for each gamma component.</p>
</td></tr>
<tr><td><code id="QB_MCMC_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating if a progress report should be printed to the console during the run.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits a quantile binary regression model via MCMC. Latent variable representation allows for Gibbs sampling of the parameters; see Benoit and Van den Poel (2017). The algorithm also includes predictor inclusion uncertainty inference (inferred via a Metroplis step) adapted from Holmes and Held (2006).
</p>
<p>The parameters of interest for this model are the regression slopes (beta) and the binary predictor inclusion indicator (gamma). For further details, see Dempsey and Wyse (2024).
</p>
<p>Note that when setting initial values and priors for gamma, the intercept must be included and therefore its initial value and prior are forced to be equal to 1. If an intercept is not used, then the first variable is forced to be included instead.
</p>


<h3>Value</h3>

<p>An &quot;MCMC_DLM&quot; object; a list containing the MCMC-derived posterior sample, as well as the data that were used.
</p>


<h3>Author(s)</h3>

<p>Daniel Dempsey (<a href="mailto:daniel.dempsey0@gmail.com">daniel.dempsey0@gmail.com</a>)
</p>


<h3>References</h3>

<p>Chris C. Holmes and Leonhard Held. 2006. &quot;Bayesian auxiliary variable models for binary and multinomial regression.&quot; Bayesian Analysis 1(1): 145-168.
</p>
<p>Daniel Dempsey and Jason Wyse. 2025. &quot;Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary Quantile and Count Data Regressions.&quot; https://doi.org/10.48550/arXiv.2403.03646
</p>
<p>Dries F. Benoit and Dirk Van den Poel. 2017. &quot;bayesQR: A Bayesian approach to quantile regression.&quot; Journal of Statistical Software 76: 1-32.
</p>


<h3>See Also</h3>

<p><a href="#topic+NB_MCMC">NB_MCMC</a>, <a href="#topic+dataframe_DLM">dataframe_DLM</a>, <a href="#topic+plot.MCMC_DLM">plot.MCMC_DLM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Set up data
set.seed( 100 )
binary_response &lt;- dlnm::chicagoNMMAPS$cvd &gt; 65
predictors &lt;- dplyr::select( dlnm::chicagoNMMAPS, c('dow', 'temp', 'dptp', 'o3') )
X &lt;- cbind( binary_response, predictors )
X &lt;- na.omit( X )
arglag &lt;- list( fun = 'bs', df = 4 )
DLM_dat &lt;- dataframe_DLM( X, lag = 40, dynamic_vars =  c('temp', 'dptp', 'o3'), arglag = arglag )

### Fit model
# NOTE: Only using 100 total iterations for illustration purposes!
myfit &lt;- QB_MCMC( binary_response ~ ., data = DLM_dat, nsamp = 50, nburn = 50 )
summary( myfit )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
