<!DOCTYPE html><html lang="en"><head><title>Help for package idiolect</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {idiolect}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calibrate_LLR'><p>Calibrate scores into Log-Likelihood Ratios</p></a></li>
<li><a href='#chunk_texts'><p>Chunk a corpus</p></a></li>
<li><a href='#concordance'><p>Qualitative examination of evidence</p></a></li>
<li><a href='#contentmask'><p>Content masking</p></a></li>
<li><a href='#create_corpus'><p>Create a corpus</p></a></li>
<li><a href='#delta'><p>Delta</p></a></li>
<li><a href='#density_plot'><p>Plot density of TRUE/FALSE distributions</p></a></li>
<li><a href='#enron.sample'><p>Enron sample</p></a></li>
<li><a href='#impostors'><p>Impostors Method</p></a></li>
<li><a href='#lambdaG'><p>Apply the LambdaG algorithm</p></a></li>
<li><a href='#lambdaG_visualize'><p>Visualize the output of the LambdaG algorithm</p></a></li>
<li><a href='#most_similar'><p>Select the most similar texts to a specific text</p></a></li>
<li><a href='#ngram_tracing'><p>N-gram tracing</p></a></li>
<li><a href='#performance'><p>Performance evaluation</p></a></li>
<li><a href='#posterior'><p>Posterior prosecution probabilities and odds</p></a></li>
<li><a href='#tokenize_sents'><p>Tokenize to sentences</p></a></li>
<li><a href='#vectorize'><p>Vectorize data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Forensic Authorship Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Carry out comparative authorship analysis of disputed and undisputed texts within the Likelihood Ratio Framework for expressing evidence in forensic science. This package contains implementations of well-known algorithms for comparative authorship analysis, such as Smith and Aldridge's (2011) Cosine Delta &lt;<a href="https://doi.org/10.1080%2F09296174.2011.533591">doi:10.1080/09296174.2011.533591</a>&gt; or Koppel and Winter's (2014) Impostors Method &lt;<a href="https://doi.org/10.1002%2Fasi.22954">doi:10.1002/asi.22954</a>&gt;, as well as functions to measure their performance and to calibrate their outputs into Log-Likelihood Ratios.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/andreanini/idiolect">https://github.com/andreanini/idiolect</a>,
<a href="https://andreanini.github.io/idiolect/">https://andreanini.github.io/idiolect/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/andreanini/idiolect/issues">https://github.com/andreanini/idiolect/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>quanteda, R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, dplyr, fdrtool, ggplot2, kgrams, pbapply, pROC, proxy,
quanteda.textstats, spacyr, stringr, textclean</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, readtext, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-24 20:41:00 UTC; mzjssan4</td>
</tr>
<tr>
<td>Author:</td>
<td>Andrea Nini <a href="https://orcid.org/0000-0003-4218-5130"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  David van Leeuwen [cph] (Author of some bundled functions from package
    ROC)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andrea Nini &lt;andrea.nini@manchester.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-28 09:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='calibrate_LLR'>Calibrate scores into Log-Likelihood Ratios</h2><span id='topic+calibrate_LLR'></span>

<h3>Description</h3>

<p>This function is used to transform the scores returned by any of the authorship analysis functions into a Log-Likelihood Ratio (LLR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate_LLR(calibration.dataset, dataset, latex = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrate_LLR_+3A_calibration.dataset">calibration.dataset</code></td>
<td>
<p>A data frame containing the calibration dataset, typically the output of an authorship analysis function like <code><a href="#topic+impostors">impostors()</a></code>.</p>
</td></tr>
<tr><td><code id="calibrate_LLR_+3A_dataset">dataset</code></td>
<td>
<p>A data frame containing the scores that have to be calibrated into LLRs using the calibration dataset. This is typically the result of applying a function like <code><a href="#topic+impostors">impostors()</a></code> to the Q texts.</p>
</td></tr>
<tr><td><code id="calibrate_LLR_+3A_latex">latex</code></td>
<td>
<p>A logical value. If FALSE (default), then the hypothesis labels are printed as plain text (Hp/Hd). If TRUE the labels are written to be read in LaTeX ($H_p$/$H_d$).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame with the LLRs (base 10), as well as the verbal label according to Marquis et al (2016) and a verbal interpretation of the results.
</p>


<h3>References</h3>

<p>Marquis, Raymond, Alex Biedermann, Liv Cadola, Christophe Champod, Line Gueissaz, Geneviève Massonnet, Williams David Mazzella, Franco Taroni &amp; Tacha Hicks. 2016. Discussion on how to implement a verbal scale in a forensic laboratory: Benefits, pitfalls and suggestions to avoid misunderstandings. Science &amp; Justice 56(5). 364–370. https://doi.org/10.1016/j.scijus.2016.05.009.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>calib &lt;- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE))
q &lt;- data.frame(score = c(0.6, 0.002))
calibrate_LLR(calib, q)

</code></pre>

<hr>
<h2 id='chunk_texts'>Chunk a corpus</h2><span id='topic+chunk_texts'></span>

<h3>Description</h3>

<p>This function can be used to chunk a corpus in order to control sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chunk_texts(corpus, size)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chunk_texts_+3A_corpus">corpus</code></td>
<td>
<p>A <code>quanteda</code> corpus.</p>
</td></tr>
<tr><td><code id="chunk_texts_+3A_size">size</code></td>
<td>
<p>The size of the chunks in number of tokens.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>quanteda</code> corpus object where each text is a chunk of the size requested.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corpus &lt;- quanteda::corpus(c("The cat sat on the mat", "The dog sat on the chair"))
quanteda::docvars(corpus, "author") &lt;- c("A", "B")
chunk_texts(corpus, size = 2)


</code></pre>

<hr>
<h2 id='concordance'>Qualitative examination of evidence</h2><span id='topic+concordance'></span>

<h3>Description</h3>

<p>This function uses <code><a href="quanteda.html#topic+kwic">quanteda::kwic()</a></code> to return a concordance for a search pattern. The function takes as input three datasets and a pattern and returns a data frame with the hits labelled for authorship.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concordance(
  q.data,
  k.data,
  reference.data,
  search,
  token.type = "word",
  window = 5,
  case_insensitive = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="concordance_+3A_q.data">q.data</code></td>
<td>
<p>A <code>quanteda</code> corpus object, such as the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>.</p>
</td></tr>
<tr><td><code id="concordance_+3A_k.data">k.data</code></td>
<td>
<p>A <code>quanteda</code> corpus object, such as the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>.</p>
</td></tr>
<tr><td><code id="concordance_+3A_reference.data">reference.data</code></td>
<td>
<p>A <code>quanteda</code> corpus object, such as the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>. This is optional.</p>
</td></tr>
<tr><td><code id="concordance_+3A_search">search</code></td>
<td>
<p>A string. It can be any sequence of characters and it also accepts the use of * as a wildcard.</p>
</td></tr>
<tr><td><code id="concordance_+3A_token.type">token.type</code></td>
<td>
<p>Choice between &quot;word&quot; (default), which searches for word or punctuation mark tokens, or &quot;character&quot;, which instead uses a single character search.</p>
</td></tr>
<tr><td><code id="concordance_+3A_window">window</code></td>
<td>
<p>The number of context items to be displayed around the keyword (a <code><a href="quanteda.html#topic+kwic">quanteda::kwic()</a></code> parameter).</p>
</td></tr>
<tr><td><code id="concordance_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>Logical; if TRUE, ignore case (a <code><a href="quanteda.html#topic+kwic">quanteda::kwic()</a></code> parameter).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame containing the concordances for the search pattern.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], "wants to", token.type = "word")

#using wildcards
concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], "want * to", token.type = "word")

#searching character sequences with wildcards
concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], "help*", token.type = "character")


</code></pre>

<hr>
<h2 id='contentmask'>Content masking</h2><span id='topic+contentmask'></span>

<h3>Description</h3>

<p>This function offers three algorithms for topic/content masking. In order to run the masking algorithms, a <code>spacy</code> tokenizer or POS-tagger has to be run first (via <code>spacyr</code>). For more information about the masking algorithms see Details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contentmask(
  corpus,
  model = "en_core_web_sm",
  algorithm = "POSnoise",
  fw_list = "eng_halvani",
  replace_non_ascii = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="contentmask_+3A_corpus">corpus</code></td>
<td>
<p>A <code>quanteda</code> corpus object, typically the output of the <code><a href="#topic+create_corpus">create_corpus()</a></code> function.</p>
</td></tr>
<tr><td><code id="contentmask_+3A_model">model</code></td>
<td>
<p>The spacy model to use. The default is &quot;en_core_web_sm&quot;.</p>
</td></tr>
<tr><td><code id="contentmask_+3A_algorithm">algorithm</code></td>
<td>
<p>A string, either &quot;POSnoise&quot; (default), &quot;frames&quot;, or &quot;textdistortion&quot;.</p>
</td></tr>
<tr><td><code id="contentmask_+3A_fw_list">fw_list</code></td>
<td>
<p>The list of function words to use for the <code>textdistortion</code> algorithm. This is either the default (&quot;eng_halvani&quot;) for the same list of function words used for <code>POSnoise</code> or it can be a vector of strings where each string is a function word to keep.</p>
</td></tr>
<tr><td><code id="contentmask_+3A_replace_non_ascii">replace_non_ascii</code></td>
<td>
<p>A logical value indicating whether to remove non-ASCII characters (including emojis). This is the default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default algorithm for content masking that this function applies is <code>POSnoise</code> (Halvani and Graner 2021). This algorithm only works for English and it transforms a text by masking tokens using their POS tag if these tokens are: nouns, verbs, adjectives, adverbs, digits, and symbols while leaving all the rest unchanged. <code>POSnoise</code> uses a list of function words for English that also includes frequent words belonging to the masked Part of Speech tags that tend to be mostly functional (e.g. make, recently, well).
</p>
<p>Another algorithm implemented is Nini's (2023) <code>frames</code> or <code style="white-space: pre;">&#8288;frame n-grams&#8288;</code>. This algorithm does not involve a special list of tokens and therefore can potentially work for any language provided that the correct <code>spacy</code> model is loaded. This algorithm consists in masking all tokens using their POS tag only when these are nouns, verbs, or personal pronouns.
</p>
<p>Finally, the last algorithm implemented is a version of <code>textdistortion</code>, as originally proposed by Stamatatos (2017). This version of the algorithm is essentially <code>POSnoise</code> but without POS tag information. The default implementation uses the same list of function words that are used for <code>POSnoise</code>. In addition to the function words provided, the function treats all punctuation marks and new line breaks as function words to keep. The basic tokenization is done using <code>spacyr</code> so the right model for the language being analysed should be selected.
</p>
<p>If you have never used <code>spacyr</code> before then please follow the instructions to set it up and install a model before using this function.
</p>
<p>The removal of non-ASCII characters is done using the <code>textclean</code> package.
</p>


<h3>Value</h3>

<p>A <code>quanteda</code> corpus object only containing functional tokens, depending on the algorithm chosen. The corpus contains the same docvars as the input. Email addresses or URLs are treated like nouns.
</p>


<h3>References</h3>

<p>Halvani, Oren &amp; Lukas Graner. 2021. POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis. In Proceedings of the 16th International Conference on Availability, Reliability and Security, 1–12. Vienna, Austria: Association for Computing Machinery. https://doi.org/10.1145/3465481.3470050.
Nini, Andrea. 2023. A Theory of Linguistic Individuality for Authorship Analysis (Elements in Forensic Linguistics). Cambridge, UK: Cambridge University Press.
Stamatatos, Efstathios. 2017. Masking topic-related information to enhance authorship attribution. Journal of the Association for Information Science and Technology. https://doi.org/10.1002/asi.23968.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
text &lt;- "The cat was on the chair. He didn't move\ncat@pets.com;\nhttp://quanteda.io/. i.e. a test "
toy.corpus &lt;- quanteda::corpus(text)
contentmask(toy.corpus, algorithm = "POSnoise")
contentmask(toy.corpus, algorithm = "textdistortion")

## End(Not run)
</code></pre>

<hr>
<h2 id='create_corpus'>Create a corpus</h2><span id='topic+create_corpus'></span>

<h3>Description</h3>

<p>Function to read in text data and turn it into a <code>quanteda</code> corpus object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_corpus(path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_corpus_+3A_path">path</code></td>
<td>
<p>A string containing the path to a folder of plain text files (ending in .txt) with their name structured as following: authorname_textname.txt (e.g. smith_text1.txt).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>quanteda</code> corpus object with the authors' names as a docvar.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
path &lt;- "path/to/data"
create_corpus(path)

## End(Not run)

</code></pre>

<hr>
<h2 id='delta'>Delta</h2><span id='topic+delta'></span>

<h3>Description</h3>

<p>This function runs a <em>Cosine Delta</em> analysis (Smith and Aldridge 2011; Evert et al. 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delta(
  q.data,
  k.data,
  tokens = "word",
  remove_punct = FALSE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  lowercase = TRUE,
  n = 1,
  trim = TRUE,
  threshold = 150,
  features = FALSE,
  cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delta_+3A_q.data">q.data</code></td>
<td>
<p>The questioned or disputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>).</p>
</td></tr>
<tr><td><code id="delta_+3A_k.data">k.data</code></td>
<td>
<p>The known or undisputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>).</p>
</td></tr>
<tr><td><code id="delta_+3A_tokens">tokens</code></td>
<td>
<p>The type of tokens to extract, either &quot;word&quot; (default) or &quot;character&quot;.</p>
</td></tr>
<tr><td><code id="delta_+3A_remove_punct">remove_punct</code></td>
<td>
<p>A logical value. FALSE (default) keeps punctuation marks.</p>
</td></tr>
<tr><td><code id="delta_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>A logical value. TRUE (default) removes symbols.</p>
</td></tr>
<tr><td><code id="delta_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>A logical value. TRUE (default) removes numbers</p>
</td></tr>
<tr><td><code id="delta_+3A_lowercase">lowercase</code></td>
<td>
<p>A logical value. TRUE (default) transforms all tokens to lower case.</p>
</td></tr>
<tr><td><code id="delta_+3A_n">n</code></td>
<td>
<p>The order or size of the n-grams being extracted. Default is 1.</p>
</td></tr>
<tr><td><code id="delta_+3A_trim">trim</code></td>
<td>
<p>A logical value. If TRUE (default) then only the most frequent tokens are kept.</p>
</td></tr>
<tr><td><code id="delta_+3A_threshold">threshold</code></td>
<td>
<p>A numeric value indicating how many most frequent tokens to keep if trim = TRUE. The default is 150.</p>
</td></tr>
<tr><td><code id="delta_+3A_features">features</code></td>
<td>
<p>Logical with default FALSE. If TRUE, then the output will contain the features used.</p>
</td></tr>
<tr><td><code id="delta_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing (the default is one).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If features is set to FALSE then the output is a data frame containing the results of all comparisons between the Q texts and the K texts. If features is set to TRUE then the output is a list containing the results data frame and the vector of features used for the analysis.
</p>


<h3>References</h3>

<p>Evert, Stefan, Thomas Proisl, Fotis Jannidis, Isabella Reger, Steffen Pielström, Christof Schöch &amp; Thorsten Vitt. 2017. Understanding and explaining Delta measures for authorship attribution. Digital Scholarship in the Humanities 32. ii4–ii16. https://doi.org/10.1093/llc/fqx023.
Smith, Peter W H &amp; W Aldridge. 2011. Improving Authorship Attribution: Optimizing Burrows’ Delta Method*. Journal of Quantitative Linguistics 18(1). 63–88. https://doi.org/10.1080/09296174.2011.533591.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Q &lt;- enron.sample[c(5:6)]
K &lt;- enron.sample[-c(5:6)]
delta(Q, K)

</code></pre>

<hr>
<h2 id='density_plot'>Plot density of TRUE/FALSE distributions</h2><span id='topic+density_plot'></span>

<h3>Description</h3>

<p>Plot density of TRUE/FALSE distributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>density_plot(dataset, q = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="density_plot_+3A_dataset">dataset</code></td>
<td>
<p>A data frame containing the calibration dataset, typically the output of an authorship analysis function like <code><a href="#topic+impostors">impostors()</a></code>.</p>
</td></tr>
<tr><td><code id="density_plot_+3A_q">q</code></td>
<td>
<p>This optional argument should be one value or a vector of values that contain the score of the disputed text(s). These are then plotted as lines crossing the density distributions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot2</code> plot with the density distributions for the scores for TRUE (typically, 'same-author') vs. FALSE (typically, 'different-author').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res &lt;- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE))
q &lt;- c(0.11, 0.7)
density_plot(res, q)

</code></pre>

<hr>
<h2 id='enron.sample'>Enron sample</h2><span id='topic+enron.sample'></span>

<h3>Description</h3>

<p>A small sample of the <em>Enron</em> corpus comprising ten authors with approximately the same amount of data. Each author has one text labelled as 'unknown' and the other texts labelled as 'known'. The data was pre-processed using the <em>POSnoise</em> algorithm to mask content (see <code><a href="#topic+contentmask">contentmask()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enron.sample
</code></pre>


<h3>Format</h3>

<p>A <code>quanteda</code> corpus object.
</p>


<h3>Source</h3>

<p>Halvani, Oren. 2021. Practice-Oriented Authorship Verification. Technical University of Darmstadt PhD Thesis.
https://tuprints.ulb.tu-darmstadt.de/19861/
</p>

<hr>
<h2 id='impostors'>Impostors Method</h2><span id='topic+impostors'></span>

<h3>Description</h3>

<p>This function runs the <em>Impostors Method</em> for authorship verification. The Impostors Method is based on calculating a similarity score and then, using a corpus of impostor texts, perform a bootstrapping analysis sampling random subsets of features and impostors in order to test the robustness of this similarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impostors(
  q.data,
  k.data,
  cand.imps,
  algorithm = "RBI",
  coefficient = "minmax",
  k = 300,
  m = 100,
  n = 25,
  features = FALSE,
  cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="impostors_+3A_q.data">q.data</code></td>
<td>
<p>The questioned or disputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>).</p>
</td></tr>
<tr><td><code id="impostors_+3A_k.data">k.data</code></td>
<td>
<p>The known or undisputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>). More than one sample for a candidate author is accepted for all algorithms except IM.</p>
</td></tr>
<tr><td><code id="impostors_+3A_cand.imps">cand.imps</code></td>
<td>
<p>The impostors data for the candidate authors, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>). This can be the same object as <code>k.data</code> (e.g. to recycle impostors).</p>
</td></tr>
<tr><td><code id="impostors_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which impostors algorithm to use, either &quot;RBI&quot; (deafult), &quot;KGI&quot;, or &quot;IM&quot;.</p>
</td></tr>
<tr><td><code id="impostors_+3A_coefficient">coefficient</code></td>
<td>
<p>A string indicating the coefficient to use, either &quot;minmax&quot; (default) or &quot;cosine&quot;. This does not apply to the algorithm KGI, where the distance is &quot;minmax&quot;.</p>
</td></tr>
<tr><td><code id="impostors_+3A_k">k</code></td>
<td>
<p>The <em>k</em> parameters for the RBI algorithm. Not used by other algorithms. The default is 300.</p>
</td></tr>
<tr><td><code id="impostors_+3A_m">m</code></td>
<td>
<p>The <em>m</em> parameter for the IM algorithm. Not used by other algorithms. The default is 100.</p>
</td></tr>
<tr><td><code id="impostors_+3A_n">n</code></td>
<td>
<p>The <em>n</em> parameter for the IM algorithm. Not used by other algorithms. The default is 25.</p>
</td></tr>
<tr><td><code id="impostors_+3A_features">features</code></td>
<td>
<p>A logical value indicating whether the important features should be retrieved or not. The default is FALSE. This only applies to the RBI algorithm.</p>
</td></tr>
<tr><td><code id="impostors_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing (the default is one).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several variants of the Impostors Method and this function can run three of them:
</p>

<ol>
<li><p> IM: this is the original Impostors Method as proposed by Koppel and Winter (2014).
</p>
</li>
<li><p> KGI: Kestemont's et al. (2016) version, which is a very popular implementation of the Impostors Method in stylometry. It is inspired by IM and by its generalized version, the General Impostors Method proposed by Seidman (2013).
</p>
</li>
<li><p> RBI: the Rank-Based Impostors Method (Potha and Stamatatos 2017, 2020), which is the default option as it is the most recent and as it tends to outperform the original.
The two data sets <code>q.data</code>, <code>k.data</code>, must be disjunct in terms of the texts that they contain otherwise an error is returned. However, <code>cand.imps</code> and <code>k.data</code> can be the same object, for example, to use the other candidates' texts as impostors. The function will always exclude impostor texts with the same author as the Q and K texts considered.
</p>
</li></ol>



<h3>Value</h3>

<p>The function will test all possible combinations of Q texts and candidate authors and return a
data frame containing a score ranging from 0 to 1, with a higher score indicating a higher likelihood that the same author produced the two sets of texts. The data frame contains a column called &quot;target&quot; with a logical value which is TRUE if the author of the Q text is the candidate and FALSE otherwise.
</p>
<p>If the RBI algorithm is selected and the features parameter is TRUE then the data frame will also contain a column with the features that are likely to have had an impact on the score. These are all those features that are consistently found to be shared by the candidate author's data and the questioned data and that also tend to be rare in the dataset of impostors.
</p>


<h3>References</h3>

<p>Kestemont, Mike, Justin Stover, Moshe Koppel, Folgert Karsdorp &amp; Walter Daelemans. 2016. Authenticating the writings of Julius Caesar. Expert Systems With Applications 63. 86–96. https://doi.org/10.1016/j.eswa.2016.06.029.
</p>
<p>Koppel, Moshe &amp; Yaron Winter. 2014. Determining if two documents are written by the same author. Journal of the Association for Information Science and Technology 65(1). 178–187.
</p>
<p>Potha, Nektaria &amp; Efstathios Stamatatos. 2017. An Improved Impostors Method for Authorship Verification. In Gareth J.FALSE. Jones, Séamus Lawless, Julio Gonzalo, Liadh Kelly, Lorraine Goeuriot, Thomas Mandl, Linda Cappellato &amp; Nicola Ferro (eds.), Experimental IR Meets Multilinguality, Multimodality, and Interaction (Lecture Notes in Computer Science), vol. 10456, 138–144. Springer, Cham. https://doi.org/10.1007/978-3-319-65813-1_14. (5 September, 2017).
</p>
<p>Potha, Nektaria &amp; Efstathios Stamatatos. 2020. Improved algorithms for extrinsic author verification. Knowledge and Information Systems 62(5). 1903–1921. https://doi.org/10.1007/s10115-019-01408-4.
</p>
<p>Seidman, Shachar. 2013. Authorship Verification Using the Impostors Method. In Pamela Forner, Roberto Navigli, Dan Tufis &amp; Nicola Ferro (eds.), Proceedings of CLEF 2013 Evaluation Labs and Workshop – Working Notes Papers, 23–26. Valencia, Spain. https://ceur-ws.org/Vol-1179/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Q &lt;- enron.sample[1]
K &lt;- enron.sample[2:3]
imps &lt;- enron.sample[4:9]
impostors(Q, K, imps, algorithm = "KGI")

</code></pre>

<hr>
<h2 id='lambdaG'>Apply the LambdaG algorithm</h2><span id='topic+lambdaG'></span>

<h3>Description</h3>

<p>This function calculates the likelihood ratio of grammar models, or <code class="reqn">\lambda_G</code>, as in Nini et al. (under review). In order to run the analysis as in this paper, all data must be preprocessed using <code><a href="#topic+contentmask">contentmask()</a></code> with the &quot;algorithm&quot; parameter set to &quot;POSnoise&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lambdaG(q.data, k.data, ref.data, N = 10, r = 30, cores = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lambdaG_+3A_q.data">q.data</code></td>
<td>
<p>The questioned or disputed data as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>).</p>
</td></tr>
<tr><td><code id="lambdaG_+3A_k.data">k.data</code></td>
<td>
<p>The known or undisputed data as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>).</p>
</td></tr>
<tr><td><code id="lambdaG_+3A_ref.data">ref.data</code></td>
<td>
<p>The reference dataset as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>). This can be the same object as <code>k.data</code>.</p>
</td></tr>
<tr><td><code id="lambdaG_+3A_n">N</code></td>
<td>
<p>The order of the model. Default is 10.</p>
</td></tr>
<tr><td><code id="lambdaG_+3A_r">r</code></td>
<td>
<p>The number of iterations. Default is 30.</p>
</td></tr>
<tr><td><code id="lambdaG_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing (the default is one).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function will test all possible combinations of Q texts and candidate authors and return a
data frame containing <code class="reqn">\lambda_G</code>, an uncalibrated log-likelihood ratio (base 10). <code class="reqn">\lambda_G</code> can then be calibrated into a likelihood ratio that expresses the strength of the evidence using <code><a href="#topic+calibrate_LLR">calibrate_LLR()</a></code>. The data frame contains a column called &quot;target&quot; with a logical value which is TRUE if the author of the Q text is the candidate and FALSE otherwise.
</p>


<h3>References</h3>

<p>Nini, A., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based on the Likelihood Ratio of Grammar Models. https://arxiv.org/abs/2403.08462v1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q.data &lt;- enron.sample[1] |&gt; quanteda::tokens("sentence")
k.data &lt;- enron.sample[2:10] |&gt; quanteda::tokens("sentence")
ref.data &lt;- enron.sample[11:ndoc(enron.sample)] |&gt; quanteda::tokens("sentence")
lambdaG(q.data, k.data, ref.data)

</code></pre>

<hr>
<h2 id='lambdaG_visualize'>Visualize the output of the LambdaG algorithm</h2><span id='topic+lambdaG_visualize'></span>

<h3>Description</h3>

<p>This function outputs a colour-coded list of sentences belonging to the input Q text ordered from highest to lowest <code class="reqn">\lambda_G</code>, as shown in Nini et al. (under review).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lambdaG_visualize(
  q.data,
  k.data,
  ref.data,
  N = 10,
  r = 30,
  output = "html",
  print = "",
  scale = "absolute",
  cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lambdaG_visualize_+3A_q.data">q.data</code></td>
<td>
<p>A single questioned or disputed text as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>).</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_k.data">k.data</code></td>
<td>
<p>A known or undisputed corpus containing exclusively a single candidate author's texts as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>).</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_ref.data">ref.data</code></td>
<td>
<p>The reference dataset as a <code>quanteda</code> tokens object with the tokens being sentences (e.g. the output of <code><a href="#topic+tokenize_sents">tokenize_sents()</a></code>).</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_n">N</code></td>
<td>
<p>The order of the model. Default is 10.</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_r">r</code></td>
<td>
<p>The number of iterations. Default is 30.</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_output">output</code></td>
<td>
<p>A string detailing the file type of the colour-coded text output. Either &quot;html&quot; (default) or &quot;latex&quot;.</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_print">print</code></td>
<td>
<p>A string indicating the path to the folder where to print a colour-coded text file. If left empty (default), then nothing is printed.</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_scale">scale</code></td>
<td>
<p>A string indicating what scale to use to colour-code the text file. If &quot;absolute&quot; (default) then the raw <code class="reqn">\lambda_G</code> is used; if &quot;relative&quot;, then the z-score of <code class="reqn">\lambda_G</code> over the Q data is used instead, thus showing relative importance.</p>
</td></tr>
<tr><td><code id="lambdaG_visualize_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing (the default is one).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function outputs a list of two objects: a data frame with each row being a token in the Q text and the values of <code class="reqn">\lambda_G</code> for the token and sentences, in decreasing order of sentence <code class="reqn">\lambda_G</code> and with the relative contribution of each token and each sentence to the final <code class="reqn">\lambda_G</code> in percentage; the raw code in html or LaTeX that generates the colour-coded file. If a path is provided for the print argument then the function will also save the colour-coded text as an html or plain text file.
</p>


<h3>References</h3>

<p>Nini, A., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based on the Likelihood Ratio of Grammar Models. https://arxiv.org/abs/2403.08462v1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>q.data &lt;- corpus_trim(enron.sample[1], "sentences", max_ntoken = 10) |&gt; quanteda::tokens("sentence")
k.data &lt;- enron.sample[2:5]|&gt; quanteda::tokens("sentence")
ref.data &lt;- enron.sample[6:ndoc(enron.sample)] |&gt; quanteda::tokens("sentence")
outputs &lt;- lambdaG_visualize(q.data, k.data, ref.data, r = 2)
outputs$table

</code></pre>

<hr>
<h2 id='most_similar'>Select the most similar texts to a specific text</h2><span id='topic+most_similar'></span>

<h3>Description</h3>

<p>Select the most similar texts to a specific text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>most_similar(sample, pool, coefficient, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="most_similar_+3A_sample">sample</code></td>
<td>
<p>This is a single row of a <code>quanteda</code> dfm representing the sample to match.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_pool">pool</code></td>
<td>
<p>This is a dfm containing all possible samples from which to select the top n.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_coefficient">coefficient</code></td>
<td>
<p>The coefficient to use for similarity. Either &quot;minmax&quot;, &quot;cosine&quot;, or &quot;Phi&quot;.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_n">n</code></td>
<td>
<p>The number of rows to extract from the pool of potential samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a dfm containing the top n most similar rows to the input sample using the minmax distance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text1 &lt;- "The cat sat on the mat"
text2 &lt;- "The dog sat on the chair"
text3 &lt;- "Violence is the last refuge of the incompetent"
c &lt;- quanteda::corpus(c(text1, text2, text3))
d &lt;- quanteda::tokens(c) |&gt; quanteda::dfm() |&gt; quanteda::dfm_weight(scheme = "prop")
most_similar(d[1,], d[-1,], coefficient = "minmax", n = 1)

</code></pre>

<hr>
<h2 id='ngram_tracing'>N-gram tracing</h2><span id='topic+ngram_tracing'></span>

<h3>Description</h3>

<p>This function runs the authorship analysis method called <em>n-gram tracing</em>, which can be used for both attribution and verification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngram_tracing(
  q.data,
  k.data,
  tokens = "character",
  remove_punct = FALSE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  lowercase = TRUE,
  n = 9,
  coefficient = "simpson",
  features = FALSE,
  cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ngram_tracing_+3A_q.data">q.data</code></td>
<td>
<p>The questioned or disputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>).</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_k.data">k.data</code></td>
<td>
<p>The known or undisputed data, either as a corpus (the output of <code><a href="#topic+create_corpus">create_corpus()</a></code>) or as a <code>quanteda</code> dfm (the output of <code><a href="#topic+vectorize">vectorize()</a></code>). More than one sample for a candidate author is accepted but the function will combine them so to make a profile.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_tokens">tokens</code></td>
<td>
<p>The type of tokens to extract, either &quot;word&quot; or &quot;character&quot; (default).</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_remove_punct">remove_punct</code></td>
<td>
<p>A logical value. FALSE (default) keeps punctuation marks.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>A logical value. TRUE (default) removes symbols.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>A logical value. TRUE (default) removes numbers.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_lowercase">lowercase</code></td>
<td>
<p>A logical value. TRUE (default) transforms all tokens to lower case.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_n">n</code></td>
<td>
<p>The order or size of the n-grams being extracted. Default is 9.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_coefficient">coefficient</code></td>
<td>
<p>The coefficient to use to compare texts, one of: &quot;simpson&quot; (default), &quot;phi&quot;, &quot;jaccard&quot;, &quot;kulczynski&quot;, or &quot;cole&quot;.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_features">features</code></td>
<td>
<p>Logical with default FALSE. If TRUE then the result table will contain the features in the overlap that are unique for that overlap in the corpus. If only two texts are present then this will return the n-grams in common.</p>
</td></tr>
<tr><td><code id="ngram_tracing_+3A_cores">cores</code></td>
<td>
<p>The number of cores to use for parallel processing (the default is one).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>N-gram tracing was originally proposed by Grieve et al (2019). Nini (2023) then proposed a mathematical reinterpretation that is compatible with Cognitive Linguistic theories of language processing. He then tested several variants of the method and found that the original version, which uses the Simpson's coefficient, tends to be outperformed by versions using the Phi coefficient, the Kulczynski's coefficient, and the Cole coefficient. This function can run the n-gram tracing method using any of these coefficients plus the Jaccard coefficient for reference, as this coefficient has been applied in several forensic linguistic studies.
</p>


<h3>Value</h3>

<p>The function will test all possible combinations of Q texts and candidate authors and return a
data frame containing the value of the similarity coefficient selected called 'score' and an optional column with the overlapping features that only occur in the Q and candidate considered and in no other Qs (ordered by length if the n-gram is of variable length). The data frame contains a column called 'target' with a logical value which is TRUE if the author of the Q text is the candidate and FALSE otherwise.
</p>


<h3>References</h3>

<p>Grieve, Jack, Emily Chiang, Isobelle Clarke, Hannah Gideon, Aninna Heini, Andrea Nini &amp; Emily Waibel. 2019. Attributing the Bixby Letter using n-gram tracing. Digital Scholarship in the Humanities 34(3). 493–512.
Nini, Andrea. 2023. A Theory of Linguistic Individuality for Authorship Analysis (Elements in Forensic Linguistics). Cambridge, UK: Cambridge University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Q &lt;- enron.sample[c(5:6)]
K &lt;- enron.sample[-c(5:6)]
ngram_tracing(Q, K, coefficient = 'phi')

</code></pre>

<hr>
<h2 id='performance'>Performance evaluation</h2><span id='topic+performance'></span>

<h3>Description</h3>

<p>This function is used to the test the performance of an authorship analysis method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance(training, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="performance_+3A_training">training</code></td>
<td>
<p>The data frame with the results to evaluate, typically the output of an authorship analysis function, such as <code><a href="#topic+impostors">impostors()</a></code>. If only training is present then the function will perform a leave-one-out cross-validation.</p>
</td></tr>
<tr><td><code id="performance_+3A_test">test</code></td>
<td>
<p>Optional data frame of results. If present then a calibration model is extracted from training and its performance is evaluated on this data set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Before applying a method to a real authorship case, it is good practice to test it on known ground truth data. This function performs this test by taking as input either a single table of results or two tables, one for training and one for the test, and then returning as output a list with the following performance statistics: the log-likelihood ratio cost (both <code class="reqn">C_{llr}</code> and <code class="reqn">C_{llr}^{min}</code>), Equal Error Rate (ERR), the mean values of the log-likelihood ratio for both the same-author (TRUE) and different-author (FALSE) cases, the Area Under the Curve (AUC), Balanced Accuracy, Precision, Recall, F1, and the full confusion matrix. The binary classification statistics are all calculated considering a Log-Likelihood Ratio score of 0 as a threshold.
</p>


<h3>Value</h3>

<p>The function returns a list containing a data frame with performance statistics, including an object that can be used to make a tippet plot using the <code>tippet.plot()</code> function of the <code>ROC</code> package (https://github.com/davidavdav/ROC).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>results &lt;- data.frame(score = c(0.5, 0.2, 0.8, 0.01), target = c(TRUE, FALSE, TRUE, FALSE))
perf &lt;- performance(results)
perf$evaluation

</code></pre>

<hr>
<h2 id='posterior'>Posterior prosecution probabilities and odds</h2><span id='topic+posterior'></span>

<h3>Description</h3>

<p>This function takes as input a value of the Log-Likelihood Ratio and returns a table that shows the impact on some simulated prior probabilities for the prosecution hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior(LLR)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="posterior_+3A_llr">LLR</code></td>
<td>
<p>One single numeric value corresponding to a Log-Likelihood Ratio (base 10).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing some simulated prior probabilities/odds for the prosecution and the resulting posterior probabilities/odds after the LLR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>posterior(LLR = 0)
posterior(LLR = 1.8)
posterior(LLR = -0.5)
posterior(LLR = 4)

</code></pre>

<hr>
<h2 id='tokenize_sents'>Tokenize to sentences</h2><span id='topic+tokenize_sents'></span>

<h3>Description</h3>

<p>This function turns a corpus of texts into a <code>quanteda</code> tokens object of sentences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_sents(corpus, model = "en_core_web_sm")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tokenize_sents_+3A_corpus">corpus</code></td>
<td>
<p>A <code>quanteda</code> corpus object, typically the output of the <code><a href="#topic+create_corpus">create_corpus()</a></code> function or the output of <code><a href="#topic+contentmask">contentmask()</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_sents_+3A_model">model</code></td>
<td>
<p>The spacy model to use. The default is &quot;en_core_web_sm&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function first split each text into paragraphs by splitting at new line markers and then uses spacy to tokenize each paragraph into sentences. The function accepts a plain text corpus input or the output of <code><a href="#topic+contentmask">contentmask()</a></code>. This function is necessary to prepare the data for <code><a href="#topic+lambdaG">lambdaG()</a></code>.
</p>


<h3>Value</h3>

<p>A <code>quanteda</code> tokens object where each token is a sentence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
toy.pos &lt;- corpus("the N was on the N . he did n't move \n N ; \n N N")
tokenize_sents(toy.pos)

## End(Not run)

</code></pre>

<hr>
<h2 id='vectorize'>Vectorize data</h2><span id='topic+vectorize'></span>

<h3>Description</h3>

<p>This function turns texts into feature vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vectorize(
  input,
  tokens,
  remove_punct,
  remove_symbols,
  remove_numbers,
  lowercase,
  n,
  weighting,
  trim,
  threshold
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vectorize_+3A_input">input</code></td>
<td>
<p>This should be a <code>quanteda</code> corpus object with the author names as a docvar called &quot;author&quot;. Typically, this is the output of the <code><a href="#topic+create_corpus">create_corpus()</a></code> function.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_tokens">tokens</code></td>
<td>
<p>The type of tokens to extract, either &quot;character&quot; or &quot;word&quot;.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_remove_punct">remove_punct</code></td>
<td>
<p>A logical value. FALSE to keep the punctuation marks or TRUE to remove them.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>A logical value. TRUE removes symbols and FALSE keeps them.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>A logical value. TRUE removes numbers and FALSE keeps them.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_lowercase">lowercase</code></td>
<td>
<p>A logical value. TRUE transforms all tokens to lower case.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_n">n</code></td>
<td>
<p>The order or size of the n-grams being extracted.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_weighting">weighting</code></td>
<td>
<p>The type of weighting to use, &quot;rel&quot; for relative frequencies, &quot;tf-idf&quot;, or &quot;boolean&quot;.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_trim">trim</code></td>
<td>
<p>A logical value. If TRUE then only the most frequent tokens are kept.</p>
</td></tr>
<tr><td><code id="vectorize_+3A_threshold">threshold</code></td>
<td>
<p>A numeric value indicating how many most frequent tokens to keep.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All the authorship analysis functions call <code>vectorize()</code> with the standard parameters for the algorithm selected. This function is therefore left only for those users who want to modify these parameters or for convenience if the same dfm has to be reused by the algorithms so to avoid vectorizing the same data many times. Most users who only need to run a standard analysis do not need use this function.
</p>


<h3>Value</h3>

<p>A dfm (document-feature matrix) containing each text as a feature vector. N-gram tokenisation does not cross sentence boundaries.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mycorpus &lt;- quanteda::corpus("The cat sat on the mat.")
quanteda::docvars(mycorpus, "author") &lt;- "author1"
matrix &lt;- vectorize(mycorpus, tokens = "character", remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, lowercase = TRUE, n = 5, weighting = "rel", trim = TRUE, threshold = 1500)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
